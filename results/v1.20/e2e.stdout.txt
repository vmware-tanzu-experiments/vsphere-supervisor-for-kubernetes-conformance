{"msg":"Test Suite starting","total":296,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1630901961 - Will randomize all specs
Will run 296 of 5667 specs

Sep  5 21:19:23.651: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 21:19:23.657: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep  5 21:19:23.691: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep  5 21:19:23.750: INFO: 30 / 30 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep  5 21:19:23.750: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Sep  5 21:19:23.750: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep  5 21:19:23.765: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep  5 21:19:23.765: INFO: e2e test version: v0.0.0-master+$Format:%h$
Sep  5 21:19:23.767: INFO: kube-apiserver version: v1.20.8+vmware.wcp.1
Sep  5 21:19:23.767: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 21:19:23.780: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:19:23.781: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
Sep  5 21:19:24.940: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Sep  5 21:19:24.968: INFO: PSP annotation exists on dry run pod: "wcp-default-psp"; assuming PodSecurityPolicy is enabled
Sep  5 21:19:25.054: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-747
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-3b03a1eb-874c-4822-9e75-fe15df23817f
STEP: Creating a pod to test consume secrets
Sep  5 21:19:25.408: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44" in namespace "projected-747" to be "Succeeded or Failed"
Sep  5 21:19:25.419: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 11.902514ms
Sep  5 21:19:27.432: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02456219s
Sep  5 21:19:29.456: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04889682s
Sep  5 21:19:31.480: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072481685s
Sep  5 21:19:33.509: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 8.101725549s
Sep  5 21:19:35.535: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 10.126997241s
Sep  5 21:19:37.547: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 12.139843342s
Sep  5 21:19:39.564: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 14.156244836s
Sep  5 21:19:41.575: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 16.167013376s
Sep  5 21:19:43.593: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 18.185138941s
Sep  5 21:19:45.611: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 20.203351399s
Sep  5 21:19:47.624: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 22.216936806s
Sep  5 21:19:49.640: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 24.232146909s
Sep  5 21:19:51.653: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 26.245125836s
Sep  5 21:19:53.665: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 28.257796228s
Sep  5 21:19:55.682: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 30.274237685s
Sep  5 21:19:57.693: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 32.285849943s
Sep  5 21:19:59.707: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 34.299246208s
Sep  5 21:20:01.720: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 36.31229001s
Sep  5 21:20:03.734: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 38.326759935s
Sep  5 21:20:05.746: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 40.33845914s
Sep  5 21:20:07.773: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 42.365763408s
Sep  5 21:20:09.791: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 44.383329235s
Sep  5 21:20:11.802: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 46.394174127s
Sep  5 21:20:13.815: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 48.407071356s
Sep  5 21:20:15.829: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 50.421840546s
Sep  5 21:20:17.849: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 52.441180628s
Sep  5 21:20:19.863: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 54.455463079s
Sep  5 21:20:21.873: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Pending", Reason="", readiness=false. Elapsed: 56.465830072s
Sep  5 21:20:23.885: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 58.477358305s
STEP: Saw pod success
Sep  5 21:20:23.885: INFO: Pod "pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44" satisfied condition "Succeeded or Failed"
Sep  5 21:20:23.892: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  5 21:20:30.434: INFO: Waiting for pod pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44 to disappear
Sep  5 21:20:30.458: INFO: Pod pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44 still exists
Sep  5 21:20:32.458: INFO: Waiting for pod pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44 to disappear
Sep  5 21:20:32.476: INFO: Pod pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44 still exists
Sep  5 21:20:34.459: INFO: Waiting for pod pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44 to disappear
Sep  5 21:20:34.477: INFO: Pod pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44 still exists
Sep  5 21:20:36.459: INFO: Waiting for pod pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44 to disappear
Sep  5 21:20:36.471: INFO: Pod pod-projected-secrets-82d769b8-0022-4432-a2b9-1137472b6b44 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:20:36.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-747" for this suite.

• [SLOW TEST:73.344 seconds]
[sig-storage] Projected secret
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":1,"skipped":17,"failed":0}
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:20:37.125: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1893
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-12766db8-c091-4e95-8284-c6407db2ec9e in namespace container-probe-1893
Sep  5 21:21:09.990: INFO: Started pod liveness-12766db8-c091-4e95-8284-c6407db2ec9e in namespace container-probe-1893
STEP: checking the pod's current state and verifying that restartCount is present
Sep  5 21:21:10.007: INFO: Initial restart count of pod liveness-12766db8-c091-4e95-8284-c6407db2ec9e is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:25:10.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1893" for this suite.

• [SLOW TEST:273.749 seconds]
[k8s.io] Probing container
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":296,"completed":2,"skipped":24,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:25:10.875: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5167
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on node default medium
Sep  5 21:25:11.505: INFO: Waiting up to 5m0s for pod "pod-222a441f-a836-467f-ab8d-bcb485414c73" in namespace "emptydir-5167" to be "Succeeded or Failed"
Sep  5 21:25:11.567: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Pending", Reason="", readiness=false. Elapsed: 61.76004ms
Sep  5 21:25:13.584: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078481058s
Sep  5 21:25:15.598: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Pending", Reason="", readiness=false. Elapsed: 4.093076817s
Sep  5 21:25:17.613: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Pending", Reason="", readiness=false. Elapsed: 6.10781055s
Sep  5 21:25:19.652: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Pending", Reason="", readiness=false. Elapsed: 8.146986915s
Sep  5 21:25:21.687: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Pending", Reason="", readiness=false. Elapsed: 10.181310958s
Sep  5 21:25:23.698: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Pending", Reason="", readiness=false. Elapsed: 12.192690786s
Sep  5 21:25:25.735: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Pending", Reason="", readiness=false. Elapsed: 14.229524168s
Sep  5 21:25:27.750: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Pending", Reason="", readiness=false. Elapsed: 16.245060265s
Sep  5 21:25:29.773: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Pending", Reason="", readiness=false. Elapsed: 18.267988149s
Sep  5 21:25:31.789: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Pending", Reason="", readiness=false. Elapsed: 20.283589021s
Sep  5 21:25:33.803: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Pending", Reason="", readiness=false. Elapsed: 22.29813588s
Sep  5 21:25:35.839: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Pending", Reason="", readiness=false. Elapsed: 24.333170215s
Sep  5 21:25:37.849: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Pending", Reason="", readiness=false. Elapsed: 26.343904143s
Sep  5 21:25:39.863: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Pending", Reason="", readiness=false. Elapsed: 28.357811866s
Sep  5 21:25:41.878: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Pending", Reason="", readiness=false. Elapsed: 30.372297867s
Sep  5 21:25:43.887: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Running", Reason="", readiness=true. Elapsed: 32.381981527s
Sep  5 21:25:45.900: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Running", Reason="", readiness=true. Elapsed: 34.394352895s
Sep  5 21:25:47.911: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Running", Reason="", readiness=true. Elapsed: 36.405320576s
Sep  5 21:25:49.927: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 38.421213746s
STEP: Saw pod success
Sep  5 21:25:49.927: INFO: Pod "pod-222a441f-a836-467f-ab8d-bcb485414c73" satisfied condition "Succeeded or Failed"
Sep  5 21:25:49.932: INFO: Trying to get logs from node sc2-10-185-226-150.eng.vmware.com pod pod-222a441f-a836-467f-ab8d-bcb485414c73 container test-container: <nil>
STEP: delete the pod
Sep  5 21:25:50.046: INFO: Waiting for pod pod-222a441f-a836-467f-ab8d-bcb485414c73 to disappear
Sep  5 21:25:50.065: INFO: Pod pod-222a441f-a836-467f-ab8d-bcb485414c73 still exists
Sep  5 21:25:52.065: INFO: Waiting for pod pod-222a441f-a836-467f-ab8d-bcb485414c73 to disappear
Sep  5 21:25:52.111: INFO: Pod pod-222a441f-a836-467f-ab8d-bcb485414c73 still exists
Sep  5 21:25:54.065: INFO: Waiting for pod pod-222a441f-a836-467f-ab8d-bcb485414c73 to disappear
Sep  5 21:25:54.081: INFO: Pod pod-222a441f-a836-467f-ab8d-bcb485414c73 still exists
Sep  5 21:25:56.066: INFO: Waiting for pod pod-222a441f-a836-467f-ab8d-bcb485414c73 to disappear
Sep  5 21:25:56.077: INFO: Pod pod-222a441f-a836-467f-ab8d-bcb485414c73 still exists
Sep  5 21:25:58.065: INFO: Waiting for pod pod-222a441f-a836-467f-ab8d-bcb485414c73 to disappear
Sep  5 21:25:58.080: INFO: Pod pod-222a441f-a836-467f-ab8d-bcb485414c73 still exists
Sep  5 21:26:00.065: INFO: Waiting for pod pod-222a441f-a836-467f-ab8d-bcb485414c73 to disappear
Sep  5 21:26:00.074: INFO: Pod pod-222a441f-a836-467f-ab8d-bcb485414c73 still exists
Sep  5 21:26:02.065: INFO: Waiting for pod pod-222a441f-a836-467f-ab8d-bcb485414c73 to disappear
Sep  5 21:26:02.086: INFO: Pod pod-222a441f-a836-467f-ab8d-bcb485414c73 still exists
Sep  5 21:26:04.065: INFO: Waiting for pod pod-222a441f-a836-467f-ab8d-bcb485414c73 to disappear
Sep  5 21:26:04.086: INFO: Pod pod-222a441f-a836-467f-ab8d-bcb485414c73 still exists
Sep  5 21:26:06.066: INFO: Waiting for pod pod-222a441f-a836-467f-ab8d-bcb485414c73 to disappear
Sep  5 21:26:06.080: INFO: Pod pod-222a441f-a836-467f-ab8d-bcb485414c73 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:26:06.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5167" for this suite.

• [SLOW TEST:55.469 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":3,"skipped":70,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:26:06.344: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3245
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Sep  5 21:26:06.746: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:26:52.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3245" for this suite.

• [SLOW TEST:46.440 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":296,"completed":4,"skipped":103,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:26:52.784: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7119
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-692a0a4b-5412-44ef-bbf6-fa5dc90863d9
STEP: Creating a pod to test consume configMaps
Sep  5 21:26:53.622: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9" in namespace "projected-7119" to be "Succeeded or Failed"
Sep  5 21:26:53.637: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9": Phase="Pending", Reason="", readiness=false. Elapsed: 15.481584ms
Sep  5 21:26:55.648: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02606786s
Sep  5 21:26:57.682: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060556041s
Sep  5 21:26:59.702: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080351107s
Sep  5 21:27:01.722: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.100140619s
Sep  5 21:27:03.752: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.130514268s
Sep  5 21:27:05.765: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.143149072s
Sep  5 21:27:07.783: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.160835735s
Sep  5 21:27:09.805: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.183189656s
Sep  5 21:27:11.819: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.197345223s
Sep  5 21:27:13.833: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.210809934s
Sep  5 21:27:15.855: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9": Phase="Pending", Reason="", readiness=false. Elapsed: 22.232889167s
Sep  5 21:27:17.870: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9": Phase="Pending", Reason="", readiness=false. Elapsed: 24.248237893s
Sep  5 21:27:19.884: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9": Phase="Pending", Reason="", readiness=false. Elapsed: 26.261932553s
Sep  5 21:27:21.899: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.27738522s
Sep  5 21:27:23.916: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9": Phase="Pending", Reason="", readiness=false. Elapsed: 30.294424181s
Sep  5 21:27:25.928: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9": Phase="Pending", Reason="", readiness=false. Elapsed: 32.306405568s
Sep  5 21:27:27.947: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9": Phase="Pending", Reason="", readiness=false. Elapsed: 34.325275585s
Sep  5 21:27:29.960: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 36.337752812s
STEP: Saw pod success
Sep  5 21:27:29.960: INFO: Pod "pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9" satisfied condition "Succeeded or Failed"
Sep  5 21:27:30.014: INFO: Trying to get logs from node sc2-10-185-226-150.eng.vmware.com pod pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9 container agnhost-container: <nil>
STEP: delete the pod
Sep  5 21:27:30.116: INFO: Waiting for pod pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9 to disappear
Sep  5 21:27:30.144: INFO: Pod pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9 still exists
Sep  5 21:27:32.145: INFO: Waiting for pod pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9 to disappear
Sep  5 21:27:32.164: INFO: Pod pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9 still exists
Sep  5 21:27:34.145: INFO: Waiting for pod pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9 to disappear
Sep  5 21:27:34.176: INFO: Pod pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9 still exists
Sep  5 21:27:36.146: INFO: Waiting for pod pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9 to disappear
Sep  5 21:27:36.160: INFO: Pod pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9 still exists
Sep  5 21:27:38.145: INFO: Waiting for pod pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9 to disappear
Sep  5 21:27:38.158: INFO: Pod pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9 still exists
Sep  5 21:27:40.145: INFO: Waiting for pod pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9 to disappear
Sep  5 21:27:40.160: INFO: Pod pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9 still exists
Sep  5 21:27:42.145: INFO: Waiting for pod pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9 to disappear
Sep  5 21:27:42.160: INFO: Pod pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9 still exists
Sep  5 21:27:44.145: INFO: Waiting for pod pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9 to disappear
Sep  5 21:27:44.159: INFO: Pod pod-projected-configmaps-0e4aa3fb-b4c6-42a3-8554-14b6c5b245a9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:27:44.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7119" for this suite.

• [SLOW TEST:51.859 seconds]
[sig-storage] Projected configMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":5,"skipped":123,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:27:44.643: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8076
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-5ad34367-7ef1-4d0d-b92e-e2a6bff2785b
STEP: Creating a pod to test consume configMaps
Sep  5 21:27:45.269: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9" in namespace "projected-8076" to be "Succeeded or Failed"
Sep  5 21:27:45.289: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.379634ms
Sep  5 21:27:47.307: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03884216s
Sep  5 21:27:49.330: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061248823s
Sep  5 21:27:51.343: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074774641s
Sep  5 21:27:53.355: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.085948404s
Sep  5 21:27:55.368: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.099384041s
Sep  5 21:27:57.384: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.114886692s
Sep  5 21:27:59.404: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.135388739s
Sep  5 21:28:01.426: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.157478339s
Sep  5 21:28:03.445: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.176694566s
Sep  5 21:28:05.464: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.195364212s
Sep  5 21:28:07.481: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9": Phase="Pending", Reason="", readiness=false. Elapsed: 22.211977252s
Sep  5 21:28:09.493: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9": Phase="Pending", Reason="", readiness=false. Elapsed: 24.224205022s
Sep  5 21:28:11.510: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9": Phase="Pending", Reason="", readiness=false. Elapsed: 26.241659643s
Sep  5 21:28:13.558: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.289657455s
Sep  5 21:28:15.575: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9": Phase="Pending", Reason="", readiness=false. Elapsed: 30.306644017s
Sep  5 21:28:17.592: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9": Phase="Pending", Reason="", readiness=false. Elapsed: 32.323584052s
Sep  5 21:28:19.614: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9": Phase="Pending", Reason="", readiness=false. Elapsed: 34.344874951s
Sep  5 21:28:21.645: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 36.376499336s
STEP: Saw pod success
Sep  5 21:28:21.645: INFO: Pod "pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9" satisfied condition "Succeeded or Failed"
Sep  5 21:28:21.655: INFO: Trying to get logs from node sc2-10-185-226-150.eng.vmware.com pod pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 21:28:27.438: INFO: Waiting for pod pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9 to disappear
Sep  5 21:28:27.468: INFO: Pod pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9 still exists
Sep  5 21:28:29.469: INFO: Waiting for pod pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9 to disappear
Sep  5 21:28:29.481: INFO: Pod pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9 still exists
Sep  5 21:28:31.469: INFO: Waiting for pod pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9 to disappear
Sep  5 21:28:31.485: INFO: Pod pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9 still exists
Sep  5 21:28:33.469: INFO: Waiting for pod pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9 to disappear
Sep  5 21:28:33.484: INFO: Pod pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9 still exists
Sep  5 21:28:35.469: INFO: Waiting for pod pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9 to disappear
Sep  5 21:28:35.483: INFO: Pod pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9 still exists
Sep  5 21:28:37.469: INFO: Waiting for pod pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9 to disappear
Sep  5 21:28:37.479: INFO: Pod pod-projected-configmaps-ea189db6-52f8-48f7-8efd-d5fd359fdcb9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:28:37.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8076" for this suite.

• [SLOW TEST:53.068 seconds]
[sig-storage] Projected configMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":296,"completed":6,"skipped":155,"failed":0}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:28:37.712: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8937
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Sep  5 21:28:38.274: INFO: Waiting up to 5m0s for pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526" in namespace "downward-api-8937" to be "Succeeded or Failed"
Sep  5 21:28:38.299: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Pending", Reason="", readiness=false. Elapsed: 25.449921ms
Sep  5 21:28:40.313: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03954426s
Sep  5 21:28:42.326: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052361475s
Sep  5 21:28:44.345: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Pending", Reason="", readiness=false. Elapsed: 6.071817025s
Sep  5 21:28:46.361: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Pending", Reason="", readiness=false. Elapsed: 8.087284481s
Sep  5 21:28:48.371: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Pending", Reason="", readiness=false. Elapsed: 10.096995845s
Sep  5 21:28:50.394: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Pending", Reason="", readiness=false. Elapsed: 12.119987419s
Sep  5 21:28:52.408: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Pending", Reason="", readiness=false. Elapsed: 14.134779879s
Sep  5 21:28:54.422: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Pending", Reason="", readiness=false. Elapsed: 16.148407076s
Sep  5 21:28:56.436: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Pending", Reason="", readiness=false. Elapsed: 18.162449506s
Sep  5 21:28:58.448: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Pending", Reason="", readiness=false. Elapsed: 20.174338706s
Sep  5 21:29:00.462: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Pending", Reason="", readiness=false. Elapsed: 22.188437455s
Sep  5 21:29:02.493: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Pending", Reason="", readiness=false. Elapsed: 24.219533895s
Sep  5 21:29:04.514: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Pending", Reason="", readiness=false. Elapsed: 26.240649144s
Sep  5 21:29:06.530: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Pending", Reason="", readiness=false. Elapsed: 28.255957278s
Sep  5 21:29:08.552: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Pending", Reason="", readiness=false. Elapsed: 30.278270339s
Sep  5 21:29:10.573: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Pending", Reason="", readiness=false. Elapsed: 32.298894138s
Sep  5 21:29:12.588: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Pending", Reason="", readiness=false. Elapsed: 34.314567343s
Sep  5 21:29:14.634: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Pending", Reason="", readiness=false. Elapsed: 36.360249065s
Sep  5 21:29:16.664: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526": Phase="Succeeded", Reason="", readiness=false. Elapsed: 38.390236453s
STEP: Saw pod success
Sep  5 21:29:16.664: INFO: Pod "downward-api-5845a4be-2329-4ceb-8070-e0401a26b526" satisfied condition "Succeeded or Failed"
Sep  5 21:29:16.673: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 container dapi-container: <nil>
STEP: delete the pod
Sep  5 21:29:16.754: INFO: Waiting for pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 to disappear
Sep  5 21:29:16.770: INFO: Pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 still exists
Sep  5 21:29:18.770: INFO: Waiting for pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 to disappear
Sep  5 21:29:18.787: INFO: Pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 still exists
Sep  5 21:29:20.770: INFO: Waiting for pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 to disappear
Sep  5 21:29:20.785: INFO: Pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 still exists
Sep  5 21:29:22.770: INFO: Waiting for pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 to disappear
Sep  5 21:29:22.788: INFO: Pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 still exists
Sep  5 21:29:24.770: INFO: Waiting for pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 to disappear
Sep  5 21:29:24.783: INFO: Pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 still exists
Sep  5 21:29:26.770: INFO: Waiting for pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 to disappear
Sep  5 21:29:26.780: INFO: Pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 still exists
Sep  5 21:29:28.771: INFO: Waiting for pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 to disappear
Sep  5 21:29:28.796: INFO: Pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 still exists
Sep  5 21:29:30.770: INFO: Waiting for pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 to disappear
Sep  5 21:29:30.784: INFO: Pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 still exists
Sep  5 21:29:32.770: INFO: Waiting for pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 to disappear
Sep  5 21:29:32.789: INFO: Pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 still exists
Sep  5 21:29:34.771: INFO: Waiting for pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 to disappear
Sep  5 21:29:34.782: INFO: Pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 still exists
Sep  5 21:29:36.770: INFO: Waiting for pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 to disappear
Sep  5 21:29:36.783: INFO: Pod downward-api-5845a4be-2329-4ceb-8070-e0401a26b526 no longer exists
[AfterEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:29:36.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8937" for this suite.

• [SLOW TEST:59.328 seconds]
[sig-node] Downward API
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":296,"completed":7,"skipped":161,"failed":0}
SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:29:37.040: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9053
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support proxy with --port 0  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting the proxy server
Sep  5 21:29:37.656: INFO: Asynchronously running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-9053 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:29:37.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9053" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":296,"completed":8,"skipped":163,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:29:38.039: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-515
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:29:56.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-515" for this suite.

• [SLOW TEST:19.246 seconds]
[sig-api-machinery] ResourceQuota
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":296,"completed":9,"skipped":208,"failed":0}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:29:57.285: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5472
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should scale a replication controller  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Sep  5 21:29:57.728: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 create -f -'
Sep  5 21:29:58.897: INFO: stderr: ""
Sep  5 21:29:58.897: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  5 21:29:58.897: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:29:59.014: INFO: stderr: ""
Sep  5 21:29:59.015: INFO: stdout: "update-demo-nautilus-tm6dg update-demo-nautilus-tw98h "
Sep  5 21:29:59.015: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-tm6dg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 21:29:59.132: INFO: stderr: ""
Sep  5 21:29:59.132: INFO: stdout: ""
Sep  5 21:29:59.132: INFO: update-demo-nautilus-tm6dg is created but not running
Sep  5 21:30:04.133: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:30:04.258: INFO: stderr: ""
Sep  5 21:30:04.258: INFO: stdout: "update-demo-nautilus-tm6dg update-demo-nautilus-tw98h "
Sep  5 21:30:04.258: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-tm6dg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 21:30:04.393: INFO: stderr: ""
Sep  5 21:30:04.393: INFO: stdout: ""
Sep  5 21:30:04.393: INFO: update-demo-nautilus-tm6dg is created but not running
Sep  5 21:30:09.393: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:30:09.513: INFO: stderr: ""
Sep  5 21:30:09.513: INFO: stdout: "update-demo-nautilus-tm6dg update-demo-nautilus-tw98h "
Sep  5 21:30:09.513: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-tm6dg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 21:30:09.653: INFO: stderr: ""
Sep  5 21:30:09.654: INFO: stdout: ""
Sep  5 21:30:09.654: INFO: update-demo-nautilus-tm6dg is created but not running
Sep  5 21:30:14.654: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:30:14.784: INFO: stderr: ""
Sep  5 21:30:14.784: INFO: stdout: "update-demo-nautilus-tm6dg update-demo-nautilus-tw98h "
Sep  5 21:30:14.784: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-tm6dg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 21:30:14.910: INFO: stderr: ""
Sep  5 21:30:14.910: INFO: stdout: ""
Sep  5 21:30:14.910: INFO: update-demo-nautilus-tm6dg is created but not running
Sep  5 21:30:19.911: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:30:20.085: INFO: stderr: ""
Sep  5 21:30:20.085: INFO: stdout: "update-demo-nautilus-tm6dg update-demo-nautilus-tw98h "
Sep  5 21:30:20.085: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-tm6dg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 21:30:20.300: INFO: stderr: ""
Sep  5 21:30:20.300: INFO: stdout: ""
Sep  5 21:30:20.300: INFO: update-demo-nautilus-tm6dg is created but not running
Sep  5 21:30:25.301: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:30:25.417: INFO: stderr: ""
Sep  5 21:30:25.417: INFO: stdout: "update-demo-nautilus-tm6dg update-demo-nautilus-tw98h "
Sep  5 21:30:25.417: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-tm6dg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 21:30:25.533: INFO: stderr: ""
Sep  5 21:30:25.533: INFO: stdout: ""
Sep  5 21:30:25.533: INFO: update-demo-nautilus-tm6dg is created but not running
Sep  5 21:30:30.534: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:30:30.681: INFO: stderr: ""
Sep  5 21:30:30.681: INFO: stdout: "update-demo-nautilus-tm6dg update-demo-nautilus-tw98h "
Sep  5 21:30:30.681: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-tm6dg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 21:30:30.783: INFO: stderr: ""
Sep  5 21:30:30.783: INFO: stdout: ""
Sep  5 21:30:30.783: INFO: update-demo-nautilus-tm6dg is created but not running
Sep  5 21:30:35.785: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:30:35.917: INFO: stderr: ""
Sep  5 21:30:35.918: INFO: stdout: "update-demo-nautilus-tm6dg update-demo-nautilus-tw98h "
Sep  5 21:30:35.918: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-tm6dg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 21:30:36.072: INFO: stderr: ""
Sep  5 21:30:36.072: INFO: stdout: ""
Sep  5 21:30:36.072: INFO: update-demo-nautilus-tm6dg is created but not running
Sep  5 21:30:41.075: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:30:41.218: INFO: stderr: ""
Sep  5 21:30:41.218: INFO: stdout: "update-demo-nautilus-tm6dg update-demo-nautilus-tw98h "
Sep  5 21:30:41.218: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-tm6dg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 21:30:41.327: INFO: stderr: ""
Sep  5 21:30:41.327: INFO: stdout: ""
Sep  5 21:30:41.328: INFO: update-demo-nautilus-tm6dg is created but not running
Sep  5 21:30:46.330: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:30:46.443: INFO: stderr: ""
Sep  5 21:30:46.443: INFO: stdout: "update-demo-nautilus-tm6dg update-demo-nautilus-tw98h "
Sep  5 21:30:46.443: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-tm6dg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 21:30:46.554: INFO: stderr: ""
Sep  5 21:30:46.554: INFO: stdout: "true"
Sep  5 21:30:46.554: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-tm6dg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  5 21:30:46.673: INFO: stderr: ""
Sep  5 21:30:46.673: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 21:30:46.673: INFO: validating pod update-demo-nautilus-tm6dg
Sep  5 21:30:46.718: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 21:30:46.718: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 21:30:46.718: INFO: update-demo-nautilus-tm6dg is verified up and running
Sep  5 21:30:46.719: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-tw98h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 21:30:46.826: INFO: stderr: ""
Sep  5 21:30:46.826: INFO: stdout: "true"
Sep  5 21:30:46.826: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-tw98h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  5 21:30:46.928: INFO: stderr: ""
Sep  5 21:30:46.928: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 21:30:46.928: INFO: validating pod update-demo-nautilus-tw98h
Sep  5 21:30:46.981: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 21:30:46.981: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 21:30:46.981: INFO: update-demo-nautilus-tw98h is verified up and running
STEP: scaling down the replication controller
Sep  5 21:30:47.019: INFO: scanned /home/worker for discovery docs: <nil>
Sep  5 21:30:47.019: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Sep  5 21:30:48.178: INFO: stderr: ""
Sep  5 21:30:48.178: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  5 21:30:48.178: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:30:48.284: INFO: stderr: ""
Sep  5 21:30:48.284: INFO: stdout: "update-demo-nautilus-tm6dg update-demo-nautilus-tw98h "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep  5 21:30:53.286: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:30:53.418: INFO: stderr: ""
Sep  5 21:30:53.418: INFO: stdout: "update-demo-nautilus-tm6dg update-demo-nautilus-tw98h "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep  5 21:30:58.419: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:30:58.542: INFO: stderr: ""
Sep  5 21:30:58.542: INFO: stdout: "update-demo-nautilus-tm6dg update-demo-nautilus-tw98h "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep  5 21:31:03.543: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:31:03.665: INFO: stderr: ""
Sep  5 21:31:03.665: INFO: stdout: "update-demo-nautilus-tm6dg update-demo-nautilus-tw98h "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep  5 21:31:08.666: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:31:08.784: INFO: stderr: ""
Sep  5 21:31:08.784: INFO: stdout: "update-demo-nautilus-tm6dg "
Sep  5 21:31:08.784: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-tm6dg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 21:31:08.891: INFO: stderr: ""
Sep  5 21:31:08.891: INFO: stdout: "true"
Sep  5 21:31:08.891: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-tm6dg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  5 21:31:09.017: INFO: stderr: ""
Sep  5 21:31:09.017: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 21:31:09.017: INFO: validating pod update-demo-nautilus-tm6dg
Sep  5 21:31:09.059: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 21:31:09.059: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 21:31:09.059: INFO: update-demo-nautilus-tm6dg is verified up and running
STEP: scaling up the replication controller
Sep  5 21:31:09.063: INFO: scanned /home/worker for discovery docs: <nil>
Sep  5 21:31:09.063: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Sep  5 21:31:10.245: INFO: stderr: ""
Sep  5 21:31:10.245: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  5 21:31:10.245: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:31:10.367: INFO: stderr: ""
Sep  5 21:31:10.367: INFO: stdout: "update-demo-nautilus-l74mf update-demo-nautilus-tm6dg "
Sep  5 21:31:10.367: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-l74mf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 21:31:10.463: INFO: stderr: ""
Sep  5 21:31:10.463: INFO: stdout: ""
Sep  5 21:31:10.463: INFO: update-demo-nautilus-l74mf is created but not running
Sep  5 21:31:15.465: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:31:15.582: INFO: stderr: ""
Sep  5 21:31:15.582: INFO: stdout: "update-demo-nautilus-l74mf update-demo-nautilus-tm6dg "
Sep  5 21:31:15.582: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-l74mf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 21:31:15.699: INFO: stderr: ""
Sep  5 21:31:15.699: INFO: stdout: ""
Sep  5 21:31:15.699: INFO: update-demo-nautilus-l74mf is created but not running
Sep  5 21:31:20.700: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:31:20.817: INFO: stderr: ""
Sep  5 21:31:20.817: INFO: stdout: "update-demo-nautilus-l74mf update-demo-nautilus-tm6dg "
Sep  5 21:31:20.817: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-l74mf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 21:31:20.917: INFO: stderr: ""
Sep  5 21:31:20.918: INFO: stdout: ""
Sep  5 21:31:20.918: INFO: update-demo-nautilus-l74mf is created but not running
Sep  5 21:31:25.918: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:31:26.025: INFO: stderr: ""
Sep  5 21:31:26.025: INFO: stdout: "update-demo-nautilus-l74mf update-demo-nautilus-tm6dg "
Sep  5 21:31:26.025: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-l74mf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 21:31:26.125: INFO: stderr: ""
Sep  5 21:31:26.125: INFO: stdout: ""
Sep  5 21:31:26.125: INFO: update-demo-nautilus-l74mf is created but not running
Sep  5 21:31:31.127: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:31:31.250: INFO: stderr: ""
Sep  5 21:31:31.250: INFO: stdout: "update-demo-nautilus-l74mf update-demo-nautilus-tm6dg "
Sep  5 21:31:31.250: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-l74mf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 21:31:31.353: INFO: stderr: ""
Sep  5 21:31:31.353: INFO: stdout: ""
Sep  5 21:31:31.353: INFO: update-demo-nautilus-l74mf is created but not running
Sep  5 21:31:36.355: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 21:31:36.471: INFO: stderr: ""
Sep  5 21:31:36.471: INFO: stdout: "update-demo-nautilus-l74mf update-demo-nautilus-tm6dg "
Sep  5 21:31:36.471: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-l74mf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 21:31:36.570: INFO: stderr: ""
Sep  5 21:31:36.570: INFO: stdout: "true"
Sep  5 21:31:36.571: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-l74mf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  5 21:31:36.699: INFO: stderr: ""
Sep  5 21:31:36.699: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 21:31:36.699: INFO: validating pod update-demo-nautilus-l74mf
Sep  5 21:31:36.763: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 21:31:36.763: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 21:31:36.763: INFO: update-demo-nautilus-l74mf is verified up and running
Sep  5 21:31:36.763: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-tm6dg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 21:31:36.874: INFO: stderr: ""
Sep  5 21:31:36.874: INFO: stdout: "true"
Sep  5 21:31:36.874: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods update-demo-nautilus-tm6dg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  5 21:31:36.968: INFO: stderr: ""
Sep  5 21:31:36.968: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 21:31:36.968: INFO: validating pod update-demo-nautilus-tm6dg
Sep  5 21:31:37.000: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 21:31:37.000: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 21:31:37.000: INFO: update-demo-nautilus-tm6dg is verified up and running
STEP: using delete to clean up resources
Sep  5 21:31:37.000: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 delete --grace-period=0 --force -f -'
Sep  5 21:31:37.119: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 21:31:37.119: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  5 21:31:37.119: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get rc,svc -l name=update-demo --no-headers'
Sep  5 21:31:37.224: INFO: stderr: "No resources found in kubectl-5472 namespace.\n"
Sep  5 21:31:37.224: INFO: stdout: ""
Sep  5 21:31:37.224: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 21:31:37.337: INFO: stderr: ""
Sep  5 21:31:37.338: INFO: stdout: "update-demo-nautilus-l74mf\nupdate-demo-nautilus-tm6dg\n"
Sep  5 21:31:37.838: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get rc,svc -l name=update-demo --no-headers'
Sep  5 21:31:37.973: INFO: stderr: "No resources found in kubectl-5472 namespace.\n"
Sep  5 21:31:37.973: INFO: stdout: ""
Sep  5 21:31:37.973: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 21:31:38.079: INFO: stderr: ""
Sep  5 21:31:38.079: INFO: stdout: "update-demo-nautilus-l74mf\nupdate-demo-nautilus-tm6dg\n"
Sep  5 21:31:38.338: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get rc,svc -l name=update-demo --no-headers'
Sep  5 21:31:38.441: INFO: stderr: "No resources found in kubectl-5472 namespace.\n"
Sep  5 21:31:38.441: INFO: stdout: ""
Sep  5 21:31:38.441: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 21:31:38.549: INFO: stderr: ""
Sep  5 21:31:38.550: INFO: stdout: "update-demo-nautilus-l74mf\nupdate-demo-nautilus-tm6dg\n"
Sep  5 21:31:38.838: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get rc,svc -l name=update-demo --no-headers'
Sep  5 21:31:38.945: INFO: stderr: "No resources found in kubectl-5472 namespace.\n"
Sep  5 21:31:38.945: INFO: stdout: ""
Sep  5 21:31:38.945: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 21:31:39.060: INFO: stderr: ""
Sep  5 21:31:39.060: INFO: stdout: "update-demo-nautilus-l74mf\nupdate-demo-nautilus-tm6dg\n"
Sep  5 21:31:39.339: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get rc,svc -l name=update-demo --no-headers'
Sep  5 21:31:39.457: INFO: stderr: "No resources found in kubectl-5472 namespace.\n"
Sep  5 21:31:39.457: INFO: stdout: ""
Sep  5 21:31:39.457: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5472 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 21:31:39.593: INFO: stderr: ""
Sep  5 21:31:39.593: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:31:39.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5472" for this suite.

• [SLOW TEST:102.971 seconds]
[sig-cli] Kubectl client
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should scale a replication controller  [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":296,"completed":10,"skipped":215,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:31:40.257: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9564
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-cff23f09-ebbf-4128-a0f0-036148701a4f
STEP: Creating a pod to test consume secrets
Sep  5 21:31:40.887: INFO: Waiting up to 5m0s for pod "pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc" in namespace "secrets-9564" to be "Succeeded or Failed"
Sep  5 21:31:40.902: INFO: Pod "pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.491153ms
Sep  5 21:31:42.914: INFO: Pod "pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026763863s
Sep  5 21:31:44.928: INFO: Pod "pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040244883s
Sep  5 21:31:46.946: INFO: Pod "pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058392415s
Sep  5 21:31:48.975: INFO: Pod "pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.087283856s
Sep  5 21:31:50.990: INFO: Pod "pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.10273912s
Sep  5 21:31:53.007: INFO: Pod "pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.119462837s
Sep  5 21:31:55.024: INFO: Pod "pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.136958858s
Sep  5 21:31:57.045: INFO: Pod "pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc": Phase="Pending", Reason="", readiness=false. Elapsed: 16.157133505s
Sep  5 21:31:59.057: INFO: Pod "pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc": Phase="Pending", Reason="", readiness=false. Elapsed: 18.1691059s
Sep  5 21:32:01.085: INFO: Pod "pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc": Phase="Pending", Reason="", readiness=false. Elapsed: 20.197359934s
Sep  5 21:32:03.154: INFO: Pod "pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc": Phase="Pending", Reason="", readiness=false. Elapsed: 22.266874769s
Sep  5 21:32:05.177: INFO: Pod "pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc": Phase="Pending", Reason="", readiness=false. Elapsed: 24.289539792s
Sep  5 21:32:07.708: INFO: Pod "pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc": Phase="Pending", Reason="", readiness=false. Elapsed: 26.820043293s
Sep  5 21:32:09.735: INFO: Pod "pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc": Phase="Running", Reason="", readiness=true. Elapsed: 28.847598912s
Sep  5 21:32:11.772: INFO: Pod "pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc": Phase="Running", Reason="", readiness=true. Elapsed: 30.884904651s
Sep  5 21:32:13.781: INFO: Pod "pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.893887809s
STEP: Saw pod success
Sep  5 21:32:13.781: INFO: Pod "pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc" satisfied condition "Succeeded or Failed"
Sep  5 21:32:13.790: INFO: Trying to get logs from node sc2-10-185-233-127.eng.vmware.com pod pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 21:32:13.908: INFO: Waiting for pod pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc to disappear
Sep  5 21:32:13.932: INFO: Pod pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc still exists
Sep  5 21:32:15.933: INFO: Waiting for pod pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc to disappear
Sep  5 21:32:15.946: INFO: Pod pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc still exists
Sep  5 21:32:17.932: INFO: Waiting for pod pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc to disappear
Sep  5 21:32:17.951: INFO: Pod pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc still exists
Sep  5 21:32:19.933: INFO: Waiting for pod pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc to disappear
Sep  5 21:32:19.956: INFO: Pod pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc still exists
Sep  5 21:32:21.933: INFO: Waiting for pod pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc to disappear
Sep  5 21:32:22.015: INFO: Pod pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc still exists
Sep  5 21:32:23.932: INFO: Waiting for pod pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc to disappear
Sep  5 21:32:23.943: INFO: Pod pod-secrets-75d1c32f-30f2-419e-839c-cee5534043cc no longer exists
[AfterEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:32:23.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9564" for this suite.

• [SLOW TEST:43.972 seconds]
[sig-storage] Secrets
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":11,"skipped":235,"failed":0}
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:32:24.230: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7531
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  5 21:32:58.477: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:32:58.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7531" for this suite.

• [SLOW TEST:34.925 seconds]
[k8s.io] Container Runtime
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":296,"completed":12,"skipped":241,"failed":0}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:32:59.155: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6719
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating cluster-info
Sep  5 21:32:59.690: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-6719 cluster-info'
Sep  5 21:32:59.806: INFO: stderr: ""
Sep  5 21:32:59.806: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.185.230.78:6443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:32:59.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6719" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","total":296,"completed":13,"skipped":245,"failed":0}

------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:33:00.055: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8951
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-4d62abd8-d75f-47fc-adc0-216d6c052e00
STEP: Creating a pod to test consume secrets
Sep  5 21:33:00.719: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597" in namespace "projected-8951" to be "Succeeded or Failed"
Sep  5 21:33:00.746: INFO: Pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597": Phase="Pending", Reason="", readiness=false. Elapsed: 26.855511ms
Sep  5 21:33:02.754: INFO: Pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035029777s
Sep  5 21:33:04.775: INFO: Pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056618041s
Sep  5 21:33:06.803: INFO: Pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084424109s
Sep  5 21:33:08.818: INFO: Pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597": Phase="Pending", Reason="", readiness=false. Elapsed: 8.098992831s
Sep  5 21:33:10.830: INFO: Pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597": Phase="Pending", Reason="", readiness=false. Elapsed: 10.111582533s
Sep  5 21:33:12.849: INFO: Pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597": Phase="Pending", Reason="", readiness=false. Elapsed: 12.130074784s
Sep  5 21:33:14.858: INFO: Pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597": Phase="Pending", Reason="", readiness=false. Elapsed: 14.139581874s
Sep  5 21:33:16.871: INFO: Pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597": Phase="Pending", Reason="", readiness=false. Elapsed: 16.152357696s
Sep  5 21:33:18.886: INFO: Pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597": Phase="Pending", Reason="", readiness=false. Elapsed: 18.166821187s
Sep  5 21:33:20.894: INFO: Pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597": Phase="Pending", Reason="", readiness=false. Elapsed: 20.175128993s
Sep  5 21:33:22.908: INFO: Pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597": Phase="Pending", Reason="", readiness=false. Elapsed: 22.189191476s
Sep  5 21:33:24.922: INFO: Pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597": Phase="Pending", Reason="", readiness=false. Elapsed: 24.203466294s
Sep  5 21:33:26.938: INFO: Pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597": Phase="Pending", Reason="", readiness=false. Elapsed: 26.219493589s
Sep  5 21:33:28.952: INFO: Pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597": Phase="Pending", Reason="", readiness=false. Elapsed: 28.23274098s
Sep  5 21:33:30.966: INFO: Pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597": Phase="Pending", Reason="", readiness=false. Elapsed: 30.247014302s
Sep  5 21:33:32.978: INFO: Pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597": Phase="Pending", Reason="", readiness=false. Elapsed: 32.259116641s
Sep  5 21:33:34.990: INFO: Pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.270707939s
STEP: Saw pod success
Sep  5 21:33:34.990: INFO: Pod "pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597" satisfied condition "Succeeded or Failed"
Sep  5 21:33:34.997: INFO: Trying to get logs from node sc2-10-185-226-150.eng.vmware.com pod pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  5 21:33:40.796: INFO: Waiting for pod pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597 to disappear
Sep  5 21:33:40.811: INFO: Pod pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597 still exists
Sep  5 21:33:42.812: INFO: Waiting for pod pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597 to disappear
Sep  5 21:33:42.833: INFO: Pod pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597 still exists
Sep  5 21:33:44.811: INFO: Waiting for pod pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597 to disappear
Sep  5 21:33:44.832: INFO: Pod pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597 still exists
Sep  5 21:33:46.812: INFO: Waiting for pod pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597 to disappear
Sep  5 21:33:46.839: INFO: Pod pod-projected-secrets-1ea87229-c2c4-411c-b785-e74344912597 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:33:46.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8951" for this suite.

• [SLOW TEST:47.100 seconds]
[sig-storage] Projected secret
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":14,"skipped":245,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:33:47.156: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4489
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-4489
STEP: creating service affinity-clusterip-transition in namespace services-4489
STEP: creating replication controller affinity-clusterip-transition in namespace services-4489
Sep  5 21:34:39.506: INFO: Creating new exec pod
Sep  5 21:35:06.587: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=services-4489 exec execpod-affinityf9tzl -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Sep  5 21:35:07.638: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Sep  5 21:35:07.638: INFO: stdout: ""
Sep  5 21:35:07.638: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=services-4489 exec execpod-affinityf9tzl -- /bin/sh -x -c nc -zv -t -w 2 172.24.70.233 80'
Sep  5 21:35:08.215: INFO: stderr: "+ nc -zv -t -w 2 172.24.70.233 80\nConnection to 172.24.70.233 80 port [tcp/http] succeeded!\n"
Sep  5 21:35:08.215: INFO: stdout: ""
Sep  5 21:35:08.319: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=services-4489 exec execpod-affinityf9tzl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.24.70.233:80/ ; done'
Sep  5 21:35:09.108: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n"
Sep  5 21:35:09.109: INFO: stdout: "\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk"
Sep  5 21:35:09.109: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:09.109: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:09.109: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:09.109: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:09.109: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:09.109: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:09.109: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:09.109: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:09.109: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:09.109: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:09.109: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:09.109: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:09.109: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:09.109: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:09.109: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:09.109: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:39.109: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=services-4489 exec execpod-affinityf9tzl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.24.70.233:80/ ; done'
Sep  5 21:35:39.715: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n"
Sep  5 21:35:39.715: INFO: stdout: "\naffinity-clusterip-transition-dsp99\naffinity-clusterip-transition-nlg74\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-dsp99\naffinity-clusterip-transition-nlg74\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-dsp99\naffinity-clusterip-transition-nlg74\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-dsp99\naffinity-clusterip-transition-nlg74\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-dsp99\naffinity-clusterip-transition-nlg74\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-dsp99"
Sep  5 21:35:39.715: INFO: Received response from host: affinity-clusterip-transition-dsp99
Sep  5 21:35:39.715: INFO: Received response from host: affinity-clusterip-transition-nlg74
Sep  5 21:35:39.715: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:39.715: INFO: Received response from host: affinity-clusterip-transition-dsp99
Sep  5 21:35:39.715: INFO: Received response from host: affinity-clusterip-transition-nlg74
Sep  5 21:35:39.715: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:39.715: INFO: Received response from host: affinity-clusterip-transition-dsp99
Sep  5 21:35:39.715: INFO: Received response from host: affinity-clusterip-transition-nlg74
Sep  5 21:35:39.715: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:39.715: INFO: Received response from host: affinity-clusterip-transition-dsp99
Sep  5 21:35:39.715: INFO: Received response from host: affinity-clusterip-transition-nlg74
Sep  5 21:35:39.715: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:39.715: INFO: Received response from host: affinity-clusterip-transition-dsp99
Sep  5 21:35:39.715: INFO: Received response from host: affinity-clusterip-transition-nlg74
Sep  5 21:35:39.715: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:39.715: INFO: Received response from host: affinity-clusterip-transition-dsp99
Sep  5 21:35:39.769: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=services-4489 exec execpod-affinityf9tzl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.24.70.233:80/ ; done'
Sep  5 21:35:40.217: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n"
Sep  5 21:35:40.217: INFO: stdout: "\naffinity-clusterip-transition-nlg74\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-dsp99\naffinity-clusterip-transition-nlg74\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-dsp99\naffinity-clusterip-transition-nlg74\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-dsp99\naffinity-clusterip-transition-nlg74\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-dsp99\naffinity-clusterip-transition-nlg74\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-dsp99\naffinity-clusterip-transition-nlg74"
Sep  5 21:35:40.217: INFO: Received response from host: affinity-clusterip-transition-nlg74
Sep  5 21:35:40.217: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:40.217: INFO: Received response from host: affinity-clusterip-transition-dsp99
Sep  5 21:35:40.217: INFO: Received response from host: affinity-clusterip-transition-nlg74
Sep  5 21:35:40.217: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:40.217: INFO: Received response from host: affinity-clusterip-transition-dsp99
Sep  5 21:35:40.217: INFO: Received response from host: affinity-clusterip-transition-nlg74
Sep  5 21:35:40.217: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:40.217: INFO: Received response from host: affinity-clusterip-transition-dsp99
Sep  5 21:35:40.217: INFO: Received response from host: affinity-clusterip-transition-nlg74
Sep  5 21:35:40.217: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:40.217: INFO: Received response from host: affinity-clusterip-transition-dsp99
Sep  5 21:35:40.217: INFO: Received response from host: affinity-clusterip-transition-nlg74
Sep  5 21:35:40.217: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:35:40.217: INFO: Received response from host: affinity-clusterip-transition-dsp99
Sep  5 21:35:40.217: INFO: Received response from host: affinity-clusterip-transition-nlg74
Sep  5 21:36:10.218: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=services-4489 exec execpod-affinityf9tzl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.24.70.233:80/ ; done'
Sep  5 21:36:10.835: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.70.233:80/\n"
Sep  5 21:36:10.835: INFO: stdout: "\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk\naffinity-clusterip-transition-zpgqk"
Sep  5 21:36:10.835: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:36:10.835: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:36:10.835: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:36:10.835: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:36:10.835: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:36:10.835: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:36:10.835: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:36:10.835: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:36:10.835: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:36:10.835: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:36:10.835: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:36:10.835: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:36:10.835: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:36:10.835: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:36:10.835: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:36:10.835: INFO: Received response from host: affinity-clusterip-transition-zpgqk
Sep  5 21:36:10.836: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4489, will wait for the garbage collector to delete the pods
Sep  5 21:36:10.954: INFO: Deleting ReplicationController affinity-clusterip-transition took: 40.368011ms
Sep  5 21:36:12.455: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 1.500796847s
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:36:31.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4489" for this suite.
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:165.059 seconds]
[sig-network] Services
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":296,"completed":15,"skipped":306,"failed":0}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:36:32.215: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7264
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:36:51.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7264" for this suite.

• [SLOW TEST:19.256 seconds]
[sig-api-machinery] ResourceQuota
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":296,"completed":16,"skipped":308,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:36:51.471: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4462
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-934
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-9998
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:38:10.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4462" for this suite.
STEP: Destroying namespace "nsdeletetest-934" for this suite.
Sep  5 21:38:10.726: INFO: Namespace nsdeletetest-934 was already deleted
STEP: Destroying namespace "nsdeletetest-9998" for this suite.

• [SLOW TEST:79.456 seconds]
[sig-api-machinery] Namespaces [Serial]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":296,"completed":17,"skipped":321,"failed":0}
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:38:10.927: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7416
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 21:38:12.348: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  5 21:38:14.405: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:38:16.421: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:38:18.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:38:20.417: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:38:22.413: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:38:24.420: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:38:26.424: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:38:28.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:38:30.668: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:38:32.418: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:38:34.432: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:38:36.413: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:38:38.415: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:38:40.433: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:38:42.416: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:38:44.463: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:38:46.446: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 38, 12, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 21:38:49.451: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:38:49.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7416" for this suite.
STEP: Destroying namespace "webhook-7416-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:40.717 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":296,"completed":18,"skipped":330,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:38:51.645: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-132
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 21:39:24.216: FAIL: while waiting for the pod container to fail
Unexpected error:
    <*errors.errorString | 0xc002404830>: {
        s: "pod was expected to be pending, but it is in the state: Failed",
    }
    pod was expected to be pending, but it is in the state: Failed
occurred

Full Stack Trace
k8s.io/kubernetes/test/e2e/common.testPodFailSubpath(0xc0009f6c60, 0x1)
	/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:376 +0x112
k8s.io/kubernetes/test/e2e/common.glob..func9.5()
	/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:175 +0x157
k8s.io/kubernetes/test/e2e.RunE2ETests(0x48f1d7)
	_output/local/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x6ae
k8s.io/kubernetes/test/e2e.TestE2E(0x408ed9)
	_output/local/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x19
testing.tRunner(0xc001480820, 0x462aaa0)
	/usr/lib/golang/src/testing/testing.go:1259 +0x102
created by testing.(*T).Run
	/usr/lib/golang/src/testing/testing.go:1306 +0x35a
Sep  5 21:39:24.217: INFO: Deleting pod "var-expansion-f37cc9a3-7fd9-4a24-8e60-d4e82ddbe25a" in namespace "var-expansion-132"
Sep  5 21:39:24.252: INFO: Wait up to 5m0s for pod "var-expansion-f37cc9a3-7fd9-4a24-8e60-d4e82ddbe25a" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
STEP: Collecting events from namespace "var-expansion-132".
STEP: Found 12 events.
Sep  5 21:39:36.300: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for var-expansion-f37cc9a3-7fd9-4a24-8e60-d4e82ddbe25a: { } Scheduled: Successfully assigned var-expansion-132/var-expansion-f37cc9a3-7fd9-4a24-8e60-d4e82ddbe25a to sc2-10-185-233-127.eng.vmware.com
Sep  5 21:39:36.300: INFO: At 2021-09-05 21:38:52 -0700 PDT - event for var-expansion-f37cc9a3-7fd9-4a24-8e60-d4e82ddbe25a: {image-controller } Image: Image busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v16807 bound successfully
Sep  5 21:39:36.300: INFO: At 2021-09-05 21:38:55 -0700 PDT - event for busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v16807: {image-controller } Status: sc2-10-185-233-127.eng.vmware.com: Image status changed to Resolving
Sep  5 21:39:36.300: INFO: At 2021-09-05 21:39:01 -0700 PDT - event for var-expansion-f37cc9a3-7fd9-4a24-8e60-d4e82ddbe25a: {nsx-container-ncp 4216b30cf7711a81a7935dfb22c7b9d2} SuccessfulRealizeNSXResource: Successfully realized NSX resource for Pod
Sep  5 21:39:36.300: INFO: At 2021-09-05 21:39:03 -0700 PDT - event for busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v16807: {image-controller } Bind: Imagedisk bind failed: Operation cannot be fulfilled on images.imagecontroller.vmware.com "busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v16807": the object has been modified; please apply your changes to the latest version and try again
Sep  5 21:39:36.300: INFO: At 2021-09-05 21:39:04 -0700 PDT - event for busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v16807: {image-controller } Bind: Imagedisk 23bc2b70b2014dec0ac22f27bb93e9babd08cdd6f1115d0c955b9ff22b382f5a-v3965207 successfully bound
Sep  5 21:39:36.300: INFO: At 2021-09-05 21:39:04 -0700 PDT - event for busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v16807: {image-controller } Resolve: sc2-10-185-233-127.eng.vmware.com: Image resolved to ChainID sha256:23bc2b70b2014dec0ac22f27bb93e9babd08cdd6f1115d0c955b9ff22b382f5a
Sep  5 21:39:36.300: INFO: At 2021-09-05 21:39:04 -0700 PDT - event for busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v16807: {image-controller } Status: Image status changed to Ready
Sep  5 21:39:36.300: INFO: At 2021-09-05 21:39:14 -0700 PDT - event for var-expansion-f37cc9a3-7fd9-4a24-8e60-d4e82ddbe25a: {kubelet sc2-10-185-233-127.eng.vmware.com} Pulling: Waiting for Image var-expansion-132/busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v16807
Sep  5 21:39:36.300: INFO: At 2021-09-05 21:39:14 -0700 PDT - event for var-expansion-f37cc9a3-7fd9-4a24-8e60-d4e82ddbe25a: {kubelet sc2-10-185-233-127.eng.vmware.com} Pulled: Image var-expansion-132/busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v16807 is ready
Sep  5 21:39:36.300: INFO: At 2021-09-05 21:39:22 -0700 PDT - event for var-expansion-f37cc9a3-7fd9-4a24-8e60-d4e82ddbe25a: {kubelet sc2-10-185-233-127.eng.vmware.com} SuccessfulMountVolume: Successfully mounted volume workdir1
Sep  5 21:39:36.300: INFO: At 2021-09-05 21:39:22 -0700 PDT - event for var-expansion-f37cc9a3-7fd9-4a24-8e60-d4e82ddbe25a: {kubelet sc2-10-185-233-127.eng.vmware.com} SuccessfulMountVolume: Successfully mounted volume default-token-pvgsn
Sep  5 21:39:36.309: INFO: POD  NODE  PHASE  GRACE  CONDITIONS
Sep  5 21:39:36.309: INFO: 
Sep  5 21:39:36.318: INFO: 
Logging node info for node 421629d5b5512e89fd5a479875bff24c
Sep  5 21:39:36.331: INFO: Node Info: &Node{ObjectMeta:{421629d5b5512e89fd5a479875bff24c   /api/v1/nodes/421629d5b5512e89fd5a479875bff24c 5392ab9e-8d36-4a02-8e7c-d62b5fbca56f 70176 0 2021-09-05 19:56:51 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:421629d5b5512e89fd5a479875bff24c kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node-role.kubernetes.io/master:] map[kubeadm.alpha.kubernetes.io/cri-socket:/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 vmware-system-workload-ip-configured: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubeadm Update v1 2021-09-05 19:57:37 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/control-plane":{},"f:node-role.kubernetes.io/master":{}}}}} {kubectl-annotate Update v1 2021-09-05 20:03:41 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-workload-ip-configured":{}}}}} {kube-controller-manager Update v1 2021-09-05 20:11:03 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:taints":{}}}} {kubelet Update v1 2021-09-05 20:11:08 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-09-05 21:36:50 -0700 PDT,LastTransitionTime:2021-09-05 20:11:07 -0700 PDT,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-09-05 21:36:50 -0700 PDT,LastTransitionTime:2021-09-05 20:11:07 -0700 PDT,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-09-05 21:36:50 -0700 PDT,LastTransitionTime:2021-09-05 20:11:07 -0700 PDT,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-05 21:36:50 -0700 PDT,LastTransitionTime:2021-09-05 20:11:07 -0700 PDT,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:172.26.0.4,},NodeAddress{Type:Hostname,Address:421629d5b5512e89fd5a479875bff24c,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:a84e0f9b757b45c3bda61dd8d7e81c47,SystemUUID:d5291642-51b5-892e-fd5a-479875bff24c,BootID:96557016-0d11-4d1b-9d79-0a2c04c403df,KernelVersion:4.19.198-1.ph3-esx,OSImage:VMware Photon OS/Linux,ContainerRuntimeVersion:containerd://1.4.4,KubeletVersion:v1.20.8+vmware.wcp.1,KubeProxyVersion:v1.20.8+vmware.wcp.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[docker.io/vmware/wcp-authproxy:0.0.11.18508287],SizeBytes:131961954,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.21.0],SizeBytes:126851027,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.20.8],SizeBytes:123038003,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.21.0],SizeBytes:121071415,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.19.12],SizeBytes:120109363,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.20.8],SizeBytes:117573957,},ContainerImage{Names:[localhost:5000/vmware/kubectl-plugin-vsphere@sha256:50972d3c755506a39b060daf001e0fb4d7bfcc47aa059a945573d6962338a8f4 localhost:5000/vmware/kubectl-plugin-vsphere:0.0.11.18508287 docker.io/vmware/kubectl-plugin-vsphere:0.0.11.18508287],SizeBytes:114408041,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.19.12],SizeBytes:112073027,},ContainerImage{Names:[projects.registry.vmware.com/tkg/etcd:v3.4.13_vmware.1],SizeBytes:105113092,},ContainerImage{Names:[docker.io/vmware/wcp-schedext:0.0.11.18508287 docker.io/vmware/wcp-schedext:v1.20.8],SizeBytes:86647081,},ContainerImage{Names:[docker.io/vmware/docker-registry:2.7.0.14110537],SizeBytes:53040781,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.21.0],SizeBytes:51910456,},ContainerImage{Names:[docker.io/vmware/wcp-fip:0.1],SizeBytes:49119461,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.20.8],SizeBytes:48519474,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.19.12],SizeBytes:47737138,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-control-plane-controller@sha256:ae28d5c4008d5ad88db2936d9b45e0248d129245810239bbafe0c66a47f2bf7e localhost:5000/vmware/kubeadm-control-plane-controller:v0.3.17_vmware.1],SizeBytes:36065259,},ContainerImage{Names:[localhost:5000/vmware/kube-proxy:active localhost:5002/vmware/kube-proxy:v1.20.8],SizeBytes:35405060,},ContainerImage{Names:[localhost:5000/vmware/capw-controller@sha256:56496b9339aba7d338f2f864f0bb946e1d57f50bbcffeee26c57c1a7be6e9cff localhost:5000/vmware/capw-controller:1.4.1-28-ge3e1d8b],SizeBytes:34792328,},ContainerImage{Names:[localhost:5000/vmware/cluster-api-controller@sha256:72a217e8de91b2e1adcbcb1266a5854e9683f0f37ea828808374563d2bf41861 localhost:5000/vmware/cluster-api-controller:v0.3.17_vmware.1],SizeBytes:34280507,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-bootstrap-controller@sha256:921fc3887b0cfaf4742eefaf2b6d60160fdc25d2600dc08e9a3389aed6fbdaca localhost:5000/vmware/kubeadm-bootstrap-controller:v0.3.17_vmware.1],SizeBytes:33862350,},ContainerImage{Names:[localhost:5000/vmware/ucs@sha256:0006afae8591e51b3ddfe6d1062caa779c2bee08dc20ee38eb8183614ccbdcd1 localhost:5000/vmware/ucs:1.0-34-g9eb2916],SizeBytes:32101483,},ContainerImage{Names:[localhost:5000/vmware/tmc-agent-installer@sha256:4d885d2a505ccc87baf5fb73958593f0b66ce00732397e935114f923367cd4ec localhost:5000/vmware/tmc-agent-installer:1.0],SizeBytes:30452862,},ContainerImage{Names:[localhost:5000/vmware/kube-rbac-proxy@sha256:ce03b0932d049e82747829680025a5d2e3c2a3f50ad27e761329d35e9e8949d5 localhost:5000/vmware/kube-rbac-proxy:v0.4.1_vmware.1 localhost:5000/vmware/kube-rbac-proxy:0.0.1],SizeBytes:29742750,},ContainerImage{Names:[localhost:5000/vmware/vmop@sha256:aa3d277bcb390735b0110d9bf3f3e63ab48db23437c9ac275ea487a2fa460dd5 localhost:5000/vmware/vmop:1.4-201-g3d42a58],SizeBytes:29542297,},ContainerImage{Names:[localhost:5000/vmware/tkgs-plugin@sha256:7160ccba94dbc3a68fdb31108319ca03163688bc5999c4da1ee20e32723ea612 localhost:5000/vmware/tkgs-plugin:prod],SizeBytes:27042759,},ContainerImage{Names:[localhost:5000/vmware/tkg-controller@sha256:700277052cbfe8be5e7a67e7819bde16ac0afceba352c473906dc3cc572cdaf9 localhost:5000/vmware/tkg-controller:1.4-196-g0e69a9e],SizeBytes:26884265,},ContainerImage{Names:[localhost:5000/vmware.io/fluent-bit@sha256:bc50adf05790809513d94105a169912371b4b5f656775350b1ab65b556ecdc32 localhost:5000/vmware.io/fluent-bit:v1.5.1_vmware.3],SizeBytes:26471308,},ContainerImage{Names:[localhost:5000/vmware/namespace-operator@sha256:fbb1177a60f33312e0415830fd55c5b565628a863a25cb500b2058fc475e8da4 localhost:5000/vmware/namespace-operator:c203fcd],SizeBytes:25804352,},ContainerImage{Names:[localhost:5000/vmware/license-operator@sha256:521924f17d102e95723a580ed084b6fbc15e3f317b982a81fd071718c3e84138 localhost:5000/vmware/license-operator:7f54d05],SizeBytes:25041264,},ContainerImage{Names:[localhost:5000/vmware/coredns@sha256:68ed1b5aea34d19af320d3390278e7719efa19e3d9a42ef12fed0b2b99446c4f localhost:5000/vmware/coredns:v1.20],SizeBytes:12884153,},ContainerImage{Names:[docker.io/vmware/pause:1.19.12 docker.io/vmware/pause:1.20.8 docker.io/vmware/pause:1.21.0],SizeBytes:685866,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Sep  5 21:39:36.332: INFO: 
Logging kubelet events for node 421629d5b5512e89fd5a479875bff24c
Sep  5 21:39:36.340: INFO: 
Logging pods the kubelet thinks is on node 421629d5b5512e89fd5a479875bff24c
Sep  5 21:39:36.442: INFO: wcp-fip-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container wcp-fip ready: true, restart count 0
Sep  5 21:39:36.442: INFO: vmware-system-tkg-controller-manager-575d95fb57-zl7sg started at 2021-09-05 20:03:54 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:36.442: INFO: 	Container manager ready: true, restart count 1
Sep  5 21:39:36.442: INFO: capi-kubeadm-bootstrap-controller-manager-5f5774d559-6cbtc started at 2021-09-05 20:03:54 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:36.442: INFO: 	Container manager ready: true, restart count 1
Sep  5 21:39:36.442: INFO: capi-kubeadm-control-plane-webhook-5f4c87b8b9-cphz4 started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:36.442: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:36.442: INFO: capi-webhook-69769f4c68-xjnm2 started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:36.442: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:36.442: INFO: kube-proxy-cwdbw started at 2021-09-05 20:04:19 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  5 21:39:36.442: INFO: docker-registry-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container docker-registry ready: true, restart count 0
Sep  5 21:39:36.442: INFO: etcd-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container etcd ready: true, restart count 0
Sep  5 21:39:36.442: INFO: kube-controller-manager-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 19:57:05 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container kube-controller-manager ready: true, restart count 2
Sep  5 21:39:36.442: INFO: kube-scheduler-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 19:57:05 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container kube-scheduler ready: true, restart count 3
Sep  5 21:39:36.442: INFO: 	Container wcp-schedext ready: true, restart count 1
Sep  5 21:39:36.442: INFO: fluentbit-rpz54 started at 2021-09-05 19:58:12 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container fluentbit ready: true, restart count 0
Sep  5 21:39:36.442: INFO: capi-kubeadm-bootstrap-webhook-6766c687f9-hjz5w started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:36.442: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:36.442: INFO: kube-apiserver-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 19:57:05 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container kube-apiserver ready: true, restart count 2
Sep  5 21:39:36.442: INFO: vmware-system-tkg-webhook-64fd4868cb-mk6zh started at 2021-09-05 20:03:54 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:36.442: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:36.442: INFO: vmware-system-vmop-controller-manager-6649dd65b-q7bqw started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:36.442: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:36.442: INFO: capi-kubeadm-control-plane-controller-manager-7c8d4b456f-l4t5v started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:36.442: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:36.442: INFO: tkgs-plugin-server-57df5fcfbf-km5pj started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:36.442: INFO: capw-webhook-58b86fb8b-vrzsj started at 2021-09-05 20:03:54 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:36.442: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:36.442: INFO: vmware-system-nsop-controller-manager-7dd8c47dcc-hbv6m started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container manager ready: true, restart count 1
Sep  5 21:39:36.442: INFO: upgrade-compatibility-service-65969fd6bc-l72w9 started at 2021-09-05 20:03:55 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container ucs ready: true, restart count 0
Sep  5 21:39:36.442: INFO: vmware-system-license-operator-controller-manager-5f5b6cddmtkxt started at 2021-09-05 20:03:55 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:36.442: INFO: masterproxy-tkgs-plugin-k5csq started at 2021-09-05 20:04:12 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container nginx ready: true, restart count 0
Sep  5 21:39:36.442: INFO: tmc-agent-installer-1630903140-j7z2c started at 2021-09-05 21:39:04 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container tmc-agent-installer ready: false, restart count 0
Sep  5 21:39:36.442: INFO: coredns-594c6dccdd-6jv2b started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container coredns ready: true, restart count 0
Sep  5 21:39:36.442: INFO: capi-controller-manager-d586f4c8-dnnr6 started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:36.442: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:36.442: INFO: kubectl-plugin-vsphere-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container kubectl-plugin-vsphere ready: true, restart count 4
Sep  5 21:39:36.442: INFO: wcp-authproxy-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container wcp-authproxy ready: true, restart count 0
Sep  5 21:39:36.442: INFO: capw-controller-manager-85b7cbb4bf-bghkv started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:36.442: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:36.442: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:37.363: INFO: 
Latency metrics for node 421629d5b5512e89fd5a479875bff24c
Sep  5 21:39:37.363: INFO: 
Logging node info for node 4216b0433daa6b68a0e3d69463d0611d
Sep  5 21:39:37.382: INFO: Node Info: &Node{ObjectMeta:{4216b0433daa6b68a0e3d69463d0611d   /api/v1/nodes/4216b0433daa6b68a0e3d69463d0611d f38a87dd-ca99-4665-a189-88c353c18242 71767 0 2021-09-05 19:55:52 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:4216b0433daa6b68a0e3d69463d0611d kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node-role.kubernetes.io/master:] map[kubeadm.alpha.kubernetes.io/cri-socket:/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 vmware-system-workload-ip-configured: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubeadm Update v1 2021-09-05 19:56:44 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/control-plane":{},"f:node-role.kubernetes.io/master":{}}}}} {kubectl-annotate Update v1 2021-09-05 20:01:13 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-workload-ip-configured":{}}}}} {kube-controller-manager Update v1 2021-09-05 20:08:11 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:taints":{}}}} {kubelet Update v1 2021-09-05 20:08:16 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-09-05 21:39:01 -0700 PDT,LastTransitionTime:2021-09-05 20:08:16 -0700 PDT,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-09-05 21:39:01 -0700 PDT,LastTransitionTime:2021-09-05 20:08:16 -0700 PDT,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-09-05 21:39:01 -0700 PDT,LastTransitionTime:2021-09-05 20:08:16 -0700 PDT,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-05 21:39:01 -0700 PDT,LastTransitionTime:2021-09-05 20:08:16 -0700 PDT,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:172.26.0.3,},NodeAddress{Type:Hostname,Address:4216b0433daa6b68a0e3d69463d0611d,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7611aba96c374b2daf54816748293442,SystemUUID:43b01642-aa3d-686b-a0e3-d69463d0611d,BootID:ebcc64ab-5283-4781-b7ac-d3f663f1dc23,KernelVersion:4.19.198-1.ph3-esx,OSImage:VMware Photon OS/Linux,ContainerRuntimeVersion:containerd://1.4.4,KubeletVersion:v1.20.8+vmware.wcp.1,KubeProxyVersion:v1.20.8+vmware.wcp.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[docker.io/vmware/wcp-authproxy:0.0.11.18508287],SizeBytes:131961954,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.21.0],SizeBytes:126851027,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.20.8],SizeBytes:123038003,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.21.0],SizeBytes:121071415,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.19.12],SizeBytes:120109363,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.20.8],SizeBytes:117573957,},ContainerImage{Names:[localhost:5000/vmware/kubectl-plugin-vsphere@sha256:50972d3c755506a39b060daf001e0fb4d7bfcc47aa059a945573d6962338a8f4 docker.io/vmware/kubectl-plugin-vsphere:0.0.11.18508287 localhost:5000/vmware/kubectl-plugin-vsphere:0.0.11.18508287],SizeBytes:114408041,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.19.12],SizeBytes:112073027,},ContainerImage{Names:[projects.registry.vmware.com/tkg/etcd:v3.4.13_vmware.1],SizeBytes:105113092,},ContainerImage{Names:[docker.io/vmware/wcp-schedext:v1.20.8 docker.io/vmware/wcp-schedext:0.0.11.18508287],SizeBytes:86647081,},ContainerImage{Names:[localhost:5000/vmware/registry-agent@sha256:a02a9f68366f57abdd0833d34f1394e402a519d3fe82ff8c1106c67fc7c80392 localhost:5000/vmware/registry-agent:0.0.11.18508287],SizeBytes:76983788,},ContainerImage{Names:[docker.io/vmware/docker-registry:2.7.0.14110537],SizeBytes:53040781,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.21.0],SizeBytes:51910456,},ContainerImage{Names:[docker.io/vmware/wcp-fip:0.1],SizeBytes:49119461,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.20.8],SizeBytes:48519474,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.19.12],SizeBytes:47737138,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-control-plane-controller@sha256:ae28d5c4008d5ad88db2936d9b45e0248d129245810239bbafe0c66a47f2bf7e localhost:5000/vmware/kubeadm-control-plane-controller:v0.3.17_vmware.1],SizeBytes:36065259,},ContainerImage{Names:[localhost:5002/vmware/kube-proxy:v1.20.8 localhost:5000/vmware/kube-proxy:active],SizeBytes:35405060,},ContainerImage{Names:[localhost:5000/vmware/capw-controller@sha256:56496b9339aba7d338f2f864f0bb946e1d57f50bbcffeee26c57c1a7be6e9cff localhost:5000/vmware/capw-controller:1.4.1-28-ge3e1d8b],SizeBytes:34792328,},ContainerImage{Names:[localhost:5000/vmware/cluster-api-controller@sha256:72a217e8de91b2e1adcbcb1266a5854e9683f0f37ea828808374563d2bf41861 localhost:5000/vmware/cluster-api-controller:v0.3.17_vmware.1],SizeBytes:34280507,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-bootstrap-controller@sha256:921fc3887b0cfaf4742eefaf2b6d60160fdc25d2600dc08e9a3389aed6fbdaca localhost:5000/vmware/kubeadm-bootstrap-controller:v0.3.17_vmware.1],SizeBytes:33862350,},ContainerImage{Names:[localhost:5000/vmware/ucs@sha256:0006afae8591e51b3ddfe6d1062caa779c2bee08dc20ee38eb8183614ccbdcd1 localhost:5000/vmware/ucs:1.0-34-g9eb2916],SizeBytes:32101483,},ContainerImage{Names:[localhost:5000/vmware/tmc-agent-installer@sha256:4d885d2a505ccc87baf5fb73958593f0b66ce00732397e935114f923367cd4ec localhost:5000/vmware/tmc-agent-installer:1.0],SizeBytes:30452862,},ContainerImage{Names:[localhost:5000/vmware/kube-rbac-proxy@sha256:ce03b0932d049e82747829680025a5d2e3c2a3f50ad27e761329d35e9e8949d5 localhost:5000/vmware/kube-rbac-proxy:v0.4.1_vmware.1 localhost:5000/vmware/kube-rbac-proxy:0.0.1],SizeBytes:29742750,},ContainerImage{Names:[localhost:5000/vmware/vmop@sha256:aa3d277bcb390735b0110d9bf3f3e63ab48db23437c9ac275ea487a2fa460dd5 localhost:5000/vmware/vmop:1.4-201-g3d42a58],SizeBytes:29542297,},ContainerImage{Names:[localhost:5000/vmware/tkgs-plugin@sha256:7160ccba94dbc3a68fdb31108319ca03163688bc5999c4da1ee20e32723ea612 localhost:5000/vmware/tkgs-plugin:prod],SizeBytes:27042759,},ContainerImage{Names:[localhost:5000/vmware/tkg-controller@sha256:700277052cbfe8be5e7a67e7819bde16ac0afceba352c473906dc3cc572cdaf9 localhost:5000/vmware/tkg-controller:1.4-196-g0e69a9e],SizeBytes:26884265,},ContainerImage{Names:[localhost:5000/vmware.io/fluent-bit@sha256:bc50adf05790809513d94105a169912371b4b5f656775350b1ab65b556ecdc32 localhost:5000/vmware.io/fluent-bit:v1.5.1_vmware.3],SizeBytes:26471308,},ContainerImage{Names:[localhost:5000/vmware/namespace-operator@sha256:fbb1177a60f33312e0415830fd55c5b565628a863a25cb500b2058fc475e8da4 localhost:5000/vmware/namespace-operator:c203fcd],SizeBytes:25804352,},ContainerImage{Names:[localhost:5000/vmware/license-operator@sha256:521924f17d102e95723a580ed084b6fbc15e3f317b982a81fd071718c3e84138 localhost:5000/vmware/license-operator:7f54d05],SizeBytes:25041264,},ContainerImage{Names:[localhost:5000/vmware/coredns@sha256:68ed1b5aea34d19af320d3390278e7719efa19e3d9a42ef12fed0b2b99446c4f localhost:5000/vmware/coredns:v1.20],SizeBytes:12884153,},ContainerImage{Names:[docker.io/vmware/pause:1.21.0 docker.io/vmware/pause:1.19.12 docker.io/vmware/pause:1.20.8],SizeBytes:685866,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Sep  5 21:39:37.383: INFO: 
Logging kubelet events for node 4216b0433daa6b68a0e3d69463d0611d
Sep  5 21:39:37.393: INFO: 
Logging pods the kubelet thinks is on node 4216b0433daa6b68a0e3d69463d0611d
Sep  5 21:39:37.528: INFO: vmware-system-tkg-webhook-64fd4868cb-c7l5v started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:37.528: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:37.528: INFO: kube-proxy-rpd77 started at 2021-09-05 20:01:38 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  5 21:39:37.528: INFO: capi-kubeadm-control-plane-controller-manager-7c8d4b456f-qsw77 started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:37.528: INFO: 	Container manager ready: true, restart count 1
Sep  5 21:39:37.528: INFO: vmware-system-nsop-controller-manager-7dd8c47dcc-fvwkc started at 2021-09-05 20:01:38 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container manager ready: true, restart count 1
Sep  5 21:39:37.528: INFO: capi-kubeadm-bootstrap-controller-manager-5f5774d559-bdxq9 started at 2021-09-05 20:01:38 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:37.528: INFO: 	Container manager ready: true, restart count 1
Sep  5 21:39:37.528: INFO: capi-kubeadm-bootstrap-webhook-6766c687f9-tfvhj started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:37.528: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:37.528: INFO: kubectl-plugin-vsphere-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:56:05 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container kubectl-plugin-vsphere ready: true, restart count 2
Sep  5 21:39:37.528: INFO: wcp-fip-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 20:01:25 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container wcp-fip ready: true, restart count 0
Sep  5 21:39:37.528: INFO: vmware-system-vmop-controller-manager-6649dd65b-6mvnw started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:37.528: INFO: 	Container manager ready: true, restart count 1
Sep  5 21:39:37.528: INFO: vmware-system-tkg-controller-manager-575d95fb57-k4slt started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:37.528: INFO: 	Container manager ready: true, restart count 1
Sep  5 21:39:37.528: INFO: tkgs-plugin-server-57df5fcfbf-pcxc7 started at 2021-09-05 20:01:25 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:37.528: INFO: capw-webhook-58b86fb8b-l8m95 started at 2021-09-05 20:01:38 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:37.528: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:37.528: INFO: masterproxy-tkgs-plugin-b7p7c started at 2021-09-05 20:01:25 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container nginx ready: true, restart count 0
Sep  5 21:39:37.528: INFO: coredns-594c6dccdd-vp82c started at 2021-09-05 20:01:37 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container coredns ready: true, restart count 0
Sep  5 21:39:37.528: INFO: capi-controller-manager-d586f4c8-m66p4 started at 2021-09-05 20:01:38 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:37.528: INFO: 	Container manager ready: true, restart count 1
Sep  5 21:39:37.528: INFO: vmware-system-license-operator-controller-manager-5f5b6cddsqckm started at 2021-09-05 20:01:38 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:37.528: INFO: vmware-registry-controller-manager-75fff77685-9v4wb started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container admin-agent ready: true, restart count 0
Sep  5 21:39:37.528: INFO: 	Container service-agent ready: true, restart count 5
Sep  5 21:39:37.528: INFO: kube-controller-manager-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:56:05 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep  5 21:39:37.528: INFO: kube-apiserver-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:56:05 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container kube-apiserver ready: true, restart count 3
Sep  5 21:39:37.528: INFO: fluentbit-8fbtb started at 2021-09-05 19:57:29 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container fluentbit ready: true, restart count 0
Sep  5 21:39:37.528: INFO: capw-controller-manager-85b7cbb4bf-n2kkl started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:37.528: INFO: 	Container manager ready: true, restart count 3
Sep  5 21:39:37.528: INFO: capi-webhook-69769f4c68-5mxkm started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:37.528: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:37.528: INFO: wcp-authproxy-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:58:14 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container wcp-authproxy ready: true, restart count 0
Sep  5 21:39:37.528: INFO: etcd-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:56:05 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container etcd ready: true, restart count 0
Sep  5 21:39:37.528: INFO: capi-kubeadm-control-plane-webhook-5f4c87b8b9-zr5kj started at 2021-09-05 20:01:38 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:37.528: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:37.528: INFO: kube-scheduler-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:56:05 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container kube-scheduler ready: true, restart count 2
Sep  5 21:39:37.528: INFO: 	Container wcp-schedext ready: true, restart count 0
Sep  5 21:39:37.528: INFO: upgrade-compatibility-service-65969fd6bc-5ztpr started at 2021-09-05 20:01:38 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container ucs ready: true, restart count 0
Sep  5 21:39:37.528: INFO: docker-registry-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:56:05 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:37.528: INFO: 	Container docker-registry ready: true, restart count 0
Sep  5 21:39:38.558: INFO: 
Latency metrics for node 4216b0433daa6b68a0e3d69463d0611d
Sep  5 21:39:38.558: INFO: 
Logging node info for node 4216b30cf7711a81a7935dfb22c7b9d2
Sep  5 21:39:38.576: INFO: Node Info: &Node{ObjectMeta:{4216b30cf7711a81a7935dfb22c7b9d2   /api/v1/nodes/4216b30cf7711a81a7935dfb22c7b9d2 ca76b1f1-4314-4242-80c1-22a5cf294536 71867 0 2021-09-05 19:45:46 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:4216b30cf7711a81a7935dfb22c7b9d2 kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node-role.kubernetes.io/master:] map[kubeadm.alpha.kubernetes.io/cri-socket:/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 vmware-system-workload-ip-configured: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubelet Update v1 2021-09-05 19:45:46 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}} {kubeadm Update v1 2021-09-05 19:45:51 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/control-plane":{},"f:node-role.kubernetes.io/master":{}}}}} {kube-controller-manager Update v1 2021-09-05 19:45:56 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:taints":{}}}} {kubectl-annotate Update v1 2021-09-05 19:55:19 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-workload-ip-configured":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-09-05 21:39:09 -0700 PDT,LastTransitionTime:2021-09-05 19:45:41 -0700 PDT,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-09-05 21:39:09 -0700 PDT,LastTransitionTime:2021-09-05 19:45:41 -0700 PDT,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-09-05 21:39:09 -0700 PDT,LastTransitionTime:2021-09-05 19:45:41 -0700 PDT,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-05 21:39:09 -0700 PDT,LastTransitionTime:2021-09-05 19:58:42 -0700 PDT,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:172.26.0.2,},NodeAddress{Type:Hostname,Address:4216b30cf7711a81a7935dfb22c7b9d2,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:a54387ba332340588c82ffb9af566e0a,SystemUUID:0cb31642-71f7-811a-a793-5dfb22c7b9d2,BootID:ac168864-03bb-44ca-934a-1f275ba10acf,KernelVersion:4.19.198-1.ph3-esx,OSImage:VMware Photon OS/Linux,ContainerRuntimeVersion:containerd://1.4.4,KubeletVersion:v1.20.8+vmware.wcp.1,KubeProxyVersion:v1.20.8+vmware.wcp.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[localhost:5000/vmware/nsx-ncp-photon@sha256:07d0445341f274674dea40d9449da3304c62615afa544f4023e3a3e21c353c89 localhost:5000/vmware/nsx-ncp-photon:3.2.0.18464816],SizeBytes:456347510,},ContainerImage{Names:[localhost:5000/vmware/vsphere-csi@sha256:5f667055674ed889a9a5a3e52e1f074d2adedb5ccf5fdc9a2cefb56e50257346 localhost:5000/vmware/vsphere-csi:vsphere70u3-3530247],SizeBytes:218324207,},ContainerImage{Names:[localhost:5000/vmware/syncer@sha256:ebb1907ad57ee343efe2a756bd7109021701469875d29008004d2e6abe615e66 localhost:5000/vmware/syncer:vsphere70u3-3530247],SizeBytes:186035682,},ContainerImage{Names:[docker.io/vmware/wcp-authproxy:0.0.11.18508287],SizeBytes:131961954,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.21.0],SizeBytes:126851027,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.20.8],SizeBytes:123038003,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.21.0],SizeBytes:121071415,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.19.12],SizeBytes:120109363,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.20.8],SizeBytes:117573957,},ContainerImage{Names:[localhost:5000/vmware/kubectl-plugin-vsphere@sha256:50972d3c755506a39b060daf001e0fb4d7bfcc47aa059a945573d6962338a8f4 docker.io/vmware/kubectl-plugin-vsphere:0.0.11.18508287 localhost:5000/vmware/kubectl-plugin-vsphere:0.0.11.18508287],SizeBytes:114408041,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.19.12],SizeBytes:112073027,},ContainerImage{Names:[projects.registry.vmware.com/tkg/etcd:v3.4.13_vmware.1],SizeBytes:105113092,},ContainerImage{Names:[localhost:5000/vmware/cert-manager-controller@sha256:ff0caa9f76178dfc2c4398046b4b08472ab90def98d1b8649833d092a1468100 localhost:5000/vmware/cert-manager-controller:v0.15.2_vmware.3],SizeBytes:94027733,},ContainerImage{Names:[localhost:5000/vmware/cert-manager-cainjector@sha256:5137cbc469ee041ef0d1f6d35a914889d50c0a44134967ca407ba7c2f1a7628d localhost:5000/vmware/cert-manager-cainjector:v0.15.2_vmware.3],SizeBytes:88001986,},ContainerImage{Names:[localhost:5000/vmware/cert-manager-webhook@sha256:da40b26c6920331288a4809415c202e2ad3a60392ee3f43f535626024b8a9845 localhost:5000/vmware/cert-manager-webhook:v0.15.2_vmware.3],SizeBytes:87704580,},ContainerImage{Names:[docker.io/vmware/wcp-schedext:0.0.11.18508287 docker.io/vmware/wcp-schedext:v1.20.8],SizeBytes:86647081,},ContainerImage{Names:[localhost:5000/vmware/wcp-appplatform-operator-v1alpha2@sha256:39f94d105b25a4f047caa8234d74645c21ccee2e13279ee4e07fb506e27d7494 localhost:5000/vmware/wcp-appplatform-operator-v1alpha2:5e0ffb0],SizeBytes:72197580,},ContainerImage{Names:[localhost:5000/vmware/psp-operator@sha256:3a831bb95bb73b3bda416fc37e3eda8eb2ed2a54109ec2e54ad6d70932f2813e localhost:5000/vmware/psp-operator:49766149],SizeBytes:59395632,},ContainerImage{Names:[docker.io/vmware/docker-registry:2.7.0.14110537],SizeBytes:53040781,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.21.0],SizeBytes:51910456,},ContainerImage{Names:[docker.io/vmware/wcp-fip:0.1],SizeBytes:49119461,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.20.8],SizeBytes:48519474,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.19.12],SizeBytes:47737138,},ContainerImage{Names:[localhost:5000/vmware/image-controller@sha256:e8bcd8932715a95d86f857fc838d67aed6953eeab4554a3dada45c80a222b2ff localhost:5000/vmware/image-controller:0.0.11.18508287],SizeBytes:38823779,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-control-plane-controller@sha256:ae28d5c4008d5ad88db2936d9b45e0248d129245810239bbafe0c66a47f2bf7e localhost:5000/vmware/kubeadm-control-plane-controller:v0.3.17_vmware.1],SizeBytes:36065259,},ContainerImage{Names:[localhost:5002/vmware/kube-proxy:v1.20.8 localhost:5000/vmware/kube-proxy:active],SizeBytes:35405060,},ContainerImage{Names:[localhost:5000/vmware/capw-controller@sha256:56496b9339aba7d338f2f864f0bb946e1d57f50bbcffeee26c57c1a7be6e9cff localhost:5000/vmware/capw-controller:1.4.1-28-ge3e1d8b],SizeBytes:34792328,},ContainerImage{Names:[localhost:5000/vmware/cluster-api-controller@sha256:72a217e8de91b2e1adcbcb1266a5854e9683f0f37ea828808374563d2bf41861 localhost:5000/vmware/cluster-api-controller:v0.3.17_vmware.1],SizeBytes:34280507,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-bootstrap-controller@sha256:921fc3887b0cfaf4742eefaf2b6d60160fdc25d2600dc08e9a3389aed6fbdaca localhost:5000/vmware/kubeadm-bootstrap-controller:v0.3.17_vmware.1],SizeBytes:33862350,},ContainerImage{Names:[localhost:5000/vmware/ucs@sha256:0006afae8591e51b3ddfe6d1062caa779c2bee08dc20ee38eb8183614ccbdcd1 localhost:5000/vmware/ucs:1.0-34-g9eb2916],SizeBytes:32101483,},ContainerImage{Names:[localhost:5000/vmware/tmc-agent-installer@sha256:4d885d2a505ccc87baf5fb73958593f0b66ce00732397e935114f923367cd4ec localhost:5000/vmware/tmc-agent-installer:1.0],SizeBytes:30452862,},ContainerImage{Names:[localhost:5000/vmware/kube-rbac-proxy@sha256:ce03b0932d049e82747829680025a5d2e3c2a3f50ad27e761329d35e9e8949d5 localhost:5000/vmware/kube-rbac-proxy:v0.4.1_vmware.1 localhost:5000/vmware/kube-rbac-proxy:0.0.1],SizeBytes:29742750,},ContainerImage{Names:[localhost:5000/vmware/vmop@sha256:aa3d277bcb390735b0110d9bf3f3e63ab48db23437c9ac275ea487a2fa460dd5 localhost:5000/vmware/vmop:1.4-201-g3d42a58],SizeBytes:29542297,},ContainerImage{Names:[localhost:5000/vmware/tkgs-plugin@sha256:7160ccba94dbc3a68fdb31108319ca03163688bc5999c4da1ee20e32723ea612 localhost:5000/vmware/tkgs-plugin:prod],SizeBytes:27042759,},ContainerImage{Names:[localhost:5000/vmware/tkg-controller@sha256:700277052cbfe8be5e7a67e7819bde16ac0afceba352c473906dc3cc572cdaf9 localhost:5000/vmware/tkg-controller:1.4-196-g0e69a9e],SizeBytes:26884265,},ContainerImage{Names:[localhost:5000/vmware.io/fluent-bit@sha256:bc50adf05790809513d94105a169912371b4b5f656775350b1ab65b556ecdc32 localhost:5000/vmware.io/fluent-bit:v1.5.1_vmware.3],SizeBytes:26471308,},ContainerImage{Names:[localhost:5000/vmware/namespace-operator@sha256:fbb1177a60f33312e0415830fd55c5b565628a863a25cb500b2058fc475e8da4 localhost:5000/vmware/namespace-operator:c203fcd],SizeBytes:25804352,},ContainerImage{Names:[localhost:5000/vmware/license-operator@sha256:521924f17d102e95723a580ed084b6fbc15e3f317b982a81fd071718c3e84138 localhost:5000/vmware/license-operator:7f54d05],SizeBytes:25041264,},ContainerImage{Names:[localhost:5000/vmware/coredns@sha256:68ed1b5aea34d19af320d3390278e7719efa19e3d9a42ef12fed0b2b99446c4f localhost:5000/vmware/coredns:v1.20],SizeBytes:12884153,},ContainerImage{Names:[localhost:5000/vmware.io/csi-attacher@sha256:ef84a54cab084305c3b6638bcf3616539a1ece71054e519ce72c74a85ed6883c localhost:5000/vmware.io/csi-attacher:v3.2.1_vmware.1],SizeBytes:12847160,},ContainerImage{Names:[localhost:5000/vmware/kubernetes-csi_external-resizer/kubernetes-csi_external-resizer@sha256:eafb68e3367ac1b840b55b115351163cd24dfc1752e402c8d83210ab35d251eb localhost:5000/vmware/kubernetes-csi_external-resizer/kubernetes-csi_external-resizer:v1.2.0_vmware.1],SizeBytes:12843921,},ContainerImage{Names:[localhost:5000/vmware/csi-provisioner/csi-provisioner@sha256:f3783faf0e57904cde3702f7f0fb1f6e959c3160c90d399ea90931acd77abc18 localhost:5000/vmware/csi-provisioner/csi-provisioner:v2.1.0_vmware.4],SizeBytes:12501308,},ContainerImage{Names:[localhost:5000/vmware.io/csi-livenessprobe@sha256:7790ca0da41bfc8cd05c35c9309a36d060f847402147fddc32ca14063f9adc89 localhost:5000/vmware.io/csi-livenessprobe:v2.3.0_vmware.1],SizeBytes:5629758,},ContainerImage{Names:[docker.io/vmware/pause:1.19.12 docker.io/vmware/pause:1.20.8 docker.io/vmware/pause:1.21.0],SizeBytes:685866,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Sep  5 21:39:38.577: INFO: 
Logging kubelet events for node 4216b30cf7711a81a7935dfb22c7b9d2
Sep  5 21:39:38.588: INFO: 
Logging pods the kubelet thinks is on node 4216b30cf7711a81a7935dfb22c7b9d2
Sep  5 21:39:38.676: INFO: capi-kubeadm-bootstrap-controller-manager-5f5774d559-whnw5 started at 2021-09-05 19:50:51 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:38.676: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:38.676: INFO: 	Container manager ready: true, restart count 4
Sep  5 21:39:38.676: INFO: capw-controller-manager-85b7cbb4bf-9kcpx started at 2021-09-05 19:50:53 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:38.676: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:38.676: INFO: 	Container manager ready: true, restart count 3
Sep  5 21:39:38.676: INFO: kube-apiserver-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:58:35 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.676: INFO: 	Container kube-apiserver ready: true, restart count 1
Sep  5 21:39:38.676: INFO: wcp-fip-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:58:35 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.676: INFO: 	Container wcp-fip ready: true, restart count 0
Sep  5 21:39:38.676: INFO: fluentbit-k2ft9 started at 2021-09-05 19:47:14 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.676: INFO: 	Container fluentbit ready: true, restart count 0
Sep  5 21:39:38.676: INFO: coredns-594c6dccdd-d29zk started at 2021-09-05 19:47:13 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.676: INFO: 	Container coredns ready: true, restart count 7
Sep  5 21:39:38.676: INFO: kube-controller-manager-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:44:40 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.676: INFO: 	Container kube-controller-manager ready: true, restart count 2
Sep  5 21:39:38.676: INFO: vmware-system-appplatform-operator-mgr-0 started at 2021-09-05 19:47:14 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.676: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:38.676: INFO: cert-manager-cainjector-69c886766f-glw2p started at 2021-09-05 19:47:17 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.676: INFO: 	Container cert-manager ready: true, restart count 5
Sep  5 21:39:38.676: INFO: tkgs-plugin-server-57df5fcfbf-6gcbl started at 2021-09-05 19:52:48 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.676: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:38.676: INFO: vsphere-csi-controller-7ff5f98858-cthb7 started at 2021-09-05 19:52:54 -0700 PDT (0+6 container statuses recorded)
Sep  5 21:39:38.676: INFO: 	Container csi-attacher ready: true, restart count 3
Sep  5 21:39:38.676: INFO: 	Container csi-provisioner ready: true, restart count 3
Sep  5 21:39:38.676: INFO: 	Container csi-resizer ready: true, restart count 5
Sep  5 21:39:38.676: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  5 21:39:38.676: INFO: 	Container vsphere-csi-controller ready: true, restart count 2
Sep  5 21:39:38.676: INFO: 	Container vsphere-syncer ready: true, restart count 2
Sep  5 21:39:38.676: INFO: upgrade-compatibility-service-65969fd6bc-wgqdt started at 2021-09-05 19:47:14 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.676: INFO: 	Container ucs ready: true, restart count 0
Sep  5 21:39:38.676: INFO: cert-manager-799b5bbfdf-52dgq started at 2021-09-05 19:47:17 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.676: INFO: 	Container cert-manager ready: true, restart count 0
Sep  5 21:39:38.676: INFO: cert-manager-webhook-74488f47f-c6nmc started at 2021-09-05 19:47:17 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.676: INFO: 	Container cert-manager ready: true, restart count 0
Sep  5 21:39:38.676: INFO: nsx-ncp-6d7f7bf559-df5sk started at 2021-09-05 19:47:14 -0700 PDT (1+1 container statuses recorded)
Sep  5 21:39:38.676: INFO: 	Init container nsx-ncp-upgrade ready: true, restart count 0
Sep  5 21:39:38.676: INFO: 	Container nsx-ncp ready: true, restart count 6
Sep  5 21:39:38.676: INFO: capi-kubeadm-bootstrap-webhook-6766c687f9-tx5q2 started at 2021-09-05 19:50:51 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:38.676: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:38.676: INFO: 	Container manager ready: true, restart count 1
Sep  5 21:39:38.676: INFO: capi-webhook-69769f4c68-p8dfp started at 2021-09-05 19:50:53 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:38.676: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:38.677: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:38.677: INFO: vmware-system-tkg-controller-manager-575d95fb57-rcjkp started at 2021-09-05 19:51:32 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:38.677: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:38.677: INFO: 	Container manager ready: true, restart count 3
Sep  5 21:39:38.677: INFO: kube-scheduler-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:44:40 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:38.677: INFO: 	Container kube-scheduler ready: true, restart count 6
Sep  5 21:39:38.677: INFO: 	Container wcp-schedext ready: true, restart count 0
Sep  5 21:39:38.677: INFO: kubectl-plugin-vsphere-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:44:40 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.677: INFO: 	Container kubectl-plugin-vsphere ready: true, restart count 3
Sep  5 21:39:38.677: INFO: wcp-authproxy-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:58:35 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.677: INFO: 	Container wcp-authproxy ready: true, restart count 0
Sep  5 21:39:38.677: INFO: vmware-system-license-operator-controller-manager-5f5b6cddq7xzp started at 2021-09-05 19:52:58 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.677: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:38.677: INFO: kube-proxy-9zz2h started at 2021-09-05 19:47:13 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.677: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  5 21:39:38.677: INFO: capi-kubeadm-control-plane-controller-manager-7c8d4b456f-g7vmk started at 2021-09-05 19:50:52 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:38.677: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:38.677: INFO: 	Container manager ready: true, restart count 2
Sep  5 21:39:38.677: INFO: capi-kubeadm-control-plane-webhook-5f4c87b8b9-wjs6z started at 2021-09-05 19:50:52 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:38.677: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:38.677: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:38.677: INFO: vmware-system-psp-operator-mgr-6cc4d85755-w7lf5 started at 2021-09-05 19:51:14 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.677: INFO: 	Container manager ready: true, restart count 8
Sep  5 21:39:38.677: INFO: vmware-system-tkg-webhook-64fd4868cb-8khb4 started at 2021-09-05 19:51:32 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:38.677: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:38.677: INFO: 	Container manager ready: true, restart count 0
Sep  5 21:39:38.677: INFO: image-controller-597bd95bc9-t5pqm started at 2021-09-05 19:47:14 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.677: INFO: 	Container image-controller ready: true, restart count 1
Sep  5 21:39:38.677: INFO: capi-controller-manager-d586f4c8-pjhgc started at 2021-09-05 19:50:51 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:38.677: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:38.677: INFO: 	Container manager ready: true, restart count 3
Sep  5 21:39:38.677: INFO: capw-webhook-58b86fb8b-nxkgl started at 2021-09-05 19:50:54 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:38.677: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:38.677: INFO: 	Container manager ready: true, restart count 1
Sep  5 21:39:38.677: INFO: masterproxy-tkgs-plugin-8x4lc started at 2021-09-05 19:53:10 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.677: INFO: 	Container nginx ready: true, restart count 0
Sep  5 21:39:38.677: INFO: vmware-system-nsop-controller-manager-7dd8c47dcc-22cs2 started at 2021-09-05 19:53:20 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.677: INFO: 	Container manager ready: true, restart count 2
Sep  5 21:39:38.677: INFO: etcd-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:44:40 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.677: INFO: 	Container etcd ready: true, restart count 0
Sep  5 21:39:38.677: INFO: docker-registry-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:58:35 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:38.677: INFO: 	Container docker-registry ready: true, restart count 0
Sep  5 21:39:38.677: INFO: vmware-system-vmop-controller-manager-6649dd65b-tzzst started at 2021-09-05 19:51:23 -0700 PDT (0+2 container statuses recorded)
Sep  5 21:39:38.677: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 21:39:38.677: INFO: 	Container manager ready: true, restart count 5
Sep  5 21:39:39.596: INFO: 
Latency metrics for node 4216b30cf7711a81a7935dfb22c7b9d2
Sep  5 21:39:39.596: INFO: 
Logging node info for node sc2-10-185-226-150.eng.vmware.com
Sep  5 21:39:39.611: INFO: Node Info: &Node{ObjectMeta:{sc2-10-185-226-150.eng.vmware.com   /api/v1/nodes/sc2-10-185-226-150.eng.vmware.com d6938cb1-1bbd-4690-b273-7b7e638d12bc 72113 0 2021-09-05 19:58:48 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:CRX kubernetes.io/arch:amd64 kubernetes.io/hostname:sc2-10-185-226-150.eng.vmware.com kubernetes.io/os:CRX node-role.kubernetes.io/agent:agent node.kubernetes.io/role:agent type:virtual-kubelet] map[ncp/transport_node:644f660e-c6b2-4fd5-a4b5-5cccdda12759 node.alpha.kubernetes.io/ttl:0 vmware-system-esxi-node-moid:host-16 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{wcpsvc Update v1 2021-09-05 19:58:49 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-esxi-node-moid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{"f:node-role.kubernetes.io/agent":{}}}}} {spherelet Update v1 2021-09-05 20:02:50 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:ncp/transport_node":{}},"f:labels":{".":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node.kubernetes.io/role":{},"f:type":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:nodeInfo":{"f:architecture":{},"f:kubeletVersion":{},"f:operatingSystem":{}}}}} {kube-controller-manager Update v1 2021-09-05 20:28:56 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}},"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:status":{"f:volumesAttached":{}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{12 0} {<nil>} 12 DecimalSI},memory: {{34354941952 0} {<nil>} 33549748Ki BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Allocatable:ResourceList{cpu: {{9 0} {<nil>} 9 DecimalSI},memory: {{4323278848 0} {<nil>} 4123Mi BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-05 21:39:34 -0700 PDT,LastTransitionTime:2021-09-05 21:39:34 -0700 PDT,Reason:KubeletReady,Message:Spherelet is ready.,},},Addresses:[]NodeAddress{NodeAddress{Type:Hostname,Address:sc2-10-185-226-150.eng.vmware.com,},NodeAddress{Type:InternalIP,Address:10.185.226.150,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:,SystemUUID:,BootID:,KernelVersion:,OSImage:,ContainerRuntimeVersion:,KubeletVersion:v1.20.2-sph-0afe203,KubeProxyVersion:,OperatingSystem:ESXi,Architecture:amd64,},Images:[]ContainerImage{},VolumesInUse:[],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/csi/csi.vsphere.vmware.com^47d4cc26-bb29-41cb-a660-79ed55715f11,DevicePath:,},},Config:nil,},}
Sep  5 21:39:39.611: INFO: 
Logging kubelet events for node sc2-10-185-226-150.eng.vmware.com
Sep  5 21:39:39.621: INFO: 
Logging pods the kubelet thinks is on node sc2-10-185-226-150.eng.vmware.com
Sep  5 21:39:39.656: INFO: wcp-sanity-busybox-6f999d6849-kspp7 started at 2021-09-05 20:17:25 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.656: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 21:39:39.656: INFO: schedext-test-node-selector-1 started at 2021-09-05 20:21:19 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.656: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:39:39.656: INFO: podwithpersistentvolume started at 2021-09-05 20:29:04 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.656: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:39:39.656: INFO: wcp-sanity-busybox-6f999d6849-6bn7v started at 2021-09-05 20:30:32 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.656: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 21:39:39.656: INFO: curl-pod started at 2021-09-05 20:38:23 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.656: INFO: 	Container curl-container ready: true, restart count 0
Sep  5 21:39:39.656: INFO: test-docker-registry started at 2021-09-05 20:41:08 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.656: INFO: 	Container test-docker-registry ready: true, restart count 0
Sep  5 21:39:39.656: INFO: schedext-test-affinity-1 started at 2021-09-05 20:22:12 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.656: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:39:39.656: INFO: schedext-test-affinity-2 started at 2021-09-05 20:22:42 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.656: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:39:39.656: INFO: helloworld started at 2021-09-05 20:32:42 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.656: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:39:39.706: INFO: 
Latency metrics for node sc2-10-185-226-150.eng.vmware.com
Sep  5 21:39:39.706: INFO: 
Logging node info for node sc2-10-185-226-233.eng.vmware.com
Sep  5 21:39:39.716: INFO: Node Info: &Node{ObjectMeta:{sc2-10-185-226-233.eng.vmware.com   /api/v1/nodes/sc2-10-185-226-233.eng.vmware.com 99eaa46a-24ac-48ad-852e-eabb653ed037 72139 0 2021-09-05 19:58:37 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:CRX kubernetes.io/arch:amd64 kubernetes.io/hostname:sc2-10-185-226-233.eng.vmware.com kubernetes.io/os:CRX node-role.kubernetes.io/agent:agent node.kubernetes.io/role:agent type:virtual-kubelet] map[ncp/transport_node:97c82e2a-be25-406d-ab4e-f897b55c19b3 node.alpha.kubernetes.io/ttl:0 vmware-system-esxi-node-moid:host-19 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{wcpsvc Update v1 2021-09-05 19:58:37 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-esxi-node-moid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{"f:node-role.kubernetes.io/agent":{}}}}} {spherelet Update v1 2021-09-05 20:02:48 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:ncp/transport_node":{}},"f:labels":{".":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node.kubernetes.io/role":{},"f:type":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:nodeInfo":{"f:architecture":{},"f:kubeletVersion":{},"f:operatingSystem":{}}}}} {kube-controller-manager Update v1 2021-09-05 20:27:52 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}},"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:status":{"f:volumesAttached":{}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{12 0} {<nil>} 12 DecimalSI},memory: {{34354941952 0} {<nil>} 33549748Ki BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Allocatable:ResourceList{cpu: {{10 0} {<nil>} 10 DecimalSI},memory: {{4772069376 0} {<nil>} 4551Mi BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-05 21:39:36 -0700 PDT,LastTransitionTime:2021-09-05 21:39:36 -0700 PDT,Reason:KubeletReady,Message:Spherelet is ready.,},},Addresses:[]NodeAddress{NodeAddress{Type:Hostname,Address:sc2-10-185-226-233.eng.vmware.com,},NodeAddress{Type:InternalIP,Address:10.185.226.233,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:,SystemUUID:,BootID:,KernelVersion:,OSImage:,ContainerRuntimeVersion:,KubeletVersion:v1.20.2-sph-0afe203,KubeProxyVersion:,OperatingSystem:ESXi,Architecture:amd64,},Images:[]ContainerImage{},VolumesInUse:[],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/csi/csi.vsphere.vmware.com^e1249417-acff-43cd-8d75-f06fb5e49590,DevicePath:,},},Config:nil,},}
Sep  5 21:39:39.716: INFO: 
Logging kubelet events for node sc2-10-185-226-233.eng.vmware.com
Sep  5 21:39:39.725: INFO: 
Logging pods the kubelet thinks is on node sc2-10-185-226-233.eng.vmware.com
Sep  5 21:39:39.762: INFO: wcp-sanity-busybox-6f999d6849-zzvcx started at 2021-09-05 20:17:23 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.762: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 21:39:39.762: INFO: schedext-test-node-selector-2 started at 2021-09-05 20:21:17 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.762: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:39:39.762: INFO: hello-web-1-6b97664bd5-h6sth started at 2021-09-05 20:37:46 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.762: INFO: 	Container hello-app ready: true, restart count 0
Sep  5 21:39:39.762: INFO: busybox started at 2021-09-05 20:40:00 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.762: INFO: 	Container busybox ready: true, restart count 0
Sep  5 21:39:39.762: INFO: helloworld started at 2021-09-05 20:31:32 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.762: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:39:39.762: INFO: hello-web-6b97664bd5-hsmdh started at 2021-09-05 20:34:39 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.762: INFO: 	Container hello-app ready: true, restart count 0
Sep  5 21:39:39.762: INFO: helloworld started at 2021-09-05 20:20:33 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.762: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:39:39.762: INFO: busybox-annotation started at 2021-09-05 20:25:00 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.762: INFO: 	Container busybox-annotation ready: true, restart count 0
Sep  5 21:39:39.762: INFO: podwithpersistentvolume started at 2021-09-05 20:27:58 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.762: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:39:39.762: INFO: wcp-sanity-busybox-6f999d6849-cdv9c started at 2021-09-05 20:30:17 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.762: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 21:39:39.813: INFO: 
Latency metrics for node sc2-10-185-226-233.eng.vmware.com
Sep  5 21:39:39.813: INFO: 
Logging node info for node sc2-10-185-233-127.eng.vmware.com
Sep  5 21:39:39.822: INFO: Node Info: &Node{ObjectMeta:{sc2-10-185-233-127.eng.vmware.com   /api/v1/nodes/sc2-10-185-233-127.eng.vmware.com 9215b2dc-bc25-43aa-b2d8-00a2b7b458d7 72143 0 2021-09-05 19:58:26 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:CRX kubernetes.io/arch:amd64 kubernetes.io/hostname:sc2-10-185-233-127.eng.vmware.com kubernetes.io/os:CRX node-role.kubernetes.io/agent:agent node.kubernetes.io/role:agent type:virtual-kubelet] map[ncp/transport_node:9e6990b2-2258-45d1-81b5-10ca792b1fac node.alpha.kubernetes.io/ttl:0 vmware-system-esxi-node-moid:host-12 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kube-controller-manager Update v1 2021-09-05 19:58:26 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}},"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}}}} {wcpsvc Update v1 2021-09-05 19:58:26 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-esxi-node-moid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{"f:node-role.kubernetes.io/agent":{}}}}} {spherelet Update v1 2021-09-05 20:02:46 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:ncp/transport_node":{}},"f:labels":{".":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node.kubernetes.io/role":{},"f:type":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:nodeInfo":{"f:architecture":{},"f:kubeletVersion":{},"f:operatingSystem":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{12 0} {<nil>} 12 DecimalSI},memory: {{34354941952 0} {<nil>} 33549748Ki BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Allocatable:ResourceList{cpu: {{10 0} {<nil>} 10 DecimalSI},memory: {{5140119552 0} {<nil>} 4902Mi BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-05 21:39:36 -0700 PDT,LastTransitionTime:2021-09-05 21:39:36 -0700 PDT,Reason:KubeletReady,Message:Spherelet is ready.,},},Addresses:[]NodeAddress{NodeAddress{Type:Hostname,Address:sc2-10-185-233-127.eng.vmware.com,},NodeAddress{Type:InternalIP,Address:10.185.233.127,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:,SystemUUID:,BootID:,KernelVersion:,OSImage:,ContainerRuntimeVersion:,KubeletVersion:v1.20.2-sph-0afe203,KubeProxyVersion:,OperatingSystem:ESXi,Architecture:amd64,},Images:[]ContainerImage{},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Sep  5 21:39:39.822: INFO: 
Logging kubelet events for node sc2-10-185-233-127.eng.vmware.com
Sep  5 21:39:39.831: INFO: 
Logging pods the kubelet thinks is on node sc2-10-185-233-127.eng.vmware.com
Sep  5 21:39:39.875: INFO: wcp-sanity-busybox-6f999d6849-564rw started at 2021-09-05 20:30:14 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.875: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 21:39:39.875: INFO: curl-pod started at 2021-09-05 20:33:45 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.875: INFO: 	Container curl-container ready: true, restart count 0
Sep  5 21:39:39.875: INFO: hello-web-2-f779cbdff-dmb55 started at 2021-09-05 20:37:45 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.875: INFO: 	Container hello-app ready: true, restart count 0
Sep  5 21:39:39.875: INFO: helloworld started at 2021-09-05 20:18:37 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.875: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:39:39.875: INFO: nginx-private started at 2021-09-05 20:19:39 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.875: INFO: 	Container nginx-private-container ready: true, restart count 0
Sep  5 21:39:39.875: INFO: schedext-test-affinity-3 started at 2021-09-05 20:22:44 -0700 PDT (0+1 container statuses recorded)
Sep  5 21:39:39.875: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:39:39.944: INFO: 
Latency metrics for node sc2-10-185-233-127.eng.vmware.com
Sep  5 21:39:39.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-132" for this suite.

• Failure [48.759 seconds]
[k8s.io] Variable Expansion
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance] [It]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629

  Sep  5 21:39:24.216: while waiting for the pod container to fail
  Unexpected error:
      <*errors.errorString | 0xc002404830>: {
          s: "pod was expected to be pending, but it is in the state: Failed",
      }
      pod was expected to be pending, but it is in the state: Failed
  occurred

  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:376
------------------------------
{"msg":"FAILED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":296,"completed":18,"skipped":373,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:39:40.404: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9942
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep  5 21:39:40.988: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996" in namespace "projected-9942" to be "Succeeded or Failed"
Sep  5 21:39:41.000: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Pending", Reason="", readiness=false. Elapsed: 12.016749ms
Sep  5 21:39:43.011: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02296058s
Sep  5 21:39:45.026: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037766349s
Sep  5 21:39:47.045: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056746305s
Sep  5 21:39:49.108: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Pending", Reason="", readiness=false. Elapsed: 8.119891392s
Sep  5 21:39:51.121: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Pending", Reason="", readiness=false. Elapsed: 10.132966997s
Sep  5 21:39:53.147: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Pending", Reason="", readiness=false. Elapsed: 12.158810665s
Sep  5 21:39:55.169: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Pending", Reason="", readiness=false. Elapsed: 14.180802951s
Sep  5 21:39:57.182: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Pending", Reason="", readiness=false. Elapsed: 16.19361054s
Sep  5 21:39:59.196: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Pending", Reason="", readiness=false. Elapsed: 18.207732959s
Sep  5 21:40:01.218: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Pending", Reason="", readiness=false. Elapsed: 20.229185092s
Sep  5 21:40:03.232: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Pending", Reason="", readiness=false. Elapsed: 22.243637069s
Sep  5 21:40:05.252: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Pending", Reason="", readiness=false. Elapsed: 24.263305424s
Sep  5 21:40:07.381: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Pending", Reason="", readiness=false. Elapsed: 26.392449206s
Sep  5 21:40:09.432: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Pending", Reason="", readiness=false. Elapsed: 28.443983614s
Sep  5 21:40:11.449: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Pending", Reason="", readiness=false. Elapsed: 30.460421523s
Sep  5 21:40:13.511: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Pending", Reason="", readiness=false. Elapsed: 32.522620063s
Sep  5 21:40:15.552: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Pending", Reason="", readiness=false. Elapsed: 34.564079074s
Sep  5 21:40:17.606: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Running", Reason="", readiness=true. Elapsed: 36.617644309s
Sep  5 21:40:19.625: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Running", Reason="", readiness=true. Elapsed: 38.6365727s
Sep  5 21:40:21.656: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Running", Reason="", readiness=true. Elapsed: 40.667272383s
Sep  5 21:40:23.673: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996": Phase="Succeeded", Reason="", readiness=false. Elapsed: 42.685115226s
STEP: Saw pod success
Sep  5 21:40:23.674: INFO: Pod "downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996" satisfied condition "Succeeded or Failed"
Sep  5 21:40:23.683: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996 container client-container: <nil>
STEP: delete the pod
Sep  5 21:40:29.889: INFO: Waiting for pod downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996 to disappear
Sep  5 21:40:29.900: INFO: Pod downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996 still exists
Sep  5 21:40:31.901: INFO: Waiting for pod downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996 to disappear
Sep  5 21:40:31.914: INFO: Pod downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996 still exists
Sep  5 21:40:33.901: INFO: Waiting for pod downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996 to disappear
Sep  5 21:40:33.915: INFO: Pod downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996 still exists
Sep  5 21:40:35.902: INFO: Waiting for pod downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996 to disappear
Sep  5 21:40:35.908: INFO: Pod downwardapi-volume-e5a27297-bd7b-48b4-bb7b-91c4282ab996 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:40:35.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9942" for this suite.

• [SLOW TEST:55.793 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":296,"completed":19,"skipped":378,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:40:36.197: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6286
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:40:36.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6286" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":296,"completed":20,"skipped":391,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:40:37.133: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3282
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep  5 21:40:37.610: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303" in namespace "downward-api-3282" to be "Succeeded or Failed"
Sep  5 21:40:37.653: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 42.949658ms
Sep  5 21:40:39.663: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052756475s
Sep  5 21:40:41.684: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073979809s
Sep  5 21:40:43.708: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 6.097794605s
Sep  5 21:40:45.717: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 8.107277605s
Sep  5 21:40:47.728: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 10.117607236s
Sep  5 21:40:49.746: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 12.135949891s
Sep  5 21:40:51.758: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 14.147792453s
Sep  5 21:40:53.783: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 16.172483448s
Sep  5 21:40:55.807: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 18.196483491s
Sep  5 21:40:57.820: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 20.21031203s
Sep  5 21:40:59.837: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 22.227137343s
Sep  5 21:41:01.853: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 24.24273677s
Sep  5 21:41:03.880: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 26.269543102s
Sep  5 21:41:05.910: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 28.299638835s
Sep  5 21:41:07.937: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 30.326887245s
Sep  5 21:41:09.950: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 32.340092093s
Sep  5 21:41:11.964: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 34.353566025s
Sep  5 21:41:13.984: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 36.373390052s
Sep  5 21:41:16.055: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 38.444944659s
Sep  5 21:41:18.075: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Pending", Reason="", readiness=false. Elapsed: 40.46531502s
Sep  5 21:41:20.092: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Running", Reason="", readiness=true. Elapsed: 42.481474478s
Sep  5 21:41:22.106: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Running", Reason="", readiness=true. Elapsed: 44.496163829s
Sep  5 21:41:24.119: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303": Phase="Succeeded", Reason="", readiness=false. Elapsed: 46.509290654s
STEP: Saw pod success
Sep  5 21:41:24.119: INFO: Pod "downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303" satisfied condition "Succeeded or Failed"
Sep  5 21:41:24.126: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303 container client-container: <nil>
STEP: delete the pod
Sep  5 21:41:30.667: INFO: Waiting for pod downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303 to disappear
Sep  5 21:41:30.686: INFO: Pod downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303 still exists
Sep  5 21:41:32.686: INFO: Waiting for pod downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303 to disappear
Sep  5 21:41:32.703: INFO: Pod downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303 still exists
Sep  5 21:41:34.686: INFO: Waiting for pod downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303 to disappear
Sep  5 21:41:34.700: INFO: Pod downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303 still exists
Sep  5 21:41:36.686: INFO: Waiting for pod downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303 to disappear
Sep  5 21:41:36.709: INFO: Pod downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303 still exists
Sep  5 21:41:38.686: INFO: Waiting for pod downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303 to disappear
Sep  5 21:41:38.700: INFO: Pod downwardapi-volume-72682c98-dd9a-4313-93c5-705dace2a303 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:41:38.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3282" for this suite.

• [SLOW TEST:62.207 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's memory limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":296,"completed":21,"skipped":399,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:41:39.341: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1679
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should delete a collection of pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pods
Sep  5 21:41:40.036: INFO: created test-pod-1
Sep  5 21:41:40.096: INFO: created test-pod-2
Sep  5 21:41:40.154: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:41:40.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1679" for this suite.
•{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":296,"completed":22,"skipped":413,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:41:40.602: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7624
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 21:41:41.092: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep  5 21:41:52.556: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7624 --namespace=crd-publish-openapi-7624 create -f -'
Sep  5 21:41:53.771: INFO: stderr: ""
Sep  5 21:41:53.771: INFO: stdout: "e2e-test-crd-publish-openapi-0-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  5 21:41:53.771: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7624 --namespace=crd-publish-openapi-7624 delete e2e-test-crd-publish-openapi-0-crds test-cr'
Sep  5 21:41:53.926: INFO: stderr: ""
Sep  5 21:41:53.926: INFO: stdout: "e2e-test-crd-publish-openapi-0-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep  5 21:41:53.927: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7624 --namespace=crd-publish-openapi-7624 apply -f -'
Sep  5 21:41:54.342: INFO: stderr: ""
Sep  5 21:41:54.342: INFO: stdout: "e2e-test-crd-publish-openapi-0-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  5 21:41:54.342: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7624 --namespace=crd-publish-openapi-7624 delete e2e-test-crd-publish-openapi-0-crds test-cr'
Sep  5 21:41:54.479: INFO: stderr: ""
Sep  5 21:41:54.479: INFO: stdout: "e2e-test-crd-publish-openapi-0-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Sep  5 21:41:54.479: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7624 explain e2e-test-crd-publish-openapi-0-crds'
Sep  5 21:41:54.885: INFO: stderr: ""
Sep  5 21:41:54.885: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-0-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:42:04.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7624" for this suite.

• [SLOW TEST:23.925 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":296,"completed":23,"skipped":522,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Events
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:42:04.528: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7577
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep  5 21:42:39.446: INFO: &Pod{ObjectMeta:{send-events-2c74235a-3bcb-485e-89dc-3e27968b2588  events-7577 /api/v1/namespaces/events-7577/pods/send-events-2c74235a-3bcb-485e-89dc-3e27968b2588 dedae6c8-5d9d-48e2-98e8-46ff87c5aa21 74200 0 2021-09-05 21:42:04 -0700 PDT <nil> <nil> map[name:foo time:271006252] map[attachment_id:2efa3e83-ac3c-4a6f-9fa6-5887f22a7be9 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:00:1c vlan:None vmware-system-ephemeral-disk-uuid:6000C295-d102-ed38-62ce-9786f9ece314 vmware-system-image-references:{"p":"agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v2681"} vmware-system-vm-moid:vm-369:3f0e15ce-7291-45c7-8a72-39d27d06e017 vmware-system-vm-uuid:5016cac7-3010-ef5f-4beb-cab2f822af1e] [] [lifecycle-controller/system.vmware.com]  [{e2e.test Update v1 2021-09-05 21:42:04 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-05 21:42:05 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {nsx-ncp-6d7f7bf559-df5sk Update v1 2021-09-05 21:42:14 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-05 21:42:28 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-05 21:42:38 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2hfj9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2hfj9,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2hfj9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-150.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 21:42:05 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 21:42:39 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 21:42:39 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 21:42:39 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.185.226.150,PodIP:172.26.1.194,StartTime:2021-09-05 21:42:35 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-05 21:42:36 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v2681,ContainerID:1e15ff3a-c6b5-464c-bd00-e58806b382fd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Sep  5 21:42:41.483: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep  5 21:42:43.555: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:42:43.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7577" for this suite.

• [SLOW TEST:39.819 seconds]
[k8s.io] [sig-node] Events
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":296,"completed":24,"skipped":524,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:42:44.347: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2157
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override arguments
Sep  5 21:42:45.585: INFO: Waiting up to 5m0s for pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44" in namespace "containers-2157" to be "Succeeded or Failed"
Sep  5 21:42:45.606: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 21.147231ms
Sep  5 21:42:47.618: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033024924s
Sep  5 21:42:49.649: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064524999s
Sep  5 21:42:51.674: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 6.088737458s
Sep  5 21:42:53.684: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 8.099326205s
Sep  5 21:42:55.695: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 10.110289963s
Sep  5 21:42:57.720: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 12.135106201s
Sep  5 21:42:59.738: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 14.153242954s
Sep  5 21:43:01.749: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 16.164145097s
Sep  5 21:43:03.765: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 18.180041658s
Sep  5 21:43:05.776: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 20.190788494s
Sep  5 21:43:07.792: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 22.207266226s
Sep  5 21:43:09.811: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 24.226232234s
Sep  5 21:43:11.827: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 26.242455113s
Sep  5 21:43:13.840: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 28.254737854s
Sep  5 21:43:15.869: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 30.284545014s
Sep  5 21:43:17.880: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 32.295685045s
Sep  5 21:43:19.892: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 34.307696734s
Sep  5 21:43:21.904: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 36.31953396s
Sep  5 21:43:23.916: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Pending", Reason="", readiness=false. Elapsed: 38.330945831s
Sep  5 21:43:25.933: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 40.347885543s
STEP: Saw pod success
Sep  5 21:43:25.933: INFO: Pod "client-containers-b139e89e-d696-4d14-aa25-df56e8981b44" satisfied condition "Succeeded or Failed"
Sep  5 21:43:25.940: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod client-containers-b139e89e-d696-4d14-aa25-df56e8981b44 container agnhost-container: <nil>
STEP: delete the pod
Sep  5 21:43:32.451: INFO: Waiting for pod client-containers-b139e89e-d696-4d14-aa25-df56e8981b44 to disappear
Sep  5 21:43:32.460: INFO: Pod client-containers-b139e89e-d696-4d14-aa25-df56e8981b44 still exists
Sep  5 21:43:34.460: INFO: Waiting for pod client-containers-b139e89e-d696-4d14-aa25-df56e8981b44 to disappear
Sep  5 21:43:34.483: INFO: Pod client-containers-b139e89e-d696-4d14-aa25-df56e8981b44 still exists
Sep  5 21:43:36.460: INFO: Waiting for pod client-containers-b139e89e-d696-4d14-aa25-df56e8981b44 to disappear
Sep  5 21:43:36.481: INFO: Pod client-containers-b139e89e-d696-4d14-aa25-df56e8981b44 still exists
Sep  5 21:43:38.462: INFO: Waiting for pod client-containers-b139e89e-d696-4d14-aa25-df56e8981b44 to disappear
Sep  5 21:43:38.474: INFO: Pod client-containers-b139e89e-d696-4d14-aa25-df56e8981b44 still exists
Sep  5 21:43:40.460: INFO: Waiting for pod client-containers-b139e89e-d696-4d14-aa25-df56e8981b44 to disappear
Sep  5 21:43:40.473: INFO: Pod client-containers-b139e89e-d696-4d14-aa25-df56e8981b44 still exists
Sep  5 21:43:42.461: INFO: Waiting for pod client-containers-b139e89e-d696-4d14-aa25-df56e8981b44 to disappear
Sep  5 21:43:42.479: INFO: Pod client-containers-b139e89e-d696-4d14-aa25-df56e8981b44 still exists
Sep  5 21:43:44.460: INFO: Waiting for pod client-containers-b139e89e-d696-4d14-aa25-df56e8981b44 to disappear
Sep  5 21:43:44.478: INFO: Pod client-containers-b139e89e-d696-4d14-aa25-df56e8981b44 still exists
Sep  5 21:43:46.460: INFO: Waiting for pod client-containers-b139e89e-d696-4d14-aa25-df56e8981b44 to disappear
Sep  5 21:43:46.471: INFO: Pod client-containers-b139e89e-d696-4d14-aa25-df56e8981b44 still exists
Sep  5 21:43:48.460: INFO: Waiting for pod client-containers-b139e89e-d696-4d14-aa25-df56e8981b44 to disappear
Sep  5 21:43:48.472: INFO: Pod client-containers-b139e89e-d696-4d14-aa25-df56e8981b44 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:43:48.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2157" for this suite.

• [SLOW TEST:64.665 seconds]
[k8s.io] Docker Containers
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":296,"completed":25,"skipped":527,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:43:49.012: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1899
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Sep  5 21:43:49.890: INFO: Waiting up to 5m0s for pod "downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb" in namespace "downward-api-1899" to be "Succeeded or Failed"
Sep  5 21:43:49.908: INFO: Pod "downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.011528ms
Sep  5 21:43:51.917: INFO: Pod "downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026631852s
Sep  5 21:43:53.937: INFO: Pod "downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046830328s
Sep  5 21:43:55.957: INFO: Pod "downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.066847184s
Sep  5 21:43:57.981: INFO: Pod "downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.090898688s
Sep  5 21:43:59.989: INFO: Pod "downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.098640202s
Sep  5 21:44:02.005: INFO: Pod "downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.115392825s
Sep  5 21:44:04.043: INFO: Pod "downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.153052668s
Sep  5 21:44:06.055: INFO: Pod "downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.164685063s
Sep  5 21:44:08.066: INFO: Pod "downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.176169234s
Sep  5 21:44:10.080: INFO: Pod "downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 20.189659375s
Sep  5 21:44:12.106: INFO: Pod "downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 22.216112831s
Sep  5 21:44:14.121: INFO: Pod "downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 24.23098983s
Sep  5 21:44:16.142: INFO: Pod "downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 26.251715674s
Sep  5 21:44:18.154: INFO: Pod "downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb": Phase="Pending", Reason="", readiness=false. Elapsed: 28.264157024s
Sep  5 21:44:20.168: INFO: Pod "downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.277884246s
STEP: Saw pod success
Sep  5 21:44:20.168: INFO: Pod "downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb" satisfied condition "Succeeded or Failed"
Sep  5 21:44:20.173: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb container dapi-container: <nil>
STEP: delete the pod
Sep  5 21:44:26.100: INFO: Waiting for pod downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb to disappear
Sep  5 21:44:26.123: INFO: Pod downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb still exists
Sep  5 21:44:28.124: INFO: Waiting for pod downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb to disappear
Sep  5 21:44:28.137: INFO: Pod downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb still exists
Sep  5 21:44:30.124: INFO: Waiting for pod downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb to disappear
Sep  5 21:44:30.137: INFO: Pod downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb still exists
Sep  5 21:44:32.124: INFO: Waiting for pod downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb to disappear
Sep  5 21:44:32.136: INFO: Pod downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb still exists
Sep  5 21:44:34.124: INFO: Waiting for pod downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb to disappear
Sep  5 21:44:34.147: INFO: Pod downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb still exists
Sep  5 21:44:36.124: INFO: Waiting for pod downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb to disappear
Sep  5 21:44:36.156: INFO: Pod downward-api-17cb0f6e-3297-4e56-afe1-3452d6558dfb no longer exists
[AfterEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:44:36.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1899" for this suite.

• [SLOW TEST:47.773 seconds]
[sig-node] Downward API
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":296,"completed":26,"skipped":530,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:44:36.785: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7885
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:44:43.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7885" for this suite.

• [SLOW TEST:6.662 seconds]
[sig-api-machinery] Watchers
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":296,"completed":27,"skipped":532,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:44:43.448: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6369
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 21:44:43.964: INFO: Creating deployment "test-recreate-deployment"
Sep  5 21:44:44.009: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep  5 21:44:44.076: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep  5 21:44:46.104: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep  5 21:44:46.126: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:44:48.140: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:44:50.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:44:52.140: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:44:54.135: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:44:56.139: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:44:58.140: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:45:00.153: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:45:02.155: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:45:04.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:45:06.154: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:45:08.139: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:45:10.166: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:45:12.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:45:14.190: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:45:16.146: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:45:18.139: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:45:20.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:45:22.146: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:45:24.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 44, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-786dd7c454\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:45:26.145: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep  5 21:45:26.175: INFO: Updating deployment test-recreate-deployment
Sep  5 21:45:26.175: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Sep  5 21:45:45.764: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6369 /apis/apps/v1/namespaces/deployment-6369/deployments/test-recreate-deployment f7bf5742-4bd6-45ab-a856-7d3489898164 76377 2 2021-09-05 21:44:43 -0700 PDT <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-09-05 21:45:26 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-05 21:45:45 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00301bfc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-09-05 21:45:26 -0700 PDT,LastTransitionTime:2021-09-05 21:45:26 -0700 PDT,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f756bcb56" is progressing.,LastUpdateTime:2021-09-05 21:45:45 -0700 PDT,LastTransitionTime:2021-09-05 21:44:44 -0700 PDT,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep  5 21:45:45.770: INFO: New ReplicaSet "test-recreate-deployment-f756bcb56" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f756bcb56  deployment-6369 /apis/apps/v1/namespaces/deployment-6369/replicasets/test-recreate-deployment-f756bcb56 60b7c1a9-ba4c-439b-9945-942b0294d88c 76376 1 2021-09-05 21:45:45 -0700 PDT <nil> <nil> map[name:sample-pod-3 pod-template-hash:f756bcb56] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment f7bf5742-4bd6-45ab-a856-7d3489898164 0xc002f0e420 0xc002f0e421}] []  [{kube-controller-manager Update apps/v1 2021-09-05 21:45:45 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7bf5742-4bd6-45ab-a856-7d3489898164\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f756bcb56,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f756bcb56] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f0e498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  5 21:45:45.770: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep  5 21:45:45.770: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-786dd7c454  deployment-6369 /apis/apps/v1/namespaces/deployment-6369/replicasets/test-recreate-deployment-786dd7c454 78478f6b-9bd3-4432-bead-006ade8bbb3f 76192 2 2021-09-05 21:44:44 -0700 PDT <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment f7bf5742-4bd6-45ab-a856-7d3489898164 0xc002f0e327 0xc002f0e328}] []  [{kube-controller-manager Update apps/v1 2021-09-05 21:45:26 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f7bf5742-4bd6-45ab-a856-7d3489898164\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 786dd7c454,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:786dd7c454] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f0e3b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  5 21:45:45.777: INFO: Pod "test-recreate-deployment-f756bcb56-q5d66" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f756bcb56-q5d66 test-recreate-deployment-f756bcb56- deployment-6369 /api/v1/namespaces/deployment-6369/pods/test-recreate-deployment-f756bcb56-q5d66 c5b115c8-c760-4079-93e3-1236c36a8e8b 76371 0 2021-09-05 21:45:45 -0700 PDT <nil> <nil> map[name:sample-pod-3 pod-template-hash:f756bcb56] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-f756bcb56 60b7c1a9-ba4c-439b-9945-942b0294d88c 0xc002f0e8c0 0xc002f0e8c1}] []  [{kube-controller-manager Update v1 2021-09-05 21:45:45 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60b7c1a9-ba4c-439b-9945-942b0294d88c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-c9pz7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-c9pz7,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-c9pz7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:45:45.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6369" for this suite.

• [SLOW TEST:62.687 seconds]
[sig-apps] Deployment
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":296,"completed":28,"skipped":565,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:45:46.135: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-2140
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 21:45:46.764: INFO: >>> kubeConfig: ./kconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:45:47.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2140" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":296,"completed":29,"skipped":583,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:45:47.915: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6546
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create and stop a working application  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating all guestbook components
Sep  5 21:45:48.364: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Sep  5 21:45:48.364: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-6546 create -f -'
Sep  5 21:45:49.095: INFO: stderr: ""
Sep  5 21:45:49.095: INFO: stdout: "service/agnhost-replica created\n"
Sep  5 21:45:49.095: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Sep  5 21:45:49.095: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-6546 create -f -'
Sep  5 21:45:49.630: INFO: stderr: ""
Sep  5 21:45:49.630: INFO: stdout: "service/agnhost-primary created\n"
Sep  5 21:45:49.630: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep  5 21:45:49.630: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-6546 create -f -'
Sep  5 21:45:50.468: INFO: stderr: ""
Sep  5 21:45:50.468: INFO: stdout: "service/frontend created\n"
Sep  5 21:45:50.468: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep  5 21:45:50.468: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-6546 create -f -'
Sep  5 21:45:50.943: INFO: stderr: ""
Sep  5 21:45:50.943: INFO: stdout: "deployment.apps/frontend created\n"
Sep  5 21:45:50.943: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  5 21:45:50.943: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-6546 create -f -'
Sep  5 21:45:51.547: INFO: stderr: ""
Sep  5 21:45:51.547: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Sep  5 21:45:51.548: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.21
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  5 21:45:51.548: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-6546 create -f -'
Sep  5 21:45:52.101: INFO: stderr: ""
Sep  5 21:45:52.101: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Sep  5 21:45:52.101: INFO: Waiting for all frontend pods to be Running.
Sep  5 21:46:47.158: INFO: Waiting for frontend to serve content.
Sep  5 21:46:47.357: INFO: Trying to add a new entry to the guestbook.
Sep  5 21:46:50.569: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Sep  5 21:46:50.726: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-6546 delete --grace-period=0 --force -f -'
Sep  5 21:46:50.999: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 21:46:50.999: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 21:46:50.999: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-6546 delete --grace-period=0 --force -f -'
Sep  5 21:46:51.242: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 21:46:51.242: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 21:46:51.243: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-6546 delete --grace-period=0 --force -f -'
Sep  5 21:46:51.453: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 21:46:51.453: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 21:46:51.453: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-6546 delete --grace-period=0 --force -f -'
Sep  5 21:46:51.606: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 21:46:51.606: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 21:46:51.606: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-6546 delete --grace-period=0 --force -f -'
Sep  5 21:46:51.738: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 21:46:51.738: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 21:46:51.738: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-6546 delete --grace-period=0 --force -f -'
Sep  5 21:46:51.851: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 21:46:51.851: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:46:51.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6546" for this suite.

• [SLOW TEST:64.486 seconds]
[sig-cli] Kubectl client
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:342
    should create and stop a working application  [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":296,"completed":30,"skipped":585,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:46:52.402: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2163
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl replace
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1554
[It] should update a single-container pod's image  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image mirror.gcr.io/library/httpd:2.4.38-alpine
Sep  5 21:46:53.047: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-2163 run e2e-test-httpd-pod --image=mirror.gcr.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Sep  5 21:46:53.245: INFO: stderr: ""
Sep  5 21:46:53.245: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Sep  5 21:47:48.304: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-2163 get pod e2e-test-httpd-pod -o json'
Sep  5 21:47:48.424: INFO: stderr: ""
Sep  5 21:47:48.424: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"attachment_id\": \"70b0c726-7647-48f1-9485-77304f8f9936\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\",\n            \"mac\": \"04:50:56:00:00:11\",\n            \"vlan\": \"None\",\n            \"vmware-system-ephemeral-disk-uuid\": \"6000C29c-61a8-e82b-ac16-c426205e3298\",\n            \"vmware-system-image-references\": \"{\\\"e2e-test-httpd-pod\\\":\\\"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v36713\\\"}\",\n            \"vmware-system-vm-moid\": \"vm-401:3f0e15ce-7291-45c7-8a72-39d27d06e017\",\n            \"vmware-system-vm-uuid\": \"50160d3c-8e51-ddd3-41cf-6fe84bdedd9b\"\n        },\n        \"creationTimestamp\": \"2021-09-06T04:46:53Z\",\n        \"finalizers\": [\n            \"lifecycle-controller/system.vmware.com\"\n        ],\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2163\",\n        \"resourceVersion\": \"78048\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2163/pods/e2e-test-httpd-pod\",\n        \"uid\": \"dc6d1353-b046-4ded-b61b-7a3f54ef813d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"mirror.gcr.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-m9fnf\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"sc2-10-185-226-233.eng.vmware.com\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-m9fnf\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-m9fnf\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-06T04:46:53Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-06T04:47:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-06T04:47:48Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-06T04:47:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"009703cb-8029-418d-b973-9f3ededd52c5\",\n                \"image\": \"mirror.gcr.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v36713\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-09-06T04:47:46Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.185.226.233\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.26.1.194\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.26.1.194\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-09-06T04:47:44Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep  5 21:47:48.424: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-2163 replace -f -'
Sep  5 21:47:49.260: INFO: stderr: ""
Sep  5 21:47:49.260: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image mirror.gcr.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
Sep  5 21:47:49.278: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-2163 delete pods e2e-test-httpd-pod'
Sep  5 21:48:06.809: INFO: stderr: ""
Sep  5 21:48:06.809: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:48:06.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2163" for this suite.

• [SLOW TEST:75.146 seconds]
[sig-cli] Kubectl client
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1551
    should update a single-container pod's image  [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":296,"completed":31,"skipped":592,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:48:07.549: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3973
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:48:44.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3973" for this suite.

• [SLOW TEST:37.107 seconds]
[k8s.io] Kubelet
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when scheduling a busybox command in a pod
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:41
    should print the output to logs [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":296,"completed":32,"skipped":600,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:48:44.656: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6697
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should delete old replica sets [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 21:48:45.260: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep  5 21:48:50.272: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  5 21:49:14.302: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Sep  5 21:49:14.363: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6697 /apis/apps/v1/namespaces/deployment-6697/deployments/test-cleanup-deployment afa3bbfd-7652-4639-880b-feb77528e88a 79034 1 2021-09-05 21:49:14 -0700 PDT <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-09-05 21:49:14 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00453f208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Sep  5 21:49:14.379: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Sep  5 21:49:14.379: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep  5 21:49:14.380: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-6697 /apis/apps/v1/namespaces/deployment-6697/replicasets/test-cleanup-controller c13b98be-b4bd-44fd-b484-457f6f980848 79036 1 2021-09-05 21:48:45 -0700 PDT <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment afa3bbfd-7652-4639-880b-feb77528e88a 0xc00453f4e7 0xc00453f4e8}] []  [{e2e.test Update apps/v1 2021-09-05 21:48:45 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-05 21:49:14 -0700 PDT FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"afa3bbfd-7652-4639-880b-feb77528e88a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00453f588 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  5 21:49:14.403: INFO: Pod "test-cleanup-controller-cmslt" is available:
&Pod{ObjectMeta:{test-cleanup-controller-cmslt test-cleanup-controller- deployment-6697 /api/v1/namespaces/deployment-6697/pods/test-cleanup-controller-cmslt d8244165-3cff-4a40-9c49-e49e16ac8729 79023 0 2021-09-05 21:48:45 -0700 PDT <nil> <nil> map[name:cleanup-pod pod:httpd] map[attachment_id:7e00f7c2-3919-4c1b-a0d8-8ca890e53c12 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:00:22 vlan:None vmware-system-ephemeral-disk-uuid:6000C294-cc21-262e-fdd4-1ad506eb510c vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v58502"} vmware-system-vm-moid:vm-409:3f0e15ce-7291-45c7-8a72-39d27d06e017 vmware-system-vm-uuid:5016e8ac-263a-f496-a70f-8f56074e9a38] [{apps/v1 ReplicaSet test-cleanup-controller c13b98be-b4bd-44fd-b484-457f6f980848 0xc000a03617 0xc000a03618}] [lifecycle-controller/system.vmware.com]  [{image-controller Update v1 2021-09-05 21:48:45 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {kube-controller-manager Update v1 2021-09-05 21:48:45 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c13b98be-b4bd-44fd-b484-457f6f980848\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-df5sk Update v1 2021-09-05 21:48:54 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-05 21:49:04 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-05 21:49:13 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-nstc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-nstc6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-nstc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-150.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 21:48:45 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 21:49:14 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 21:49:14 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 21:49:14 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.185.226.150,PodIP:172.26.1.194,StartTime:2021-09-05 21:49:11 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-05 21:49:13 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v58502,ContainerID:53010d39-4654-429d-8df4-0516fed2cdfc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:49:14.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6697" for this suite.

• [SLOW TEST:30.333 seconds]
[sig-apps] Deployment
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":296,"completed":33,"skipped":601,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:49:14.989: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3741
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
Sep  5 21:50:20.780: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:50:20.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3741" for this suite.

• [SLOW TEST:66.462 seconds]
[sig-api-machinery] Garbage collector
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":296,"completed":34,"skipped":613,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:50:21.451: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7118
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-e8154449-6ac8-45b7-89f4-9804349c8ff5
STEP: Creating configMap with name cm-test-opt-upd-3f534f71-11c1-418e-ab7e-91a16f4306b9
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e8154449-6ac8-45b7-89f4-9804349c8ff5
STEP: Updating configmap cm-test-opt-upd-3f534f71-11c1-418e-ab7e-91a16f4306b9
STEP: Creating configMap with name cm-test-opt-create-55aa8f07-24cc-4d1c-a53f-fa405c487994
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:51:35.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7118" for this suite.

• [SLOW TEST:74.720 seconds]
[sig-storage] Projected configMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":296,"completed":35,"skipped":632,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:51:36.172: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-1806
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:51:36.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-1806" for this suite.
•{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":296,"completed":36,"skipped":662,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:51:37.129: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2101
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 21:51:39.288: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  5 21:51:41.341: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:51:43.370: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:51:45.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:51:47.356: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:51:49.358: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:51:51.358: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:51:53.353: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:51:55.358: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:51:57.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:51:59.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:01.379: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:03.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:05.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:07.366: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:09.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:11.416: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:13.362: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:15.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:17.419: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:19.358: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 51, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 21:52:22.435: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Sep  5 21:52:54.735: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=webhook-2101 attach --namespace=webhook-2101 to-be-attached-pod -i -c=container1'
Sep  5 21:52:55.577: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:52:55.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2101" for this suite.
STEP: Destroying namespace "webhook-2101-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:79.586 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":296,"completed":37,"skipped":663,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:52:56.715: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5023
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep  5 21:52:57.272: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba" in namespace "downward-api-5023" to be "Succeeded or Failed"
Sep  5 21:52:57.286: INFO: Pod "downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba": Phase="Pending", Reason="", readiness=false. Elapsed: 13.69372ms
Sep  5 21:52:59.294: INFO: Pod "downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021872698s
Sep  5 21:53:01.382: INFO: Pod "downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba": Phase="Pending", Reason="", readiness=false. Elapsed: 4.10947179s
Sep  5 21:53:03.402: INFO: Pod "downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba": Phase="Pending", Reason="", readiness=false. Elapsed: 6.129451148s
Sep  5 21:53:05.422: INFO: Pod "downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba": Phase="Pending", Reason="", readiness=false. Elapsed: 8.149805475s
Sep  5 21:53:07.444: INFO: Pod "downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba": Phase="Pending", Reason="", readiness=false. Elapsed: 10.171708344s
Sep  5 21:53:09.523: INFO: Pod "downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba": Phase="Pending", Reason="", readiness=false. Elapsed: 12.250349932s
Sep  5 21:53:11.583: INFO: Pod "downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba": Phase="Pending", Reason="", readiness=false. Elapsed: 14.310443543s
Sep  5 21:53:13.601: INFO: Pod "downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba": Phase="Pending", Reason="", readiness=false. Elapsed: 16.328227624s
Sep  5 21:53:15.619: INFO: Pod "downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba": Phase="Pending", Reason="", readiness=false. Elapsed: 18.346421823s
Sep  5 21:53:17.658: INFO: Pod "downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba": Phase="Pending", Reason="", readiness=false. Elapsed: 20.386099526s
Sep  5 21:53:19.678: INFO: Pod "downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba": Phase="Pending", Reason="", readiness=false. Elapsed: 22.406028888s
Sep  5 21:53:21.692: INFO: Pod "downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba": Phase="Pending", Reason="", readiness=false. Elapsed: 24.419547473s
Sep  5 21:53:23.709: INFO: Pod "downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba": Phase="Pending", Reason="", readiness=false. Elapsed: 26.436279438s
Sep  5 21:53:25.734: INFO: Pod "downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba": Phase="Pending", Reason="", readiness=false. Elapsed: 28.461423596s
Sep  5 21:53:27.743: INFO: Pod "downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba": Phase="Pending", Reason="", readiness=false. Elapsed: 30.470229898s
Sep  5 21:53:29.765: INFO: Pod "downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.493195602s
STEP: Saw pod success
Sep  5 21:53:29.766: INFO: Pod "downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba" satisfied condition "Succeeded or Failed"
Sep  5 21:53:29.789: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba container client-container: <nil>
STEP: delete the pod
Sep  5 21:53:29.897: INFO: Waiting for pod downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba to disappear
Sep  5 21:53:29.920: INFO: Pod downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba still exists
Sep  5 21:53:31.921: INFO: Waiting for pod downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba to disappear
Sep  5 21:53:31.935: INFO: Pod downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba still exists
Sep  5 21:53:33.921: INFO: Waiting for pod downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba to disappear
Sep  5 21:53:33.934: INFO: Pod downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba still exists
Sep  5 21:53:35.921: INFO: Waiting for pod downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba to disappear
Sep  5 21:53:35.931: INFO: Pod downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba still exists
Sep  5 21:53:37.920: INFO: Waiting for pod downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba to disappear
Sep  5 21:53:37.932: INFO: Pod downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba still exists
Sep  5 21:53:39.920: INFO: Waiting for pod downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba to disappear
Sep  5 21:53:39.933: INFO: Pod downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba still exists
Sep  5 21:53:41.921: INFO: Waiting for pod downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba to disappear
Sep  5 21:53:41.934: INFO: Pod downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba still exists
Sep  5 21:53:43.921: INFO: Waiting for pod downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba to disappear
Sep  5 21:53:43.931: INFO: Pod downwardapi-volume-72448278-2172-4877-b2ac-a6194f9a21ba no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:53:43.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5023" for this suite.

• [SLOW TEST:47.505 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide podname only [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":296,"completed":38,"skipped":669,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:53:44.221: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9519
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap that has name configmap-test-emptyKey-8d62fae1-dc6d-46e2-a62b-6f6ee658838f
[AfterEach] [sig-node] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:53:44.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9519" for this suite.
•{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":296,"completed":39,"skipped":693,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:53:44.993: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3432
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3432.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3432.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3432.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3432.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3432.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3432.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 21:54:57.702: INFO: DNS probes using dns-3432/dns-test-7785537e-d15a-4479-9e71-26a816932361 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:54:57.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3432" for this suite.

• [SLOW TEST:73.728 seconds]
[sig-network] DNS
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":296,"completed":40,"skipped":694,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] server version
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:54:58.721: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-2864
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Request ServerVersion
STEP: Confirm major version
Sep  5 21:54:59.463: INFO: Major version: 1
STEP: Confirm minor version
Sep  5 21:54:59.463: INFO: cleanMinorVersion: 20
Sep  5 21:54:59.463: INFO: Minor version: 20
[AfterEach] [sig-api-machinery] server version
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:54:59.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-2864" for this suite.
•{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":296,"completed":41,"skipped":703,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:54:59.760: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5687
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 21:55:02.087: INFO: Waiting up to 5m0s for pod "busybox-user-65534-6d9c033a-4496-4f23-a331-ba32c9789454" in namespace "security-context-test-5687" to be "Succeeded or Failed"
Sep  5 21:55:02.287: INFO: Pod "busybox-user-65534-6d9c033a-4496-4f23-a331-ba32c9789454": Phase="Pending", Reason="", readiness=false. Elapsed: 200.381093ms
Sep  5 21:55:04.305: INFO: Pod "busybox-user-65534-6d9c033a-4496-4f23-a331-ba32c9789454": Phase="Pending", Reason="", readiness=false. Elapsed: 2.218369116s
Sep  5 21:55:06.331: INFO: Pod "busybox-user-65534-6d9c033a-4496-4f23-a331-ba32c9789454": Phase="Pending", Reason="", readiness=false. Elapsed: 4.24373858s
Sep  5 21:55:08.433: INFO: Pod "busybox-user-65534-6d9c033a-4496-4f23-a331-ba32c9789454": Phase="Pending", Reason="", readiness=false. Elapsed: 6.346406077s
Sep  5 21:55:10.448: INFO: Pod "busybox-user-65534-6d9c033a-4496-4f23-a331-ba32c9789454": Phase="Pending", Reason="", readiness=false. Elapsed: 8.361009613s
Sep  5 21:55:12.539: INFO: Pod "busybox-user-65534-6d9c033a-4496-4f23-a331-ba32c9789454": Phase="Pending", Reason="", readiness=false. Elapsed: 10.452335832s
Sep  5 21:55:14.593: INFO: Pod "busybox-user-65534-6d9c033a-4496-4f23-a331-ba32c9789454": Phase="Pending", Reason="", readiness=false. Elapsed: 12.505570255s
Sep  5 21:55:16.620: INFO: Pod "busybox-user-65534-6d9c033a-4496-4f23-a331-ba32c9789454": Phase="Pending", Reason="", readiness=false. Elapsed: 14.53326232s
Sep  5 21:55:18.637: INFO: Pod "busybox-user-65534-6d9c033a-4496-4f23-a331-ba32c9789454": Phase="Pending", Reason="", readiness=false. Elapsed: 16.549763423s
Sep  5 21:55:20.654: INFO: Pod "busybox-user-65534-6d9c033a-4496-4f23-a331-ba32c9789454": Phase="Pending", Reason="", readiness=false. Elapsed: 18.56711554s
Sep  5 21:55:22.670: INFO: Pod "busybox-user-65534-6d9c033a-4496-4f23-a331-ba32c9789454": Phase="Pending", Reason="", readiness=false. Elapsed: 20.583365754s
Sep  5 21:55:24.734: INFO: Pod "busybox-user-65534-6d9c033a-4496-4f23-a331-ba32c9789454": Phase="Pending", Reason="", readiness=false. Elapsed: 22.647114059s
Sep  5 21:55:26.759: INFO: Pod "busybox-user-65534-6d9c033a-4496-4f23-a331-ba32c9789454": Phase="Pending", Reason="", readiness=false. Elapsed: 24.671995081s
Sep  5 21:55:28.777: INFO: Pod "busybox-user-65534-6d9c033a-4496-4f23-a331-ba32c9789454": Phase="Pending", Reason="", readiness=false. Elapsed: 26.689450856s
Sep  5 21:55:30.788: INFO: Pod "busybox-user-65534-6d9c033a-4496-4f23-a331-ba32c9789454": Phase="Pending", Reason="", readiness=false. Elapsed: 28.70091631s
Sep  5 21:55:32.811: INFO: Pod "busybox-user-65534-6d9c033a-4496-4f23-a331-ba32c9789454": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.723931016s
Sep  5 21:55:32.811: INFO: Pod "busybox-user-65534-6d9c033a-4496-4f23-a331-ba32c9789454" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:55:32.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5687" for this suite.

• [SLOW TEST:33.321 seconds]
[k8s.io] Security Context
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  When creating a container with runAsUser
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:45
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":42,"skipped":719,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:55:33.082: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1089
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  5 21:55:33.647: INFO: Waiting up to 5m0s for pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2" in namespace "emptydir-1089" to be "Succeeded or Failed"
Sep  5 21:55:33.704: INFO: Pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2": Phase="Pending", Reason="", readiness=false. Elapsed: 56.365336ms
Sep  5 21:55:35.726: INFO: Pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.07881043s
Sep  5 21:55:37.735: INFO: Pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.087580729s
Sep  5 21:55:39.748: INFO: Pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.101169103s
Sep  5 21:55:41.773: INFO: Pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.125580973s
Sep  5 21:55:43.791: INFO: Pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.14378639s
Sep  5 21:55:45.803: INFO: Pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.155632635s
Sep  5 21:55:47.820: INFO: Pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.173175661s
Sep  5 21:55:49.829: INFO: Pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.181725731s
Sep  5 21:55:51.842: INFO: Pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.194353496s
Sep  5 21:55:53.854: INFO: Pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.206688228s
Sep  5 21:55:55.864: INFO: Pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.217284316s
Sep  5 21:55:57.878: INFO: Pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.230767212s
Sep  5 21:55:59.891: INFO: Pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2": Phase="Pending", Reason="", readiness=false. Elapsed: 26.243349563s
Sep  5 21:56:01.974: INFO: Pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2": Phase="Pending", Reason="", readiness=false. Elapsed: 28.326435356s
Sep  5 21:56:03.987: INFO: Pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2": Phase="Pending", Reason="", readiness=false. Elapsed: 30.339875295s
Sep  5 21:56:06.002: INFO: Pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2": Phase="Pending", Reason="", readiness=false. Elapsed: 32.354718454s
Sep  5 21:56:08.018: INFO: Pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.370881682s
STEP: Saw pod success
Sep  5 21:56:08.018: INFO: Pod "pod-9cc10676-7e9c-47f3-8083-4732c23e87d2" satisfied condition "Succeeded or Failed"
Sep  5 21:56:08.026: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-9cc10676-7e9c-47f3-8083-4732c23e87d2 container test-container: <nil>
STEP: delete the pod
Sep  5 21:56:14.555: INFO: Waiting for pod pod-9cc10676-7e9c-47f3-8083-4732c23e87d2 to disappear
Sep  5 21:56:14.583: INFO: Pod pod-9cc10676-7e9c-47f3-8083-4732c23e87d2 still exists
Sep  5 21:56:16.583: INFO: Waiting for pod pod-9cc10676-7e9c-47f3-8083-4732c23e87d2 to disappear
Sep  5 21:56:16.595: INFO: Pod pod-9cc10676-7e9c-47f3-8083-4732c23e87d2 still exists
Sep  5 21:56:18.583: INFO: Waiting for pod pod-9cc10676-7e9c-47f3-8083-4732c23e87d2 to disappear
Sep  5 21:56:18.595: INFO: Pod pod-9cc10676-7e9c-47f3-8083-4732c23e87d2 still exists
Sep  5 21:56:20.583: INFO: Waiting for pod pod-9cc10676-7e9c-47f3-8083-4732c23e87d2 to disappear
Sep  5 21:56:20.592: INFO: Pod pod-9cc10676-7e9c-47f3-8083-4732c23e87d2 still exists
Sep  5 21:56:22.583: INFO: Waiting for pod pod-9cc10676-7e9c-47f3-8083-4732c23e87d2 to disappear
Sep  5 21:56:22.594: INFO: Pod pod-9cc10676-7e9c-47f3-8083-4732c23e87d2 still exists
Sep  5 21:56:24.583: INFO: Waiting for pod pod-9cc10676-7e9c-47f3-8083-4732c23e87d2 to disappear
Sep  5 21:56:24.600: INFO: Pod pod-9cc10676-7e9c-47f3-8083-4732c23e87d2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:56:24.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1089" for this suite.

• [SLOW TEST:52.048 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":43,"skipped":744,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:56:25.130: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6204
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep  5 21:56:25.870: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6204 /api/v1/namespaces/watch-6204/configmaps/e2e-watch-test-label-changed 13ef125d-a6ef-4d8d-8eb6-7098399bc5df 84040 0 2021-09-05 21:56:25 -0700 PDT <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-05 21:56:25 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  5 21:56:25.870: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6204 /api/v1/namespaces/watch-6204/configmaps/e2e-watch-test-label-changed 13ef125d-a6ef-4d8d-8eb6-7098399bc5df 84043 0 2021-09-05 21:56:25 -0700 PDT <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-05 21:56:25 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  5 21:56:25.871: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6204 /api/v1/namespaces/watch-6204/configmaps/e2e-watch-test-label-changed 13ef125d-a6ef-4d8d-8eb6-7098399bc5df 84044 0 2021-09-05 21:56:25 -0700 PDT <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-05 21:56:25 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep  5 21:56:35.985: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6204 /api/v1/namespaces/watch-6204/configmaps/e2e-watch-test-label-changed 13ef125d-a6ef-4d8d-8eb6-7098399bc5df 84171 0 2021-09-05 21:56:25 -0700 PDT <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-05 21:56:25 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  5 21:56:35.985: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6204 /api/v1/namespaces/watch-6204/configmaps/e2e-watch-test-label-changed 13ef125d-a6ef-4d8d-8eb6-7098399bc5df 84172 0 2021-09-05 21:56:25 -0700 PDT <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-05 21:56:25 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  5 21:56:35.985: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6204 /api/v1/namespaces/watch-6204/configmaps/e2e-watch-test-label-changed 13ef125d-a6ef-4d8d-8eb6-7098399bc5df 84173 0 2021-09-05 21:56:25 -0700 PDT <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-05 21:56:25 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:56:35.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6204" for this suite.

• [SLOW TEST:11.100 seconds]
[sig-api-machinery] Watchers
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":296,"completed":44,"skipped":757,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:56:36.230: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5107
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create deployment with httpd image
Sep  5 21:56:36.682: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5107 create -f -'
Sep  5 21:56:37.189: INFO: stderr: ""
Sep  5 21:56:37.189: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Sep  5 21:56:37.189: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5107 diff -f -'
Sep  5 21:56:37.597: INFO: rc: 1
Sep  5 21:56:37.597: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-5107 delete -f -'
Sep  5 21:56:37.716: INFO: stderr: ""
Sep  5 21:56:37.716: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:56:37.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5107" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":296,"completed":45,"skipped":776,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:56:37.957: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2468
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-6c0c8b53-5776-4ac7-a0b7-afa8a6a540f6 in namespace container-probe-2468
Sep  5 21:57:16.529: INFO: Started pod busybox-6c0c8b53-5776-4ac7-a0b7-afa8a6a540f6 in namespace container-probe-2468
STEP: checking the pod's current state and verifying that restartCount is present
Sep  5 21:57:16.541: INFO: Initial restart count of pod busybox-6c0c8b53-5776-4ac7-a0b7-afa8a6a540f6 is 0
Sep  5 21:57:30.712: INFO: Restart count of pod container-probe-2468/busybox-6c0c8b53-5776-4ac7-a0b7-afa8a6a540f6 is now 1 (14.170193326s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:57:30.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2468" for this suite.

• [SLOW TEST:53.087 seconds]
[k8s.io] Probing container
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":296,"completed":46,"skipped":798,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:57:31.044: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5883
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-4dfc6109-a245-4ca1-a68c-15b5819e9120
STEP: Creating a pod to test consume secrets
Sep  5 21:57:31.716: INFO: Waiting up to 5m0s for pod "pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea" in namespace "secrets-5883" to be "Succeeded or Failed"
Sep  5 21:57:31.729: INFO: Pod "pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea": Phase="Pending", Reason="", readiness=false. Elapsed: 13.727401ms
Sep  5 21:57:33.741: INFO: Pod "pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025517485s
Sep  5 21:57:35.757: INFO: Pod "pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040868257s
Sep  5 21:57:37.767: INFO: Pod "pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051445184s
Sep  5 21:57:39.782: INFO: Pod "pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea": Phase="Pending", Reason="", readiness=false. Elapsed: 8.066338703s
Sep  5 21:57:41.790: INFO: Pod "pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea": Phase="Pending", Reason="", readiness=false. Elapsed: 10.074703127s
Sep  5 21:57:43.806: INFO: Pod "pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea": Phase="Pending", Reason="", readiness=false. Elapsed: 12.090452764s
Sep  5 21:57:45.818: INFO: Pod "pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea": Phase="Pending", Reason="", readiness=false. Elapsed: 14.102397067s
Sep  5 21:57:47.849: INFO: Pod "pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea": Phase="Pending", Reason="", readiness=false. Elapsed: 16.132905489s
Sep  5 21:57:49.860: INFO: Pod "pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea": Phase="Pending", Reason="", readiness=false. Elapsed: 18.144487822s
Sep  5 21:57:51.872: INFO: Pod "pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea": Phase="Pending", Reason="", readiness=false. Elapsed: 20.156225315s
Sep  5 21:57:53.898: INFO: Pod "pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea": Phase="Pending", Reason="", readiness=false. Elapsed: 22.182455248s
Sep  5 21:57:55.917: INFO: Pod "pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea": Phase="Pending", Reason="", readiness=false. Elapsed: 24.201055067s
Sep  5 21:57:57.932: INFO: Pod "pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea": Phase="Pending", Reason="", readiness=false. Elapsed: 26.216238559s
Sep  5 21:57:59.952: INFO: Pod "pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea": Phase="Pending", Reason="", readiness=false. Elapsed: 28.236666356s
Sep  5 21:58:01.970: INFO: Pod "pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea": Phase="Pending", Reason="", readiness=false. Elapsed: 30.254233117s
Sep  5 21:58:03.980: INFO: Pod "pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.264655356s
STEP: Saw pod success
Sep  5 21:58:03.980: INFO: Pod "pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea" satisfied condition "Succeeded or Failed"
Sep  5 21:58:04.000: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 21:58:09.901: INFO: Waiting for pod pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea to disappear
Sep  5 21:58:09.922: INFO: Pod pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea still exists
Sep  5 21:58:11.923: INFO: Waiting for pod pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea to disappear
Sep  5 21:58:11.937: INFO: Pod pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea still exists
Sep  5 21:58:13.923: INFO: Waiting for pod pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea to disappear
Sep  5 21:58:13.954: INFO: Pod pod-secrets-0fabdb84-45eb-46f2-89e3-4cb37c28efea no longer exists
[AfterEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:58:13.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5883" for this suite.

• [SLOW TEST:43.166 seconds]
[sig-storage] Secrets
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":296,"completed":47,"skipped":811,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:58:14.211: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-128
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep  5 21:58:14.771: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7" in namespace "downward-api-128" to be "Succeeded or Failed"
Sep  5 21:58:14.791: INFO: Pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.171785ms
Sep  5 21:58:16.806: INFO: Pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0349613s
Sep  5 21:58:18.820: INFO: Pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049484193s
Sep  5 21:58:20.831: INFO: Pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.059617464s
Sep  5 21:58:22.844: INFO: Pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.072743452s
Sep  5 21:58:24.862: INFO: Pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.090864123s
Sep  5 21:58:26.876: INFO: Pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.104628385s
Sep  5 21:58:28.892: INFO: Pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 14.12075414s
Sep  5 21:58:30.901: INFO: Pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 16.129506118s
Sep  5 21:58:32.914: INFO: Pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 18.142895339s
Sep  5 21:58:34.933: INFO: Pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 20.162378804s
Sep  5 21:58:36.944: INFO: Pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 22.172960165s
Sep  5 21:58:38.959: INFO: Pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 24.188337327s
Sep  5 21:58:40.973: INFO: Pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 26.201639511s
Sep  5 21:58:42.984: INFO: Pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7": Phase="Pending", Reason="", readiness=false. Elapsed: 28.213411547s
Sep  5 21:58:45.004: INFO: Pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7": Phase="Running", Reason="", readiness=true. Elapsed: 30.233233218s
Sep  5 21:58:47.018: INFO: Pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7": Phase="Running", Reason="", readiness=true. Elapsed: 32.246520563s
Sep  5 21:58:49.029: INFO: Pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.258381067s
STEP: Saw pod success
Sep  5 21:58:49.029: INFO: Pod "downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7" satisfied condition "Succeeded or Failed"
Sep  5 21:58:49.037: INFO: Trying to get logs from node sc2-10-185-226-150.eng.vmware.com pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 container client-container: <nil>
STEP: delete the pod
Sep  5 21:58:49.107: INFO: Waiting for pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 to disappear
Sep  5 21:58:49.122: INFO: Pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 still exists
Sep  5 21:58:51.123: INFO: Waiting for pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 to disappear
Sep  5 21:58:51.131: INFO: Pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 still exists
Sep  5 21:58:53.123: INFO: Waiting for pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 to disappear
Sep  5 21:58:53.137: INFO: Pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 still exists
Sep  5 21:58:55.124: INFO: Waiting for pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 to disappear
Sep  5 21:58:55.135: INFO: Pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 still exists
Sep  5 21:58:57.123: INFO: Waiting for pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 to disappear
Sep  5 21:58:57.137: INFO: Pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 still exists
Sep  5 21:58:59.122: INFO: Waiting for pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 to disappear
Sep  5 21:58:59.137: INFO: Pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 still exists
Sep  5 21:59:01.124: INFO: Waiting for pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 to disappear
Sep  5 21:59:01.135: INFO: Pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 still exists
Sep  5 21:59:03.123: INFO: Waiting for pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 to disappear
Sep  5 21:59:03.140: INFO: Pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 still exists
Sep  5 21:59:05.124: INFO: Waiting for pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 to disappear
Sep  5 21:59:05.133: INFO: Pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 still exists
Sep  5 21:59:07.123: INFO: Waiting for pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 to disappear
Sep  5 21:59:07.135: INFO: Pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 still exists
Sep  5 21:59:09.123: INFO: Waiting for pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 to disappear
Sep  5 21:59:09.136: INFO: Pod downwardapi-volume-94f31056-b610-4b49-a448-482853301cc7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:59:09.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-128" for this suite.

• [SLOW TEST:55.334 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":48,"skipped":954,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:59:09.546: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5628
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod test-webserver-8da486cc-1112-430e-9e18-4a3f9a4bb9bf in namespace container-probe-5628
Sep  5 21:59:38.211: INFO: Started pod test-webserver-8da486cc-1112-430e-9e18-4a3f9a4bb9bf in namespace container-probe-5628
STEP: checking the pod's current state and verifying that restartCount is present
Sep  5 21:59:38.224: INFO: Initial restart count of pod test-webserver-8da486cc-1112-430e-9e18-4a3f9a4bb9bf is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:03:38.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5628" for this suite.

• [SLOW TEST:269.471 seconds]
[k8s.io] Probing container
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":296,"completed":49,"skipped":984,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:03:39.017: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8749
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service externalname-service with the type=ExternalName in namespace services-8749
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-8749
Sep  5 22:04:12.957: INFO: Creating new exec pod
Sep  5 22:04:42.031: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=services-8749 exec execpodc22tm -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep  5 22:04:43.083: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  5 22:04:43.083: INFO: stdout: ""
Sep  5 22:04:43.084: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=services-8749 exec execpodc22tm -- /bin/sh -x -c nc -zv -t -w 2 172.24.176.184 80'
Sep  5 22:04:43.352: INFO: stderr: "+ nc -zv -t -w 2 172.24.176.184 80\nConnection to 172.24.176.184 80 port [tcp/http] succeeded!\n"
Sep  5 22:04:43.352: INFO: stdout: ""
Sep  5 22:04:43.352: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:04:43.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8749" for this suite.
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:65.447 seconds]
[sig-network] Services
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":296,"completed":50,"skipped":987,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Aggregator
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:04:44.464: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-9188
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Sep  5 22:04:44.982: INFO: >>> kubeConfig: ./kconfig.yaml
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the sample API server.
Sep  5 22:04:45.912: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
Sep  5 22:04:48.152: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:04:50.167: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:04:52.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:04:54.168: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:04:56.164: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:04:58.191: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:00.230: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:02.161: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:04.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:06.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:08.166: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:10.168: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:12.178: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:14.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:16.168: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:18.447: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:20.173: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:22.165: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:24.170: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:26.179: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:28.170: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:30.174: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:32.198: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:34.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:36.166: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:38.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:40.164: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:42.165: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:44.166: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:46.165: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:48.169: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:50.180: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 4, 46, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 4, 45, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:05:56.623: INFO: Waited 4.431890495s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:05:58.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9188" for this suite.

• [SLOW TEST:75.470 seconds]
[sig-api-machinery] Aggregator
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":296,"completed":51,"skipped":1018,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:05:59.935: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7550
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 22:06:33.482: INFO: Waiting up to 5m0s for pod "client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465" in namespace "pods-7550" to be "Succeeded or Failed"
Sep  5 22:06:33.501: INFO: Pod "client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465": Phase="Pending", Reason="", readiness=false. Elapsed: 19.158168ms
Sep  5 22:06:35.514: INFO: Pod "client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032069159s
Sep  5 22:06:37.530: INFO: Pod "client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047835613s
Sep  5 22:06:39.563: INFO: Pod "client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465": Phase="Pending", Reason="", readiness=false. Elapsed: 6.080614923s
Sep  5 22:06:41.578: INFO: Pod "client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465": Phase="Pending", Reason="", readiness=false. Elapsed: 8.095656078s
Sep  5 22:06:43.595: INFO: Pod "client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465": Phase="Pending", Reason="", readiness=false. Elapsed: 10.113089564s
Sep  5 22:06:45.610: INFO: Pod "client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465": Phase="Pending", Reason="", readiness=false. Elapsed: 12.127798258s
Sep  5 22:06:47.623: INFO: Pod "client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465": Phase="Pending", Reason="", readiness=false. Elapsed: 14.140667996s
Sep  5 22:06:49.641: INFO: Pod "client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465": Phase="Pending", Reason="", readiness=false. Elapsed: 16.158907118s
Sep  5 22:06:51.678: INFO: Pod "client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465": Phase="Pending", Reason="", readiness=false. Elapsed: 18.195325964s
Sep  5 22:06:53.709: INFO: Pod "client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465": Phase="Pending", Reason="", readiness=false. Elapsed: 20.226813327s
Sep  5 22:06:55.725: INFO: Pod "client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465": Phase="Pending", Reason="", readiness=false. Elapsed: 22.242390494s
Sep  5 22:06:57.739: INFO: Pod "client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465": Phase="Pending", Reason="", readiness=false. Elapsed: 24.256809554s
Sep  5 22:06:59.752: INFO: Pod "client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465": Phase="Pending", Reason="", readiness=false. Elapsed: 26.269312941s
Sep  5 22:07:01.767: INFO: Pod "client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.285062327s
STEP: Saw pod success
Sep  5 22:07:01.767: INFO: Pod "client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465" satisfied condition "Succeeded or Failed"
Sep  5 22:07:01.780: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465 container env3cont: <nil>
STEP: delete the pod
Sep  5 22:07:07.025: INFO: Waiting for pod client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465 to disappear
Sep  5 22:07:07.036: INFO: Pod client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465 still exists
Sep  5 22:07:09.037: INFO: Waiting for pod client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465 to disappear
Sep  5 22:07:09.053: INFO: Pod client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465 still exists
Sep  5 22:07:11.038: INFO: Waiting for pod client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465 to disappear
Sep  5 22:07:11.052: INFO: Pod client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465 still exists
Sep  5 22:07:13.037: INFO: Waiting for pod client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465 to disappear
Sep  5 22:07:13.048: INFO: Pod client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465 still exists
Sep  5 22:07:15.037: INFO: Waiting for pod client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465 to disappear
Sep  5 22:07:15.053: INFO: Pod client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465 still exists
Sep  5 22:07:17.037: INFO: Waiting for pod client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465 to disappear
Sep  5 22:07:17.048: INFO: Pod client-envvars-6ff8f604-82b0-4b7b-815a-00ceec3c4465 no longer exists
[AfterEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:07:17.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7550" for this suite.

• [SLOW TEST:77.553 seconds]
[k8s.io] Pods
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should contain environment variables for services [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":296,"completed":52,"skipped":1020,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:07:17.489: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4958
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Sep  5 22:08:26.626: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:08:26.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4958" for this suite.

• [SLOW TEST:69.624 seconds]
[sig-api-machinery] Garbage collector
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":296,"completed":53,"skipped":1028,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:08:27.114: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8575
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 22:08:27.661: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep  5 22:08:36.719: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-8575 --namespace=crd-publish-openapi-8575 create -f -'
Sep  5 22:08:37.791: INFO: stderr: ""
Sep  5 22:08:37.791: INFO: stdout: "e2e-test-crd-publish-openapi-5294-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  5 22:08:37.791: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-8575 --namespace=crd-publish-openapi-8575 delete e2e-test-crd-publish-openapi-5294-crds test-cr'
Sep  5 22:08:37.902: INFO: stderr: ""
Sep  5 22:08:37.902: INFO: stdout: "e2e-test-crd-publish-openapi-5294-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep  5 22:08:37.902: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-8575 --namespace=crd-publish-openapi-8575 apply -f -'
Sep  5 22:08:38.308: INFO: stderr: ""
Sep  5 22:08:38.308: INFO: stdout: "e2e-test-crd-publish-openapi-5294-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  5 22:08:38.308: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-8575 --namespace=crd-publish-openapi-8575 delete e2e-test-crd-publish-openapi-5294-crds test-cr'
Sep  5 22:08:38.436: INFO: stderr: ""
Sep  5 22:08:38.436: INFO: stdout: "e2e-test-crd-publish-openapi-5294-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep  5 22:08:38.436: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-8575 explain e2e-test-crd-publish-openapi-5294-crds'
Sep  5 22:08:38.791: INFO: stderr: ""
Sep  5 22:08:38.791: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5294-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:08:46.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8575" for this suite.

• [SLOW TEST:19.984 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":296,"completed":54,"skipped":1039,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:08:47.098: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-733
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name projected-secret-test-164ad52b-020f-4a64-a897-c877d444829a
STEP: Creating a pod to test consume secrets
Sep  5 22:08:47.663: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817" in namespace "projected-733" to be "Succeeded or Failed"
Sep  5 22:08:47.673: INFO: Pod "pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817": Phase="Pending", Reason="", readiness=false. Elapsed: 9.728078ms
Sep  5 22:08:49.714: INFO: Pod "pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051250762s
Sep  5 22:08:51.727: INFO: Pod "pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064290065s
Sep  5 22:08:53.737: INFO: Pod "pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073896137s
Sep  5 22:08:55.751: INFO: Pod "pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817": Phase="Pending", Reason="", readiness=false. Elapsed: 8.087782227s
Sep  5 22:08:57.758: INFO: Pod "pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817": Phase="Pending", Reason="", readiness=false. Elapsed: 10.094701202s
Sep  5 22:08:59.770: INFO: Pod "pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817": Phase="Pending", Reason="", readiness=false. Elapsed: 12.107105718s
Sep  5 22:09:01.782: INFO: Pod "pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817": Phase="Pending", Reason="", readiness=false. Elapsed: 14.119084518s
Sep  5 22:09:03.798: INFO: Pod "pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817": Phase="Pending", Reason="", readiness=false. Elapsed: 16.135309236s
Sep  5 22:09:05.815: INFO: Pod "pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817": Phase="Pending", Reason="", readiness=false. Elapsed: 18.151970105s
Sep  5 22:09:07.827: INFO: Pod "pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817": Phase="Pending", Reason="", readiness=false. Elapsed: 20.164169742s
Sep  5 22:09:09.840: INFO: Pod "pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817": Phase="Pending", Reason="", readiness=false. Elapsed: 22.176517224s
Sep  5 22:09:11.851: INFO: Pod "pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817": Phase="Pending", Reason="", readiness=false. Elapsed: 24.187781049s
Sep  5 22:09:13.869: INFO: Pod "pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817": Phase="Running", Reason="", readiness=true. Elapsed: 26.206218428s
Sep  5 22:09:15.889: INFO: Pod "pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817": Phase="Running", Reason="", readiness=true. Elapsed: 28.225982655s
Sep  5 22:09:17.907: INFO: Pod "pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.243515023s
STEP: Saw pod success
Sep  5 22:09:17.907: INFO: Pod "pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817" satisfied condition "Succeeded or Failed"
Sep  5 22:09:17.927: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 22:09:25.399: INFO: Waiting for pod pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817 to disappear
Sep  5 22:09:25.414: INFO: Pod pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817 still exists
Sep  5 22:09:27.414: INFO: Waiting for pod pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817 to disappear
Sep  5 22:09:27.429: INFO: Pod pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817 still exists
Sep  5 22:09:29.414: INFO: Waiting for pod pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817 to disappear
Sep  5 22:09:29.429: INFO: Pod pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817 still exists
Sep  5 22:09:31.414: INFO: Waiting for pod pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817 to disappear
Sep  5 22:09:31.426: INFO: Pod pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817 still exists
Sep  5 22:09:33.415: INFO: Waiting for pod pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817 to disappear
Sep  5 22:09:33.427: INFO: Pod pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817 still exists
Sep  5 22:09:35.415: INFO: Waiting for pod pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817 to disappear
Sep  5 22:09:35.427: INFO: Pod pod-projected-secrets-8d52d78b-0a2f-4df8-a3f1-7e2ba9c8c817 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:09:35.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-733" for this suite.

• [SLOW TEST:48.819 seconds]
[sig-storage] Projected secret
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":296,"completed":55,"skipped":1053,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:09:35.917: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9617
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should test the lifecycle of an Endpoint [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:09:36.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9617" for this suite.
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":296,"completed":56,"skipped":1061,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:09:36.799: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3470
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Sep  5 22:09:37.315: INFO: Waiting up to 5m0s for pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5" in namespace "downward-api-3470" to be "Succeeded or Failed"
Sep  5 22:09:37.322: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.583013ms
Sep  5 22:09:39.339: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023879315s
Sep  5 22:09:41.355: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039797776s
Sep  5 22:09:43.370: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055483264s
Sep  5 22:09:45.388: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.073449236s
Sep  5 22:09:47.404: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.089034918s
Sep  5 22:09:49.419: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.103780607s
Sep  5 22:09:51.434: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.119528092s
Sep  5 22:09:53.453: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.138127503s
Sep  5 22:09:55.465: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.15002316s
Sep  5 22:09:57.522: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.207519922s
Sep  5 22:09:59.543: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.227884758s
Sep  5 22:10:01.567: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.251916814s
Sep  5 22:10:03.580: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.26543296s
Sep  5 22:10:05.600: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.285001585s
Sep  5 22:10:07.616: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.301590085s
Sep  5 22:10:09.659: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.344282214s
Sep  5 22:10:11.685: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.370653088s
Sep  5 22:10:13.702: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 36.387019175s
STEP: Saw pod success
Sep  5 22:10:13.702: INFO: Pod "downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5" satisfied condition "Succeeded or Failed"
Sep  5 22:10:13.726: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5 container dapi-container: <nil>
STEP: delete the pod
Sep  5 22:10:19.744: INFO: Waiting for pod downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5 to disappear
Sep  5 22:10:19.755: INFO: Pod downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5 still exists
Sep  5 22:10:21.757: INFO: Waiting for pod downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5 to disappear
Sep  5 22:10:21.769: INFO: Pod downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5 still exists
Sep  5 22:10:23.757: INFO: Waiting for pod downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5 to disappear
Sep  5 22:10:23.771: INFO: Pod downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5 still exists
Sep  5 22:10:25.757: INFO: Waiting for pod downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5 to disappear
Sep  5 22:10:25.771: INFO: Pod downward-api-bcaa47aa-5627-46fd-bbfe-2fdd5905c2f5 no longer exists
[AfterEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:10:25.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3470" for this suite.

• [SLOW TEST:49.232 seconds]
[sig-node] Downward API
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide host IP as an env var [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":296,"completed":57,"skipped":1074,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:10:26.031: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9089
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-c2febdc7-3b3a-4d49-bde5-4fd0abdceeed-5289
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:10:27.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9089" for this suite.
STEP: Destroying namespace "nspatchtest-c2febdc7-3b3a-4d49-bde5-4fd0abdceeed-5289" for this suite.
•{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":296,"completed":58,"skipped":1088,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:10:28.042: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9106
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 22:10:28.686: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-df73a85e-da77-415b-b825-406691936a59" in namespace "security-context-test-9106" to be "Succeeded or Failed"
Sep  5 22:10:28.701: INFO: Pod "busybox-readonly-false-df73a85e-da77-415b-b825-406691936a59": Phase="Pending", Reason="", readiness=false. Elapsed: 15.387697ms
Sep  5 22:10:30.740: INFO: Pod "busybox-readonly-false-df73a85e-da77-415b-b825-406691936a59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054481402s
Sep  5 22:10:32.750: INFO: Pod "busybox-readonly-false-df73a85e-da77-415b-b825-406691936a59": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064375766s
Sep  5 22:10:34.764: INFO: Pod "busybox-readonly-false-df73a85e-da77-415b-b825-406691936a59": Phase="Pending", Reason="", readiness=false. Elapsed: 6.078684243s
Sep  5 22:10:36.776: INFO: Pod "busybox-readonly-false-df73a85e-da77-415b-b825-406691936a59": Phase="Pending", Reason="", readiness=false. Elapsed: 8.089883969s
Sep  5 22:10:38.787: INFO: Pod "busybox-readonly-false-df73a85e-da77-415b-b825-406691936a59": Phase="Pending", Reason="", readiness=false. Elapsed: 10.101552045s
Sep  5 22:10:40.803: INFO: Pod "busybox-readonly-false-df73a85e-da77-415b-b825-406691936a59": Phase="Pending", Reason="", readiness=false. Elapsed: 12.116906996s
Sep  5 22:10:42.812: INFO: Pod "busybox-readonly-false-df73a85e-da77-415b-b825-406691936a59": Phase="Pending", Reason="", readiness=false. Elapsed: 14.126053322s
Sep  5 22:10:44.824: INFO: Pod "busybox-readonly-false-df73a85e-da77-415b-b825-406691936a59": Phase="Pending", Reason="", readiness=false. Elapsed: 16.138596198s
Sep  5 22:10:46.845: INFO: Pod "busybox-readonly-false-df73a85e-da77-415b-b825-406691936a59": Phase="Pending", Reason="", readiness=false. Elapsed: 18.159717843s
Sep  5 22:10:48.856: INFO: Pod "busybox-readonly-false-df73a85e-da77-415b-b825-406691936a59": Phase="Pending", Reason="", readiness=false. Elapsed: 20.170696615s
Sep  5 22:10:50.871: INFO: Pod "busybox-readonly-false-df73a85e-da77-415b-b825-406691936a59": Phase="Pending", Reason="", readiness=false. Elapsed: 22.184879915s
Sep  5 22:10:52.881: INFO: Pod "busybox-readonly-false-df73a85e-da77-415b-b825-406691936a59": Phase="Pending", Reason="", readiness=false. Elapsed: 24.195623194s
Sep  5 22:10:54.894: INFO: Pod "busybox-readonly-false-df73a85e-da77-415b-b825-406691936a59": Phase="Pending", Reason="", readiness=false. Elapsed: 26.208295227s
Sep  5 22:10:56.911: INFO: Pod "busybox-readonly-false-df73a85e-da77-415b-b825-406691936a59": Phase="Pending", Reason="", readiness=false. Elapsed: 28.225320448s
Sep  5 22:10:58.923: INFO: Pod "busybox-readonly-false-df73a85e-da77-415b-b825-406691936a59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.237787526s
Sep  5 22:10:58.924: INFO: Pod "busybox-readonly-false-df73a85e-da77-415b-b825-406691936a59" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:10:58.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9106" for this suite.

• [SLOW TEST:31.324 seconds]
[k8s.io] Security Context
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  When creating a pod with readOnlyRootFilesystem
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:166
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":296,"completed":59,"skipped":1090,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:10:59.367: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2975
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 22:11:01.103: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  5 22:11:03.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:11:05.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:11:07.170: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:11:09.146: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:11:11.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:11:13.157: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:11:15.144: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:11:17.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:11:19.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:11:21.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:11:23.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:11:25.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:11:27.141: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:11:29.140: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 11, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 22:11:32.182: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:11:43.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2975" for this suite.
STEP: Destroying namespace "webhook-2975-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:45.322 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":296,"completed":60,"skipped":1095,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints 
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:11:44.689: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename sched-preemption
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-6175
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:90
Sep  5 22:11:45.212: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  5 22:12:45.428: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:12:45.443: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename sched-preemption-path
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-preemption-path-3366
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] PriorityClass endpoints
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:679
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 22:12:45.924: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: Value: Forbidden: may not be changed in an update.
Sep  5 22:12:45.934: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: Value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:12:46.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3366" for this suite.
[AfterEach] PriorityClass endpoints
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:693
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:12:46.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6175" for this suite.
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:78

• [SLOW TEST:62.010 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/preemption.go:673
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","total":296,"completed":61,"skipped":1096,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:12:46.699: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2537
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Sep  5 22:13:59.602: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Sep  5 22:13:59.602: INFO: Deleting pod "simpletest-rc-to-be-deleted-5w8g9" in namespace "gc-2537"
Sep  5 22:13:59.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-8drsw" in namespace "gc-2537"
Sep  5 22:13:59.697: INFO: Deleting pod "simpletest-rc-to-be-deleted-bbjzf" in namespace "gc-2537"
Sep  5 22:13:59.738: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcm88" in namespace "gc-2537"
Sep  5 22:13:59.778: INFO: Deleting pod "simpletest-rc-to-be-deleted-h949g" in namespace "gc-2537"
[AfterEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:13:59.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2537" for this suite.

• [SLOW TEST:74.908 seconds]
[sig-api-machinery] Garbage collector
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":296,"completed":62,"skipped":1100,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:14:01.607: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-9871
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 22:14:02.736: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b210bf76-e2b7-4a1d-a1ec-3d37b665cffb" in namespace "security-context-test-9871" to be "Succeeded or Failed"
Sep  5 22:14:02.751: INFO: Pod "busybox-privileged-false-b210bf76-e2b7-4a1d-a1ec-3d37b665cffb": Phase="Pending", Reason="", readiness=false. Elapsed: 15.036202ms
Sep  5 22:14:04.764: INFO: Pod "busybox-privileged-false-b210bf76-e2b7-4a1d-a1ec-3d37b665cffb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027944821s
Sep  5 22:14:06.779: INFO: Pod "busybox-privileged-false-b210bf76-e2b7-4a1d-a1ec-3d37b665cffb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043034385s
Sep  5 22:14:08.790: INFO: Pod "busybox-privileged-false-b210bf76-e2b7-4a1d-a1ec-3d37b665cffb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054405151s
Sep  5 22:14:10.834: INFO: Pod "busybox-privileged-false-b210bf76-e2b7-4a1d-a1ec-3d37b665cffb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.098182925s
Sep  5 22:14:12.848: INFO: Pod "busybox-privileged-false-b210bf76-e2b7-4a1d-a1ec-3d37b665cffb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.11231381s
Sep  5 22:14:14.863: INFO: Pod "busybox-privileged-false-b210bf76-e2b7-4a1d-a1ec-3d37b665cffb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.127161486s
Sep  5 22:14:16.879: INFO: Pod "busybox-privileged-false-b210bf76-e2b7-4a1d-a1ec-3d37b665cffb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.143511309s
Sep  5 22:14:18.895: INFO: Pod "busybox-privileged-false-b210bf76-e2b7-4a1d-a1ec-3d37b665cffb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.158740528s
Sep  5 22:14:20.907: INFO: Pod "busybox-privileged-false-b210bf76-e2b7-4a1d-a1ec-3d37b665cffb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.17072119s
Sep  5 22:14:22.924: INFO: Pod "busybox-privileged-false-b210bf76-e2b7-4a1d-a1ec-3d37b665cffb": Phase="Pending", Reason="", readiness=false. Elapsed: 20.188049512s
Sep  5 22:14:24.934: INFO: Pod "busybox-privileged-false-b210bf76-e2b7-4a1d-a1ec-3d37b665cffb": Phase="Pending", Reason="", readiness=false. Elapsed: 22.198498086s
Sep  5 22:14:26.951: INFO: Pod "busybox-privileged-false-b210bf76-e2b7-4a1d-a1ec-3d37b665cffb": Phase="Pending", Reason="", readiness=false. Elapsed: 24.214992349s
Sep  5 22:14:28.962: INFO: Pod "busybox-privileged-false-b210bf76-e2b7-4a1d-a1ec-3d37b665cffb": Phase="Pending", Reason="", readiness=false. Elapsed: 26.226144682s
Sep  5 22:14:30.976: INFO: Pod "busybox-privileged-false-b210bf76-e2b7-4a1d-a1ec-3d37b665cffb": Phase="Pending", Reason="", readiness=false. Elapsed: 28.240060738s
Sep  5 22:14:32.986: INFO: Pod "busybox-privileged-false-b210bf76-e2b7-4a1d-a1ec-3d37b665cffb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.250440356s
Sep  5 22:14:32.986: INFO: Pod "busybox-privileged-false-b210bf76-e2b7-4a1d-a1ec-3d37b665cffb" satisfied condition "Succeeded or Failed"
Sep  5 22:14:39.145: INFO: Got logs for pod "busybox-privileged-false-b210bf76-e2b7-4a1d-a1ec-3d37b665cffb": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:14:39.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-9871" for this suite.

• [SLOW TEST:37.932 seconds]
[k8s.io] Security Context
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  When creating a pod with privileged
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:227
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":63,"skipped":1101,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:14:39.540: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7170
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 22:14:41.922: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  5 22:14:43.961: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:14:45.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:14:47.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:14:49.974: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:14:51.975: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:14:53.982: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:14:55.986: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:14:57.985: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:14:59.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:15:01.980: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:15:03.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:15:05.987: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:15:07.993: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:15:09.973: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:15:11.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:15:14.090: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 14, 42, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 14, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 22:15:17.095: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 22:15:17.106: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1677-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:15:18.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7170" for this suite.
STEP: Destroying namespace "webhook-7170-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:41.171 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":296,"completed":64,"skipped":1161,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:15:20.711: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8014
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  5 22:15:21.477: INFO: Waiting up to 5m0s for pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8" in namespace "emptydir-8014" to be "Succeeded or Failed"
Sep  5 22:15:21.512: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 35.190591ms
Sep  5 22:15:23.589: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.111704263s
Sep  5 22:15:25.598: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.121076887s
Sep  5 22:15:27.644: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.167117962s
Sep  5 22:15:29.657: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.179824807s
Sep  5 22:15:31.679: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.202072658s
Sep  5 22:15:33.727: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.250231536s
Sep  5 22:15:35.759: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.281879005s
Sep  5 22:15:37.769: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.291808195s
Sep  5 22:15:39.782: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.305017461s
Sep  5 22:15:41.806: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 20.328295663s
Sep  5 22:15:43.819: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.341630328s
Sep  5 22:15:45.835: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 24.358240074s
Sep  5 22:15:47.847: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.369829916s
Sep  5 22:15:49.870: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 28.392857677s
Sep  5 22:15:51.889: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8": Phase="Pending", Reason="", readiness=false. Elapsed: 30.411645193s
Sep  5 22:15:53.902: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8": Phase="Running", Reason="", readiness=true. Elapsed: 32.42479274s
Sep  5 22:15:55.918: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8": Phase="Running", Reason="", readiness=true. Elapsed: 34.440886364s
Sep  5 22:15:57.938: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 36.460683717s
STEP: Saw pod success
Sep  5 22:15:57.938: INFO: Pod "pod-d4832b04-f34b-4350-999a-05847dfec9e8" satisfied condition "Succeeded or Failed"
Sep  5 22:15:57.961: INFO: Trying to get logs from node sc2-10-185-226-150.eng.vmware.com pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 container test-container: <nil>
STEP: delete the pod
Sep  5 22:15:58.024: INFO: Waiting for pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 to disappear
Sep  5 22:15:58.039: INFO: Pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 still exists
Sep  5 22:16:00.040: INFO: Waiting for pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 to disappear
Sep  5 22:16:00.052: INFO: Pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 still exists
Sep  5 22:16:02.039: INFO: Waiting for pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 to disappear
Sep  5 22:16:02.049: INFO: Pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 still exists
Sep  5 22:16:04.040: INFO: Waiting for pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 to disappear
Sep  5 22:16:04.046: INFO: Pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 still exists
Sep  5 22:16:06.041: INFO: Waiting for pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 to disappear
Sep  5 22:16:06.060: INFO: Pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 still exists
Sep  5 22:16:08.040: INFO: Waiting for pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 to disappear
Sep  5 22:16:08.050: INFO: Pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 still exists
Sep  5 22:16:10.040: INFO: Waiting for pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 to disappear
Sep  5 22:16:10.056: INFO: Pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 still exists
Sep  5 22:16:12.040: INFO: Waiting for pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 to disappear
Sep  5 22:16:12.103: INFO: Pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 still exists
Sep  5 22:16:14.040: INFO: Waiting for pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 to disappear
Sep  5 22:16:14.153: INFO: Pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 still exists
Sep  5 22:16:16.040: INFO: Waiting for pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 to disappear
Sep  5 22:16:16.050: INFO: Pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 still exists
Sep  5 22:16:18.040: INFO: Waiting for pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 to disappear
Sep  5 22:16:18.051: INFO: Pod pod-d4832b04-f34b-4350-999a-05847dfec9e8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:16:18.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8014" for this suite.

• [SLOW TEST:57.582 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":65,"skipped":1185,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:16:18.293: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7918
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-972bdaf6-1f91-4bd3-b59e-f1e5acf00839
STEP: Creating a pod to test consume secrets
Sep  5 22:16:18.782: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf" in namespace "projected-7918" to be "Succeeded or Failed"
Sep  5 22:16:18.812: INFO: Pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf": Phase="Pending", Reason="", readiness=false. Elapsed: 29.715065ms
Sep  5 22:16:20.826: INFO: Pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043651487s
Sep  5 22:16:22.836: INFO: Pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054194207s
Sep  5 22:16:24.846: INFO: Pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063630464s
Sep  5 22:16:26.857: INFO: Pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.074883108s
Sep  5 22:16:28.867: INFO: Pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf": Phase="Pending", Reason="", readiness=false. Elapsed: 10.084670981s
Sep  5 22:16:30.880: INFO: Pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf": Phase="Pending", Reason="", readiness=false. Elapsed: 12.097506835s
Sep  5 22:16:32.891: INFO: Pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf": Phase="Pending", Reason="", readiness=false. Elapsed: 14.109033819s
Sep  5 22:16:34.903: INFO: Pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf": Phase="Pending", Reason="", readiness=false. Elapsed: 16.12073642s
Sep  5 22:16:36.920: INFO: Pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf": Phase="Pending", Reason="", readiness=false. Elapsed: 18.138009953s
Sep  5 22:16:38.935: INFO: Pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf": Phase="Pending", Reason="", readiness=false. Elapsed: 20.152562436s
Sep  5 22:16:40.946: INFO: Pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf": Phase="Pending", Reason="", readiness=false. Elapsed: 22.163617754s
Sep  5 22:16:42.958: INFO: Pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf": Phase="Pending", Reason="", readiness=false. Elapsed: 24.176204397s
Sep  5 22:16:44.969: INFO: Pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf": Phase="Pending", Reason="", readiness=false. Elapsed: 26.186975704s
Sep  5 22:16:46.985: INFO: Pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf": Phase="Running", Reason="", readiness=true. Elapsed: 28.202635337s
Sep  5 22:16:49.001: INFO: Pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf": Phase="Running", Reason="", readiness=true. Elapsed: 30.218786397s
Sep  5 22:16:51.014: INFO: Pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf": Phase="Running", Reason="", readiness=true. Elapsed: 32.231714847s
Sep  5 22:16:53.026: INFO: Pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.244222724s
STEP: Saw pod success
Sep  5 22:16:53.026: INFO: Pod "pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf" satisfied condition "Succeeded or Failed"
Sep  5 22:16:53.034: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  5 22:16:59.566: INFO: Waiting for pod pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf to disappear
Sep  5 22:16:59.587: INFO: Pod pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf still exists
Sep  5 22:17:01.588: INFO: Waiting for pod pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf to disappear
Sep  5 22:17:01.600: INFO: Pod pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf still exists
Sep  5 22:17:03.589: INFO: Waiting for pod pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf to disappear
Sep  5 22:17:03.620: INFO: Pod pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf still exists
Sep  5 22:17:05.588: INFO: Waiting for pod pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf to disappear
Sep  5 22:17:05.611: INFO: Pod pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf still exists
Sep  5 22:17:07.588: INFO: Waiting for pod pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf to disappear
Sep  5 22:17:07.611: INFO: Pod pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf still exists
Sep  5 22:17:09.587: INFO: Waiting for pod pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf to disappear
Sep  5 22:17:09.619: INFO: Pod pod-projected-secrets-531d62bd-8358-4919-965f-0a73651675bf no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:17:09.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7918" for this suite.

• [SLOW TEST:52.052 seconds]
[sig-storage] Projected secret
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":66,"skipped":1200,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:17:10.345: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1547
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check is all data is printed  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 22:17:10.929: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-1547 version'
Sep  5 22:17:11.027: INFO: stderr: ""
Sep  5 22:17:11.027: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.0+vmware.wcp.2\", GitCommit:\"d5bb17833505d15ce5f40815bb14fede978fe8c1\", GitTreeState:\"clean\", BuildDate:\"2021-08-14T16:46:51Z\", GoVersion:\"go1.16.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.20.8+vmware.wcp.1\", GitCommit:\"e6c9e1282afaff09bfb0f25cddf7bc3f9b0e680d\", GitTreeState:\"clean\", BuildDate:\"2021-08-14T16:42:17Z\", GoVersion:\"go1.15.13\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:17:11.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1547" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":296,"completed":67,"skipped":1251,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:17:11.330: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6687
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:130
[It] should run and stop simple daemon [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  5 22:17:12.192: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:12.192: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:12.192: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:12.205: INFO: Number of nodes with available pods: 0
Sep  5 22:17:12.205: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:13.220: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:13.220: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:13.220: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:13.230: INFO: Number of nodes with available pods: 0
Sep  5 22:17:13.230: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:14.268: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:14.268: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:14.268: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:14.284: INFO: Number of nodes with available pods: 0
Sep  5 22:17:14.284: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:15.254: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:15.254: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:15.254: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:15.263: INFO: Number of nodes with available pods: 0
Sep  5 22:17:15.263: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:16.290: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:16.291: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:16.291: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:16.337: INFO: Number of nodes with available pods: 0
Sep  5 22:17:16.337: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:17.225: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:17.225: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:17.225: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:17.244: INFO: Number of nodes with available pods: 0
Sep  5 22:17:17.244: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:18.234: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:18.234: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:18.234: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:18.281: INFO: Number of nodes with available pods: 0
Sep  5 22:17:18.281: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:19.246: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:19.246: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:19.246: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:19.272: INFO: Number of nodes with available pods: 0
Sep  5 22:17:19.272: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:20.232: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:20.233: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:20.233: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:20.248: INFO: Number of nodes with available pods: 0
Sep  5 22:17:20.248: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:21.219: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:21.219: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:21.219: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:21.240: INFO: Number of nodes with available pods: 0
Sep  5 22:17:21.240: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:22.231: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:22.231: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:22.231: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:22.258: INFO: Number of nodes with available pods: 0
Sep  5 22:17:22.258: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:23.258: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:23.259: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:23.259: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:23.293: INFO: Number of nodes with available pods: 0
Sep  5 22:17:23.293: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:24.226: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:24.226: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:24.226: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:24.241: INFO: Number of nodes with available pods: 0
Sep  5 22:17:24.241: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:25.218: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:25.218: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:25.218: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:25.247: INFO: Number of nodes with available pods: 0
Sep  5 22:17:25.247: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:26.218: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:26.218: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:26.218: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:26.228: INFO: Number of nodes with available pods: 0
Sep  5 22:17:26.228: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:27.228: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:27.229: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:27.229: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:27.238: INFO: Number of nodes with available pods: 0
Sep  5 22:17:27.238: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:28.218: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:28.219: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:28.219: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:28.228: INFO: Number of nodes with available pods: 0
Sep  5 22:17:28.228: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:29.221: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:29.221: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:29.221: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:29.235: INFO: Number of nodes with available pods: 0
Sep  5 22:17:29.235: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:30.224: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:30.224: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:30.224: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:30.235: INFO: Number of nodes with available pods: 0
Sep  5 22:17:30.235: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:31.230: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:31.230: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:31.230: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:31.249: INFO: Number of nodes with available pods: 0
Sep  5 22:17:31.249: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:32.240: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:32.240: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:32.240: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:32.264: INFO: Number of nodes with available pods: 0
Sep  5 22:17:32.264: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:33.217: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:33.217: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:33.217: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:33.225: INFO: Number of nodes with available pods: 0
Sep  5 22:17:33.225: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:34.224: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:34.224: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:34.224: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:34.242: INFO: Number of nodes with available pods: 0
Sep  5 22:17:34.242: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:35.217: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:35.217: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:35.217: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:35.230: INFO: Number of nodes with available pods: 0
Sep  5 22:17:35.230: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:36.235: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:36.235: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:36.235: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:36.254: INFO: Number of nodes with available pods: 0
Sep  5 22:17:36.254: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:37.230: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:37.231: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:37.231: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:37.237: INFO: Number of nodes with available pods: 0
Sep  5 22:17:37.237: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:38.232: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:38.232: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:38.232: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:38.244: INFO: Number of nodes with available pods: 0
Sep  5 22:17:38.244: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:39.266: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:39.266: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:39.266: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:39.296: INFO: Number of nodes with available pods: 0
Sep  5 22:17:39.296: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:40.218: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:40.218: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:40.218: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:40.228: INFO: Number of nodes with available pods: 0
Sep  5 22:17:40.228: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:41.228: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:41.228: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:41.228: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:41.240: INFO: Number of nodes with available pods: 0
Sep  5 22:17:41.240: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:42.217: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:42.217: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:42.217: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:42.231: INFO: Number of nodes with available pods: 0
Sep  5 22:17:42.231: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:43.217: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:43.217: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:43.217: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:43.224: INFO: Number of nodes with available pods: 1
Sep  5 22:17:43.224: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:44.218: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:44.218: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:44.219: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:44.235: INFO: Number of nodes with available pods: 2
Sep  5 22:17:44.235: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:45.217: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:45.217: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:45.217: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:45.225: INFO: Number of nodes with available pods: 2
Sep  5 22:17:45.225: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:46.221: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:46.221: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:46.222: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:46.231: INFO: Number of nodes with available pods: 2
Sep  5 22:17:46.231: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:47.227: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:47.227: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:47.227: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:47.235: INFO: Number of nodes with available pods: 3
Sep  5 22:17:47.235: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep  5 22:17:47.295: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:47.295: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:47.295: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:47.304: INFO: Number of nodes with available pods: 2
Sep  5 22:17:47.304: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:48.331: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:48.331: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:48.331: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:48.344: INFO: Number of nodes with available pods: 2
Sep  5 22:17:48.344: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:49.327: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:49.327: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:49.327: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:49.338: INFO: Number of nodes with available pods: 2
Sep  5 22:17:49.338: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:50.340: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:50.340: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:50.340: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:50.350: INFO: Number of nodes with available pods: 2
Sep  5 22:17:50.350: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:51.322: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:51.322: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:51.322: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:51.337: INFO: Number of nodes with available pods: 2
Sep  5 22:17:51.337: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:52.320: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:52.320: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:52.320: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:52.338: INFO: Number of nodes with available pods: 2
Sep  5 22:17:52.338: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:53.319: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:53.319: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:53.319: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:53.342: INFO: Number of nodes with available pods: 2
Sep  5 22:17:53.342: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:54.340: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:54.340: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:54.340: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:54.350: INFO: Number of nodes with available pods: 2
Sep  5 22:17:54.350: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:55.320: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:55.320: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:55.320: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:55.337: INFO: Number of nodes with available pods: 2
Sep  5 22:17:55.337: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:56.320: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:56.320: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:56.320: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:56.332: INFO: Number of nodes with available pods: 2
Sep  5 22:17:56.332: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:57.337: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:57.337: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:57.337: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:57.359: INFO: Number of nodes with available pods: 2
Sep  5 22:17:57.359: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:58.313: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:58.313: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:58.313: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:58.320: INFO: Number of nodes with available pods: 2
Sep  5 22:17:58.320: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:17:59.317: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:59.317: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:59.317: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:17:59.331: INFO: Number of nodes with available pods: 2
Sep  5 22:17:59.331: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:00.348: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:00.348: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:00.348: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:00.371: INFO: Number of nodes with available pods: 2
Sep  5 22:18:00.371: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:01.315: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:01.315: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:01.315: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:01.330: INFO: Number of nodes with available pods: 2
Sep  5 22:18:01.330: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:02.315: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:02.316: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:02.316: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:02.325: INFO: Number of nodes with available pods: 2
Sep  5 22:18:02.325: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:03.319: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:03.319: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:03.319: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:03.334: INFO: Number of nodes with available pods: 2
Sep  5 22:18:03.334: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:04.318: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:04.318: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:04.318: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:04.325: INFO: Number of nodes with available pods: 2
Sep  5 22:18:04.325: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:05.315: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:05.315: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:05.315: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:05.322: INFO: Number of nodes with available pods: 2
Sep  5 22:18:05.322: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:06.318: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:06.318: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:06.318: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:06.326: INFO: Number of nodes with available pods: 2
Sep  5 22:18:06.326: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:07.314: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:07.314: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:07.314: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:07.327: INFO: Number of nodes with available pods: 2
Sep  5 22:18:07.327: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:08.324: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:08.324: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:08.324: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:08.330: INFO: Number of nodes with available pods: 2
Sep  5 22:18:08.330: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:09.316: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:09.316: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:09.316: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:09.324: INFO: Number of nodes with available pods: 2
Sep  5 22:18:09.325: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:10.315: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:10.315: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:10.315: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:10.321: INFO: Number of nodes with available pods: 2
Sep  5 22:18:10.321: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:11.322: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:11.322: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:11.322: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:11.329: INFO: Number of nodes with available pods: 2
Sep  5 22:18:11.329: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:12.314: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:12.314: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:12.314: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:12.322: INFO: Number of nodes with available pods: 2
Sep  5 22:18:12.322: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:13.316: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:13.316: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:13.316: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:13.323: INFO: Number of nodes with available pods: 2
Sep  5 22:18:13.323: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:14.322: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:14.322: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:14.322: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:14.331: INFO: Number of nodes with available pods: 2
Sep  5 22:18:14.331: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:15.318: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:15.318: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:15.318: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:15.324: INFO: Number of nodes with available pods: 2
Sep  5 22:18:15.324: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:16.315: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:16.315: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:16.315: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:16.321: INFO: Number of nodes with available pods: 2
Sep  5 22:18:16.321: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:17.315: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:17.315: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:17.315: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:17.321: INFO: Number of nodes with available pods: 2
Sep  5 22:18:17.321: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:18.326: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:18.326: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:18.326: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:18.332: INFO: Number of nodes with available pods: 2
Sep  5 22:18:18.332: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:19.317: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:19.318: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:19.318: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:19.326: INFO: Number of nodes with available pods: 2
Sep  5 22:18:19.326: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:20.319: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:20.319: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:20.319: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:20.330: INFO: Number of nodes with available pods: 2
Sep  5 22:18:20.330: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:21.317: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:21.318: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:21.318: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:21.327: INFO: Number of nodes with available pods: 2
Sep  5 22:18:21.327: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:22.314: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:22.314: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:22.314: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:22.333: INFO: Number of nodes with available pods: 2
Sep  5 22:18:22.333: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:23.315: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:23.315: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:23.315: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:23.323: INFO: Number of nodes with available pods: 2
Sep  5 22:18:23.323: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:24.317: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:24.317: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:24.317: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:24.324: INFO: Number of nodes with available pods: 2
Sep  5 22:18:24.324: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  5 22:18:25.316: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:25.316: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:25.316: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:18:25.325: INFO: Number of nodes with available pods: 3
Sep  5 22:18:25.325: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:96
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6687, will wait for the garbage collector to delete the pods
Sep  5 22:18:25.411: INFO: Deleting DaemonSet.extensions daemon-set took: 21.342815ms
Sep  5 22:18:25.511: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.49449ms
Sep  5 22:18:39.431: INFO: Number of nodes with available pods: 0
Sep  5 22:18:39.431: INFO: Number of running nodes: 0, number of available pods: 0
Sep  5 22:18:39.439: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6687/daemonsets","resourceVersion":"99510"},"items":null}

Sep  5 22:18:39.445: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6687/pods","resourceVersion":"99510"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:18:39.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6687" for this suite.

• [SLOW TEST:88.718 seconds]
[sig-apps] Daemon set [Serial]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":296,"completed":68,"skipped":1262,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:18:40.048: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-4188
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 22:18:41.087: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41" in namespace "security-context-test-4188" to be "Succeeded or Failed"
Sep  5 22:18:41.102: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Pending", Reason="", readiness=false. Elapsed: 14.927359ms
Sep  5 22:18:43.116: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029086922s
Sep  5 22:18:45.162: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Pending", Reason="", readiness=false. Elapsed: 4.075337725s
Sep  5 22:18:47.172: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0845502s
Sep  5 22:18:49.195: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Pending", Reason="", readiness=false. Elapsed: 8.108005322s
Sep  5 22:18:51.267: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Pending", Reason="", readiness=false. Elapsed: 10.179765087s
Sep  5 22:18:53.280: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Pending", Reason="", readiness=false. Elapsed: 12.193156007s
Sep  5 22:18:55.292: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Pending", Reason="", readiness=false. Elapsed: 14.20490967s
Sep  5 22:18:57.304: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Pending", Reason="", readiness=false. Elapsed: 16.217315262s
Sep  5 22:18:59.323: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Pending", Reason="", readiness=false. Elapsed: 18.236059171s
Sep  5 22:19:01.336: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Pending", Reason="", readiness=false. Elapsed: 20.249225516s
Sep  5 22:19:03.347: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Pending", Reason="", readiness=false. Elapsed: 22.26017561s
Sep  5 22:19:05.373: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Pending", Reason="", readiness=false. Elapsed: 24.285696501s
Sep  5 22:19:07.387: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Pending", Reason="", readiness=false. Elapsed: 26.29956561s
Sep  5 22:19:09.403: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Pending", Reason="", readiness=false. Elapsed: 28.31620868s
Sep  5 22:19:11.420: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Pending", Reason="", readiness=false. Elapsed: 30.332687892s
Sep  5 22:19:13.436: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Pending", Reason="", readiness=false. Elapsed: 32.349302109s
Sep  5 22:19:15.446: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Pending", Reason="", readiness=false. Elapsed: 34.358796528s
Sep  5 22:19:17.461: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Pending", Reason="", readiness=false. Elapsed: 36.373857438s
Sep  5 22:19:19.472: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 38.385206727s
Sep  5 22:19:19.472: INFO: Pod "alpine-nnp-false-394d865f-3102-42f0-95e0-33b7c5313a41" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:19:19.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4188" for this suite.

• [SLOW TEST:39.726 seconds]
[k8s.io] Security Context
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when creating containers with AllowPrivilegeEscalation
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":69,"skipped":1272,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:19:19.775: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7725
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl logs
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1392
STEP: creating an pod
Sep  5 22:19:20.509: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-7725 run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.21 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep  5 22:19:21.162: INFO: stderr: ""
Sep  5 22:19:21.162: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Waiting for log generator to start.
Sep  5 22:19:21.162: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep  5 22:19:21.162: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7725" to be "running and ready, or succeeded"
Sep  5 22:19:21.171: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 9.142711ms
Sep  5 22:19:23.182: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019712909s
Sep  5 22:19:25.191: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02944747s
Sep  5 22:19:27.205: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0429406s
Sep  5 22:19:29.222: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.060498738s
Sep  5 22:19:31.241: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.078998862s
Sep  5 22:19:33.256: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 12.094701417s
Sep  5 22:19:35.266: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 14.10406973s
Sep  5 22:19:37.285: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 16.123246837s
Sep  5 22:19:39.301: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 18.139646534s
Sep  5 22:19:41.312: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 20.149969146s
Sep  5 22:19:43.324: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 22.161731699s
Sep  5 22:19:45.333: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 24.170950203s
Sep  5 22:19:47.379: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 26.216954063s
Sep  5 22:19:49.393: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 28.231302477s
Sep  5 22:19:51.408: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 30.245815209s
Sep  5 22:19:51.408: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep  5 22:19:51.408: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Sep  5 22:19:51.408: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-7725 logs logs-generator logs-generator'
Sep  5 22:19:51.581: INFO: stderr: ""
Sep  5 22:19:51.581: INFO: stdout: "I0906 05:19:52.781742       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/xq8 425\nI0906 05:19:52.973630       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/2qzg 358\n"
STEP: limiting log lines
Sep  5 22:19:51.581: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-7725 logs logs-generator logs-generator --tail=1'
Sep  5 22:19:51.780: INFO: stderr: ""
Sep  5 22:19:51.780: INFO: stdout: "I0906 05:19:53.173488       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/5h5s 488\n"
Sep  5 22:19:51.780: INFO: got output "I0906 05:19:53.173488       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/5h5s 488\n"
STEP: limiting log bytes
Sep  5 22:19:51.780: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-7725 logs logs-generator logs-generator --limit-bytes=1'
Sep  5 22:19:51.923: INFO: stderr: ""
Sep  5 22:19:51.924: INFO: stdout: "I"
Sep  5 22:19:51.924: INFO: got output "I"
STEP: exposing timestamps
Sep  5 22:19:51.924: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-7725 logs logs-generator logs-generator --tail=1 --timestamps'
Sep  5 22:19:52.060: INFO: stderr: ""
Sep  5 22:19:52.060: INFO: stdout: "2021-09-06T05:19:53.57372596Z I0906 05:19:53.573561       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/qwl 212\n"
Sep  5 22:19:52.060: INFO: got output "2021-09-06T05:19:53.57372596Z I0906 05:19:53.573561       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/qwl 212\n"
STEP: restricting to a time range
Sep  5 22:19:54.561: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-7725 logs logs-generator logs-generator --since=1s'
Sep  5 22:19:54.712: INFO: stderr: ""
Sep  5 22:19:54.712: INFO: stdout: "I0906 05:19:55.374388       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/6xk 266\nI0906 05:19:55.573620       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/ws8 411\nI0906 05:19:55.773667       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/v2j9 412\nI0906 05:19:55.973991       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/lmz 431\nI0906 05:19:56.173687       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/wzc 591\n"
Sep  5 22:19:54.712: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-7725 logs logs-generator logs-generator --since=24h'
Sep  5 22:19:54.878: INFO: stderr: ""
Sep  5 22:19:54.878: INFO: stdout: "I0906 05:19:52.781742       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/xq8 425\nI0906 05:19:52.973630       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/2qzg 358\nI0906 05:19:53.173488       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/5h5s 488\nI0906 05:19:53.373661       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/5h6t 257\nI0906 05:19:53.573561       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/qwl 212\nI0906 05:19:53.773642       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/hftr 574\nI0906 05:19:53.973661       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/94mz 483\nI0906 05:19:54.173789       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/lnz9 294\nI0906 05:19:54.373928       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/jvz 521\nI0906 05:19:54.574108       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/vv5 487\nI0906 05:19:54.773657       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/l5w7 343\nI0906 05:19:54.973729       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/67n 249\nI0906 05:19:55.173762       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/b4qc 416\nI0906 05:19:55.374388       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/6xk 266\nI0906 05:19:55.573620       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/ws8 411\nI0906 05:19:55.773667       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/v2j9 412\nI0906 05:19:55.973991       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/lmz 431\nI0906 05:19:56.173687       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/wzc 591\nI0906 05:19:56.373981       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/lcm5 297\n"
[AfterEach] Kubectl logs
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
Sep  5 22:19:54.878: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-7725 delete pod logs-generator'
Sep  5 22:20:08.102: INFO: stderr: ""
Sep  5 22:20:08.102: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:20:08.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7725" for this suite.

• [SLOW TEST:48.795 seconds]
[sig-cli] Kubectl client
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1389
    should be able to retrieve and filter logs  [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":296,"completed":70,"skipped":1331,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:20:08.571: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7266
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:20:25.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7266" for this suite.

• [SLOW TEST:17.111 seconds]
[sig-api-machinery] ResourceQuota
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":296,"completed":71,"skipped":1360,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:20:25.682: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6341
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:23:39.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6341" for this suite.

• [SLOW TEST:194.199 seconds]
[k8s.io] Container Runtime
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":296,"completed":72,"skipped":1364,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:23:39.882: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9803
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Sep  5 22:24:45.903: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:24:45.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9803" for this suite.

• [SLOW TEST:66.384 seconds]
[sig-api-machinery] Garbage collector
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":296,"completed":73,"skipped":1365,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Service endpoints latency
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:24:46.266: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-569
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 22:24:46.718: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: creating replication controller svc-latency-rc in namespace svc-latency-569
Sep  5 22:25:12.965: INFO: Created: latency-svc-k6k9m
Sep  5 22:25:13.038: INFO: Got endpoints: latency-svc-k6k9m [124.877212ms]
Sep  5 22:25:13.104: INFO: Created: latency-svc-v8cnc
Sep  5 22:25:13.147: INFO: Got endpoints: latency-svc-v8cnc [107.982244ms]
Sep  5 22:25:13.180: INFO: Created: latency-svc-m2spl
Sep  5 22:25:13.282: INFO: Got endpoints: latency-svc-m2spl [243.316261ms]
Sep  5 22:25:13.303: INFO: Created: latency-svc-66zfv
Sep  5 22:25:13.448: INFO: Created: latency-svc-lhssm
Sep  5 22:25:13.457: INFO: Got endpoints: latency-svc-66zfv [417.565939ms]
Sep  5 22:25:13.513: INFO: Got endpoints: latency-svc-lhssm [473.616933ms]
Sep  5 22:25:13.541: INFO: Created: latency-svc-qt89d
Sep  5 22:25:13.588: INFO: Got endpoints: latency-svc-qt89d [548.985965ms]
Sep  5 22:25:13.651: INFO: Created: latency-svc-vxfm2
Sep  5 22:25:14.390: INFO: Got endpoints: latency-svc-vxfm2 [1.35096656s]
Sep  5 22:25:14.406: INFO: Created: latency-svc-tjff4
Sep  5 22:25:14.416: INFO: Created: latency-svc-k87l2
Sep  5 22:25:14.526: INFO: Created: latency-svc-gjjjr
Sep  5 22:25:14.595: INFO: Got endpoints: latency-svc-tjff4 [1.555107656s]
Sep  5 22:25:14.595: INFO: Got endpoints: latency-svc-k87l2 [1.555269836s]
Sep  5 22:25:14.638: INFO: Got endpoints: latency-svc-gjjjr [1.598309722s]
Sep  5 22:25:14.687: INFO: Created: latency-svc-hr88h
Sep  5 22:25:14.763: INFO: Created: latency-svc-gq5wq
Sep  5 22:25:14.774: INFO: Got endpoints: latency-svc-hr88h [1.735014443s]
Sep  5 22:25:14.850: INFO: Got endpoints: latency-svc-gq5wq [254.925549ms]
Sep  5 22:25:14.851: INFO: Created: latency-svc-9r6s8
Sep  5 22:25:14.881: INFO: Got endpoints: latency-svc-9r6s8 [1.841176516s]
Sep  5 22:25:14.901: INFO: Created: latency-svc-56k5v
Sep  5 22:25:14.961: INFO: Got endpoints: latency-svc-56k5v [1.921011336s]
Sep  5 22:25:14.989: INFO: Created: latency-svc-fvzs6
Sep  5 22:25:15.060: INFO: Got endpoints: latency-svc-fvzs6 [2.020477343s]
Sep  5 22:25:15.079: INFO: Created: latency-svc-q2g6q
Sep  5 22:25:15.111: INFO: Created: latency-svc-gcxb8
Sep  5 22:25:15.169: INFO: Got endpoints: latency-svc-q2g6q [2.129600706s]
Sep  5 22:25:15.223: INFO: Got endpoints: latency-svc-gcxb8 [2.183610074s]
Sep  5 22:25:15.260: INFO: Created: latency-svc-wlvtd
Sep  5 22:25:15.287: INFO: Got endpoints: latency-svc-wlvtd [2.140596823s]
Sep  5 22:25:15.358: INFO: Created: latency-svc-4fbmm
Sep  5 22:25:15.451: INFO: Created: latency-svc-4lhl5
Sep  5 22:25:15.454: INFO: Got endpoints: latency-svc-4fbmm [2.171914232s]
Sep  5 22:25:15.500: INFO: Got endpoints: latency-svc-4lhl5 [2.043063999s]
Sep  5 22:25:15.522: INFO: Created: latency-svc-vtnwp
Sep  5 22:25:15.535: INFO: Got endpoints: latency-svc-vtnwp [2.021960999s]
Sep  5 22:25:15.554: INFO: Created: latency-svc-xdlm6
Sep  5 22:25:15.591: INFO: Got endpoints: latency-svc-xdlm6 [2.002970208s]
Sep  5 22:25:15.657: INFO: Created: latency-svc-qdwpl
Sep  5 22:25:15.718: INFO: Created: latency-svc-97xrk
Sep  5 22:25:15.732: INFO: Got endpoints: latency-svc-qdwpl [1.342072414s]
Sep  5 22:25:15.789: INFO: Created: latency-svc-tdt4t
Sep  5 22:25:15.832: INFO: Got endpoints: latency-svc-tdt4t [1.194229316s]
Sep  5 22:25:15.832: INFO: Got endpoints: latency-svc-97xrk [1.237686896s]
Sep  5 22:25:15.898: INFO: Created: latency-svc-7zwkw
Sep  5 22:25:15.932: INFO: Created: latency-svc-n2nbw
Sep  5 22:25:15.969: INFO: Got endpoints: latency-svc-7zwkw [1.194451637s]
Sep  5 22:25:16.029: INFO: Created: latency-svc-tzpzw
Sep  5 22:25:16.057: INFO: Got endpoints: latency-svc-n2nbw [1.207762037s]
Sep  5 22:25:16.104: INFO: Created: latency-svc-l85cs
Sep  5 22:25:16.192: INFO: Got endpoints: latency-svc-l85cs [1.231192279s]
Sep  5 22:25:16.204: INFO: Got endpoints: latency-svc-tzpzw [1.323588111s]
Sep  5 22:25:16.235: INFO: Created: latency-svc-l8r9q
Sep  5 22:25:16.292: INFO: Got endpoints: latency-svc-l8r9q [1.23243331s]
Sep  5 22:25:16.319: INFO: Created: latency-svc-clwzl
Sep  5 22:25:16.366: INFO: Got endpoints: latency-svc-clwzl [1.196969993s]
Sep  5 22:25:16.402: INFO: Created: latency-svc-lmvnz
Sep  5 22:25:16.460: INFO: Got endpoints: latency-svc-lmvnz [1.237046218s]
Sep  5 22:25:16.479: INFO: Created: latency-svc-wfwhd
Sep  5 22:25:16.523: INFO: Got endpoints: latency-svc-wfwhd [1.236156599s]
Sep  5 22:25:16.545: INFO: Created: latency-svc-g68gp
Sep  5 22:25:16.595: INFO: Got endpoints: latency-svc-g68gp [1.140816214s]
Sep  5 22:25:16.618: INFO: Created: latency-svc-696xz
Sep  5 22:25:16.671: INFO: Created: latency-svc-dlqms
Sep  5 22:25:16.675: INFO: Got endpoints: latency-svc-696xz [1.175670944s]
Sep  5 22:25:16.718: INFO: Got endpoints: latency-svc-dlqms [1.182770341s]
Sep  5 22:25:16.785: INFO: Created: latency-svc-htnxg
Sep  5 22:25:16.857: INFO: Got endpoints: latency-svc-htnxg [1.266148012s]
Sep  5 22:25:16.928: INFO: Created: latency-svc-qwnx5
Sep  5 22:25:16.986: INFO: Got endpoints: latency-svc-qwnx5 [1.25319781s]
Sep  5 22:25:16.994: INFO: Created: latency-svc-l24l5
Sep  5 22:25:17.286: INFO: Created: latency-svc-dvfmg
Sep  5 22:25:17.308: INFO: Got endpoints: latency-svc-l24l5 [1.475458993s]
Sep  5 22:25:17.365: INFO: Got endpoints: latency-svc-dvfmg [1.533085591s]
Sep  5 22:25:17.410: INFO: Created: latency-svc-k86hl
Sep  5 22:25:17.469: INFO: Got endpoints: latency-svc-k86hl [1.499601017s]
Sep  5 22:25:17.483: INFO: Created: latency-svc-qmf4w
Sep  5 22:25:17.541: INFO: Got endpoints: latency-svc-qmf4w [1.483316889s]
Sep  5 22:25:17.559: INFO: Created: latency-svc-fv6q6
Sep  5 22:25:17.613: INFO: Got endpoints: latency-svc-fv6q6 [1.40862017s]
Sep  5 22:25:17.614: INFO: Created: latency-svc-klmtz
Sep  5 22:25:17.674: INFO: Got endpoints: latency-svc-klmtz [1.482211207s]
Sep  5 22:25:17.676: INFO: Created: latency-svc-9qxhs
Sep  5 22:25:17.687: INFO: Created: latency-svc-vptlz
Sep  5 22:25:17.702: INFO: Got endpoints: latency-svc-9qxhs [1.409602298s]
Sep  5 22:25:17.723: INFO: Got endpoints: latency-svc-vptlz [1.357025271s]
Sep  5 22:25:17.761: INFO: Created: latency-svc-ntfv9
Sep  5 22:25:17.795: INFO: Created: latency-svc-fd5q2
Sep  5 22:25:17.796: INFO: Got endpoints: latency-svc-ntfv9 [1.336142711s]
Sep  5 22:25:17.825: INFO: Got endpoints: latency-svc-fd5q2 [1.301604372s]
Sep  5 22:25:17.837: INFO: Created: latency-svc-hxqw8
Sep  5 22:25:17.863: INFO: Created: latency-svc-tqv76
Sep  5 22:25:17.887: INFO: Got endpoints: latency-svc-hxqw8 [1.291495087s]
Sep  5 22:25:17.934: INFO: Got endpoints: latency-svc-tqv76 [1.258146254s]
Sep  5 22:25:17.994: INFO: Created: latency-svc-48ft5
Sep  5 22:25:18.020: INFO: Created: latency-svc-8jdbc
Sep  5 22:25:18.053: INFO: Created: latency-svc-lls67
Sep  5 22:25:18.056: INFO: Got endpoints: latency-svc-48ft5 [1.338472411s]
Sep  5 22:25:18.081: INFO: Got endpoints: latency-svc-8jdbc [1.223148064s]
Sep  5 22:25:18.098: INFO: Created: latency-svc-sqt7f
Sep  5 22:25:18.127: INFO: Created: latency-svc-db7cp
Sep  5 22:25:18.144: INFO: Got endpoints: latency-svc-sqt7f [836.706487ms]
Sep  5 22:25:18.144: INFO: Got endpoints: latency-svc-lls67 [1.158527513s]
Sep  5 22:25:18.183: INFO: Got endpoints: latency-svc-db7cp [817.853211ms]
Sep  5 22:25:18.206: INFO: Created: latency-svc-jwjcn
Sep  5 22:25:18.206: INFO: Created: latency-svc-g724h
Sep  5 22:25:18.239: INFO: Got endpoints: latency-svc-g724h [698.094583ms]
Sep  5 22:25:18.239: INFO: Got endpoints: latency-svc-jwjcn [770.687515ms]
Sep  5 22:25:18.273: INFO: Created: latency-svc-9gdvj
Sep  5 22:25:18.299: INFO: Created: latency-svc-h8gnm
Sep  5 22:25:18.319: INFO: Got endpoints: latency-svc-9gdvj [705.800205ms]
Sep  5 22:25:18.335: INFO: Got endpoints: latency-svc-h8gnm [660.951855ms]
Sep  5 22:25:18.369: INFO: Created: latency-svc-z9gzg
Sep  5 22:25:18.407: INFO: Got endpoints: latency-svc-z9gzg [704.396721ms]
Sep  5 22:25:18.422: INFO: Created: latency-svc-njfgl
Sep  5 22:25:18.443: INFO: Got endpoints: latency-svc-njfgl [719.267052ms]
Sep  5 22:25:18.461: INFO: Created: latency-svc-d75pw
Sep  5 22:25:18.494: INFO: Got endpoints: latency-svc-d75pw [697.642839ms]
Sep  5 22:25:18.545: INFO: Created: latency-svc-vwcb5
Sep  5 22:25:18.560: INFO: Created: latency-svc-hcn6n
Sep  5 22:25:18.603: INFO: Got endpoints: latency-svc-vwcb5 [778.03012ms]
Sep  5 22:25:18.613: INFO: Got endpoints: latency-svc-hcn6n [726.055316ms]
Sep  5 22:25:18.683: INFO: Created: latency-svc-tvvhl
Sep  5 22:25:18.732: INFO: Got endpoints: latency-svc-tvvhl [798.705445ms]
Sep  5 22:25:18.765: INFO: Created: latency-svc-gl5rr
Sep  5 22:25:18.768: INFO: Created: latency-svc-ssstt
Sep  5 22:25:18.828: INFO: Created: latency-svc-85j7w
Sep  5 22:25:18.832: INFO: Got endpoints: latency-svc-ssstt [751.724352ms]
Sep  5 22:25:18.833: INFO: Got endpoints: latency-svc-gl5rr [776.308039ms]
Sep  5 22:25:18.858: INFO: Got endpoints: latency-svc-85j7w [713.938551ms]
Sep  5 22:25:18.887: INFO: Created: latency-svc-xv72p
Sep  5 22:25:18.943: INFO: Created: latency-svc-hvxr5
Sep  5 22:25:18.961: INFO: Got endpoints: latency-svc-xv72p [816.203111ms]
Sep  5 22:25:18.972: INFO: Got endpoints: latency-svc-hvxr5 [788.847327ms]
Sep  5 22:25:18.996: INFO: Created: latency-svc-dnk9g
Sep  5 22:25:19.041: INFO: Got endpoints: latency-svc-dnk9g [802.07324ms]
Sep  5 22:25:19.061: INFO: Created: latency-svc-zh9v2
Sep  5 22:25:19.109: INFO: Got endpoints: latency-svc-zh9v2 [870.160373ms]
Sep  5 22:25:19.149: INFO: Created: latency-svc-x2c76
Sep  5 22:25:19.157: INFO: Got endpoints: latency-svc-x2c76 [838.379237ms]
Sep  5 22:25:19.170: INFO: Created: latency-svc-vbr2n
Sep  5 22:25:19.311: INFO: Got endpoints: latency-svc-vbr2n [975.762847ms]
Sep  5 22:25:19.356: INFO: Created: latency-svc-gc76b
Sep  5 22:25:19.406: INFO: Created: latency-svc-82894
Sep  5 22:25:19.452: INFO: Got endpoints: latency-svc-gc76b [1.045766342s]
Sep  5 22:25:19.453: INFO: Created: latency-svc-26z64
Sep  5 22:25:19.470: INFO: Got endpoints: latency-svc-82894 [1.027705961s]
Sep  5 22:25:19.502: INFO: Got endpoints: latency-svc-26z64 [1.007639993s]
Sep  5 22:25:19.545: INFO: Created: latency-svc-cw9zl
Sep  5 22:25:19.596: INFO: Got endpoints: latency-svc-cw9zl [983.494681ms]
Sep  5 22:25:19.608: INFO: Created: latency-svc-vd8wh
Sep  5 22:25:19.638: INFO: Got endpoints: latency-svc-vd8wh [1.03486578s]
Sep  5 22:25:19.684: INFO: Created: latency-svc-jh9dz
Sep  5 22:25:19.761: INFO: Created: latency-svc-w6tm5
Sep  5 22:25:19.789: INFO: Got endpoints: latency-svc-jh9dz [1.056672569s]
Sep  5 22:25:19.810: INFO: Got endpoints: latency-svc-w6tm5 [977.379933ms]
Sep  5 22:25:19.851: INFO: Created: latency-svc-vm6xb
Sep  5 22:25:19.884: INFO: Got endpoints: latency-svc-vm6xb [1.051549076s]
Sep  5 22:25:19.926: INFO: Created: latency-svc-98hp5
Sep  5 22:25:19.952: INFO: Created: latency-svc-4jm6w
Sep  5 22:25:19.980: INFO: Got endpoints: latency-svc-98hp5 [1.121808849s]
Sep  5 22:25:20.021: INFO: Got endpoints: latency-svc-4jm6w [1.059698494s]
Sep  5 22:25:20.049: INFO: Created: latency-svc-dpq8c
Sep  5 22:25:20.102: INFO: Created: latency-svc-w6bnk
Sep  5 22:25:20.138: INFO: Got endpoints: latency-svc-w6bnk [1.096631802s]
Sep  5 22:25:20.138: INFO: Got endpoints: latency-svc-dpq8c [1.165857343s]
Sep  5 22:25:20.159: INFO: Created: latency-svc-qvtkr
Sep  5 22:25:20.217: INFO: Created: latency-svc-wxlpq
Sep  5 22:25:20.239: INFO: Got endpoints: latency-svc-qvtkr [1.129693127s]
Sep  5 22:25:20.247: INFO: Got endpoints: latency-svc-wxlpq [1.089134026s]
Sep  5 22:25:20.248: INFO: Created: latency-svc-2mlc7
Sep  5 22:25:20.284: INFO: Got endpoints: latency-svc-2mlc7 [972.713673ms]
Sep  5 22:25:20.342: INFO: Created: latency-svc-vrrmz
Sep  5 22:25:20.406: INFO: Created: latency-svc-74hjp
Sep  5 22:25:20.408: INFO: Got endpoints: latency-svc-vrrmz [955.955322ms]
Sep  5 22:25:20.490: INFO: Created: latency-svc-nbd65
Sep  5 22:25:20.497: INFO: Got endpoints: latency-svc-74hjp [1.027061905s]
Sep  5 22:25:20.529: INFO: Got endpoints: latency-svc-nbd65 [1.027521769s]
Sep  5 22:25:20.556: INFO: Created: latency-svc-sgzbj
Sep  5 22:25:20.589: INFO: Created: latency-svc-blkdp
Sep  5 22:25:20.602: INFO: Got endpoints: latency-svc-sgzbj [1.005552025s]
Sep  5 22:25:20.622: INFO: Created: latency-svc-n5k97
Sep  5 22:25:20.639: INFO: Got endpoints: latency-svc-blkdp [1.000578757s]
Sep  5 22:25:20.660: INFO: Got endpoints: latency-svc-n5k97 [870.741868ms]
Sep  5 22:25:20.683: INFO: Created: latency-svc-pqbtb
Sep  5 22:25:20.717: INFO: Got endpoints: latency-svc-pqbtb [906.965695ms]
Sep  5 22:25:20.735: INFO: Created: latency-svc-f6rjz
Sep  5 22:25:20.773: INFO: Created: latency-svc-8ngzd
Sep  5 22:25:20.785: INFO: Got endpoints: latency-svc-f6rjz [900.554171ms]
Sep  5 22:25:20.829: INFO: Got endpoints: latency-svc-8ngzd [848.549548ms]
Sep  5 22:25:20.842: INFO: Created: latency-svc-lgxnk
Sep  5 22:25:20.865: INFO: Got endpoints: latency-svc-lgxnk [844.180968ms]
Sep  5 22:25:20.890: INFO: Created: latency-svc-g2rln
Sep  5 22:25:20.916: INFO: Got endpoints: latency-svc-g2rln [777.756426ms]
Sep  5 22:25:20.935: INFO: Created: latency-svc-kszb8
Sep  5 22:25:20.982: INFO: Got endpoints: latency-svc-kszb8 [843.837082ms]
Sep  5 22:25:21.020: INFO: Created: latency-svc-qt5v9
Sep  5 22:25:21.042: INFO: Got endpoints: latency-svc-qt5v9 [802.556687ms]
Sep  5 22:25:21.055: INFO: Created: latency-svc-sqxrk
Sep  5 22:25:21.085: INFO: Created: latency-svc-c5pz7
Sep  5 22:25:21.099: INFO: Got endpoints: latency-svc-sqxrk [852.252286ms]
Sep  5 22:25:21.122: INFO: Got endpoints: latency-svc-c5pz7 [837.823333ms]
Sep  5 22:25:21.149: INFO: Created: latency-svc-5q889
Sep  5 22:25:21.185: INFO: Created: latency-svc-6jg7m
Sep  5 22:25:21.202: INFO: Got endpoints: latency-svc-5q889 [793.023344ms]
Sep  5 22:25:21.216: INFO: Got endpoints: latency-svc-6jg7m [718.629541ms]
Sep  5 22:25:21.275: INFO: Created: latency-svc-plvd9
Sep  5 22:25:21.321: INFO: Got endpoints: latency-svc-plvd9 [791.185077ms]
Sep  5 22:25:21.355: INFO: Created: latency-svc-pjbdm
Sep  5 22:25:21.392: INFO: Got endpoints: latency-svc-pjbdm [790.3446ms]
Sep  5 22:25:21.400: INFO: Created: latency-svc-m7p5d
Sep  5 22:25:21.445: INFO: Got endpoints: latency-svc-m7p5d [806.403236ms]
Sep  5 22:25:21.465: INFO: Created: latency-svc-9mdms
Sep  5 22:25:21.496: INFO: Got endpoints: latency-svc-9mdms [836.438745ms]
Sep  5 22:25:21.514: INFO: Created: latency-svc-gz7w9
Sep  5 22:25:21.555: INFO: Got endpoints: latency-svc-gz7w9 [837.782438ms]
Sep  5 22:25:21.594: INFO: Created: latency-svc-6tfvr
Sep  5 22:25:21.606: INFO: Created: latency-svc-p9xwk
Sep  5 22:25:21.622: INFO: Got endpoints: latency-svc-6tfvr [836.927361ms]
Sep  5 22:25:21.637: INFO: Got endpoints: latency-svc-p9xwk [807.759303ms]
Sep  5 22:25:21.679: INFO: Created: latency-svc-z575x
Sep  5 22:25:21.709: INFO: Got endpoints: latency-svc-z575x [843.899422ms]
Sep  5 22:25:21.712: INFO: Created: latency-svc-79tg7
Sep  5 22:25:21.750: INFO: Got endpoints: latency-svc-79tg7 [833.725388ms]
Sep  5 22:25:21.767: INFO: Created: latency-svc-9vwrl
Sep  5 22:25:21.783: INFO: Got endpoints: latency-svc-9vwrl [800.96607ms]
Sep  5 22:25:21.811: INFO: Created: latency-svc-bgsrj
Sep  5 22:25:21.829: INFO: Got endpoints: latency-svc-bgsrj [786.963777ms]
Sep  5 22:25:21.844: INFO: Created: latency-svc-kvdc5
Sep  5 22:25:21.874: INFO: Got endpoints: latency-svc-kvdc5 [774.963041ms]
Sep  5 22:25:21.888: INFO: Created: latency-svc-ng4f5
Sep  5 22:25:21.923: INFO: Got endpoints: latency-svc-ng4f5 [801.294255ms]
Sep  5 22:25:21.935: INFO: Created: latency-svc-h9jpr
Sep  5 22:25:21.964: INFO: Got endpoints: latency-svc-h9jpr [762.874709ms]
Sep  5 22:25:21.977: INFO: Created: latency-svc-2tsgl
Sep  5 22:25:22.007: INFO: Got endpoints: latency-svc-2tsgl [790.827294ms]
Sep  5 22:25:22.015: INFO: Created: latency-svc-cp6dm
Sep  5 22:25:22.059: INFO: Created: latency-svc-754q2
Sep  5 22:25:22.067: INFO: Got endpoints: latency-svc-cp6dm [746.219652ms]
Sep  5 22:25:22.090: INFO: Got endpoints: latency-svc-754q2 [697.895408ms]
Sep  5 22:25:22.166: INFO: Created: latency-svc-58fq2
Sep  5 22:25:22.216: INFO: Got endpoints: latency-svc-58fq2 [770.789831ms]
Sep  5 22:25:22.259: INFO: Created: latency-svc-hc87g
Sep  5 22:25:22.268: INFO: Created: latency-svc-92mn5
Sep  5 22:25:22.300: INFO: Got endpoints: latency-svc-hc87g [745.23641ms]
Sep  5 22:25:22.325: INFO: Got endpoints: latency-svc-92mn5 [828.09521ms]
Sep  5 22:25:22.340: INFO: Created: latency-svc-h2s5p
Sep  5 22:25:22.401: INFO: Got endpoints: latency-svc-h2s5p [779.284502ms]
Sep  5 22:25:22.416: INFO: Created: latency-svc-kmt6p
Sep  5 22:25:22.446: INFO: Got endpoints: latency-svc-kmt6p [809.737713ms]
Sep  5 22:25:22.474: INFO: Created: latency-svc-xg6l4
Sep  5 22:25:22.516: INFO: Got endpoints: latency-svc-xg6l4 [807.026623ms]
Sep  5 22:25:22.554: INFO: Created: latency-svc-bg79v
Sep  5 22:25:22.599: INFO: Created: latency-svc-6sd9d
Sep  5 22:25:22.619: INFO: Got endpoints: latency-svc-bg79v [868.873487ms]
Sep  5 22:25:22.649: INFO: Got endpoints: latency-svc-6sd9d [866.220692ms]
Sep  5 22:25:22.660: INFO: Created: latency-svc-xrvpf
Sep  5 22:25:22.690: INFO: Created: latency-svc-mqq6k
Sep  5 22:25:22.721: INFO: Got endpoints: latency-svc-xrvpf [892.704728ms]
Sep  5 22:25:22.760: INFO: Created: latency-svc-6xrpr
Sep  5 22:25:22.773: INFO: Got endpoints: latency-svc-mqq6k [899.028599ms]
Sep  5 22:25:22.815: INFO: Got endpoints: latency-svc-6xrpr [892.137807ms]
Sep  5 22:25:22.828: INFO: Created: latency-svc-4tq7v
Sep  5 22:25:22.859: INFO: Created: latency-svc-kl5ng
Sep  5 22:25:22.883: INFO: Got endpoints: latency-svc-4tq7v [918.279728ms]
Sep  5 22:25:22.889: INFO: Created: latency-svc-j6r9w
Sep  5 22:25:22.928: INFO: Got endpoints: latency-svc-kl5ng [920.382536ms]
Sep  5 22:25:22.945: INFO: Got endpoints: latency-svc-j6r9w [878.204923ms]
Sep  5 22:25:22.960: INFO: Created: latency-svc-zm29v
Sep  5 22:25:22.992: INFO: Got endpoints: latency-svc-zm29v [901.912719ms]
Sep  5 22:25:23.021: INFO: Created: latency-svc-mshrm
Sep  5 22:25:23.082: INFO: Got endpoints: latency-svc-mshrm [866.286769ms]
Sep  5 22:25:23.101: INFO: Created: latency-svc-jk9l2
Sep  5 22:25:23.119: INFO: Created: latency-svc-7wbw4
Sep  5 22:25:23.141: INFO: Got endpoints: latency-svc-jk9l2 [840.45722ms]
Sep  5 22:25:23.181: INFO: Created: latency-svc-qlpzh
Sep  5 22:25:23.241: INFO: Got endpoints: latency-svc-qlpzh [840.091514ms]
Sep  5 22:25:23.241: INFO: Got endpoints: latency-svc-7wbw4 [916.617476ms]
Sep  5 22:25:23.258: INFO: Created: latency-svc-787kl
Sep  5 22:25:23.318: INFO: Created: latency-svc-775kd
Sep  5 22:25:23.321: INFO: Got endpoints: latency-svc-787kl [874.994349ms]
Sep  5 22:25:23.334: INFO: Got endpoints: latency-svc-775kd [818.230058ms]
Sep  5 22:25:23.370: INFO: Created: latency-svc-b4dbc
Sep  5 22:25:23.388: INFO: Got endpoints: latency-svc-b4dbc [769.240883ms]
Sep  5 22:25:23.430: INFO: Created: latency-svc-cvbdg
Sep  5 22:25:23.496: INFO: Created: latency-svc-wx868
Sep  5 22:25:23.535: INFO: Got endpoints: latency-svc-cvbdg [885.848254ms]
Sep  5 22:25:23.544: INFO: Got endpoints: latency-svc-wx868 [822.962462ms]
Sep  5 22:25:23.590: INFO: Created: latency-svc-kzpfp
Sep  5 22:25:23.626: INFO: Created: latency-svc-9ppn6
Sep  5 22:25:23.650: INFO: Got endpoints: latency-svc-kzpfp [876.979364ms]
Sep  5 22:25:23.702: INFO: Got endpoints: latency-svc-9ppn6 [886.608839ms]
Sep  5 22:25:23.723: INFO: Created: latency-svc-hwknv
Sep  5 22:25:23.785: INFO: Created: latency-svc-d685c
Sep  5 22:25:23.804: INFO: Got endpoints: latency-svc-hwknv [921.627474ms]
Sep  5 22:25:23.853: INFO: Created: latency-svc-ccbxt
Sep  5 22:25:23.920: INFO: Created: latency-svc-fzjd6
Sep  5 22:25:23.920: INFO: Got endpoints: latency-svc-d685c [992.541697ms]
Sep  5 22:25:23.920: INFO: Got endpoints: latency-svc-ccbxt [974.901379ms]
Sep  5 22:25:23.920: INFO: Got endpoints: latency-svc-fzjd6 [927.97427ms]
Sep  5 22:25:23.921: INFO: Created: latency-svc-rbbrh
Sep  5 22:25:23.950: INFO: Got endpoints: latency-svc-rbbrh [867.84029ms]
Sep  5 22:25:23.965: INFO: Created: latency-svc-p4rtb
Sep  5 22:25:23.992: INFO: Got endpoints: latency-svc-p4rtb [851.532788ms]
Sep  5 22:25:24.042: INFO: Created: latency-svc-pnjg7
Sep  5 22:25:24.082: INFO: Created: latency-svc-xk25b
Sep  5 22:25:24.097: INFO: Got endpoints: latency-svc-pnjg7 [855.356725ms]
Sep  5 22:25:24.106: INFO: Created: latency-svc-d9hff
Sep  5 22:25:24.144: INFO: Got endpoints: latency-svc-xk25b [902.430344ms]
Sep  5 22:25:24.182: INFO: Got endpoints: latency-svc-d9hff [860.303772ms]
Sep  5 22:25:24.191: INFO: Created: latency-svc-q84l7
Sep  5 22:25:24.206: INFO: Created: latency-svc-cpdhg
Sep  5 22:25:24.229: INFO: Got endpoints: latency-svc-q84l7 [895.205162ms]
Sep  5 22:25:24.243: INFO: Got endpoints: latency-svc-cpdhg [854.775765ms]
Sep  5 22:25:24.256: INFO: Created: latency-svc-2ml4z
Sep  5 22:25:24.301: INFO: Got endpoints: latency-svc-2ml4z [765.882055ms]
Sep  5 22:25:24.310: INFO: Created: latency-svc-npswt
Sep  5 22:25:24.338: INFO: Got endpoints: latency-svc-npswt [794.084394ms]
Sep  5 22:25:24.354: INFO: Created: latency-svc-l9rv2
Sep  5 22:25:24.383: INFO: Got endpoints: latency-svc-l9rv2 [732.592136ms]
Sep  5 22:25:24.407: INFO: Created: latency-svc-bsl84
Sep  5 22:25:24.431: INFO: Got endpoints: latency-svc-bsl84 [729.242718ms]
Sep  5 22:25:24.452: INFO: Created: latency-svc-n8xrh
Sep  5 22:25:24.528: INFO: Got endpoints: latency-svc-n8xrh [723.012234ms]
Sep  5 22:25:24.537: INFO: Created: latency-svc-28wz5
Sep  5 22:25:24.662: INFO: Got endpoints: latency-svc-28wz5 [741.804935ms]
Sep  5 22:25:24.665: INFO: Created: latency-svc-5tb7t
Sep  5 22:25:24.701: INFO: Got endpoints: latency-svc-5tb7t [781.075671ms]
Sep  5 22:25:24.730: INFO: Created: latency-svc-phxh4
Sep  5 22:25:24.762: INFO: Got endpoints: latency-svc-phxh4 [841.721934ms]
Sep  5 22:25:24.766: INFO: Created: latency-svc-9q82r
Sep  5 22:25:24.789: INFO: Got endpoints: latency-svc-9q82r [838.967117ms]
Sep  5 22:25:24.814: INFO: Created: latency-svc-59c4v
Sep  5 22:25:24.849: INFO: Got endpoints: latency-svc-59c4v [856.684404ms]
Sep  5 22:25:24.850: INFO: Created: latency-svc-lmxbm
Sep  5 22:25:24.882: INFO: Got endpoints: latency-svc-lmxbm [785.214401ms]
Sep  5 22:25:24.882: INFO: Created: latency-svc-f6crc
Sep  5 22:25:24.905: INFO: Created: latency-svc-k4gsv
Sep  5 22:25:24.921: INFO: Got endpoints: latency-svc-f6crc [776.710685ms]
Sep  5 22:25:24.932: INFO: Got endpoints: latency-svc-k4gsv [750.497789ms]
Sep  5 22:25:24.952: INFO: Created: latency-svc-vrp7t
Sep  5 22:25:24.972: INFO: Got endpoints: latency-svc-vrp7t [742.246176ms]
Sep  5 22:25:24.990: INFO: Created: latency-svc-kgqcw
Sep  5 22:25:25.028: INFO: Got endpoints: latency-svc-kgqcw [785.53577ms]
Sep  5 22:25:25.055: INFO: Created: latency-svc-7nv9r
Sep  5 22:25:25.076: INFO: Got endpoints: latency-svc-7nv9r [774.892099ms]
Sep  5 22:25:25.077: INFO: Created: latency-svc-gzj5l
Sep  5 22:25:25.121: INFO: Created: latency-svc-b5wmf
Sep  5 22:25:25.148: INFO: Got endpoints: latency-svc-gzj5l [809.694489ms]
Sep  5 22:25:25.161: INFO: Got endpoints: latency-svc-b5wmf [778.272783ms]
Sep  5 22:25:25.194: INFO: Created: latency-svc-cdzm9
Sep  5 22:25:25.220: INFO: Got endpoints: latency-svc-cdzm9 [788.968408ms]
Sep  5 22:25:25.228: INFO: Created: latency-svc-dls8g
Sep  5 22:25:25.265: INFO: Got endpoints: latency-svc-dls8g [737.21787ms]
Sep  5 22:25:25.318: INFO: Created: latency-svc-cd9rs
Sep  5 22:25:25.370: INFO: Got endpoints: latency-svc-cd9rs [707.353443ms]
Sep  5 22:25:25.375: INFO: Created: latency-svc-jddgv
Sep  5 22:25:25.408: INFO: Got endpoints: latency-svc-jddgv [706.373605ms]
Sep  5 22:25:25.428: INFO: Created: latency-svc-5kdjr
Sep  5 22:25:25.460: INFO: Got endpoints: latency-svc-5kdjr [698.21435ms]
Sep  5 22:25:25.474: INFO: Created: latency-svc-pcptl
Sep  5 22:25:25.505: INFO: Got endpoints: latency-svc-pcptl [715.931523ms]
Sep  5 22:25:25.526: INFO: Created: latency-svc-6cd4s
Sep  5 22:25:25.556: INFO: Got endpoints: latency-svc-6cd4s [706.765528ms]
Sep  5 22:25:25.574: INFO: Created: latency-svc-hzlf8
Sep  5 22:25:25.612: INFO: Got endpoints: latency-svc-hzlf8 [729.802358ms]
Sep  5 22:25:25.620: INFO: Created: latency-svc-56vfp
Sep  5 22:25:25.658: INFO: Got endpoints: latency-svc-56vfp [737.328946ms]
Sep  5 22:25:25.660: INFO: Created: latency-svc-dnsh5
Sep  5 22:25:25.701: INFO: Created: latency-svc-trf5h
Sep  5 22:25:25.716: INFO: Got endpoints: latency-svc-dnsh5 [783.282321ms]
Sep  5 22:25:25.729: INFO: Got endpoints: latency-svc-trf5h [756.992401ms]
Sep  5 22:25:25.741: INFO: Created: latency-svc-crp67
Sep  5 22:25:25.777: INFO: Got endpoints: latency-svc-crp67 [748.849555ms]
Sep  5 22:25:25.780: INFO: Created: latency-svc-dshht
Sep  5 22:25:25.807: INFO: Got endpoints: latency-svc-dshht [730.402863ms]
Sep  5 22:25:25.815: INFO: Created: latency-svc-mzgq6
Sep  5 22:25:25.848: INFO: Got endpoints: latency-svc-mzgq6 [699.983447ms]
Sep  5 22:25:25.875: INFO: Created: latency-svc-bnqk6
Sep  5 22:25:25.910: INFO: Created: latency-svc-j4hw6
Sep  5 22:25:25.923: INFO: Got endpoints: latency-svc-bnqk6 [761.513912ms]
Sep  5 22:25:25.953: INFO: Created: latency-svc-8w5sd
Sep  5 22:25:25.965: INFO: Got endpoints: latency-svc-j4hw6 [744.538689ms]
Sep  5 22:25:25.999: INFO: Got endpoints: latency-svc-8w5sd [734.293982ms]
Sep  5 22:25:26.006: INFO: Created: latency-svc-vxfms
Sep  5 22:25:26.263: INFO: Created: latency-svc-7w6tq
Sep  5 22:25:26.264: INFO: Got endpoints: latency-svc-vxfms [894.809277ms]
Sep  5 22:25:26.370: INFO: Created: latency-svc-v4s9f
Sep  5 22:25:26.424: INFO: Got endpoints: latency-svc-7w6tq [1.01640279s]
Sep  5 22:25:26.482: INFO: Created: latency-svc-2qcs5
Sep  5 22:25:26.485: INFO: Got endpoints: latency-svc-v4s9f [1.02512504s]
Sep  5 22:25:26.529: INFO: Created: latency-svc-6b4t2
Sep  5 22:25:26.598: INFO: Got endpoints: latency-svc-2qcs5 [1.093001165s]
Sep  5 22:25:26.618: INFO: Got endpoints: latency-svc-6b4t2 [1.061839139s]
Sep  5 22:25:26.638: INFO: Created: latency-svc-fl5kc
Sep  5 22:25:26.675: INFO: Got endpoints: latency-svc-fl5kc [1.063328768s]
Sep  5 22:25:26.675: INFO: Latencies: [107.982244ms 243.316261ms 254.925549ms 417.565939ms 473.616933ms 548.985965ms 660.951855ms 697.642839ms 697.895408ms 698.094583ms 698.21435ms 699.983447ms 704.396721ms 705.800205ms 706.373605ms 706.765528ms 707.353443ms 713.938551ms 715.931523ms 718.629541ms 719.267052ms 723.012234ms 726.055316ms 729.242718ms 729.802358ms 730.402863ms 732.592136ms 734.293982ms 737.21787ms 737.328946ms 741.804935ms 742.246176ms 744.538689ms 745.23641ms 746.219652ms 748.849555ms 750.497789ms 751.724352ms 756.992401ms 761.513912ms 762.874709ms 765.882055ms 769.240883ms 770.687515ms 770.789831ms 774.892099ms 774.963041ms 776.308039ms 776.710685ms 777.756426ms 778.03012ms 778.272783ms 779.284502ms 781.075671ms 783.282321ms 785.214401ms 785.53577ms 786.963777ms 788.847327ms 788.968408ms 790.3446ms 790.827294ms 791.185077ms 793.023344ms 794.084394ms 798.705445ms 800.96607ms 801.294255ms 802.07324ms 802.556687ms 806.403236ms 807.026623ms 807.759303ms 809.694489ms 809.737713ms 816.203111ms 817.853211ms 818.230058ms 822.962462ms 828.09521ms 833.725388ms 836.438745ms 836.706487ms 836.927361ms 837.782438ms 837.823333ms 838.379237ms 838.967117ms 840.091514ms 840.45722ms 841.721934ms 843.837082ms 843.899422ms 844.180968ms 848.549548ms 851.532788ms 852.252286ms 854.775765ms 855.356725ms 856.684404ms 860.303772ms 866.220692ms 866.286769ms 867.84029ms 868.873487ms 870.160373ms 870.741868ms 874.994349ms 876.979364ms 878.204923ms 885.848254ms 886.608839ms 892.137807ms 892.704728ms 894.809277ms 895.205162ms 899.028599ms 900.554171ms 901.912719ms 902.430344ms 906.965695ms 916.617476ms 918.279728ms 920.382536ms 921.627474ms 927.97427ms 955.955322ms 972.713673ms 974.901379ms 975.762847ms 977.379933ms 983.494681ms 992.541697ms 1.000578757s 1.005552025s 1.007639993s 1.01640279s 1.02512504s 1.027061905s 1.027521769s 1.027705961s 1.03486578s 1.045766342s 1.051549076s 1.056672569s 1.059698494s 1.061839139s 1.063328768s 1.089134026s 1.093001165s 1.096631802s 1.121808849s 1.129693127s 1.140816214s 1.158527513s 1.165857343s 1.175670944s 1.182770341s 1.194229316s 1.194451637s 1.196969993s 1.207762037s 1.223148064s 1.231192279s 1.23243331s 1.236156599s 1.237046218s 1.237686896s 1.25319781s 1.258146254s 1.266148012s 1.291495087s 1.301604372s 1.323588111s 1.336142711s 1.338472411s 1.342072414s 1.35096656s 1.357025271s 1.40862017s 1.409602298s 1.475458993s 1.482211207s 1.483316889s 1.499601017s 1.533085591s 1.555107656s 1.555269836s 1.598309722s 1.735014443s 1.841176516s 1.921011336s 2.002970208s 2.020477343s 2.021960999s 2.043063999s 2.129600706s 2.140596823s 2.171914232s 2.183610074s]
Sep  5 22:25:26.675: INFO: 50 %ile: 860.303772ms
Sep  5 22:25:26.675: INFO: 90 %ile: 1.409602298s
Sep  5 22:25:26.675: INFO: 99 %ile: 2.171914232s
Sep  5 22:25:26.675: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:25:26.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-569" for this suite.

• [SLOW TEST:41.104 seconds]
[sig-network] Service endpoints latency
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":296,"completed":74,"skipped":1392,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:25:27.370: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8402
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod with failed condition
STEP: updating the pod
Sep  5 22:27:28.545: INFO: Successfully updated pod "var-expansion-02bb1491-8eb6-410a-8db5-1efdeffb8573"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Sep  5 22:28:34.580: INFO: Deleting pod "var-expansion-02bb1491-8eb6-410a-8db5-1efdeffb8573" in namespace "var-expansion-8402"
Sep  5 22:28:34.607: INFO: Wait up to 5m0s for pod "var-expansion-02bb1491-8eb6-410a-8db5-1efdeffb8573" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:29:18.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8402" for this suite.

• [SLOW TEST:231.996 seconds]
[k8s.io] Variable Expansion
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":296,"completed":75,"skipped":1396,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:29:19.366: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2582
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:30:20.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2582" for this suite.

• [SLOW TEST:61.286 seconds]
[k8s.io] Probing container
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":296,"completed":76,"skipped":1400,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SS
------------------------------
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] IngressClass API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:30:20.653: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-6013
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep  5 22:30:21.321: INFO: starting watch
STEP: patching
STEP: updating
Sep  5 22:30:21.366: INFO: waiting for watch events with expected annotations
Sep  5 22:30:21.366: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:30:21.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-6013" for this suite.
•{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":296,"completed":77,"skipped":1402,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] PreStop
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:30:21.769: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-7043
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:157
[It] should call prestop when killing a pod  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating server pod server in namespace prestop-7043
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7043
STEP: Deleting pre-stop pod
Sep  5 22:31:31.423: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:31:31.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7043" for this suite.

• [SLOW TEST:70.214 seconds]
[k8s.io] [sig-node] PreStop
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should call prestop when killing a pod  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":296,"completed":78,"skipped":1427,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:31:31.984: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5501
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep  5 22:31:32.585: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff" in namespace "projected-5501" to be "Succeeded or Failed"
Sep  5 22:31:32.598: INFO: Pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff": Phase="Pending", Reason="", readiness=false. Elapsed: 12.405325ms
Sep  5 22:31:34.614: INFO: Pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028512926s
Sep  5 22:31:36.631: INFO: Pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046315703s
Sep  5 22:31:38.648: INFO: Pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff": Phase="Pending", Reason="", readiness=false. Elapsed: 6.062640574s
Sep  5 22:31:40.658: INFO: Pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff": Phase="Pending", Reason="", readiness=false. Elapsed: 8.072597943s
Sep  5 22:31:42.674: INFO: Pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff": Phase="Pending", Reason="", readiness=false. Elapsed: 10.089323324s
Sep  5 22:31:44.692: INFO: Pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff": Phase="Pending", Reason="", readiness=false. Elapsed: 12.106605474s
Sep  5 22:31:46.705: INFO: Pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff": Phase="Pending", Reason="", readiness=false. Elapsed: 14.119642181s
Sep  5 22:31:48.717: INFO: Pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff": Phase="Pending", Reason="", readiness=false. Elapsed: 16.13180217s
Sep  5 22:31:50.746: INFO: Pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff": Phase="Pending", Reason="", readiness=false. Elapsed: 18.160673907s
Sep  5 22:31:52.764: INFO: Pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff": Phase="Pending", Reason="", readiness=false. Elapsed: 20.179090871s
Sep  5 22:31:54.777: INFO: Pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff": Phase="Pending", Reason="", readiness=false. Elapsed: 22.192389111s
Sep  5 22:31:56.805: INFO: Pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff": Phase="Pending", Reason="", readiness=false. Elapsed: 24.220118164s
Sep  5 22:31:58.820: INFO: Pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff": Phase="Pending", Reason="", readiness=false. Elapsed: 26.23468721s
Sep  5 22:32:00.841: INFO: Pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff": Phase="Running", Reason="", readiness=true. Elapsed: 28.25580387s
Sep  5 22:32:02.881: INFO: Pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff": Phase="Running", Reason="", readiness=true. Elapsed: 30.295685498s
Sep  5 22:32:04.906: INFO: Pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff": Phase="Running", Reason="", readiness=true. Elapsed: 32.320610083s
Sep  5 22:32:06.929: INFO: Pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.34362341s
STEP: Saw pod success
Sep  5 22:32:06.929: INFO: Pod "downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff" satisfied condition "Succeeded or Failed"
Sep  5 22:32:06.936: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff container client-container: <nil>
STEP: delete the pod
Sep  5 22:32:14.509: INFO: Waiting for pod downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff to disappear
Sep  5 22:32:14.525: INFO: Pod downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff still exists
Sep  5 22:32:16.526: INFO: Waiting for pod downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff to disappear
Sep  5 22:32:16.554: INFO: Pod downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff still exists
Sep  5 22:32:18.527: INFO: Waiting for pod downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff to disappear
Sep  5 22:32:18.543: INFO: Pod downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff still exists
Sep  5 22:32:20.526: INFO: Waiting for pod downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff to disappear
Sep  5 22:32:20.552: INFO: Pod downwardapi-volume-9fcb6398-05f8-420b-b26b-e958fbe8aaff no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:32:20.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5501" for this suite.

• [SLOW TEST:49.072 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":79,"skipped":1427,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:32:21.056: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8838
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should add annotations for pods in rc  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Sep  5 22:32:21.738: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-8838 create -f -'
Sep  5 22:32:22.877: INFO: stderr: ""
Sep  5 22:32:22.877: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep  5 22:32:23.889: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:23.889: INFO: Found 0 / 1
Sep  5 22:32:24.886: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:24.887: INFO: Found 0 / 1
Sep  5 22:32:25.893: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:25.893: INFO: Found 0 / 1
Sep  5 22:32:26.884: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:26.884: INFO: Found 0 / 1
Sep  5 22:32:27.894: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:27.894: INFO: Found 0 / 1
Sep  5 22:32:28.889: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:28.889: INFO: Found 0 / 1
Sep  5 22:32:29.888: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:29.888: INFO: Found 0 / 1
Sep  5 22:32:30.888: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:30.888: INFO: Found 0 / 1
Sep  5 22:32:31.891: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:31.891: INFO: Found 0 / 1
Sep  5 22:32:32.894: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:32.894: INFO: Found 0 / 1
Sep  5 22:32:33.893: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:33.893: INFO: Found 0 / 1
Sep  5 22:32:34.903: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:34.903: INFO: Found 0 / 1
Sep  5 22:32:35.888: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:35.888: INFO: Found 0 / 1
Sep  5 22:32:36.902: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:36.902: INFO: Found 0 / 1
Sep  5 22:32:37.885: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:37.885: INFO: Found 0 / 1
Sep  5 22:32:38.890: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:38.891: INFO: Found 0 / 1
Sep  5 22:32:39.886: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:39.886: INFO: Found 0 / 1
Sep  5 22:32:40.886: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:40.886: INFO: Found 0 / 1
Sep  5 22:32:41.890: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:41.890: INFO: Found 0 / 1
Sep  5 22:32:42.893: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:42.893: INFO: Found 0 / 1
Sep  5 22:32:43.892: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:43.892: INFO: Found 0 / 1
Sep  5 22:32:44.886: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:44.886: INFO: Found 0 / 1
Sep  5 22:32:45.889: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:45.889: INFO: Found 0 / 1
Sep  5 22:32:47.127: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:47.127: INFO: Found 0 / 1
Sep  5 22:32:47.907: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:47.907: INFO: Found 0 / 1
Sep  5 22:32:48.897: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:48.897: INFO: Found 0 / 1
Sep  5 22:32:49.892: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:49.892: INFO: Found 0 / 1
Sep  5 22:32:50.888: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:50.888: INFO: Found 0 / 1
Sep  5 22:32:51.908: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:51.908: INFO: Found 1 / 1
Sep  5 22:32:51.908: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep  5 22:32:51.922: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:51.922: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  5 22:32:51.922: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-8838 patch pod agnhost-primary-89tlt -p {"metadata":{"annotations":{"x":"y"}}}'
Sep  5 22:32:52.086: INFO: stderr: ""
Sep  5 22:32:52.086: INFO: stdout: "pod/agnhost-primary-89tlt patched\n"
STEP: checking annotations
Sep  5 22:32:52.094: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:32:52.094: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:32:52.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8838" for this suite.

• [SLOW TEST:31.382 seconds]
[sig-cli] Kubectl client
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1466
    should add annotations for pods in rc  [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":296,"completed":80,"skipped":1448,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:32:52.438: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8556
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-2876
STEP: Creating secret with name secret-test-5a5e8b0b-9c05-4728-bd84-e6f6547accbf
STEP: Creating a pod to test consume secrets
Sep  5 22:32:53.956: INFO: Waiting up to 5m0s for pod "pod-secrets-f52e4af0-700b-405d-8099-efffd39baade" in namespace "secrets-8556" to be "Succeeded or Failed"
Sep  5 22:32:53.967: INFO: Pod "pod-secrets-f52e4af0-700b-405d-8099-efffd39baade": Phase="Pending", Reason="", readiness=false. Elapsed: 10.039676ms
Sep  5 22:32:55.976: INFO: Pod "pod-secrets-f52e4af0-700b-405d-8099-efffd39baade": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019282101s
Sep  5 22:32:57.994: INFO: Pod "pod-secrets-f52e4af0-700b-405d-8099-efffd39baade": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037129316s
Sep  5 22:33:00.004: INFO: Pod "pod-secrets-f52e4af0-700b-405d-8099-efffd39baade": Phase="Pending", Reason="", readiness=false. Elapsed: 6.047892678s
Sep  5 22:33:02.021: INFO: Pod "pod-secrets-f52e4af0-700b-405d-8099-efffd39baade": Phase="Pending", Reason="", readiness=false. Elapsed: 8.064655858s
Sep  5 22:33:04.030: INFO: Pod "pod-secrets-f52e4af0-700b-405d-8099-efffd39baade": Phase="Pending", Reason="", readiness=false. Elapsed: 10.073887767s
Sep  5 22:33:06.056: INFO: Pod "pod-secrets-f52e4af0-700b-405d-8099-efffd39baade": Phase="Pending", Reason="", readiness=false. Elapsed: 12.099032954s
Sep  5 22:33:08.074: INFO: Pod "pod-secrets-f52e4af0-700b-405d-8099-efffd39baade": Phase="Pending", Reason="", readiness=false. Elapsed: 14.117748306s
Sep  5 22:33:10.087: INFO: Pod "pod-secrets-f52e4af0-700b-405d-8099-efffd39baade": Phase="Pending", Reason="", readiness=false. Elapsed: 16.130333379s
Sep  5 22:33:12.098: INFO: Pod "pod-secrets-f52e4af0-700b-405d-8099-efffd39baade": Phase="Pending", Reason="", readiness=false. Elapsed: 18.141112641s
Sep  5 22:33:14.116: INFO: Pod "pod-secrets-f52e4af0-700b-405d-8099-efffd39baade": Phase="Pending", Reason="", readiness=false. Elapsed: 20.159019161s
Sep  5 22:33:16.132: INFO: Pod "pod-secrets-f52e4af0-700b-405d-8099-efffd39baade": Phase="Pending", Reason="", readiness=false. Elapsed: 22.175186721s
Sep  5 22:33:18.149: INFO: Pod "pod-secrets-f52e4af0-700b-405d-8099-efffd39baade": Phase="Pending", Reason="", readiness=false. Elapsed: 24.192330934s
Sep  5 22:33:20.180: INFO: Pod "pod-secrets-f52e4af0-700b-405d-8099-efffd39baade": Phase="Pending", Reason="", readiness=false. Elapsed: 26.223490692s
Sep  5 22:33:22.195: INFO: Pod "pod-secrets-f52e4af0-700b-405d-8099-efffd39baade": Phase="Running", Reason="", readiness=true. Elapsed: 28.238908853s
Sep  5 22:33:24.206: INFO: Pod "pod-secrets-f52e4af0-700b-405d-8099-efffd39baade": Phase="Running", Reason="", readiness=true. Elapsed: 30.249294306s
Sep  5 22:33:26.233: INFO: Pod "pod-secrets-f52e4af0-700b-405d-8099-efffd39baade": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.276413324s
STEP: Saw pod success
Sep  5 22:33:26.233: INFO: Pod "pod-secrets-f52e4af0-700b-405d-8099-efffd39baade" satisfied condition "Succeeded or Failed"
Sep  5 22:33:26.243: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-secrets-f52e4af0-700b-405d-8099-efffd39baade container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 22:33:26.312: INFO: Waiting for pod pod-secrets-f52e4af0-700b-405d-8099-efffd39baade to disappear
Sep  5 22:33:26.324: INFO: Pod pod-secrets-f52e4af0-700b-405d-8099-efffd39baade still exists
Sep  5 22:33:28.326: INFO: Waiting for pod pod-secrets-f52e4af0-700b-405d-8099-efffd39baade to disappear
Sep  5 22:33:28.334: INFO: Pod pod-secrets-f52e4af0-700b-405d-8099-efffd39baade still exists
Sep  5 22:33:30.325: INFO: Waiting for pod pod-secrets-f52e4af0-700b-405d-8099-efffd39baade to disappear
Sep  5 22:33:30.336: INFO: Pod pod-secrets-f52e4af0-700b-405d-8099-efffd39baade still exists
Sep  5 22:33:32.325: INFO: Waiting for pod pod-secrets-f52e4af0-700b-405d-8099-efffd39baade to disappear
Sep  5 22:33:32.337: INFO: Pod pod-secrets-f52e4af0-700b-405d-8099-efffd39baade still exists
Sep  5 22:33:34.325: INFO: Waiting for pod pod-secrets-f52e4af0-700b-405d-8099-efffd39baade to disappear
Sep  5 22:33:34.337: INFO: Pod pod-secrets-f52e4af0-700b-405d-8099-efffd39baade no longer exists
[AfterEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:33:34.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8556" for this suite.
STEP: Destroying namespace "secret-namespace-2876" for this suite.

• [SLOW TEST:42.590 seconds]
[sig-storage] Secrets
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":296,"completed":81,"skipped":1452,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected combined
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:33:35.029: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3691
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-projected-all-test-volume-a680c26b-9905-4c9e-bc7e-a101c4aa45b8
STEP: Creating secret with name secret-projected-all-test-volume-4308912b-2654-49fc-b71c-ecfce94615e8
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep  5 22:33:35.637: INFO: Waiting up to 5m0s for pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053" in namespace "projected-3691" to be "Succeeded or Failed"
Sep  5 22:33:35.652: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053": Phase="Pending", Reason="", readiness=false. Elapsed: 14.216521ms
Sep  5 22:33:37.686: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048538268s
Sep  5 22:33:39.700: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062182761s
Sep  5 22:33:41.725: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053": Phase="Pending", Reason="", readiness=false. Elapsed: 6.087414212s
Sep  5 22:33:43.736: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053": Phase="Pending", Reason="", readiness=false. Elapsed: 8.098270821s
Sep  5 22:33:45.748: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053": Phase="Pending", Reason="", readiness=false. Elapsed: 10.110804194s
Sep  5 22:33:47.768: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053": Phase="Pending", Reason="", readiness=false. Elapsed: 12.130459631s
Sep  5 22:33:49.810: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053": Phase="Pending", Reason="", readiness=false. Elapsed: 14.172365114s
Sep  5 22:33:51.849: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053": Phase="Pending", Reason="", readiness=false. Elapsed: 16.211871706s
Sep  5 22:33:53.908: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053": Phase="Pending", Reason="", readiness=false. Elapsed: 18.270506389s
Sep  5 22:33:55.928: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053": Phase="Pending", Reason="", readiness=false. Elapsed: 20.290169775s
Sep  5 22:33:57.947: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053": Phase="Pending", Reason="", readiness=false. Elapsed: 22.309425437s
Sep  5 22:33:59.957: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053": Phase="Pending", Reason="", readiness=false. Elapsed: 24.319832705s
Sep  5 22:34:01.975: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053": Phase="Pending", Reason="", readiness=false. Elapsed: 26.337412353s
Sep  5 22:34:04.027: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053": Phase="Pending", Reason="", readiness=false. Elapsed: 28.389171771s
Sep  5 22:34:06.040: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053": Phase="Pending", Reason="", readiness=false. Elapsed: 30.402004192s
Sep  5 22:34:08.057: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053": Phase="Pending", Reason="", readiness=false. Elapsed: 32.419480508s
Sep  5 22:34:10.071: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053": Phase="Pending", Reason="", readiness=false. Elapsed: 34.432997086s
Sep  5 22:34:12.086: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053": Phase="Succeeded", Reason="", readiness=false. Elapsed: 36.448163358s
STEP: Saw pod success
Sep  5 22:34:12.086: INFO: Pod "projected-volume-860726dc-40f5-4dfc-895d-27be0a052053" satisfied condition "Succeeded or Failed"
Sep  5 22:34:12.093: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod projected-volume-860726dc-40f5-4dfc-895d-27be0a052053 container projected-all-volume-test: <nil>
STEP: delete the pod
Sep  5 22:34:17.893: INFO: Waiting for pod projected-volume-860726dc-40f5-4dfc-895d-27be0a052053 to disappear
Sep  5 22:34:17.912: INFO: Pod projected-volume-860726dc-40f5-4dfc-895d-27be0a052053 still exists
Sep  5 22:34:19.912: INFO: Waiting for pod projected-volume-860726dc-40f5-4dfc-895d-27be0a052053 to disappear
Sep  5 22:34:19.930: INFO: Pod projected-volume-860726dc-40f5-4dfc-895d-27be0a052053 still exists
Sep  5 22:34:21.913: INFO: Waiting for pod projected-volume-860726dc-40f5-4dfc-895d-27be0a052053 to disappear
Sep  5 22:34:21.936: INFO: Pod projected-volume-860726dc-40f5-4dfc-895d-27be0a052053 still exists
Sep  5 22:34:23.912: INFO: Waiting for pod projected-volume-860726dc-40f5-4dfc-895d-27be0a052053 to disappear
Sep  5 22:34:23.934: INFO: Pod projected-volume-860726dc-40f5-4dfc-895d-27be0a052053 still exists
Sep  5 22:34:25.913: INFO: Waiting for pod projected-volume-860726dc-40f5-4dfc-895d-27be0a052053 to disappear
Sep  5 22:34:25.967: INFO: Pod projected-volume-860726dc-40f5-4dfc-895d-27be0a052053 still exists
Sep  5 22:34:27.913: INFO: Waiting for pod projected-volume-860726dc-40f5-4dfc-895d-27be0a052053 to disappear
Sep  5 22:34:27.927: INFO: Pod projected-volume-860726dc-40f5-4dfc-895d-27be0a052053 no longer exists
[AfterEach] [sig-storage] Projected combined
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:34:27.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3691" for this suite.

• [SLOW TEST:53.883 seconds]
[sig-storage] Projected combined
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":296,"completed":82,"skipped":1479,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:34:28.912: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4796
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  5 22:34:29.614: INFO: Waiting up to 5m0s for pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072" in namespace "emptydir-4796" to be "Succeeded or Failed"
Sep  5 22:34:29.649: INFO: Pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072": Phase="Pending", Reason="", readiness=false. Elapsed: 34.503025ms
Sep  5 22:34:31.681: INFO: Pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066400672s
Sep  5 22:34:33.754: INFO: Pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072": Phase="Pending", Reason="", readiness=false. Elapsed: 4.140083448s
Sep  5 22:34:35.776: INFO: Pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072": Phase="Pending", Reason="", readiness=false. Elapsed: 6.161980598s
Sep  5 22:34:37.864: INFO: Pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072": Phase="Pending", Reason="", readiness=false. Elapsed: 8.249520298s
Sep  5 22:34:39.884: INFO: Pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072": Phase="Pending", Reason="", readiness=false. Elapsed: 10.269781901s
Sep  5 22:34:41.903: INFO: Pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072": Phase="Pending", Reason="", readiness=false. Elapsed: 12.289041228s
Sep  5 22:34:43.919: INFO: Pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072": Phase="Pending", Reason="", readiness=false. Elapsed: 14.305164486s
Sep  5 22:34:45.938: INFO: Pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072": Phase="Pending", Reason="", readiness=false. Elapsed: 16.323753312s
Sep  5 22:34:47.954: INFO: Pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072": Phase="Pending", Reason="", readiness=false. Elapsed: 18.339429126s
Sep  5 22:34:49.968: INFO: Pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072": Phase="Pending", Reason="", readiness=false. Elapsed: 20.353292837s
Sep  5 22:34:51.981: INFO: Pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072": Phase="Pending", Reason="", readiness=false. Elapsed: 22.367118807s
Sep  5 22:34:53.996: INFO: Pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072": Phase="Pending", Reason="", readiness=false. Elapsed: 24.381651809s
Sep  5 22:34:56.029: INFO: Pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072": Phase="Pending", Reason="", readiness=false. Elapsed: 26.414270834s
Sep  5 22:34:58.040: INFO: Pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072": Phase="Running", Reason="", readiness=true. Elapsed: 28.426231765s
Sep  5 22:35:00.066: INFO: Pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072": Phase="Running", Reason="", readiness=true. Elapsed: 30.452057099s
Sep  5 22:35:02.080: INFO: Pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072": Phase="Running", Reason="", readiness=true. Elapsed: 32.465453573s
Sep  5 22:35:04.094: INFO: Pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.47951215s
STEP: Saw pod success
Sep  5 22:35:04.094: INFO: Pod "pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072" satisfied condition "Succeeded or Failed"
Sep  5 22:35:04.103: INFO: Trying to get logs from node sc2-10-185-226-150.eng.vmware.com pod pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072 container test-container: <nil>
STEP: delete the pod
Sep  5 22:35:10.479: INFO: Waiting for pod pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072 to disappear
Sep  5 22:35:10.506: INFO: Pod pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072 still exists
Sep  5 22:35:12.506: INFO: Waiting for pod pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072 to disappear
Sep  5 22:35:12.526: INFO: Pod pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072 still exists
Sep  5 22:35:14.507: INFO: Waiting for pod pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072 to disappear
Sep  5 22:35:14.539: INFO: Pod pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072 still exists
Sep  5 22:35:16.506: INFO: Waiting for pod pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072 to disappear
Sep  5 22:35:16.529: INFO: Pod pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072 still exists
Sep  5 22:35:18.506: INFO: Waiting for pod pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072 to disappear
Sep  5 22:35:18.515: INFO: Pod pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072 still exists
Sep  5 22:35:20.506: INFO: Waiting for pod pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072 to disappear
Sep  5 22:35:20.523: INFO: Pod pod-c5364e9c-23c2-4dcd-aa01-8f6f8cd39072 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:35:20.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4796" for this suite.

• [SLOW TEST:52.129 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":83,"skipped":1527,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:35:21.041: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7769
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 22:35:23.030: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  5 22:35:25.065: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:35:27.079: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:35:29.089: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:35:31.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:35:33.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:35:35.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:35:37.078: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:35:39.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:35:41.083: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:35:43.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:35:45.080: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:35:47.089: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:35:49.075: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:35:51.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:35:53.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:35:55.082: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:35:57.077: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 35, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 22:36:00.133: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:36:00.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7769" for this suite.
STEP: Destroying namespace "webhook-7769-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:40.230 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":296,"completed":84,"skipped":1527,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:36:01.271: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1358
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 22:36:01.923: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep  5 22:36:04.070: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:36:05.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1358" for this suite.
•{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":296,"completed":85,"skipped":1535,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:36:05.327: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9252
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Sep  5 22:36:41.304: INFO: Successfully updated pod "labelsupdate28fbb450-637e-46b9-acdd-5ba30e7aef3b"
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:36:43.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9252" for this suite.

• [SLOW TEST:38.853 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":296,"completed":86,"skipped":1546,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:36:44.180: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6382
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 22:36:45.039: INFO: >>> kubeConfig: ./kconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:36:46.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6382" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":296,"completed":87,"skipped":1547,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:36:47.176: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2787
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2787
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2787
STEP: creating replication controller externalsvc in namespace services-2787
STEP: changing the ClusterIP service to type=ExternalName
Sep  5 22:37:30.967: INFO: Creating new exec pod
Sep  5 22:37:55.054: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=services-2787 exec execpodt92lr -- /bin/sh -x -c nslookup clusterip-service.services-2787.svc.cluster.local'
Sep  5 22:37:55.682: INFO: stderr: "+ nslookup clusterip-service.services-2787.svc.cluster.local\n"
Sep  5 22:37:55.682: INFO: stdout: "Server:\t\t172.24.0.10\nAddress:\t172.24.0.10#53\n\nclusterip-service.services-2787.svc.cluster.local\tcanonical name = externalsvc.services-2787.svc.cluster.local.\nName:\texternalsvc.services-2787.svc.cluster.local\nAddress: 172.24.139.222\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2787, will wait for the garbage collector to delete the pods
Sep  5 22:37:55.779: INFO: Deleting ReplicationController externalsvc took: 38.742098ms
Sep  5 22:37:57.979: INFO: Terminating ReplicationController externalsvc pods took: 2.200456315s
Sep  5 22:38:19.480: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:38:19.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2787" for this suite.
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:92.776 seconds]
[sig-network] Services
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":296,"completed":88,"skipped":1588,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:38:19.952: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7114
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 22:38:20.611: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Sep  5 22:38:29.889: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7114 --namespace=crd-publish-openapi-7114 create -f -'
Sep  5 22:38:31.004: INFO: stderr: ""
Sep  5 22:38:31.004: INFO: stdout: "e2e-test-crd-publish-openapi-7283-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  5 22:38:31.004: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7114 --namespace=crd-publish-openapi-7114 delete e2e-test-crd-publish-openapi-7283-crds test-foo'
Sep  5 22:38:31.132: INFO: stderr: ""
Sep  5 22:38:31.132: INFO: stdout: "e2e-test-crd-publish-openapi-7283-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep  5 22:38:31.132: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7114 --namespace=crd-publish-openapi-7114 apply -f -'
Sep  5 22:38:31.651: INFO: stderr: ""
Sep  5 22:38:31.652: INFO: stdout: "e2e-test-crd-publish-openapi-7283-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  5 22:38:31.652: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7114 --namespace=crd-publish-openapi-7114 delete e2e-test-crd-publish-openapi-7283-crds test-foo'
Sep  5 22:38:31.789: INFO: stderr: ""
Sep  5 22:38:31.789: INFO: stdout: "e2e-test-crd-publish-openapi-7283-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Sep  5 22:38:31.789: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7114 --namespace=crd-publish-openapi-7114 create -f -'
Sep  5 22:38:32.216: INFO: rc: 1
Sep  5 22:38:32.216: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7114 --namespace=crd-publish-openapi-7114 apply -f -'
Sep  5 22:38:32.643: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Sep  5 22:38:32.643: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7114 --namespace=crd-publish-openapi-7114 create -f -'
Sep  5 22:38:33.005: INFO: rc: 1
Sep  5 22:38:33.005: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7114 --namespace=crd-publish-openapi-7114 apply -f -'
Sep  5 22:38:33.378: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Sep  5 22:38:33.378: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7114 explain e2e-test-crd-publish-openapi-7283-crds'
Sep  5 22:38:33.784: INFO: stderr: ""
Sep  5 22:38:33.784: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7283-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Sep  5 22:38:33.784: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7114 explain e2e-test-crd-publish-openapi-7283-crds.metadata'
Sep  5 22:38:34.147: INFO: stderr: ""
Sep  5 22:38:34.148: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7283-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep  5 22:38:34.148: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7114 explain e2e-test-crd-publish-openapi-7283-crds.spec'
Sep  5 22:38:34.552: INFO: stderr: ""
Sep  5 22:38:34.552: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7283-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep  5 22:38:34.552: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7114 explain e2e-test-crd-publish-openapi-7283-crds.spec.bars'
Sep  5 22:38:34.970: INFO: stderr: ""
Sep  5 22:38:34.970: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7283-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Sep  5 22:38:34.970: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7114 explain e2e-test-crd-publish-openapi-7283-crds.spec.bars2'
Sep  5 22:38:35.391: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:38:44.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7114" for this suite.

• [SLOW TEST:24.853 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":296,"completed":89,"skipped":1607,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:38:44.806: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4153
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl run pod
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1520
[It] should create a pod from an image when restart is Never  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image mirror.gcr.io/library/httpd:2.4.38-alpine
Sep  5 22:38:45.215: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-4153 run e2e-test-httpd-pod --restart=Never --image=mirror.gcr.io/library/httpd:2.4.38-alpine'
Sep  5 22:38:45.408: INFO: stderr: ""
Sep  5 22:38:45.408: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1524
Sep  5 22:38:45.418: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-4153 delete pods e2e-test-httpd-pod'
Sep  5 22:38:45.597: INFO: stderr: ""
Sep  5 22:38:45.597: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:38:45.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4153" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":296,"completed":90,"skipped":1615,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:38:46.150: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1808
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-584e31d5-7ce8-40f3-a51b-01ba07c9d8c7
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:39:20.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1808" for this suite.

• [SLOW TEST:35.111 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":296,"completed":91,"skipped":1636,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:39:21.261: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7363
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should provide secure master service  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:39:21.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7363" for this suite.
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":296,"completed":92,"skipped":1642,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:39:22.229: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-411
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:130
[It] should run and stop complex daemon [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 22:39:23.109: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep  5 22:39:23.135: INFO: Number of nodes with available pods: 0
Sep  5 22:39:23.135: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep  5 22:39:23.202: INFO: Number of nodes with available pods: 0
Sep  5 22:39:23.202: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:24.209: INFO: Number of nodes with available pods: 0
Sep  5 22:39:24.209: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:25.212: INFO: Number of nodes with available pods: 0
Sep  5 22:39:25.212: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:26.230: INFO: Number of nodes with available pods: 0
Sep  5 22:39:26.230: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:27.212: INFO: Number of nodes with available pods: 0
Sep  5 22:39:27.212: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:28.221: INFO: Number of nodes with available pods: 0
Sep  5 22:39:28.221: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:29.243: INFO: Number of nodes with available pods: 0
Sep  5 22:39:29.243: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:30.216: INFO: Number of nodes with available pods: 0
Sep  5 22:39:30.216: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:31.216: INFO: Number of nodes with available pods: 0
Sep  5 22:39:31.216: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:32.211: INFO: Number of nodes with available pods: 0
Sep  5 22:39:32.211: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:33.441: INFO: Number of nodes with available pods: 0
Sep  5 22:39:33.441: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:34.216: INFO: Number of nodes with available pods: 0
Sep  5 22:39:34.216: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:35.244: INFO: Number of nodes with available pods: 0
Sep  5 22:39:35.244: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:36.232: INFO: Number of nodes with available pods: 0
Sep  5 22:39:36.232: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:37.219: INFO: Number of nodes with available pods: 0
Sep  5 22:39:37.219: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:38.214: INFO: Number of nodes with available pods: 0
Sep  5 22:39:38.214: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:39.222: INFO: Number of nodes with available pods: 0
Sep  5 22:39:39.222: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:40.218: INFO: Number of nodes with available pods: 0
Sep  5 22:39:40.218: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:41.215: INFO: Number of nodes with available pods: 0
Sep  5 22:39:41.215: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:42.217: INFO: Number of nodes with available pods: 0
Sep  5 22:39:42.217: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:43.235: INFO: Number of nodes with available pods: 0
Sep  5 22:39:43.235: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:44.230: INFO: Number of nodes with available pods: 0
Sep  5 22:39:44.231: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:45.215: INFO: Number of nodes with available pods: 0
Sep  5 22:39:45.215: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:46.227: INFO: Number of nodes with available pods: 0
Sep  5 22:39:46.227: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:47.219: INFO: Number of nodes with available pods: 0
Sep  5 22:39:47.220: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:48.219: INFO: Number of nodes with available pods: 0
Sep  5 22:39:48.219: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:49.219: INFO: Number of nodes with available pods: 0
Sep  5 22:39:49.219: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:50.221: INFO: Number of nodes with available pods: 0
Sep  5 22:39:50.221: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:51.213: INFO: Number of nodes with available pods: 0
Sep  5 22:39:51.213: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:52.219: INFO: Number of nodes with available pods: 0
Sep  5 22:39:52.219: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:53.218: INFO: Number of nodes with available pods: 0
Sep  5 22:39:53.218: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:54.214: INFO: Number of nodes with available pods: 0
Sep  5 22:39:54.214: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:55.216: INFO: Number of nodes with available pods: 1
Sep  5 22:39:55.216: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep  5 22:39:55.271: INFO: Number of nodes with available pods: 1
Sep  5 22:39:55.272: INFO: Number of running nodes: 0, number of available pods: 1
Sep  5 22:39:56.288: INFO: Number of nodes with available pods: 0
Sep  5 22:39:56.288: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep  5 22:39:56.361: INFO: Number of nodes with available pods: 0
Sep  5 22:39:56.361: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:57.382: INFO: Number of nodes with available pods: 0
Sep  5 22:39:57.382: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:58.374: INFO: Number of nodes with available pods: 0
Sep  5 22:39:58.374: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:39:59.374: INFO: Number of nodes with available pods: 0
Sep  5 22:39:59.374: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:00.385: INFO: Number of nodes with available pods: 0
Sep  5 22:40:00.385: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:01.376: INFO: Number of nodes with available pods: 0
Sep  5 22:40:01.376: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:02.387: INFO: Number of nodes with available pods: 0
Sep  5 22:40:02.387: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:03.377: INFO: Number of nodes with available pods: 0
Sep  5 22:40:03.377: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:04.371: INFO: Number of nodes with available pods: 0
Sep  5 22:40:04.371: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:05.381: INFO: Number of nodes with available pods: 0
Sep  5 22:40:05.381: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:06.371: INFO: Number of nodes with available pods: 0
Sep  5 22:40:06.371: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:07.378: INFO: Number of nodes with available pods: 0
Sep  5 22:40:07.378: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:08.394: INFO: Number of nodes with available pods: 0
Sep  5 22:40:08.394: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:09.389: INFO: Number of nodes with available pods: 0
Sep  5 22:40:09.389: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:10.383: INFO: Number of nodes with available pods: 0
Sep  5 22:40:10.383: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:11.375: INFO: Number of nodes with available pods: 0
Sep  5 22:40:11.375: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:12.386: INFO: Number of nodes with available pods: 0
Sep  5 22:40:12.386: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:13.372: INFO: Number of nodes with available pods: 0
Sep  5 22:40:13.372: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:14.375: INFO: Number of nodes with available pods: 0
Sep  5 22:40:14.375: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:15.374: INFO: Number of nodes with available pods: 0
Sep  5 22:40:15.374: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:16.373: INFO: Number of nodes with available pods: 0
Sep  5 22:40:16.373: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:17.377: INFO: Number of nodes with available pods: 0
Sep  5 22:40:17.377: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:18.379: INFO: Number of nodes with available pods: 0
Sep  5 22:40:18.379: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:19.373: INFO: Number of nodes with available pods: 0
Sep  5 22:40:19.373: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:20.382: INFO: Number of nodes with available pods: 0
Sep  5 22:40:20.382: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:21.371: INFO: Number of nodes with available pods: 0
Sep  5 22:40:21.371: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:22.371: INFO: Number of nodes with available pods: 0
Sep  5 22:40:22.371: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:23.374: INFO: Number of nodes with available pods: 0
Sep  5 22:40:23.374: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:24.377: INFO: Number of nodes with available pods: 0
Sep  5 22:40:24.377: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:25.375: INFO: Number of nodes with available pods: 0
Sep  5 22:40:25.375: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:26.385: INFO: Number of nodes with available pods: 0
Sep  5 22:40:26.385: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:27.383: INFO: Number of nodes with available pods: 0
Sep  5 22:40:27.383: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:28.399: INFO: Number of nodes with available pods: 0
Sep  5 22:40:28.399: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:29.388: INFO: Number of nodes with available pods: 0
Sep  5 22:40:29.388: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:30.378: INFO: Number of nodes with available pods: 0
Sep  5 22:40:30.378: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:31.379: INFO: Number of nodes with available pods: 0
Sep  5 22:40:31.379: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:32.403: INFO: Number of nodes with available pods: 0
Sep  5 22:40:32.403: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:33.381: INFO: Number of nodes with available pods: 0
Sep  5 22:40:33.381: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:34.385: INFO: Number of nodes with available pods: 0
Sep  5 22:40:34.385: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:35.374: INFO: Number of nodes with available pods: 0
Sep  5 22:40:35.374: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:36.382: INFO: Number of nodes with available pods: 0
Sep  5 22:40:36.382: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:37.380: INFO: Number of nodes with available pods: 0
Sep  5 22:40:37.380: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:38.379: INFO: Number of nodes with available pods: 0
Sep  5 22:40:38.379: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:39.382: INFO: Number of nodes with available pods: 0
Sep  5 22:40:39.382: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  5 22:40:40.416: INFO: Number of nodes with available pods: 1
Sep  5 22:40:40.416: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:96
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-411, will wait for the garbage collector to delete the pods
Sep  5 22:40:40.578: INFO: Deleting DaemonSet.extensions daemon-set took: 27.576925ms
Sep  5 22:40:42.978: INFO: Terminating DaemonSet.extensions daemon-set pods took: 2.400177199s
Sep  5 22:41:00.392: INFO: Number of nodes with available pods: 0
Sep  5 22:41:00.392: INFO: Number of running nodes: 0, number of available pods: 0
Sep  5 22:41:00.404: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-411/daemonsets","resourceVersion":"116364"},"items":null}

Sep  5 22:41:00.415: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-411/pods","resourceVersion":"116364"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:41:00.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-411" for this suite.

• [SLOW TEST:99.154 seconds]
[sig-apps] Daemon set [Serial]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":296,"completed":93,"skipped":1662,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:41:01.383: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-9451
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Sep  5 22:41:02.435: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Sep  5 22:41:32.486: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 22:41:41.084: INFO: >>> kubeConfig: ./kconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:42:11.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9451" for this suite.

• [SLOW TEST:70.261 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":296,"completed":94,"skipped":1666,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:42:11.644: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-777
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be updated [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  5 22:42:48.723: INFO: Successfully updated pod "pod-update-b8904acc-ecaa-46fd-af42-c159ae6edd47"
STEP: verifying the updated pod is in kubernetes
Sep  5 22:42:48.741: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:42:48.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-777" for this suite.

• [SLOW TEST:37.651 seconds]
[k8s.io] Pods
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be updated [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":296,"completed":95,"skipped":1687,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:42:49.297: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-1530
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Sep  5 22:42:49.998: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  5 22:43:50.211: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 22:43:50.233: INFO: Starting informer...
STEP: Starting pods...
Sep  5 22:43:50.567: INFO: Pod1 is running on sc2-10-185-226-233.eng.vmware.com. Tainting Node
Sep  5 22:44:22.840: INFO: Pod2 is running on sc2-10-185-226-233.eng.vmware.com. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Sep  5 22:44:43.844: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep  5 22:45:12.030: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:45:12.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-1530" for this suite.

• [SLOW TEST:143.356 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":296,"completed":96,"skipped":1735,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:45:12.653: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should be submitted and removed [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep  5 22:45:13.319: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:46:14.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8230" for this suite.

• [SLOW TEST:62.479 seconds]
[k8s.io] Pods
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be submitted and removed [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":296,"completed":97,"skipped":1737,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Ingress API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:46:15.132: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-6926
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep  5 22:46:15.820: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Sep  5 22:46:15.832: INFO: starting watch
STEP: patching
STEP: updating
Sep  5 22:46:15.868: INFO: waiting for watch events with expected annotations
Sep  5 22:46:15.868: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:46:16.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-6926" for this suite.
•{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":296,"completed":98,"skipped":1738,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:46:16.208: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2437
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Sep  5 22:46:49.353: INFO: Successfully updated pod "annotationupdateb81102f3-24bb-454a-9750-11dc12b44ad6"
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:46:51.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2437" for this suite.

• [SLOW TEST:35.752 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":296,"completed":99,"skipped":1757,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:46:51.960: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-7647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep  5 22:46:53.133: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Sep  5 22:46:55.167: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:46:57.197: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:46:59.178: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:47:01.189: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:47:03.209: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:47:05.187: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:47:07.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:47:09.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:47:11.181: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:47:13.208: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:47:15.189: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:47:17.181: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:47:19.188: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:47:21.178: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:47:23.181: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:47:25.199: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:47:27.180: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 46, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 22:47:30.282: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 22:47:30.302: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:47:32.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-7647" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:41.842 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":296,"completed":100,"skipped":1758,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:47:33.802: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8335
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep  5 22:47:34.806: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8335 /api/v1/namespaces/watch-8335/configmaps/e2e-watch-test-watch-closed 7ed455cd-6d5d-474c-bb74-171be7ccfa6b 120957 0 2021-09-05 22:47:34 -0700 PDT <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-05 22:47:34 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  5 22:47:34.806: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8335 /api/v1/namespaces/watch-8335/configmaps/e2e-watch-test-watch-closed 7ed455cd-6d5d-474c-bb74-171be7ccfa6b 120962 0 2021-09-05 22:47:34 -0700 PDT <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-05 22:47:34 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep  5 22:47:35.045: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8335 /api/v1/namespaces/watch-8335/configmaps/e2e-watch-test-watch-closed 7ed455cd-6d5d-474c-bb74-171be7ccfa6b 120964 0 2021-09-05 22:47:34 -0700 PDT <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-05 22:47:34 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  5 22:47:35.049: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8335 /api/v1/namespaces/watch-8335/configmaps/e2e-watch-test-watch-closed 7ed455cd-6d5d-474c-bb74-171be7ccfa6b 120966 0 2021-09-05 22:47:34 -0700 PDT <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-05 22:47:34 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:47:35.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8335" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":296,"completed":101,"skipped":1764,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:47:35.517: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5929
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide podname only [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep  5 22:47:36.658: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0" in namespace "projected-5929" to be "Succeeded or Failed"
Sep  5 22:47:36.696: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 38.373844ms
Sep  5 22:47:38.718: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060472503s
Sep  5 22:47:40.758: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.100114003s
Sep  5 22:47:42.782: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.124669242s
Sep  5 22:47:44.790: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.132485374s
Sep  5 22:47:46.904: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.246621132s
Sep  5 22:47:48.919: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.260994393s
Sep  5 22:47:50.932: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.274826337s
Sep  5 22:47:52.953: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.29508708s
Sep  5 22:47:54.965: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.307455732s
Sep  5 22:47:56.979: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.321065532s
Sep  5 22:47:58.996: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.338607129s
Sep  5 22:48:01.010: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.352514745s
Sep  5 22:48:03.023: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 26.365766793s
Sep  5 22:48:05.034: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 28.376311659s
Sep  5 22:48:07.079: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.421880798s
Sep  5 22:48:09.122: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 32.464790504s
Sep  5 22:48:11.133: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 34.475269855s
Sep  5 22:48:13.143: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 36.48563926s
Sep  5 22:48:15.152: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 38.494782783s
STEP: Saw pod success
Sep  5 22:48:15.152: INFO: Pod "downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0" satisfied condition "Succeeded or Failed"
Sep  5 22:48:15.158: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0 container client-container: <nil>
STEP: delete the pod
Sep  5 22:48:20.649: INFO: Waiting for pod downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0 to disappear
Sep  5 22:48:20.662: INFO: Pod downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0 still exists
Sep  5 22:48:22.663: INFO: Waiting for pod downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0 to disappear
Sep  5 22:48:22.674: INFO: Pod downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0 still exists
Sep  5 22:48:24.662: INFO: Waiting for pod downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0 to disappear
Sep  5 22:48:24.679: INFO: Pod downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0 still exists
Sep  5 22:48:26.662: INFO: Waiting for pod downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0 to disappear
Sep  5 22:48:26.692: INFO: Pod downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0 still exists
Sep  5 22:48:28.663: INFO: Waiting for pod downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0 to disappear
Sep  5 22:48:28.701: INFO: Pod downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0 still exists
Sep  5 22:48:30.663: INFO: Waiting for pod downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0 to disappear
Sep  5 22:48:30.677: INFO: Pod downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0 still exists
Sep  5 22:48:32.663: INFO: Waiting for pod downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0 to disappear
Sep  5 22:48:32.677: INFO: Pod downwardapi-volume-6a82781e-2ad1-4661-a7b4-5f7acac00fd0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:48:32.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5929" for this suite.

• [SLOW TEST:57.720 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide podname only [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":296,"completed":102,"skipped":1770,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:48:33.237: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6291
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in volume subpath
Sep  5 22:48:33.972: INFO: Waiting up to 5m0s for pod "var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855" in namespace "var-expansion-6291" to be "Succeeded or Failed"
Sep  5 22:48:33.979: INFO: Pod "var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855": Phase="Pending", Reason="", readiness=false. Elapsed: 7.033287ms
Sep  5 22:48:36.000: INFO: Pod "var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028196374s
Sep  5 22:48:38.048: INFO: Pod "var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076104s
Sep  5 22:48:40.060: INFO: Pod "var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855": Phase="Pending", Reason="", readiness=false. Elapsed: 6.088524943s
Sep  5 22:48:42.069: INFO: Pod "var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855": Phase="Pending", Reason="", readiness=false. Elapsed: 8.097486111s
Sep  5 22:48:44.078: INFO: Pod "var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855": Phase="Pending", Reason="", readiness=false. Elapsed: 10.106635222s
Sep  5 22:48:46.103: INFO: Pod "var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855": Phase="Pending", Reason="", readiness=false. Elapsed: 12.131477729s
Sep  5 22:48:48.117: INFO: Pod "var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855": Phase="Pending", Reason="", readiness=false. Elapsed: 14.144915599s
Sep  5 22:48:50.149: INFO: Pod "var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855": Phase="Pending", Reason="", readiness=false. Elapsed: 16.176968701s
Sep  5 22:48:52.162: INFO: Pod "var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855": Phase="Pending", Reason="", readiness=false. Elapsed: 18.190474665s
Sep  5 22:48:54.176: INFO: Pod "var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855": Phase="Pending", Reason="", readiness=false. Elapsed: 20.204285861s
Sep  5 22:48:56.200: INFO: Pod "var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855": Phase="Pending", Reason="", readiness=false. Elapsed: 22.227755505s
Sep  5 22:48:58.217: INFO: Pod "var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855": Phase="Pending", Reason="", readiness=false. Elapsed: 24.245292776s
Sep  5 22:49:00.257: INFO: Pod "var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855": Phase="Pending", Reason="", readiness=false. Elapsed: 26.284786273s
Sep  5 22:49:02.274: INFO: Pod "var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855": Phase="Pending", Reason="", readiness=false. Elapsed: 28.302646563s
Sep  5 22:49:04.300: INFO: Pod "var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.328566443s
STEP: Saw pod success
Sep  5 22:49:04.300: INFO: Pod "var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855" satisfied condition "Succeeded or Failed"
Sep  5 22:49:04.315: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855 container dapi-container: <nil>
STEP: delete the pod
Sep  5 22:49:11.419: INFO: Waiting for pod var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855 to disappear
Sep  5 22:49:11.446: INFO: Pod var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855 still exists
Sep  5 22:49:13.447: INFO: Waiting for pod var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855 to disappear
Sep  5 22:49:13.458: INFO: Pod var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855 still exists
Sep  5 22:49:15.447: INFO: Waiting for pod var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855 to disappear
Sep  5 22:49:15.475: INFO: Pod var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855 still exists
Sep  5 22:49:17.447: INFO: Waiting for pod var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855 to disappear
Sep  5 22:49:17.458: INFO: Pod var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855 still exists
Sep  5 22:49:19.447: INFO: Waiting for pod var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855 to disappear
Sep  5 22:49:19.460: INFO: Pod var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855 still exists
Sep  5 22:49:21.448: INFO: Waiting for pod var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855 to disappear
Sep  5 22:49:21.460: INFO: Pod var-expansion-21f762de-b9fe-4c2b-9ee8-11abb2236855 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:49:21.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6291" for this suite.

• [SLOW TEST:48.700 seconds]
[k8s.io] Variable Expansion
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":296,"completed":103,"skipped":1779,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:49:21.938: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9277
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  5 22:49:53.963: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:49:54.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9277" for this suite.

• [SLOW TEST:32.350 seconds]
[k8s.io] Container Runtime
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":296,"completed":104,"skipped":1819,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:49:54.288: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3994
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  5 22:49:54.742: INFO: Waiting up to 5m0s for pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507" in namespace "emptydir-3994" to be "Succeeded or Failed"
Sep  5 22:49:54.761: INFO: Pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507": Phase="Pending", Reason="", readiness=false. Elapsed: 19.145737ms
Sep  5 22:49:56.771: INFO: Pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029024851s
Sep  5 22:49:58.799: INFO: Pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056736492s
Sep  5 22:50:00.819: INFO: Pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507": Phase="Pending", Reason="", readiness=false. Elapsed: 6.077344267s
Sep  5 22:50:02.833: INFO: Pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507": Phase="Pending", Reason="", readiness=false. Elapsed: 8.090974075s
Sep  5 22:50:04.861: INFO: Pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507": Phase="Pending", Reason="", readiness=false. Elapsed: 10.118944083s
Sep  5 22:50:06.875: INFO: Pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507": Phase="Pending", Reason="", readiness=false. Elapsed: 12.13321877s
Sep  5 22:50:08.891: INFO: Pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507": Phase="Pending", Reason="", readiness=false. Elapsed: 14.148745485s
Sep  5 22:50:10.904: INFO: Pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507": Phase="Pending", Reason="", readiness=false. Elapsed: 16.162236698s
Sep  5 22:50:12.916: INFO: Pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507": Phase="Pending", Reason="", readiness=false. Elapsed: 18.174167566s
Sep  5 22:50:14.930: INFO: Pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507": Phase="Pending", Reason="", readiness=false. Elapsed: 20.188425485s
Sep  5 22:50:16.944: INFO: Pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507": Phase="Pending", Reason="", readiness=false. Elapsed: 22.201925708s
Sep  5 22:50:18.957: INFO: Pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507": Phase="Pending", Reason="", readiness=false. Elapsed: 24.215300105s
Sep  5 22:50:20.970: INFO: Pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507": Phase="Pending", Reason="", readiness=false. Elapsed: 26.227874121s
Sep  5 22:50:22.982: INFO: Pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507": Phase="Pending", Reason="", readiness=false. Elapsed: 28.240507271s
Sep  5 22:50:25.001: INFO: Pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507": Phase="Pending", Reason="", readiness=false. Elapsed: 30.258698514s
Sep  5 22:50:27.019: INFO: Pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507": Phase="Pending", Reason="", readiness=false. Elapsed: 32.277585256s
Sep  5 22:50:29.031: INFO: Pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.289162244s
STEP: Saw pod success
Sep  5 22:50:29.031: INFO: Pod "pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507" satisfied condition "Succeeded or Failed"
Sep  5 22:50:29.037: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507 container test-container: <nil>
STEP: delete the pod
Sep  5 22:50:29.093: INFO: Waiting for pod pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507 to disappear
Sep  5 22:50:29.116: INFO: Pod pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507 still exists
Sep  5 22:50:31.117: INFO: Waiting for pod pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507 to disappear
Sep  5 22:50:31.132: INFO: Pod pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507 still exists
Sep  5 22:50:33.117: INFO: Waiting for pod pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507 to disappear
Sep  5 22:50:33.125: INFO: Pod pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507 still exists
Sep  5 22:50:35.116: INFO: Waiting for pod pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507 to disappear
Sep  5 22:50:35.144: INFO: Pod pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507 still exists
Sep  5 22:50:37.116: INFO: Waiting for pod pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507 to disappear
Sep  5 22:50:37.131: INFO: Pod pod-e3ad79d5-c3ee-4ce6-8dba-063289f4a507 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:50:37.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3994" for this suite.

• [SLOW TEST:43.160 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":105,"skipped":1823,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:50:37.449: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-4672
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
Sep  5 22:50:38.557: INFO: created pod pod-service-account-defaultsa
Sep  5 22:50:38.557: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep  5 22:50:38.594: INFO: created pod pod-service-account-mountsa
Sep  5 22:50:38.594: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep  5 22:50:38.636: INFO: created pod pod-service-account-nomountsa
Sep  5 22:50:38.636: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep  5 22:50:38.669: INFO: created pod pod-service-account-defaultsa-mountspec
Sep  5 22:50:38.669: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep  5 22:50:38.698: INFO: created pod pod-service-account-mountsa-mountspec
Sep  5 22:50:38.698: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep  5 22:50:38.761: INFO: created pod pod-service-account-nomountsa-mountspec
Sep  5 22:50:38.761: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep  5 22:50:38.828: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep  5 22:50:38.828: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep  5 22:50:38.893: INFO: created pod pod-service-account-mountsa-nomountspec
Sep  5 22:50:38.893: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep  5 22:50:38.924: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep  5 22:50:38.924: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:50:38.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4672" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":296,"completed":106,"skipped":1870,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:50:39.203: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7652
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test env composition
Sep  5 22:50:39.872: INFO: Waiting up to 5m0s for pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d" in namespace "var-expansion-7652" to be "Succeeded or Failed"
Sep  5 22:50:39.883: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.70714ms
Sep  5 22:50:41.919: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04774961s
Sep  5 22:50:43.943: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071181869s
Sep  5 22:50:45.961: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.08984801s
Sep  5 22:50:48.141: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.268961792s
Sep  5 22:50:50.157: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.285512209s
Sep  5 22:50:52.171: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.299632538s
Sep  5 22:50:54.185: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.313667022s
Sep  5 22:50:56.205: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.333287426s
Sep  5 22:50:58.219: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.346981688s
Sep  5 22:51:00.238: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 20.366348456s
Sep  5 22:51:02.249: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 22.377819104s
Sep  5 22:51:04.269: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 24.396995904s
Sep  5 22:51:06.306: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 26.434689573s
Sep  5 22:51:08.324: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 28.452241345s
Sep  5 22:51:10.360: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 30.488653404s
Sep  5 22:51:12.369: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 32.497868373s
Sep  5 22:51:14.388: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 34.516684512s
Sep  5 22:51:16.409: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 36.537101455s
Sep  5 22:51:18.431: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 38.55963851s
Sep  5 22:51:20.477: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Pending", Reason="", readiness=false. Elapsed: 40.605537406s
Sep  5 22:51:22.495: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 42.623089558s
STEP: Saw pod success
Sep  5 22:51:22.495: INFO: Pod "var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d" satisfied condition "Succeeded or Failed"
Sep  5 22:51:22.509: INFO: Trying to get logs from node sc2-10-185-226-150.eng.vmware.com pod var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d container dapi-container: <nil>
STEP: delete the pod
Sep  5 22:51:22.649: INFO: Waiting for pod var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d to disappear
Sep  5 22:51:22.687: INFO: Pod var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d still exists
Sep  5 22:51:24.688: INFO: Waiting for pod var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d to disappear
Sep  5 22:51:24.749: INFO: Pod var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d still exists
Sep  5 22:51:26.688: INFO: Waiting for pod var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d to disappear
Sep  5 22:51:26.701: INFO: Pod var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d still exists
Sep  5 22:51:28.687: INFO: Waiting for pod var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d to disappear
Sep  5 22:51:28.703: INFO: Pod var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d still exists
Sep  5 22:51:30.688: INFO: Waiting for pod var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d to disappear
Sep  5 22:51:30.703: INFO: Pod var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d still exists
Sep  5 22:51:32.688: INFO: Waiting for pod var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d to disappear
Sep  5 22:51:32.697: INFO: Pod var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d still exists
Sep  5 22:51:34.687: INFO: Waiting for pod var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d to disappear
Sep  5 22:51:34.700: INFO: Pod var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d still exists
Sep  5 22:51:36.687: INFO: Waiting for pod var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d to disappear
Sep  5 22:51:36.699: INFO: Pod var-expansion-7264d32f-864a-4de9-9d90-b918e76cdb9d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:51:36.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7652" for this suite.

• [SLOW TEST:58.077 seconds]
[k8s.io] Variable Expansion
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":296,"completed":107,"skipped":1876,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:51:37.281: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-899
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-2773a7c3-00af-415a-a3be-6d16edcfa918
STEP: Creating a pod to test consume secrets
Sep  5 22:51:38.003: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed" in namespace "projected-899" to be "Succeeded or Failed"
Sep  5 22:51:38.035: INFO: Pod "pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed": Phase="Pending", Reason="", readiness=false. Elapsed: 31.093616ms
Sep  5 22:51:40.051: INFO: Pod "pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047999549s
Sep  5 22:51:42.065: INFO: Pod "pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061583131s
Sep  5 22:51:44.080: INFO: Pod "pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076871539s
Sep  5 22:51:46.095: INFO: Pod "pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed": Phase="Pending", Reason="", readiness=false. Elapsed: 8.09199894s
Sep  5 22:51:48.113: INFO: Pod "pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed": Phase="Pending", Reason="", readiness=false. Elapsed: 10.109302498s
Sep  5 22:51:50.149: INFO: Pod "pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed": Phase="Pending", Reason="", readiness=false. Elapsed: 12.145548766s
Sep  5 22:51:52.157: INFO: Pod "pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed": Phase="Pending", Reason="", readiness=false. Elapsed: 14.153031482s
Sep  5 22:51:54.170: INFO: Pod "pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed": Phase="Pending", Reason="", readiness=false. Elapsed: 16.166135542s
Sep  5 22:51:56.186: INFO: Pod "pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed": Phase="Pending", Reason="", readiness=false. Elapsed: 18.182858039s
Sep  5 22:51:58.199: INFO: Pod "pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed": Phase="Pending", Reason="", readiness=false. Elapsed: 20.195184443s
Sep  5 22:52:00.227: INFO: Pod "pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed": Phase="Pending", Reason="", readiness=false. Elapsed: 22.223313594s
Sep  5 22:52:02.251: INFO: Pod "pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed": Phase="Pending", Reason="", readiness=false. Elapsed: 24.247010228s
Sep  5 22:52:04.273: INFO: Pod "pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed": Phase="Running", Reason="", readiness=true. Elapsed: 26.269818314s
Sep  5 22:52:06.290: INFO: Pod "pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed": Phase="Running", Reason="", readiness=true. Elapsed: 28.286824723s
Sep  5 22:52:08.309: INFO: Pod "pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed": Phase="Running", Reason="", readiness=true. Elapsed: 30.305866985s
Sep  5 22:52:10.336: INFO: Pod "pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.332222127s
STEP: Saw pod success
Sep  5 22:52:10.336: INFO: Pod "pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed" satisfied condition "Succeeded or Failed"
Sep  5 22:52:10.346: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  5 22:52:16.521: INFO: Waiting for pod pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed to disappear
Sep  5 22:52:16.534: INFO: Pod pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed still exists
Sep  5 22:52:18.536: INFO: Waiting for pod pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed to disappear
Sep  5 22:52:18.552: INFO: Pod pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed still exists
Sep  5 22:52:20.534: INFO: Waiting for pod pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed to disappear
Sep  5 22:52:20.547: INFO: Pod pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed still exists
Sep  5 22:52:22.534: INFO: Waiting for pod pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed to disappear
Sep  5 22:52:22.544: INFO: Pod pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed still exists
Sep  5 22:52:24.535: INFO: Waiting for pod pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed to disappear
Sep  5 22:52:24.545: INFO: Pod pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed still exists
Sep  5 22:52:26.536: INFO: Waiting for pod pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed to disappear
Sep  5 22:52:26.547: INFO: Pod pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed still exists
Sep  5 22:52:28.535: INFO: Waiting for pod pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed to disappear
Sep  5 22:52:28.548: INFO: Pod pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed still exists
Sep  5 22:52:30.535: INFO: Waiting for pod pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed to disappear
Sep  5 22:52:30.552: INFO: Pod pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed still exists
Sep  5 22:52:32.534: INFO: Waiting for pod pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed to disappear
Sep  5 22:52:32.557: INFO: Pod pod-projected-secrets-6bf67774-cb10-46ea-9ec2-aab79fc56fed no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:52:32.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-899" for this suite.

• [SLOW TEST:55.822 seconds]
[sig-storage] Projected secret
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":296,"completed":108,"skipped":1883,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:52:33.103: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5236
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name projected-secret-test-map-15c6f206-45da-425e-8e32-80c259bb5303
STEP: Creating a pod to test consume secrets
Sep  5 22:52:34.194: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423" in namespace "projected-5236" to be "Succeeded or Failed"
Sep  5 22:52:34.237: INFO: Pod "pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423": Phase="Pending", Reason="", readiness=false. Elapsed: 42.785735ms
Sep  5 22:52:36.247: INFO: Pod "pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052465176s
Sep  5 22:52:38.266: INFO: Pod "pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071977086s
Sep  5 22:52:40.283: INFO: Pod "pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423": Phase="Pending", Reason="", readiness=false. Elapsed: 6.088878041s
Sep  5 22:52:42.300: INFO: Pod "pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423": Phase="Pending", Reason="", readiness=false. Elapsed: 8.105334588s
Sep  5 22:52:44.309: INFO: Pod "pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423": Phase="Pending", Reason="", readiness=false. Elapsed: 10.114060944s
Sep  5 22:52:46.320: INFO: Pod "pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423": Phase="Pending", Reason="", readiness=false. Elapsed: 12.12553612s
Sep  5 22:52:48.331: INFO: Pod "pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423": Phase="Pending", Reason="", readiness=false. Elapsed: 14.136656451s
Sep  5 22:52:50.357: INFO: Pod "pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423": Phase="Pending", Reason="", readiness=false. Elapsed: 16.162118785s
Sep  5 22:52:52.405: INFO: Pod "pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423": Phase="Pending", Reason="", readiness=false. Elapsed: 18.210751694s
Sep  5 22:52:54.416: INFO: Pod "pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423": Phase="Pending", Reason="", readiness=false. Elapsed: 20.221060446s
Sep  5 22:52:56.427: INFO: Pod "pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423": Phase="Pending", Reason="", readiness=false. Elapsed: 22.232690569s
Sep  5 22:52:58.439: INFO: Pod "pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423": Phase="Pending", Reason="", readiness=false. Elapsed: 24.244414243s
Sep  5 22:53:00.452: INFO: Pod "pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423": Phase="Pending", Reason="", readiness=false. Elapsed: 26.258032513s
Sep  5 22:53:02.469: INFO: Pod "pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423": Phase="Pending", Reason="", readiness=false. Elapsed: 28.274134044s
Sep  5 22:53:04.481: INFO: Pod "pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.286287894s
STEP: Saw pod success
Sep  5 22:53:04.481: INFO: Pod "pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423" satisfied condition "Succeeded or Failed"
Sep  5 22:53:04.492: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  5 22:53:04.555: INFO: Waiting for pod pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423 to disappear
Sep  5 22:53:04.579: INFO: Pod pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423 still exists
Sep  5 22:53:06.581: INFO: Waiting for pod pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423 to disappear
Sep  5 22:53:06.595: INFO: Pod pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423 still exists
Sep  5 22:53:08.581: INFO: Waiting for pod pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423 to disappear
Sep  5 22:53:08.598: INFO: Pod pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423 still exists
Sep  5 22:53:10.580: INFO: Waiting for pod pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423 to disappear
Sep  5 22:53:10.592: INFO: Pod pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423 still exists
Sep  5 22:53:12.580: INFO: Waiting for pod pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423 to disappear
Sep  5 22:53:12.593: INFO: Pod pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423 still exists
Sep  5 22:53:14.580: INFO: Waiting for pod pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423 to disappear
Sep  5 22:53:14.599: INFO: Pod pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423 still exists
Sep  5 22:53:16.580: INFO: Waiting for pod pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423 to disappear
Sep  5 22:53:16.595: INFO: Pod pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423 still exists
Sep  5 22:53:18.580: INFO: Waiting for pod pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423 to disappear
Sep  5 22:53:18.601: INFO: Pod pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423 still exists
Sep  5 22:53:20.580: INFO: Waiting for pod pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423 to disappear
Sep  5 22:53:20.599: INFO: Pod pod-projected-secrets-f7e7508a-46a7-48c6-bc55-211135d6b423 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:53:20.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5236" for this suite.

• [SLOW TEST:47.977 seconds]
[sig-storage] Projected secret
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":296,"completed":109,"skipped":1885,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:53:21.081: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4620
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-c40a88c1-c19f-46ed-9356-cfee61b08c79
STEP: Creating a pod to test consume configMaps
Sep  5 22:53:21.672: INFO: Waiting up to 5m0s for pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60" in namespace "configmap-4620" to be "Succeeded or Failed"
Sep  5 22:53:21.704: INFO: Pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60": Phase="Pending", Reason="", readiness=false. Elapsed: 32.392014ms
Sep  5 22:53:23.734: INFO: Pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.062047761s
Sep  5 22:53:25.752: INFO: Pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080172202s
Sep  5 22:53:27.782: INFO: Pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60": Phase="Pending", Reason="", readiness=false. Elapsed: 6.110196313s
Sep  5 22:53:29.797: INFO: Pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60": Phase="Pending", Reason="", readiness=false. Elapsed: 8.125110043s
Sep  5 22:53:31.812: INFO: Pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60": Phase="Pending", Reason="", readiness=false. Elapsed: 10.139795725s
Sep  5 22:53:33.829: INFO: Pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60": Phase="Pending", Reason="", readiness=false. Elapsed: 12.157380539s
Sep  5 22:53:35.842: INFO: Pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60": Phase="Pending", Reason="", readiness=false. Elapsed: 14.170346197s
Sep  5 22:53:37.859: INFO: Pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60": Phase="Pending", Reason="", readiness=false. Elapsed: 16.186662832s
Sep  5 22:53:39.881: INFO: Pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60": Phase="Pending", Reason="", readiness=false. Elapsed: 18.20933227s
Sep  5 22:53:41.890: INFO: Pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60": Phase="Pending", Reason="", readiness=false. Elapsed: 20.217735595s
Sep  5 22:53:43.900: INFO: Pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60": Phase="Pending", Reason="", readiness=false. Elapsed: 22.227647192s
Sep  5 22:53:45.911: INFO: Pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60": Phase="Pending", Reason="", readiness=false. Elapsed: 24.238729218s
Sep  5 22:53:47.928: INFO: Pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60": Phase="Pending", Reason="", readiness=false. Elapsed: 26.255943047s
Sep  5 22:53:49.938: INFO: Pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60": Phase="Pending", Reason="", readiness=false. Elapsed: 28.265751475s
Sep  5 22:53:51.951: INFO: Pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60": Phase="Pending", Reason="", readiness=false. Elapsed: 30.278681157s
Sep  5 22:53:53.961: INFO: Pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60": Phase="Pending", Reason="", readiness=false. Elapsed: 32.289579598s
Sep  5 22:53:55.977: INFO: Pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.304628527s
STEP: Saw pod success
Sep  5 22:53:55.977: INFO: Pod "pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60" satisfied condition "Succeeded or Failed"
Sep  5 22:53:55.985: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60 container agnhost-container: <nil>
STEP: delete the pod
Sep  5 22:54:02.312: INFO: Waiting for pod pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60 to disappear
Sep  5 22:54:02.336: INFO: Pod pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60 still exists
Sep  5 22:54:04.337: INFO: Waiting for pod pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60 to disappear
Sep  5 22:54:04.347: INFO: Pod pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60 still exists
Sep  5 22:54:06.338: INFO: Waiting for pod pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60 to disappear
Sep  5 22:54:06.354: INFO: Pod pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60 still exists
Sep  5 22:54:08.336: INFO: Waiting for pod pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60 to disappear
Sep  5 22:54:08.352: INFO: Pod pod-configmaps-2122cac3-16b7-4a0c-9c1d-ec9fc9f04b60 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:54:08.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4620" for this suite.

• [SLOW TEST:47.541 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":110,"skipped":1888,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:54:08.622: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6176
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 22:54:10.303: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  5 22:54:12.328: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:54:14.339: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:54:16.346: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:54:18.340: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:54:20.339: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:54:22.351: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:54:24.340: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:54:26.350: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:54:28.343: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:54:30.343: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:54:32.349: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:54:34.341: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:54:36.341: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:54:38.344: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:54:40.344: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:54:42.341: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:54:44.339: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 54, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 22:54:47.377: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 22:54:47.389: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:54:49.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6176" for this suite.
STEP: Destroying namespace "webhook-6176-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:41.876 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":296,"completed":111,"skipped":1899,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:54:50.498: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1417
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-1417
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  5 22:54:51.114: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  5 22:54:51.518: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:54:53.529: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:54:55.534: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:54:57.538: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:54:59.562: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:55:01.533: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:55:03.528: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:55:05.531: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:55:07.533: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:55:09.538: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:55:11.531: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:55:13.532: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:55:15.534: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:55:17.533: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:55:19.530: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:55:21.527: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:55:23.527: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:55:25.530: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 22:55:27.534: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 22:55:29.543: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 22:55:31.530: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 22:55:33.539: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 22:55:35.537: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep  5 22:55:35.553: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep  5 22:55:37.565: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep  5 22:55:39.566: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep  5 22:55:41.590: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep  5 22:55:41.622: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep  5 22:56:05.734: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Sep  5 22:56:05.737: INFO: Breadth first check of 172.26.1.194 on host 10.185.226.150...
Sep  5 22:56:05.746: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.26.1.197:9080/dial?request=hostname&protocol=http&host=172.26.1.194&port=8080&tries=1'] Namespace:pod-network-test-1417 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  5 22:56:05.746: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 22:56:06.327: INFO: Waiting for responses: map[]
Sep  5 22:56:06.328: INFO: reached 172.26.1.194 after 0/1 tries
Sep  5 22:56:06.328: INFO: Breadth first check of 172.26.1.195 on host 10.185.226.233...
Sep  5 22:56:06.341: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.26.1.197:9080/dial?request=hostname&protocol=http&host=172.26.1.195&port=8080&tries=1'] Namespace:pod-network-test-1417 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  5 22:56:06.341: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 22:56:06.532: INFO: Waiting for responses: map[]
Sep  5 22:56:06.532: INFO: reached 172.26.1.195 after 0/1 tries
Sep  5 22:56:06.532: INFO: Breadth first check of 172.26.1.196 on host 10.185.233.127...
Sep  5 22:56:06.555: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.26.1.197:9080/dial?request=hostname&protocol=http&host=172.26.1.196&port=8080&tries=1'] Namespace:pod-network-test-1417 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  5 22:56:06.555: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 22:56:06.757: INFO: Waiting for responses: map[]
Sep  5 22:56:06.757: INFO: reached 172.26.1.196 after 0/1 tries
Sep  5 22:56:06.757: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:56:06.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1417" for this suite.

• [SLOW TEST:76.940 seconds]
[sig-network] Networking
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":296,"completed":112,"skipped":1902,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:56:07.439: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8157
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Sep  5 22:56:08.346: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 22:56:22.249: INFO: >>> kubeConfig: ./kconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:56:52.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8157" for this suite.

• [SLOW TEST:44.955 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":296,"completed":113,"skipped":1926,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:56:52.394: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7545
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  5 22:57:27.469: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d1dd01a7-a9dc-49e8-b413-66a08d16636b"
Sep  5 22:57:27.469: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d1dd01a7-a9dc-49e8-b413-66a08d16636b" in namespace "pods-7545" to be "terminated due to deadline exceeded"
Sep  5 22:57:27.487: INFO: Pod "pod-update-activedeadlineseconds-d1dd01a7-a9dc-49e8-b413-66a08d16636b": Phase="Running", Reason="", readiness=true. Elapsed: 17.709365ms
Sep  5 22:57:29.507: INFO: Pod "pod-update-activedeadlineseconds-d1dd01a7-a9dc-49e8-b413-66a08d16636b": Phase="Running", Reason="", readiness=true. Elapsed: 2.037497035s
Sep  5 22:57:31.520: INFO: Pod "pod-update-activedeadlineseconds-d1dd01a7-a9dc-49e8-b413-66a08d16636b": Phase="Failed", Reason="DeadlineExceeded", readiness=true. Elapsed: 4.050802879s
Sep  5 22:57:31.520: INFO: Pod "pod-update-activedeadlineseconds-d1dd01a7-a9dc-49e8-b413-66a08d16636b" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:57:31.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7545" for this suite.

• [SLOW TEST:39.588 seconds]
[k8s.io] Pods
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":296,"completed":114,"skipped":1935,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:57:31.982: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2909
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-77ac8ea7-d388-413a-b7bb-4f26e294919e
STEP: Creating a pod to test consume configMaps
Sep  5 22:57:32.508: INFO: Waiting up to 5m0s for pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37" in namespace "configmap-2909" to be "Succeeded or Failed"
Sep  5 22:57:32.541: INFO: Pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37": Phase="Pending", Reason="", readiness=false. Elapsed: 32.279184ms
Sep  5 22:57:34.555: INFO: Pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046356496s
Sep  5 22:57:36.578: INFO: Pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06935273s
Sep  5 22:57:38.593: INFO: Pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084994481s
Sep  5 22:57:40.609: INFO: Pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37": Phase="Pending", Reason="", readiness=false. Elapsed: 8.100403607s
Sep  5 22:57:42.620: INFO: Pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37": Phase="Pending", Reason="", readiness=false. Elapsed: 10.111515206s
Sep  5 22:57:44.654: INFO: Pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37": Phase="Pending", Reason="", readiness=false. Elapsed: 12.145442675s
Sep  5 22:57:46.677: INFO: Pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37": Phase="Pending", Reason="", readiness=false. Elapsed: 14.168198957s
Sep  5 22:57:48.692: INFO: Pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37": Phase="Pending", Reason="", readiness=false. Elapsed: 16.1839391s
Sep  5 22:57:50.706: INFO: Pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37": Phase="Pending", Reason="", readiness=false. Elapsed: 18.197974003s
Sep  5 22:57:52.719: INFO: Pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37": Phase="Pending", Reason="", readiness=false. Elapsed: 20.21091213s
Sep  5 22:57:54.731: INFO: Pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37": Phase="Pending", Reason="", readiness=false. Elapsed: 22.222751348s
Sep  5 22:57:56.750: INFO: Pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37": Phase="Pending", Reason="", readiness=false. Elapsed: 24.24189665s
Sep  5 22:57:58.762: INFO: Pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37": Phase="Pending", Reason="", readiness=false. Elapsed: 26.253155982s
Sep  5 22:58:00.775: INFO: Pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37": Phase="Pending", Reason="", readiness=false. Elapsed: 28.266801865s
Sep  5 22:58:02.863: INFO: Pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37": Phase="Pending", Reason="", readiness=false. Elapsed: 30.354066085s
Sep  5 22:58:04.877: INFO: Pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37": Phase="Pending", Reason="", readiness=false. Elapsed: 32.368170153s
Sep  5 22:58:06.901: INFO: Pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.392764961s
STEP: Saw pod success
Sep  5 22:58:06.901: INFO: Pod "pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37" satisfied condition "Succeeded or Failed"
Sep  5 22:58:06.910: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37 container agnhost-container: <nil>
STEP: delete the pod
Sep  5 22:58:07.038: INFO: Waiting for pod pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37 to disappear
Sep  5 22:58:07.064: INFO: Pod pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37 still exists
Sep  5 22:58:09.065: INFO: Waiting for pod pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37 to disappear
Sep  5 22:58:09.078: INFO: Pod pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37 still exists
Sep  5 22:58:11.065: INFO: Waiting for pod pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37 to disappear
Sep  5 22:58:11.078: INFO: Pod pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37 still exists
Sep  5 22:58:13.065: INFO: Waiting for pod pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37 to disappear
Sep  5 22:58:13.081: INFO: Pod pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37 still exists
Sep  5 22:58:15.065: INFO: Waiting for pod pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37 to disappear
Sep  5 22:58:15.075: INFO: Pod pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37 still exists
Sep  5 22:58:17.065: INFO: Waiting for pod pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37 to disappear
Sep  5 22:58:17.077: INFO: Pod pod-configmaps-4b07f368-92c2-4ad0-95e2-cc23de531d37 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:58:17.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2909" for this suite.

• [SLOW TEST:45.322 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":296,"completed":115,"skipped":1966,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:58:17.305: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-46
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  5 22:58:17.797: INFO: Waiting up to 5m0s for pod "pod-3337fb33-d7a6-46c5-bd1d-515decad709a" in namespace "emptydir-46" to be "Succeeded or Failed"
Sep  5 22:58:17.810: INFO: Pod "pod-3337fb33-d7a6-46c5-bd1d-515decad709a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.882837ms
Sep  5 22:58:19.825: INFO: Pod "pod-3337fb33-d7a6-46c5-bd1d-515decad709a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028239846s
Sep  5 22:58:21.838: INFO: Pod "pod-3337fb33-d7a6-46c5-bd1d-515decad709a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041469375s
Sep  5 22:58:23.851: INFO: Pod "pod-3337fb33-d7a6-46c5-bd1d-515decad709a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053619405s
Sep  5 22:58:25.875: INFO: Pod "pod-3337fb33-d7a6-46c5-bd1d-515decad709a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.077986603s
Sep  5 22:58:27.906: INFO: Pod "pod-3337fb33-d7a6-46c5-bd1d-515decad709a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.108705525s
Sep  5 22:58:29.919: INFO: Pod "pod-3337fb33-d7a6-46c5-bd1d-515decad709a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.121571012s
Sep  5 22:58:31.932: INFO: Pod "pod-3337fb33-d7a6-46c5-bd1d-515decad709a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.134643301s
Sep  5 22:58:33.943: INFO: Pod "pod-3337fb33-d7a6-46c5-bd1d-515decad709a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.145798985s
Sep  5 22:58:35.957: INFO: Pod "pod-3337fb33-d7a6-46c5-bd1d-515decad709a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.159644285s
Sep  5 22:58:37.973: INFO: Pod "pod-3337fb33-d7a6-46c5-bd1d-515decad709a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.176120069s
Sep  5 22:58:39.988: INFO: Pod "pod-3337fb33-d7a6-46c5-bd1d-515decad709a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.190530467s
Sep  5 22:58:42.007: INFO: Pod "pod-3337fb33-d7a6-46c5-bd1d-515decad709a": Phase="Pending", Reason="", readiness=false. Elapsed: 24.209685309s
Sep  5 22:58:44.016: INFO: Pod "pod-3337fb33-d7a6-46c5-bd1d-515decad709a": Phase="Pending", Reason="", readiness=false. Elapsed: 26.218904179s
Sep  5 22:58:46.030: INFO: Pod "pod-3337fb33-d7a6-46c5-bd1d-515decad709a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.233370596s
Sep  5 22:58:48.043: INFO: Pod "pod-3337fb33-d7a6-46c5-bd1d-515decad709a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.245791988s
STEP: Saw pod success
Sep  5 22:58:48.043: INFO: Pod "pod-3337fb33-d7a6-46c5-bd1d-515decad709a" satisfied condition "Succeeded or Failed"
Sep  5 22:58:48.051: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-3337fb33-d7a6-46c5-bd1d-515decad709a container test-container: <nil>
STEP: delete the pod
Sep  5 22:58:54.384: INFO: Waiting for pod pod-3337fb33-d7a6-46c5-bd1d-515decad709a to disappear
Sep  5 22:58:54.396: INFO: Pod pod-3337fb33-d7a6-46c5-bd1d-515decad709a still exists
Sep  5 22:58:56.397: INFO: Waiting for pod pod-3337fb33-d7a6-46c5-bd1d-515decad709a to disappear
Sep  5 22:58:56.413: INFO: Pod pod-3337fb33-d7a6-46c5-bd1d-515decad709a still exists
Sep  5 22:58:58.397: INFO: Waiting for pod pod-3337fb33-d7a6-46c5-bd1d-515decad709a to disappear
Sep  5 22:58:58.408: INFO: Pod pod-3337fb33-d7a6-46c5-bd1d-515decad709a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:58:58.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-46" for this suite.

• [SLOW TEST:41.345 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":116,"skipped":2039,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:58:58.650: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 22:59:00.002: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  5 22:59:02.028: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:59:04.047: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:59:06.057: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:59:08.041: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:59:10.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:59:12.044: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:59:14.043: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:59:16.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:59:18.056: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:59:20.049: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:59:22.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:59:24.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:59:26.042: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:59:28.038: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:59:30.042: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 59, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 22:59:33.084: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:59:33.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3498" for this suite.
STEP: Destroying namespace "webhook-3498-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:35.825 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":296,"completed":117,"skipped":2055,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:59:34.475: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8219
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-downwardapi-688n
STEP: Creating a pod to test atomic-volume-subpath
Sep  5 22:59:34.935: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-688n" in namespace "subpath-8219" to be "Succeeded or Failed"
Sep  5 22:59:34.962: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Pending", Reason="", readiness=false. Elapsed: 26.916618ms
Sep  5 22:59:36.971: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035921281s
Sep  5 22:59:38.985: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049788711s
Sep  5 22:59:40.997: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Pending", Reason="", readiness=false. Elapsed: 6.062547196s
Sep  5 22:59:43.013: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Pending", Reason="", readiness=false. Elapsed: 8.078256781s
Sep  5 22:59:45.020: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Pending", Reason="", readiness=false. Elapsed: 10.085518179s
Sep  5 22:59:47.035: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Pending", Reason="", readiness=false. Elapsed: 12.10012542s
Sep  5 22:59:49.055: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Pending", Reason="", readiness=false. Elapsed: 14.120404399s
Sep  5 22:59:51.077: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Pending", Reason="", readiness=false. Elapsed: 16.142241506s
Sep  5 22:59:53.090: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Pending", Reason="", readiness=false. Elapsed: 18.155509202s
Sep  5 22:59:55.101: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Pending", Reason="", readiness=false. Elapsed: 20.165993884s
Sep  5 22:59:57.111: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Pending", Reason="", readiness=false. Elapsed: 22.175612206s
Sep  5 22:59:59.130: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Pending", Reason="", readiness=false. Elapsed: 24.195447512s
Sep  5 23:00:01.148: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Pending", Reason="", readiness=false. Elapsed: 26.213324168s
Sep  5 23:00:03.193: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Pending", Reason="", readiness=false. Elapsed: 28.258012685s
Sep  5 23:00:05.209: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Pending", Reason="", readiness=false. Elapsed: 30.273835057s
Sep  5 23:00:07.246: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Running", Reason="", readiness=true. Elapsed: 32.310968976s
Sep  5 23:00:09.289: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Running", Reason="", readiness=true. Elapsed: 34.354175812s
Sep  5 23:00:11.309: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Running", Reason="", readiness=true. Elapsed: 36.374190346s
Sep  5 23:00:13.323: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Running", Reason="", readiness=true. Elapsed: 38.388097461s
Sep  5 23:00:15.339: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Running", Reason="", readiness=true. Elapsed: 40.403712829s
Sep  5 23:00:17.361: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Running", Reason="", readiness=true. Elapsed: 42.426518714s
Sep  5 23:00:19.375: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Running", Reason="", readiness=true. Elapsed: 44.43977114s
Sep  5 23:00:21.388: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Running", Reason="", readiness=true. Elapsed: 46.452940756s
Sep  5 23:00:23.402: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Running", Reason="", readiness=true. Elapsed: 48.466779991s
Sep  5 23:00:25.421: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Running", Reason="", readiness=true. Elapsed: 50.486070742s
Sep  5 23:00:27.456: INFO: Pod "pod-subpath-test-downwardapi-688n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 52.521496596s
STEP: Saw pod success
Sep  5 23:00:27.456: INFO: Pod "pod-subpath-test-downwardapi-688n" satisfied condition "Succeeded or Failed"
Sep  5 23:00:27.467: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-subpath-test-downwardapi-688n container test-container-subpath-downwardapi-688n: <nil>
STEP: delete the pod
Sep  5 23:00:27.559: INFO: Waiting for pod pod-subpath-test-downwardapi-688n to disappear
Sep  5 23:00:27.571: INFO: Pod pod-subpath-test-downwardapi-688n still exists
Sep  5 23:00:29.572: INFO: Waiting for pod pod-subpath-test-downwardapi-688n to disappear
Sep  5 23:00:29.581: INFO: Pod pod-subpath-test-downwardapi-688n still exists
Sep  5 23:00:31.572: INFO: Waiting for pod pod-subpath-test-downwardapi-688n to disappear
Sep  5 23:00:31.595: INFO: Pod pod-subpath-test-downwardapi-688n still exists
Sep  5 23:00:33.572: INFO: Waiting for pod pod-subpath-test-downwardapi-688n to disappear
Sep  5 23:00:33.592: INFO: Pod pod-subpath-test-downwardapi-688n still exists
Sep  5 23:00:35.572: INFO: Waiting for pod pod-subpath-test-downwardapi-688n to disappear
Sep  5 23:00:35.588: INFO: Pod pod-subpath-test-downwardapi-688n still exists
Sep  5 23:00:37.572: INFO: Waiting for pod pod-subpath-test-downwardapi-688n to disappear
Sep  5 23:00:37.579: INFO: Pod pod-subpath-test-downwardapi-688n no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-688n
Sep  5 23:00:37.580: INFO: Deleting pod "pod-subpath-test-downwardapi-688n" in namespace "subpath-8219"
[AfterEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:00:37.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8219" for this suite.

• [SLOW TEST:63.935 seconds]
[sig-storage] Subpath
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":296,"completed":118,"skipped":2091,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:00:38.411: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3182
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3182.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3182.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3182.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3182.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 23:01:23.305: INFO: DNS probes using dns-test-018c406c-b47f-4153-9a37-93b8d4f89ef4 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3182.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3182.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3182.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3182.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 23:01:53.591: INFO: Unable to read wheezy_udp@dns-test-service-3.dns-3182.svc.cluster.local from pod dns-3182/dns-test-fdb71d06-19e2-4202-a4dd-272ff00bcb97: Get "https://10.185.230.78:6443/api/v1/namespaces/dns-3182/pods/dns-test-fdb71d06-19e2-4202-a4dd-272ff00bcb97/proxy/results/wheezy_udp@dns-test-service-3.dns-3182.svc.cluster.local": stream error: stream ID 10477; INTERNAL_ERROR
Sep  5 23:01:53.610: INFO: Lookups using dns-3182/dns-test-fdb71d06-19e2-4202-a4dd-272ff00bcb97 failed for: [wheezy_udp@dns-test-service-3.dns-3182.svc.cluster.local]

Sep  5 23:01:58.648: INFO: DNS probes using dns-test-fdb71d06-19e2-4202-a4dd-272ff00bcb97 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3182.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3182.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3182.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3182.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 23:02:22.876: INFO: Unable to read wheezy_udp@dns-test-service-3.dns-3182.svc.cluster.local from pod dns-3182/dns-test-59eeb453-dc88-49b8-b8b3-dcbe307aa45e: an error on the server ("unknown") has prevented the request from succeeding (get pods dns-test-59eeb453-dc88-49b8-b8b3-dcbe307aa45e)
Sep  5 23:02:22.894: INFO: Unable to read jessie_udp@dns-test-service-3.dns-3182.svc.cluster.local from pod dns-3182/dns-test-59eeb453-dc88-49b8-b8b3-dcbe307aa45e: an error on the server ("unknown") has prevented the request from succeeding (get pods dns-test-59eeb453-dc88-49b8-b8b3-dcbe307aa45e)
Sep  5 23:02:22.895: INFO: Lookups using dns-3182/dns-test-59eeb453-dc88-49b8-b8b3-dcbe307aa45e failed for: [wheezy_udp@dns-test-service-3.dns-3182.svc.cluster.local jessie_udp@dns-test-service-3.dns-3182.svc.cluster.local]

Sep  5 23:02:27.953: INFO: DNS probes using dns-test-59eeb453-dc88-49b8-b8b3-dcbe307aa45e succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:02:28.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3182" for this suite.

• [SLOW TEST:110.233 seconds]
[sig-network] DNS
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":296,"completed":119,"skipped":2128,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:02:28.644: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-271
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  5 23:02:29.337: INFO: Waiting up to 5m0s for pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc" in namespace "emptydir-271" to be "Succeeded or Failed"
Sep  5 23:02:29.358: INFO: Pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc": Phase="Pending", Reason="", readiness=false. Elapsed: 21.198258ms
Sep  5 23:02:31.367: INFO: Pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030174781s
Sep  5 23:02:33.379: INFO: Pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041929596s
Sep  5 23:02:35.408: INFO: Pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070751248s
Sep  5 23:02:37.416: INFO: Pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.079402201s
Sep  5 23:02:39.423: INFO: Pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.085992081s
Sep  5 23:02:41.459: INFO: Pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.121583651s
Sep  5 23:02:43.482: INFO: Pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.144996967s
Sep  5 23:02:45.504: INFO: Pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc": Phase="Pending", Reason="", readiness=false. Elapsed: 16.167305692s
Sep  5 23:02:47.526: INFO: Pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc": Phase="Pending", Reason="", readiness=false. Elapsed: 18.189013152s
Sep  5 23:02:49.535: INFO: Pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc": Phase="Pending", Reason="", readiness=false. Elapsed: 20.197551922s
Sep  5 23:02:51.554: INFO: Pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc": Phase="Pending", Reason="", readiness=false. Elapsed: 22.217304269s
Sep  5 23:02:53.588: INFO: Pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc": Phase="Pending", Reason="", readiness=false. Elapsed: 24.250601281s
Sep  5 23:02:55.602: INFO: Pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc": Phase="Pending", Reason="", readiness=false. Elapsed: 26.264554052s
Sep  5 23:02:57.618: INFO: Pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc": Phase="Pending", Reason="", readiness=false. Elapsed: 28.281069522s
Sep  5 23:02:59.630: INFO: Pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc": Phase="Running", Reason="", readiness=true. Elapsed: 30.293069637s
Sep  5 23:03:01.699: INFO: Pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc": Phase="Running", Reason="", readiness=true. Elapsed: 32.361721454s
Sep  5 23:03:03.716: INFO: Pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.379131466s
STEP: Saw pod success
Sep  5 23:03:03.716: INFO: Pod "pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc" satisfied condition "Succeeded or Failed"
Sep  5 23:03:03.726: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc container test-container: <nil>
STEP: delete the pod
Sep  5 23:03:10.600: INFO: Waiting for pod pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc to disappear
Sep  5 23:03:10.631: INFO: Pod pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc still exists
Sep  5 23:03:12.631: INFO: Waiting for pod pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc to disappear
Sep  5 23:03:12.649: INFO: Pod pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc still exists
Sep  5 23:03:14.632: INFO: Waiting for pod pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc to disappear
Sep  5 23:03:14.644: INFO: Pod pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc still exists
Sep  5 23:03:16.631: INFO: Waiting for pod pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc to disappear
Sep  5 23:03:16.653: INFO: Pod pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc still exists
Sep  5 23:03:18.631: INFO: Waiting for pod pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc to disappear
Sep  5 23:03:18.655: INFO: Pod pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc still exists
Sep  5 23:03:20.631: INFO: Waiting for pod pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc to disappear
Sep  5 23:03:20.645: INFO: Pod pod-080ed45c-0ae9-4937-92c1-f9bbc5e064cc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:03:20.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-271" for this suite.

• [SLOW TEST:52.541 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":120,"skipped":2148,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:03:21.186: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2252
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2252
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating stateful set ss in namespace statefulset-2252
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2252
Sep  5 23:03:21.818: INFO: Found 0 stateful pods, waiting for 1
Sep  5 23:03:31.827: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 23:03:41.870: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 23:03:53.331: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 23:04:01.858: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep  5 23:04:01.873: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 23:04:02.971: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 23:04:02.971: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 23:04:02.971: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  5 23:04:03.015: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  5 23:04:13.040: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 23:04:13.040: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 23:04:13.107: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Sep  5 23:04:13.107: INFO: ss-0  sc2-10-185-226-233.eng.vmware.com  Running         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:22 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:54 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:04 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:04 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:04:13.107: INFO: 
Sep  5 23:04:13.107: INFO: StatefulSet ss has not reached scale 3, at 1
Sep  5 23:04:14.121: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.984483732s
Sep  5 23:04:15.138: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.969773243s
Sep  5 23:04:16.149: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.95387087s
Sep  5 23:04:17.161: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.942517912s
Sep  5 23:04:18.174: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.930321476s
Sep  5 23:04:19.190: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.916286047s
Sep  5 23:04:20.205: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.90162039s
Sep  5 23:04:21.224: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.886057477s
Sep  5 23:04:22.238: INFO: Verifying statefulset ss doesn't scale past 3 for another 867.190286ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2252
Sep  5 23:04:23.250: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:04:23.592: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  5 23:04:23.592: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  5 23:04:23.592: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  5 23:04:23.592: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:04:23.808: INFO: rc: 1
Sep  5 23:04:23.808: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server: 

error:
exit status 1
Sep  5 23:04:33.809: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:04:34.075: INFO: rc: 1
Sep  5 23:04:34.075: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server: 

error:
exit status 1
Sep  5 23:04:44.077: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:04:44.468: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  5 23:04:44.468: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  5 23:04:44.468: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  5 23:04:44.468: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:04:44.734: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  5 23:04:44.735: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  5 23:04:44.735: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  5 23:04:44.756: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:04:44.756: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:04:44.756: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep  5 23:04:44.776: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 23:04:45.053: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 23:04:45.053: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 23:04:45.053: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  5 23:04:45.053: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 23:04:45.348: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 23:04:45.348: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 23:04:45.348: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  5 23:04:45.348: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 23:04:45.647: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 23:04:45.647: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 23:04:45.647: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  5 23:04:45.647: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 23:04:45.658: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep  5 23:04:55.685: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 23:04:55.685: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 23:04:55.685: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 23:04:55.751: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Sep  5 23:04:55.751: INFO: ss-0  sc2-10-185-226-233.eng.vmware.com  Running         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:22 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:54 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:04:55.751: INFO: ss-1  sc2-10-185-226-233.eng.vmware.com  Running         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:12 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:39 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:04:55.751: INFO: ss-2  sc2-10-185-226-233.eng.vmware.com  Running         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:13 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:40 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:04:55.751: INFO: 
Sep  5 23:04:55.751: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 23:04:56.766: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Sep  5 23:04:56.766: INFO: ss-0  sc2-10-185-226-233.eng.vmware.com  Running  30s    [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:22 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:54 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:04:56.766: INFO: ss-1  sc2-10-185-226-233.eng.vmware.com  Running  30s    [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:12 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:39 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:04:56.766: INFO: ss-2  sc2-10-185-226-233.eng.vmware.com  Running  30s    [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:13 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:40 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:04:56.766: INFO: 
Sep  5 23:04:56.766: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 23:04:57.788: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Sep  5 23:04:57.788: INFO: ss-0  sc2-10-185-226-233.eng.vmware.com  Running  30s    [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:22 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:54 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:04:57.788: INFO: ss-1  sc2-10-185-226-233.eng.vmware.com  Running  30s    [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:12 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:39 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:04:57.789: INFO: ss-2  sc2-10-185-226-233.eng.vmware.com  Running  30s    [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:13 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:40 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:04:57.789: INFO: 
Sep  5 23:04:57.789: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 23:04:58.847: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Sep  5 23:04:58.847: INFO: ss-0  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:22 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:54 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:04:58.847: INFO: ss-1  sc2-10-185-226-233.eng.vmware.com  Running  30s    [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:12 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:39 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:04:58.847: INFO: ss-2  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:13 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:40 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:04:58.847: INFO: 
Sep  5 23:04:58.847: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 23:04:59.862: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Sep  5 23:04:59.862: INFO: ss-0  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:22 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:54 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:04:59.862: INFO: ss-1  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:12 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:39 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:04:59.862: INFO: ss-2  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:13 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:40 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:04:59.862: INFO: 
Sep  5 23:04:59.862: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 23:05:00.892: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Sep  5 23:05:00.892: INFO: ss-0  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:22 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:54 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:05:00.892: INFO: ss-1  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:12 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:39 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:05:00.892: INFO: ss-2  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:13 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:40 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:05:00.892: INFO: 
Sep  5 23:05:00.892: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 23:05:01.920: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Sep  5 23:05:01.920: INFO: ss-0  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:22 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:54 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:05:01.920: INFO: ss-1  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:12 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:39 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:05:01.920: INFO: ss-2  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:13 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:40 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:05:01.920: INFO: 
Sep  5 23:05:01.920: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 23:05:02.999: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Sep  5 23:05:02.999: INFO: ss-0  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:22 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:54 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:05:02.999: INFO: ss-1  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:12 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:39 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:05:02.999: INFO: ss-2  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:13 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:40 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:05:02.999: INFO: 
Sep  5 23:05:02.999: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 23:05:04.026: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Sep  5 23:05:04.026: INFO: ss-0  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:22 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:54 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:05:04.026: INFO: ss-1  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:12 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:39 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:05:04.026: INFO: ss-2  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:13 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:40 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:05:04.026: INFO: 
Sep  5 23:05:04.026: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 23:05:05.045: INFO: POD   NODE                               PHASE    GRACE  CONDITIONS
Sep  5 23:05:05.045: INFO: ss-0  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:22 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:03:54 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:05:05.045: INFO: ss-1  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:12 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:39 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:05:05.045: INFO: ss-2  sc2-10-185-226-233.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:13 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:40 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:04:50 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:05:05.045: INFO: 
Sep  5 23:05:05.045: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2252
Sep  5 23:05:06.063: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:05:06.263: INFO: rc: 1
Sep  5 23:05:06.263: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server: 

error:
exit status 1
Sep  5 23:05:16.263: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:05:16.371: INFO: rc: 1
Sep  5 23:05:16.371: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:05:26.372: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:05:26.488: INFO: rc: 1
Sep  5 23:05:26.488: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:05:36.488: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:05:36.641: INFO: rc: 1
Sep  5 23:05:36.641: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:05:46.641: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:05:46.759: INFO: rc: 1
Sep  5 23:05:46.760: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:05:56.761: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:05:56.886: INFO: rc: 1
Sep  5 23:05:56.886: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:06:06.887: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:06:07.003: INFO: rc: 1
Sep  5 23:06:07.003: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:06:17.004: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:06:17.133: INFO: rc: 1
Sep  5 23:06:17.133: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:06:27.133: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:06:27.251: INFO: rc: 1
Sep  5 23:06:27.251: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:06:37.252: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:06:37.379: INFO: rc: 1
Sep  5 23:06:37.379: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:06:47.380: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:06:47.521: INFO: rc: 1
Sep  5 23:06:47.521: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:06:57.522: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:06:57.638: INFO: rc: 1
Sep  5 23:06:57.638: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:07:07.639: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:07:07.768: INFO: rc: 1
Sep  5 23:07:07.768: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:07:17.769: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:07:17.905: INFO: rc: 1
Sep  5 23:07:17.905: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:07:27.906: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:07:28.019: INFO: rc: 1
Sep  5 23:07:28.019: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:07:38.020: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:07:38.184: INFO: rc: 1
Sep  5 23:07:38.184: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:07:48.185: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:07:48.305: INFO: rc: 1
Sep  5 23:07:48.305: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:07:58.306: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:07:58.430: INFO: rc: 1
Sep  5 23:07:58.430: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:08:08.431: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:08:08.548: INFO: rc: 1
Sep  5 23:08:08.549: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:08:18.549: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:08:18.671: INFO: rc: 1
Sep  5 23:08:18.671: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:08:28.671: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:08:28.844: INFO: rc: 1
Sep  5 23:08:28.844: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:08:38.845: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:08:38.952: INFO: rc: 1
Sep  5 23:08:38.952: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:08:48.952: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:08:49.066: INFO: rc: 1
Sep  5 23:08:49.066: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:08:59.067: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:08:59.170: INFO: rc: 1
Sep  5 23:08:59.170: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:09:09.172: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:09:09.291: INFO: rc: 1
Sep  5 23:09:09.291: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:09:19.291: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:09:19.402: INFO: rc: 1
Sep  5 23:09:19.402: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:09:29.405: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:09:29.523: INFO: rc: 1
Sep  5 23:09:29.523: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:09:39.524: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:09:39.656: INFO: rc: 1
Sep  5 23:09:39.656: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:09:49.658: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:09:49.781: INFO: rc: 1
Sep  5 23:09:49.781: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:09:59.782: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:09:59.979: INFO: rc: 1
Sep  5 23:09:59.979: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:10:09.980: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-2252 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:10:10.128: INFO: rc: 1
Sep  5 23:10:10.128: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Sep  5 23:10:10.128: INFO: Scaling statefulset ss to 0
Sep  5 23:10:10.205: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep  5 23:10:10.212: INFO: Deleting all statefulset in ns statefulset-2252
Sep  5 23:10:10.221: INFO: Scaling statefulset ss to 0
Sep  5 23:10:10.251: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 23:10:10.264: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:10:10.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2252" for this suite.

• [SLOW TEST:409.902 seconds]
[sig-apps] StatefulSet
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":296,"completed":121,"skipped":2165,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:10:11.088: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6981
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 23:10:11.948: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:10:46.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6981" for this suite.

• [SLOW TEST:35.647 seconds]
[k8s.io] Pods
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":296,"completed":122,"skipped":2168,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:10:46.735: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3232
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 23:10:47.326: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep  5 23:10:57.092: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-3232 --namespace=crd-publish-openapi-3232 create -f -'
Sep  5 23:10:58.217: INFO: stderr: ""
Sep  5 23:10:58.217: INFO: stdout: "e2e-test-crd-publish-openapi-4535-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  5 23:10:58.217: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-3232 --namespace=crd-publish-openapi-3232 delete e2e-test-crd-publish-openapi-4535-crds test-cr'
Sep  5 23:10:58.357: INFO: stderr: ""
Sep  5 23:10:58.357: INFO: stdout: "e2e-test-crd-publish-openapi-4535-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep  5 23:10:58.357: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-3232 --namespace=crd-publish-openapi-3232 apply -f -'
Sep  5 23:10:58.877: INFO: stderr: ""
Sep  5 23:10:58.877: INFO: stdout: "e2e-test-crd-publish-openapi-4535-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  5 23:10:58.877: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-3232 --namespace=crd-publish-openapi-3232 delete e2e-test-crd-publish-openapi-4535-crds test-cr'
Sep  5 23:10:59.009: INFO: stderr: ""
Sep  5 23:10:59.009: INFO: stdout: "e2e-test-crd-publish-openapi-4535-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep  5 23:10:59.009: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-3232 explain e2e-test-crd-publish-openapi-4535-crds'
Sep  5 23:10:59.436: INFO: stderr: ""
Sep  5 23:10:59.436: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4535-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:11:09.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3232" for this suite.

• [SLOW TEST:22.666 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":296,"completed":123,"skipped":2176,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:11:09.401: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3151
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-6abc76d8-9fb3-483f-a3f4-8a9dd508081a
STEP: Creating secret with name s-test-opt-upd-d81c99ed-9413-49e8-8c01-91dc7b0412ae
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-6abc76d8-9fb3-483f-a3f4-8a9dd508081a
STEP: Updating secret s-test-opt-upd-d81c99ed-9413-49e8-8c01-91dc7b0412ae
STEP: Creating secret with name s-test-opt-create-5d4b4567-f1ef-46cf-80cb-76b1199e5787
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:12:35.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3151" for this suite.

• [SLOW TEST:86.774 seconds]
[sig-storage] Projected secret
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":296,"completed":124,"skipped":2207,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:12:36.175: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4265
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4265.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4265.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4265.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4265.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4265.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4265.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4265.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4265.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4265.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4265.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4265.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4265.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4265.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4265.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4265.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4265.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4265.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4265.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 23:13:14.960: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4265.svc.cluster.local from pod dns-4265/dns-test-d7fbea92-fece-4c2f-9843-ab0f025dab9c: the server could not find the requested resource (get pods dns-test-d7fbea92-fece-4c2f-9843-ab0f025dab9c)
Sep  5 23:13:14.974: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4265.svc.cluster.local from pod dns-4265/dns-test-d7fbea92-fece-4c2f-9843-ab0f025dab9c: the server could not find the requested resource (get pods dns-test-d7fbea92-fece-4c2f-9843-ab0f025dab9c)
Sep  5 23:13:14.987: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4265.svc.cluster.local from pod dns-4265/dns-test-d7fbea92-fece-4c2f-9843-ab0f025dab9c: the server could not find the requested resource (get pods dns-test-d7fbea92-fece-4c2f-9843-ab0f025dab9c)
Sep  5 23:13:15.005: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4265.svc.cluster.local from pod dns-4265/dns-test-d7fbea92-fece-4c2f-9843-ab0f025dab9c: the server could not find the requested resource (get pods dns-test-d7fbea92-fece-4c2f-9843-ab0f025dab9c)
Sep  5 23:13:15.075: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4265.svc.cluster.local from pod dns-4265/dns-test-d7fbea92-fece-4c2f-9843-ab0f025dab9c: the server could not find the requested resource (get pods dns-test-d7fbea92-fece-4c2f-9843-ab0f025dab9c)
Sep  5 23:13:15.103: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4265.svc.cluster.local from pod dns-4265/dns-test-d7fbea92-fece-4c2f-9843-ab0f025dab9c: the server could not find the requested resource (get pods dns-test-d7fbea92-fece-4c2f-9843-ab0f025dab9c)
Sep  5 23:13:15.119: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4265.svc.cluster.local from pod dns-4265/dns-test-d7fbea92-fece-4c2f-9843-ab0f025dab9c: the server could not find the requested resource (get pods dns-test-d7fbea92-fece-4c2f-9843-ab0f025dab9c)
Sep  5 23:13:15.147: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4265.svc.cluster.local from pod dns-4265/dns-test-d7fbea92-fece-4c2f-9843-ab0f025dab9c: the server could not find the requested resource (get pods dns-test-d7fbea92-fece-4c2f-9843-ab0f025dab9c)
Sep  5 23:13:15.174: INFO: Lookups using dns-4265/dns-test-d7fbea92-fece-4c2f-9843-ab0f025dab9c failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4265.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4265.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4265.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4265.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4265.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4265.svc.cluster.local jessie_udp@dns-test-service-2.dns-4265.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4265.svc.cluster.local]

Sep  5 23:13:20.485: INFO: DNS probes using dns-4265/dns-test-d7fbea92-fece-4c2f-9843-ab0f025dab9c succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:13:20.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4265" for this suite.

• [SLOW TEST:44.895 seconds]
[sig-network] DNS
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":296,"completed":125,"skipped":2214,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:13:21.071: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3533
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Update Demo
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:299
[It] should create and stop a replication controller  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a replication controller
Sep  5 23:13:21.557: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 create -f -'
Sep  5 23:13:22.197: INFO: stderr: ""
Sep  5 23:13:22.197: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  5 23:13:22.197: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 23:13:22.309: INFO: stderr: ""
Sep  5 23:13:22.309: INFO: stdout: "update-demo-nautilus-6vnbb update-demo-nautilus-nm94x "
Sep  5 23:13:22.309: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods update-demo-nautilus-6vnbb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 23:13:22.413: INFO: stderr: ""
Sep  5 23:13:22.413: INFO: stdout: ""
Sep  5 23:13:22.413: INFO: update-demo-nautilus-6vnbb is created but not running
Sep  5 23:13:27.414: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 23:13:27.577: INFO: stderr: ""
Sep  5 23:13:27.577: INFO: stdout: "update-demo-nautilus-6vnbb update-demo-nautilus-nm94x "
Sep  5 23:13:27.577: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods update-demo-nautilus-6vnbb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 23:13:27.689: INFO: stderr: ""
Sep  5 23:13:27.689: INFO: stdout: ""
Sep  5 23:13:27.689: INFO: update-demo-nautilus-6vnbb is created but not running
Sep  5 23:13:32.690: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 23:13:32.806: INFO: stderr: ""
Sep  5 23:13:32.806: INFO: stdout: "update-demo-nautilus-6vnbb update-demo-nautilus-nm94x "
Sep  5 23:13:32.806: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods update-demo-nautilus-6vnbb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 23:13:32.909: INFO: stderr: ""
Sep  5 23:13:32.909: INFO: stdout: ""
Sep  5 23:13:32.909: INFO: update-demo-nautilus-6vnbb is created but not running
Sep  5 23:13:37.909: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 23:13:38.038: INFO: stderr: ""
Sep  5 23:13:38.038: INFO: stdout: "update-demo-nautilus-6vnbb update-demo-nautilus-nm94x "
Sep  5 23:13:38.038: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods update-demo-nautilus-6vnbb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 23:13:38.159: INFO: stderr: ""
Sep  5 23:13:38.159: INFO: stdout: ""
Sep  5 23:13:38.159: INFO: update-demo-nautilus-6vnbb is created but not running
Sep  5 23:13:43.160: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 23:13:43.272: INFO: stderr: ""
Sep  5 23:13:43.272: INFO: stdout: "update-demo-nautilus-6vnbb update-demo-nautilus-nm94x "
Sep  5 23:13:43.272: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods update-demo-nautilus-6vnbb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 23:13:43.413: INFO: stderr: ""
Sep  5 23:13:43.413: INFO: stdout: ""
Sep  5 23:13:43.413: INFO: update-demo-nautilus-6vnbb is created but not running
Sep  5 23:13:48.413: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 23:13:48.522: INFO: stderr: ""
Sep  5 23:13:48.522: INFO: stdout: "update-demo-nautilus-6vnbb update-demo-nautilus-nm94x "
Sep  5 23:13:48.523: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods update-demo-nautilus-6vnbb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 23:13:48.631: INFO: stderr: ""
Sep  5 23:13:48.631: INFO: stdout: ""
Sep  5 23:13:48.631: INFO: update-demo-nautilus-6vnbb is created but not running
Sep  5 23:13:53.632: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 23:13:53.741: INFO: stderr: ""
Sep  5 23:13:53.741: INFO: stdout: "update-demo-nautilus-6vnbb update-demo-nautilus-nm94x "
Sep  5 23:13:53.741: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods update-demo-nautilus-6vnbb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 23:13:53.837: INFO: stderr: ""
Sep  5 23:13:53.837: INFO: stdout: ""
Sep  5 23:13:53.837: INFO: update-demo-nautilus-6vnbb is created but not running
Sep  5 23:13:58.838: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 23:13:58.945: INFO: stderr: ""
Sep  5 23:13:58.945: INFO: stdout: "update-demo-nautilus-6vnbb update-demo-nautilus-nm94x "
Sep  5 23:13:58.945: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods update-demo-nautilus-6vnbb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 23:13:59.049: INFO: stderr: ""
Sep  5 23:13:59.049: INFO: stdout: ""
Sep  5 23:13:59.049: INFO: update-demo-nautilus-6vnbb is created but not running
Sep  5 23:14:04.049: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Sep  5 23:14:04.162: INFO: stderr: ""
Sep  5 23:14:04.162: INFO: stdout: "update-demo-nautilus-6vnbb update-demo-nautilus-nm94x "
Sep  5 23:14:04.162: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods update-demo-nautilus-6vnbb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 23:14:04.262: INFO: stderr: ""
Sep  5 23:14:04.262: INFO: stdout: "true"
Sep  5 23:14:04.262: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods update-demo-nautilus-6vnbb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  5 23:14:04.396: INFO: stderr: ""
Sep  5 23:14:04.396: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 23:14:04.396: INFO: validating pod update-demo-nautilus-6vnbb
Sep  5 23:14:04.427: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 23:14:04.427: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 23:14:04.427: INFO: update-demo-nautilus-6vnbb is verified up and running
Sep  5 23:14:04.427: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods update-demo-nautilus-nm94x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Sep  5 23:14:04.526: INFO: stderr: ""
Sep  5 23:14:04.526: INFO: stdout: "true"
Sep  5 23:14:04.526: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods update-demo-nautilus-nm94x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Sep  5 23:14:04.640: INFO: stderr: ""
Sep  5 23:14:04.640: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 23:14:04.640: INFO: validating pod update-demo-nautilus-nm94x
Sep  5 23:14:04.674: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 23:14:04.674: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 23:14:04.674: INFO: update-demo-nautilus-nm94x is verified up and running
STEP: using delete to clean up resources
Sep  5 23:14:04.674: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 delete --grace-period=0 --force -f -'
Sep  5 23:14:04.798: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 23:14:04.798: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  5 23:14:04.798: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get rc,svc -l name=update-demo --no-headers'
Sep  5 23:14:04.905: INFO: stderr: "No resources found in kubectl-3533 namespace.\n"
Sep  5 23:14:04.905: INFO: stdout: ""
Sep  5 23:14:04.905: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 23:14:05.023: INFO: stderr: ""
Sep  5 23:14:05.023: INFO: stdout: "update-demo-nautilus-6vnbb\nupdate-demo-nautilus-nm94x\n"
Sep  5 23:14:05.524: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get rc,svc -l name=update-demo --no-headers'
Sep  5 23:14:05.636: INFO: stderr: "No resources found in kubectl-3533 namespace.\n"
Sep  5 23:14:05.636: INFO: stdout: ""
Sep  5 23:14:05.636: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 23:14:05.754: INFO: stderr: ""
Sep  5 23:14:05.754: INFO: stdout: "update-demo-nautilus-6vnbb\nupdate-demo-nautilus-nm94x\n"
Sep  5 23:14:06.023: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get rc,svc -l name=update-demo --no-headers'
Sep  5 23:14:06.150: INFO: stderr: "No resources found in kubectl-3533 namespace.\n"
Sep  5 23:14:06.150: INFO: stdout: ""
Sep  5 23:14:06.150: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 23:14:06.261: INFO: stderr: ""
Sep  5 23:14:06.261: INFO: stdout: "update-demo-nautilus-6vnbb\nupdate-demo-nautilus-nm94x\n"
Sep  5 23:14:06.523: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get rc,svc -l name=update-demo --no-headers'
Sep  5 23:14:06.637: INFO: stderr: "No resources found in kubectl-3533 namespace.\n"
Sep  5 23:14:06.637: INFO: stdout: ""
Sep  5 23:14:06.637: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 23:14:06.749: INFO: stderr: ""
Sep  5 23:14:06.749: INFO: stdout: "update-demo-nautilus-6vnbb\nupdate-demo-nautilus-nm94x\n"
Sep  5 23:14:07.024: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get rc,svc -l name=update-demo --no-headers'
Sep  5 23:14:07.137: INFO: stderr: "No resources found in kubectl-3533 namespace.\n"
Sep  5 23:14:07.137: INFO: stdout: ""
Sep  5 23:14:07.137: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-3533 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 23:14:07.247: INFO: stderr: ""
Sep  5 23:14:07.247: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:14:07.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3533" for this suite.

• [SLOW TEST:46.556 seconds]
[sig-cli] Kubectl client
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:297
    should create and stop a replication controller  [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":296,"completed":126,"skipped":2215,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:14:07.628: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8630
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  5 23:14:08.174: INFO: Waiting up to 5m0s for pod "pod-2277e862-4889-4864-ab92-3c79674a0d8b" in namespace "emptydir-8630" to be "Succeeded or Failed"
Sep  5 23:14:08.190: INFO: Pod "pod-2277e862-4889-4864-ab92-3c79674a0d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.652229ms
Sep  5 23:14:10.203: INFO: Pod "pod-2277e862-4889-4864-ab92-3c79674a0d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029264724s
Sep  5 23:14:12.214: INFO: Pod "pod-2277e862-4889-4864-ab92-3c79674a0d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040442384s
Sep  5 23:14:14.233: INFO: Pod "pod-2277e862-4889-4864-ab92-3c79674a0d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.059444127s
Sep  5 23:14:16.248: INFO: Pod "pod-2277e862-4889-4864-ab92-3c79674a0d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.074378938s
Sep  5 23:14:18.257: INFO: Pod "pod-2277e862-4889-4864-ab92-3c79674a0d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.083702819s
Sep  5 23:14:20.283: INFO: Pod "pod-2277e862-4889-4864-ab92-3c79674a0d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.109720289s
Sep  5 23:14:22.320: INFO: Pod "pod-2277e862-4889-4864-ab92-3c79674a0d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.146657129s
Sep  5 23:14:24.350: INFO: Pod "pod-2277e862-4889-4864-ab92-3c79674a0d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.176061343s
Sep  5 23:14:26.393: INFO: Pod "pod-2277e862-4889-4864-ab92-3c79674a0d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.218875058s
Sep  5 23:14:28.402: INFO: Pod "pod-2277e862-4889-4864-ab92-3c79674a0d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.228640264s
Sep  5 23:14:30.416: INFO: Pod "pod-2277e862-4889-4864-ab92-3c79674a0d8b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.242277757s
Sep  5 23:14:32.430: INFO: Pod "pod-2277e862-4889-4864-ab92-3c79674a0d8b": Phase="Running", Reason="", readiness=true. Elapsed: 24.256579831s
Sep  5 23:14:34.443: INFO: Pod "pod-2277e862-4889-4864-ab92-3c79674a0d8b": Phase="Running", Reason="", readiness=true. Elapsed: 26.269337531s
Sep  5 23:14:36.453: INFO: Pod "pod-2277e862-4889-4864-ab92-3c79674a0d8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.279515995s
STEP: Saw pod success
Sep  5 23:14:36.453: INFO: Pod "pod-2277e862-4889-4864-ab92-3c79674a0d8b" satisfied condition "Succeeded or Failed"
Sep  5 23:14:36.459: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-2277e862-4889-4864-ab92-3c79674a0d8b container test-container: <nil>
STEP: delete the pod
Sep  5 23:14:36.525: INFO: Waiting for pod pod-2277e862-4889-4864-ab92-3c79674a0d8b to disappear
Sep  5 23:14:36.540: INFO: Pod pod-2277e862-4889-4864-ab92-3c79674a0d8b still exists
Sep  5 23:14:38.541: INFO: Waiting for pod pod-2277e862-4889-4864-ab92-3c79674a0d8b to disappear
Sep  5 23:14:38.548: INFO: Pod pod-2277e862-4889-4864-ab92-3c79674a0d8b still exists
Sep  5 23:14:40.540: INFO: Waiting for pod pod-2277e862-4889-4864-ab92-3c79674a0d8b to disappear
Sep  5 23:14:40.563: INFO: Pod pod-2277e862-4889-4864-ab92-3c79674a0d8b still exists
Sep  5 23:14:42.540: INFO: Waiting for pod pod-2277e862-4889-4864-ab92-3c79674a0d8b to disappear
Sep  5 23:14:42.563: INFO: Pod pod-2277e862-4889-4864-ab92-3c79674a0d8b still exists
Sep  5 23:14:44.540: INFO: Waiting for pod pod-2277e862-4889-4864-ab92-3c79674a0d8b to disappear
Sep  5 23:14:44.555: INFO: Pod pod-2277e862-4889-4864-ab92-3c79674a0d8b still exists
Sep  5 23:14:46.540: INFO: Waiting for pod pod-2277e862-4889-4864-ab92-3c79674a0d8b to disappear
Sep  5 23:14:46.552: INFO: Pod pod-2277e862-4889-4864-ab92-3c79674a0d8b still exists
Sep  5 23:14:48.541: INFO: Waiting for pod pod-2277e862-4889-4864-ab92-3c79674a0d8b to disappear
Sep  5 23:14:48.549: INFO: Pod pod-2277e862-4889-4864-ab92-3c79674a0d8b still exists
Sep  5 23:14:50.540: INFO: Waiting for pod pod-2277e862-4889-4864-ab92-3c79674a0d8b to disappear
Sep  5 23:14:50.552: INFO: Pod pod-2277e862-4889-4864-ab92-3c79674a0d8b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:14:50.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8630" for this suite.

• [SLOW TEST:44.078 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":127,"skipped":2216,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:14:51.706: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9710
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-2aee2bef-bffa-4e32-a5bb-15615ecd4345
STEP: Creating a pod to test consume secrets
Sep  5 23:14:52.356: INFO: Waiting up to 5m0s for pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643" in namespace "secrets-9710" to be "Succeeded or Failed"
Sep  5 23:14:52.367: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 10.196106ms
Sep  5 23:14:54.384: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027231285s
Sep  5 23:14:56.430: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073981668s
Sep  5 23:14:58.441: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084968872s
Sep  5 23:15:00.457: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 8.100710992s
Sep  5 23:15:02.478: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 10.122037016s
Sep  5 23:15:04.490: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 12.13405752s
Sep  5 23:15:06.508: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 14.151672596s
Sep  5 23:15:08.517: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 16.161012918s
Sep  5 23:15:10.529: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 18.173102673s
Sep  5 23:15:12.547: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 20.19033342s
Sep  5 23:15:14.565: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 22.208505441s
Sep  5 23:15:16.618: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 24.261630667s
Sep  5 23:15:18.633: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 26.277005522s
Sep  5 23:15:20.650: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 28.293208018s
Sep  5 23:15:22.664: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 30.307544526s
Sep  5 23:15:24.676: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 32.31943855s
Sep  5 23:15:26.698: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 34.341404842s
Sep  5 23:15:28.705: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 36.349067669s
Sep  5 23:15:30.720: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Pending", Reason="", readiness=false. Elapsed: 38.363495852s
Sep  5 23:15:32.732: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643": Phase="Succeeded", Reason="", readiness=false. Elapsed: 40.375777326s
STEP: Saw pod success
Sep  5 23:15:32.732: INFO: Pod "pod-secrets-306e7811-487c-438e-a7c0-46612528d643" satisfied condition "Succeeded or Failed"
Sep  5 23:15:32.738: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-secrets-306e7811-487c-438e-a7c0-46612528d643 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 23:15:38.637: INFO: Waiting for pod pod-secrets-306e7811-487c-438e-a7c0-46612528d643 to disappear
Sep  5 23:15:38.647: INFO: Pod pod-secrets-306e7811-487c-438e-a7c0-46612528d643 still exists
Sep  5 23:15:40.649: INFO: Waiting for pod pod-secrets-306e7811-487c-438e-a7c0-46612528d643 to disappear
Sep  5 23:15:40.665: INFO: Pod pod-secrets-306e7811-487c-438e-a7c0-46612528d643 still exists
Sep  5 23:15:42.648: INFO: Waiting for pod pod-secrets-306e7811-487c-438e-a7c0-46612528d643 to disappear
Sep  5 23:15:42.659: INFO: Pod pod-secrets-306e7811-487c-438e-a7c0-46612528d643 still exists
Sep  5 23:15:44.648: INFO: Waiting for pod pod-secrets-306e7811-487c-438e-a7c0-46612528d643 to disappear
Sep  5 23:15:44.666: INFO: Pod pod-secrets-306e7811-487c-438e-a7c0-46612528d643 still exists
Sep  5 23:15:46.649: INFO: Waiting for pod pod-secrets-306e7811-487c-438e-a7c0-46612528d643 to disappear
Sep  5 23:15:46.661: INFO: Pod pod-secrets-306e7811-487c-438e-a7c0-46612528d643 still exists
Sep  5 23:15:48.649: INFO: Waiting for pod pod-secrets-306e7811-487c-438e-a7c0-46612528d643 to disappear
Sep  5 23:15:48.659: INFO: Pod pod-secrets-306e7811-487c-438e-a7c0-46612528d643 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:15:48.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9710" for this suite.

• [SLOW TEST:57.546 seconds]
[sig-storage] Secrets
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":296,"completed":128,"skipped":2243,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:15:49.253: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8665
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating replication controller my-hostname-basic-17a7d61d-2de4-444b-8ea7-bf8471e82f4e
Sep  5 23:15:49.696: INFO: Pod name my-hostname-basic-17a7d61d-2de4-444b-8ea7-bf8471e82f4e: Found 0 pods out of 1
Sep  5 23:15:54.717: INFO: Pod name my-hostname-basic-17a7d61d-2de4-444b-8ea7-bf8471e82f4e: Found 1 pods out of 1
Sep  5 23:15:54.717: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-17a7d61d-2de4-444b-8ea7-bf8471e82f4e" are running
Sep  5 23:16:22.739: INFO: Pod "my-hostname-basic-17a7d61d-2de4-444b-8ea7-bf8471e82f4e-d6tfn" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-05 23:15:49 -0700 PDT Reason: Message:}])
Sep  5 23:16:22.739: INFO: Trying to dial the pod
Sep  5 23:16:27.816: INFO: Controller my-hostname-basic-17a7d61d-2de4-444b-8ea7-bf8471e82f4e: Got expected result from replica 1 [my-hostname-basic-17a7d61d-2de4-444b-8ea7-bf8471e82f4e-d6tfn]: "my-hostname-basic-17a7d61d-2de4-444b-8ea7-bf8471e82f4e-d6tfn", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:16:27.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8665" for this suite.

• [SLOW TEST:39.070 seconds]
[sig-apps] ReplicationController
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":296,"completed":129,"skipped":2248,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:16:28.323: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8416
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-map-a00e71fe-5fe2-4de7-be0d-c7c8ef700895
STEP: Creating a pod to test consume secrets
Sep  5 23:16:28.850: INFO: Waiting up to 5m0s for pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08" in namespace "secrets-8416" to be "Succeeded or Failed"
Sep  5 23:16:28.857: INFO: Pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08": Phase="Pending", Reason="", readiness=false. Elapsed: 7.36422ms
Sep  5 23:16:30.869: INFO: Pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018973267s
Sep  5 23:16:32.884: INFO: Pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033880482s
Sep  5 23:16:34.895: INFO: Pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08": Phase="Pending", Reason="", readiness=false. Elapsed: 6.045138679s
Sep  5 23:16:36.907: INFO: Pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08": Phase="Pending", Reason="", readiness=false. Elapsed: 8.05680788s
Sep  5 23:16:38.915: INFO: Pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08": Phase="Pending", Reason="", readiness=false. Elapsed: 10.065489608s
Sep  5 23:16:40.968: INFO: Pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08": Phase="Pending", Reason="", readiness=false. Elapsed: 12.118199798s
Sep  5 23:16:42.980: INFO: Pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08": Phase="Pending", Reason="", readiness=false. Elapsed: 14.129762569s
Sep  5 23:16:44.994: INFO: Pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08": Phase="Pending", Reason="", readiness=false. Elapsed: 16.143648794s
Sep  5 23:16:47.008: INFO: Pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08": Phase="Pending", Reason="", readiness=false. Elapsed: 18.158415999s
Sep  5 23:16:49.021: INFO: Pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08": Phase="Pending", Reason="", readiness=false. Elapsed: 20.171334515s
Sep  5 23:16:51.043: INFO: Pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08": Phase="Pending", Reason="", readiness=false. Elapsed: 22.193071309s
Sep  5 23:16:53.054: INFO: Pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08": Phase="Pending", Reason="", readiness=false. Elapsed: 24.204320124s
Sep  5 23:16:55.065: INFO: Pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08": Phase="Pending", Reason="", readiness=false. Elapsed: 26.214868559s
Sep  5 23:16:57.080: INFO: Pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08": Phase="Pending", Reason="", readiness=false. Elapsed: 28.229630544s
Sep  5 23:16:59.113: INFO: Pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08": Phase="Pending", Reason="", readiness=false. Elapsed: 30.262618073s
Sep  5 23:17:01.125: INFO: Pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08": Phase="Pending", Reason="", readiness=false. Elapsed: 32.275487905s
Sep  5 23:17:03.145: INFO: Pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.295031413s
STEP: Saw pod success
Sep  5 23:17:03.145: INFO: Pod "pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08" satisfied condition "Succeeded or Failed"
Sep  5 23:17:03.156: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 23:17:03.237: INFO: Waiting for pod pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08 to disappear
Sep  5 23:17:03.258: INFO: Pod pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08 still exists
Sep  5 23:17:05.258: INFO: Waiting for pod pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08 to disappear
Sep  5 23:17:05.269: INFO: Pod pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08 still exists
Sep  5 23:17:07.258: INFO: Waiting for pod pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08 to disappear
Sep  5 23:17:07.269: INFO: Pod pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08 still exists
Sep  5 23:17:09.258: INFO: Waiting for pod pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08 to disappear
Sep  5 23:17:09.268: INFO: Pod pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08 still exists
Sep  5 23:17:11.258: INFO: Waiting for pod pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08 to disappear
Sep  5 23:17:11.290: INFO: Pod pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08 still exists
Sep  5 23:17:13.258: INFO: Waiting for pod pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08 to disappear
Sep  5 23:17:13.269: INFO: Pod pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08 still exists
Sep  5 23:17:15.258: INFO: Waiting for pod pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08 to disappear
Sep  5 23:17:15.272: INFO: Pod pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08 still exists
Sep  5 23:17:17.259: INFO: Waiting for pod pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08 to disappear
Sep  5 23:17:17.273: INFO: Pod pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08 still exists
Sep  5 23:17:19.260: INFO: Waiting for pod pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08 to disappear
Sep  5 23:17:19.273: INFO: Pod pod-secrets-8052fdef-0a17-4bb1-916a-4e308e79fc08 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:17:19.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8416" for this suite.

• [SLOW TEST:51.335 seconds]
[sig-storage] Secrets
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":130,"skipped":2249,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:17:19.658: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5819
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating Pod
STEP: Reading file content from the nginx-container
Sep  5 23:17:52.174: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5819 PodName:pod-sharedvolume-4b403f20-63ee-47bd-9764-cdd9f26f7623 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  5 23:17:52.174: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 23:17:52.355: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:17:52.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5819" for this suite.

• [SLOW TEST:33.300 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  pod should support shared volumes between containers [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":296,"completed":131,"skipped":2274,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:17:52.959: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3219
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3219.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3219.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3219.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3219.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3219.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3219.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 23:18:37.857: INFO: DNS probes using dns-3219/dns-test-632cf0e5-73d7-40b7-92bb-e09af6a9d3d0 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:18:38.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3219" for this suite.

• [SLOW TEST:45.520 seconds]
[sig-network] DNS
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":296,"completed":132,"skipped":2296,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:18:38.479: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2716
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep  5 23:18:39.254: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264" in namespace "projected-2716" to be "Succeeded or Failed"
Sep  5 23:18:39.277: INFO: Pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264": Phase="Pending", Reason="", readiness=false. Elapsed: 22.219858ms
Sep  5 23:18:41.291: INFO: Pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036608384s
Sep  5 23:18:43.308: INFO: Pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053559293s
Sep  5 23:18:45.325: INFO: Pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070527879s
Sep  5 23:18:47.346: INFO: Pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091519569s
Sep  5 23:18:49.354: INFO: Pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264": Phase="Pending", Reason="", readiness=false. Elapsed: 10.100067375s
Sep  5 23:18:51.373: INFO: Pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264": Phase="Pending", Reason="", readiness=false. Elapsed: 12.118209084s
Sep  5 23:18:53.390: INFO: Pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264": Phase="Pending", Reason="", readiness=false. Elapsed: 14.135664835s
Sep  5 23:18:55.404: INFO: Pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264": Phase="Pending", Reason="", readiness=false. Elapsed: 16.149116787s
Sep  5 23:18:57.424: INFO: Pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264": Phase="Pending", Reason="", readiness=false. Elapsed: 18.169591769s
Sep  5 23:18:59.443: INFO: Pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264": Phase="Pending", Reason="", readiness=false. Elapsed: 20.188900073s
Sep  5 23:19:01.468: INFO: Pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264": Phase="Pending", Reason="", readiness=false. Elapsed: 22.213739915s
Sep  5 23:19:03.485: INFO: Pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264": Phase="Pending", Reason="", readiness=false. Elapsed: 24.230823729s
Sep  5 23:19:05.500: INFO: Pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264": Phase="Pending", Reason="", readiness=false. Elapsed: 26.245183603s
Sep  5 23:19:07.514: INFO: Pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264": Phase="Pending", Reason="", readiness=false. Elapsed: 28.259829408s
Sep  5 23:19:09.532: INFO: Pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264": Phase="Running", Reason="", readiness=true. Elapsed: 30.277999186s
Sep  5 23:19:11.544: INFO: Pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264": Phase="Running", Reason="", readiness=true. Elapsed: 32.289482592s
Sep  5 23:19:13.565: INFO: Pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.311060152s
STEP: Saw pod success
Sep  5 23:19:13.566: INFO: Pod "downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264" satisfied condition "Succeeded or Failed"
Sep  5 23:19:13.578: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264 container client-container: <nil>
STEP: delete the pod
Sep  5 23:19:13.713: INFO: Waiting for pod downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264 to disappear
Sep  5 23:19:13.732: INFO: Pod downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264 still exists
Sep  5 23:19:15.733: INFO: Waiting for pod downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264 to disappear
Sep  5 23:19:15.747: INFO: Pod downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264 still exists
Sep  5 23:19:17.733: INFO: Waiting for pod downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264 to disappear
Sep  5 23:19:17.757: INFO: Pod downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264 still exists
Sep  5 23:19:19.733: INFO: Waiting for pod downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264 to disappear
Sep  5 23:19:19.747: INFO: Pod downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264 still exists
Sep  5 23:19:21.734: INFO: Waiting for pod downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264 to disappear
Sep  5 23:19:21.745: INFO: Pod downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264 still exists
Sep  5 23:19:23.734: INFO: Waiting for pod downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264 to disappear
Sep  5 23:19:23.747: INFO: Pod downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264 still exists
Sep  5 23:19:25.733: INFO: Waiting for pod downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264 to disappear
Sep  5 23:19:25.749: INFO: Pod downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264 still exists
Sep  5 23:19:27.733: INFO: Waiting for pod downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264 to disappear
Sep  5 23:19:27.744: INFO: Pod downwardapi-volume-ad39bc6a-a391-435e-991c-4e9ad5d0d264 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:19:27.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2716" for this suite.

• [SLOW TEST:49.851 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":296,"completed":133,"skipped":2297,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:19:28.331: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3850
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  5 23:19:28.891: INFO: Waiting up to 5m0s for pod "pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515" in namespace "emptydir-3850" to be "Succeeded or Failed"
Sep  5 23:19:28.903: INFO: Pod "pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515": Phase="Pending", Reason="", readiness=false. Elapsed: 11.554049ms
Sep  5 23:19:30.917: INFO: Pod "pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025193442s
Sep  5 23:19:32.933: INFO: Pod "pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041254153s
Sep  5 23:19:34.968: INFO: Pod "pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515": Phase="Pending", Reason="", readiness=false. Elapsed: 6.076748295s
Sep  5 23:19:37.014: INFO: Pod "pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515": Phase="Pending", Reason="", readiness=false. Elapsed: 8.122884239s
Sep  5 23:19:39.066: INFO: Pod "pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515": Phase="Pending", Reason="", readiness=false. Elapsed: 10.174952089s
Sep  5 23:19:41.079: INFO: Pod "pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515": Phase="Pending", Reason="", readiness=false. Elapsed: 12.187459355s
Sep  5 23:19:43.095: INFO: Pod "pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515": Phase="Pending", Reason="", readiness=false. Elapsed: 14.203422374s
Sep  5 23:19:45.109: INFO: Pod "pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515": Phase="Pending", Reason="", readiness=false. Elapsed: 16.218017928s
Sep  5 23:19:47.126: INFO: Pod "pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515": Phase="Pending", Reason="", readiness=false. Elapsed: 18.234187298s
Sep  5 23:19:49.141: INFO: Pod "pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515": Phase="Pending", Reason="", readiness=false. Elapsed: 20.250151156s
Sep  5 23:19:51.155: INFO: Pod "pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515": Phase="Pending", Reason="", readiness=false. Elapsed: 22.263691591s
Sep  5 23:19:53.177: INFO: Pod "pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515": Phase="Pending", Reason="", readiness=false. Elapsed: 24.285938398s
Sep  5 23:19:55.188: INFO: Pod "pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515": Phase="Pending", Reason="", readiness=false. Elapsed: 26.296535264s
Sep  5 23:19:57.205: INFO: Pod "pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515": Phase="Pending", Reason="", readiness=false. Elapsed: 28.313229585s
Sep  5 23:19:59.221: INFO: Pod "pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.329237449s
STEP: Saw pod success
Sep  5 23:19:59.221: INFO: Pod "pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515" satisfied condition "Succeeded or Failed"
Sep  5 23:19:59.235: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 container test-container: <nil>
STEP: delete the pod
Sep  5 23:19:59.315: INFO: Waiting for pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 to disappear
Sep  5 23:19:59.336: INFO: Pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 still exists
Sep  5 23:20:01.337: INFO: Waiting for pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 to disappear
Sep  5 23:20:01.372: INFO: Pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 still exists
Sep  5 23:20:03.338: INFO: Waiting for pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 to disappear
Sep  5 23:20:03.363: INFO: Pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 still exists
Sep  5 23:20:05.337: INFO: Waiting for pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 to disappear
Sep  5 23:20:05.361: INFO: Pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 still exists
Sep  5 23:20:07.337: INFO: Waiting for pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 to disappear
Sep  5 23:20:07.355: INFO: Pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 still exists
Sep  5 23:20:09.338: INFO: Waiting for pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 to disappear
Sep  5 23:20:09.352: INFO: Pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 still exists
Sep  5 23:20:11.338: INFO: Waiting for pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 to disappear
Sep  5 23:20:11.356: INFO: Pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 still exists
Sep  5 23:20:13.337: INFO: Waiting for pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 to disappear
Sep  5 23:20:13.356: INFO: Pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 still exists
Sep  5 23:20:15.337: INFO: Waiting for pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 to disappear
Sep  5 23:20:15.352: INFO: Pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 still exists
Sep  5 23:20:17.337: INFO: Waiting for pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 to disappear
Sep  5 23:20:17.375: INFO: Pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 still exists
Sep  5 23:20:19.338: INFO: Waiting for pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 to disappear
Sep  5 23:20:19.347: INFO: Pod pod-5ea912ff-1fd4-4db3-bcc8-050cd2778515 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:20:19.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3850" for this suite.

• [SLOW TEST:51.824 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":134,"skipped":2298,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:20:20.156: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9546
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name s-test-opt-del-c3f26442-aea3-4649-bc91-972c473571be
STEP: Creating secret with name s-test-opt-upd-21693c22-4879-45cc-8f5d-cb9cb995aaa8
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-c3f26442-aea3-4649-bc91-972c473571be
STEP: Updating secret s-test-opt-upd-21693c22-4879-45cc-8f5d-cb9cb995aaa8
STEP: Creating secret with name s-test-opt-create-ed39419e-52a4-4547-a8be-13ed91925020
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:21:33.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9546" for this suite.

• [SLOW TEST:74.483 seconds]
[sig-storage] Secrets
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":296,"completed":135,"skipped":2300,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:21:34.639: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5749
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override command
Sep  5 23:21:35.235: INFO: Waiting up to 5m0s for pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b" in namespace "containers-5749" to be "Succeeded or Failed"
Sep  5 23:21:35.250: INFO: Pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.410814ms
Sep  5 23:21:37.260: INFO: Pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024910781s
Sep  5 23:21:39.275: INFO: Pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039124021s
Sep  5 23:21:41.286: INFO: Pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050799137s
Sep  5 23:21:43.317: INFO: Pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081173478s
Sep  5 23:21:45.325: INFO: Pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.0893578s
Sep  5 23:21:47.342: INFO: Pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.107004715s
Sep  5 23:21:49.361: INFO: Pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.125658425s
Sep  5 23:21:51.375: INFO: Pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.139831578s
Sep  5 23:21:53.400: INFO: Pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.164464745s
Sep  5 23:21:55.413: INFO: Pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.177223096s
Sep  5 23:21:57.425: INFO: Pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.18935439s
Sep  5 23:21:59.438: INFO: Pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.202809204s
Sep  5 23:22:01.451: INFO: Pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b": Phase="Pending", Reason="", readiness=false. Elapsed: 26.21583867s
Sep  5 23:22:03.463: INFO: Pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b": Phase="Pending", Reason="", readiness=false. Elapsed: 28.227554978s
Sep  5 23:22:05.476: INFO: Pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b": Phase="Pending", Reason="", readiness=false. Elapsed: 30.240366533s
Sep  5 23:22:07.489: INFO: Pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b": Phase="Pending", Reason="", readiness=false. Elapsed: 32.253346859s
Sep  5 23:22:09.501: INFO: Pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.265435488s
STEP: Saw pod success
Sep  5 23:22:09.501: INFO: Pod "client-containers-60bb7790-55ef-433f-b6c4-147be40a108b" satisfied condition "Succeeded or Failed"
Sep  5 23:22:09.509: INFO: Trying to get logs from node sc2-10-185-226-150.eng.vmware.com pod client-containers-60bb7790-55ef-433f-b6c4-147be40a108b container agnhost-container: <nil>
STEP: delete the pod
Sep  5 23:22:15.779: INFO: Waiting for pod client-containers-60bb7790-55ef-433f-b6c4-147be40a108b to disappear
Sep  5 23:22:15.802: INFO: Pod client-containers-60bb7790-55ef-433f-b6c4-147be40a108b still exists
Sep  5 23:22:17.804: INFO: Waiting for pod client-containers-60bb7790-55ef-433f-b6c4-147be40a108b to disappear
Sep  5 23:22:17.818: INFO: Pod client-containers-60bb7790-55ef-433f-b6c4-147be40a108b still exists
Sep  5 23:22:19.803: INFO: Waiting for pod client-containers-60bb7790-55ef-433f-b6c4-147be40a108b to disappear
Sep  5 23:22:19.829: INFO: Pod client-containers-60bb7790-55ef-433f-b6c4-147be40a108b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:22:19.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5749" for this suite.

• [SLOW TEST:45.832 seconds]
[k8s.io] Docker Containers
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":296,"completed":136,"skipped":2307,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:22:20.472: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2588
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Sep  5 23:24:03.600: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Sep  5 23:24:03.600: INFO: Deleting pod "simpletest.rc-2xqws" in namespace "gc-2588"
Sep  5 23:24:03.680: INFO: Deleting pod "simpletest.rc-679v2" in namespace "gc-2588"
Sep  5 23:24:03.741: INFO: Deleting pod "simpletest.rc-kg4zm" in namespace "gc-2588"
Sep  5 23:24:03.813: INFO: Deleting pod "simpletest.rc-q7vsb" in namespace "gc-2588"
Sep  5 23:24:03.886: INFO: Deleting pod "simpletest.rc-qch82" in namespace "gc-2588"
Sep  5 23:24:03.952: INFO: Deleting pod "simpletest.rc-rrt87" in namespace "gc-2588"
Sep  5 23:24:04.116: INFO: Deleting pod "simpletest.rc-v8z9w" in namespace "gc-2588"
Sep  5 23:24:04.175: INFO: Deleting pod "simpletest.rc-w6g7v" in namespace "gc-2588"
Sep  5 23:24:04.277: INFO: Deleting pod "simpletest.rc-wnqt4" in namespace "gc-2588"
Sep  5 23:24:04.437: INFO: Deleting pod "simpletest.rc-xcnzf" in namespace "gc-2588"
[AfterEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:24:04.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2588" for this suite.

• [SLOW TEST:104.929 seconds]
[sig-api-machinery] Garbage collector
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":296,"completed":137,"skipped":2351,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:24:05.401: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6823
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 23:24:06.496: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"5d44596e-c992-4d9e-b7e4-648ccaa4f72b", Controller:(*bool)(0xc001f04d4a), BlockOwnerDeletion:(*bool)(0xc001f04d4b)}}
Sep  5 23:24:06.571: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"0fb16af7-b4ea-4136-b2a1-56f8adfbcbe1", Controller:(*bool)(0xc001f04fba), BlockOwnerDeletion:(*bool)(0xc001f04fbb)}}
Sep  5 23:24:06.607: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"2d59c45c-17fe-4bec-abd4-537fa3726524", Controller:(*bool)(0xc003da9942), BlockOwnerDeletion:(*bool)(0xc003da9943)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:24:11.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6823" for this suite.

• [SLOW TEST:6.677 seconds]
[sig-api-machinery] Garbage collector
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":296,"completed":138,"skipped":2352,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:24:12.079: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9847
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if v1 is in available api versions  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: validating api versions
Sep  5 23:24:12.803: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-9847 api-versions'
Sep  5 23:24:12.912: INFO: stderr: ""
Sep  5 23:24:12.912: INFO: stdout: "acme.cert-manager.io/v1alpha2\nacme.cert-manager.io/v1alpha3\naddons.cluster.x-k8s.io/v1alpha3\nadmissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\nappplatform.wcp.vmware.com/v1alpha1\nappplatform.wcp.vmware.com/v1alpha2\nappplatform.wcp.vmware.com/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbootstrap.cluster.x-k8s.io/v1alpha2\nbootstrap.cluster.x-k8s.io/v1alpha3\ncert-manager.io/v1alpha2\ncert-manager.io/v1alpha3\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncluster.x-k8s.io/v1alpha2\ncluster.x-k8s.io/v1alpha3\ncns.vmware.com/v1alpha1\ncontrolplane.cluster.x-k8s.io/v1alpha3\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nexp.cluster.x-k8s.io/v1alpha3\nextensions/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta1\nimagecontroller.vmware.com/v1\ninfrastructure.cluster.vmware.com/v1alpha2\ninfrastructure.cluster.vmware.com/v1alpha3\ninstallers.tmc.cloud.vmware.com/v1alpha1\nk8s.cni.cncf.io/v1\nlicenseoperator.vmware.com/v1alpha1\nnetoperator.vmware.com/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnetworking.x-k8s.io/v1alpha1pre1\nnode.k8s.io/v1\nnode.k8s.io/v1beta1\nnsx.vmware.com/v1\nnsx.vmware.com/v1alpha1\npolicy/v1beta1\npsp.wcp.vmware.com/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nregistryagent.vmware.com/v1alpha1\nrun.tanzu.vmware.com/v1alpha1\nrun.tanzu.vmware.com/v1alpha2\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntopology.tanzu.vmware.com/v1alpha1\nv1\nvmoperator.vmware.com/v1alpha1\nvmware.com/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:24:12.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9847" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":296,"completed":139,"skipped":2388,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:24:13.216: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7031
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  5 23:24:13.902: INFO: Waiting up to 5m0s for pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb" in namespace "emptydir-7031" to be "Succeeded or Failed"
Sep  5 23:24:13.935: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 32.552046ms
Sep  5 23:24:15.958: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055113374s
Sep  5 23:24:18.004: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.101508362s
Sep  5 23:24:20.025: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.122310883s
Sep  5 23:24:22.050: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.14742026s
Sep  5 23:24:24.112: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.209197464s
Sep  5 23:24:26.125: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.222349918s
Sep  5 23:24:28.141: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.23897129s
Sep  5 23:24:30.163: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.261031124s
Sep  5 23:24:32.194: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.29157519s
Sep  5 23:24:34.207: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 20.304533969s
Sep  5 23:24:36.231: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 22.328156095s
Sep  5 23:24:38.249: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 24.346511149s
Sep  5 23:24:40.265: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 26.362087228s
Sep  5 23:24:42.280: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 28.377485534s
Sep  5 23:24:44.298: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 30.395699423s
Sep  5 23:24:46.336: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 32.433625837s
Sep  5 23:24:48.350: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 34.447613008s
Sep  5 23:24:50.371: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 36.468619822s
Sep  5 23:24:52.386: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 38.483483334s
Sep  5 23:24:54.404: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 40.501605885s
Sep  5 23:24:56.428: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 42.526064944s
Sep  5 23:24:58.449: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Pending", Reason="", readiness=false. Elapsed: 44.546859104s
Sep  5 23:25:00.469: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 46.566793594s
STEP: Saw pod success
Sep  5 23:25:00.469: INFO: Pod "pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb" satisfied condition "Succeeded or Failed"
Sep  5 23:25:00.482: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb container test-container: <nil>
STEP: delete the pod
Sep  5 23:25:08.107: INFO: Waiting for pod pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb to disappear
Sep  5 23:25:08.130: INFO: Pod pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb still exists
Sep  5 23:25:10.130: INFO: Waiting for pod pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb to disappear
Sep  5 23:25:10.155: INFO: Pod pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb still exists
Sep  5 23:25:12.130: INFO: Waiting for pod pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb to disappear
Sep  5 23:25:12.146: INFO: Pod pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb still exists
Sep  5 23:25:14.131: INFO: Waiting for pod pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb to disappear
Sep  5 23:25:14.144: INFO: Pod pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb still exists
Sep  5 23:25:16.131: INFO: Waiting for pod pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb to disappear
Sep  5 23:25:16.146: INFO: Pod pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb still exists
Sep  5 23:25:18.130: INFO: Waiting for pod pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb to disappear
Sep  5 23:25:18.141: INFO: Pod pod-e4a1300f-cd3d-4cec-b807-d39f71689fbb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:25:18.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7031" for this suite.

• [SLOW TEST:65.170 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":140,"skipped":2446,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:25:18.386: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2188
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  5 23:25:50.453: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:25:50.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2188" for this suite.

• [SLOW TEST:32.696 seconds]
[k8s.io] Container Runtime
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":296,"completed":141,"skipped":2463,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:25:51.083: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1035
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep  5 23:25:53.072: INFO: Pod name wrapped-volume-race-65f597b0-ee0a-4d5c-9d48-2036dddffa90: Found 0 pods out of 5
Sep  5 23:25:58.109: INFO: Pod name wrapped-volume-race-65f597b0-ee0a-4d5c-9d48-2036dddffa90: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-65f597b0-ee0a-4d5c-9d48-2036dddffa90 in namespace emptydir-wrapper-1035, will wait for the garbage collector to delete the pods
Sep  5 23:26:52.307: INFO: Deleting ReplicationController wrapped-volume-race-65f597b0-ee0a-4d5c-9d48-2036dddffa90 took: 18.904114ms
Sep  5 23:26:54.608: INFO: Terminating ReplicationController wrapped-volume-race-65f597b0-ee0a-4d5c-9d48-2036dddffa90 pods took: 2.301046729s
STEP: Creating RC which spawns configmap-volume pods
Sep  5 23:27:09.789: INFO: Pod name wrapped-volume-race-14bed8b1-ed4b-447b-af3b-e19ba8277e4a: Found 0 pods out of 5
Sep  5 23:27:14.815: INFO: Pod name wrapped-volume-race-14bed8b1-ed4b-447b-af3b-e19ba8277e4a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-14bed8b1-ed4b-447b-af3b-e19ba8277e4a in namespace emptydir-wrapper-1035, will wait for the garbage collector to delete the pods
Sep  5 23:27:49.262: INFO: Deleting ReplicationController wrapped-volume-race-14bed8b1-ed4b-447b-af3b-e19ba8277e4a took: 122.263693ms
Sep  5 23:27:51.663: INFO: Terminating ReplicationController wrapped-volume-race-14bed8b1-ed4b-447b-af3b-e19ba8277e4a pods took: 2.401172196s
STEP: Creating RC which spawns configmap-volume pods
Sep  5 23:28:12.758: INFO: Pod name wrapped-volume-race-96e20276-cb9d-487f-8ae2-6aab6453df0c: Found 0 pods out of 5
Sep  5 23:28:17.810: INFO: Pod name wrapped-volume-race-96e20276-cb9d-487f-8ae2-6aab6453df0c: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-96e20276-cb9d-487f-8ae2-6aab6453df0c in namespace emptydir-wrapper-1035, will wait for the garbage collector to delete the pods
Sep  5 23:28:51.990: INFO: Deleting ReplicationController wrapped-volume-race-96e20276-cb9d-487f-8ae2-6aab6453df0c took: 20.647585ms
Sep  5 23:28:54.291: INFO: Terminating ReplicationController wrapped-volume-race-96e20276-cb9d-487f-8ae2-6aab6453df0c pods took: 2.301208037s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:29:10.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1035" for this suite.

• [SLOW TEST:199.612 seconds]
[sig-storage] EmptyDir wrapper volumes
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":296,"completed":142,"skipped":2477,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:29:10.695: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5617
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Docker Containers
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:29:45.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5617" for this suite.

• [SLOW TEST:35.866 seconds]
[k8s.io] Docker Containers
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":296,"completed":143,"skipped":2533,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:29:46.562: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-9492
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep  5 23:29:47.964: INFO: starting watch
STEP: patching
STEP: updating
Sep  5 23:29:48.023: INFO: waiting for watch events with expected annotations
Sep  5 23:29:48.023: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:29:48.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-9492" for this suite.
•{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":296,"completed":144,"skipped":2567,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:29:48.518: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3943
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-projected-qd8k
STEP: Creating a pod to test atomic-volume-subpath
Sep  5 23:29:49.169: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-qd8k" in namespace "subpath-3943" to be "Succeeded or Failed"
Sep  5 23:29:49.189: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 20.736752ms
Sep  5 23:29:51.200: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031104613s
Sep  5 23:29:53.215: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046137597s
Sep  5 23:29:55.225: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056336798s
Sep  5 23:29:57.241: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 8.072483812s
Sep  5 23:29:59.252: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 10.083911448s
Sep  5 23:30:01.307: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 12.138046972s
Sep  5 23:30:03.322: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 14.153058441s
Sep  5 23:30:05.344: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 16.175180221s
Sep  5 23:30:07.401: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 18.232238941s
Sep  5 23:30:09.422: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 20.253767638s
Sep  5 23:30:11.454: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 22.285936653s
Sep  5 23:30:13.473: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 24.304201948s
Sep  5 23:30:15.487: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 26.318327404s
Sep  5 23:30:17.502: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 28.333477696s
Sep  5 23:30:19.534: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 30.365042811s
Sep  5 23:30:21.552: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 32.383445823s
Sep  5 23:30:23.579: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 34.410073501s
Sep  5 23:30:25.596: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 36.427166937s
Sep  5 23:30:27.624: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Pending", Reason="", readiness=false. Elapsed: 38.455897734s
Sep  5 23:30:29.638: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Running", Reason="", readiness=true. Elapsed: 40.469788038s
Sep  5 23:30:31.658: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Running", Reason="", readiness=true. Elapsed: 42.488967814s
Sep  5 23:30:33.673: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Running", Reason="", readiness=true. Elapsed: 44.504053406s
Sep  5 23:30:35.692: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Running", Reason="", readiness=true. Elapsed: 46.523815207s
Sep  5 23:30:37.713: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Running", Reason="", readiness=true. Elapsed: 48.544302946s
Sep  5 23:30:39.732: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Running", Reason="", readiness=true. Elapsed: 50.563797522s
Sep  5 23:30:41.751: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Running", Reason="", readiness=true. Elapsed: 52.582850368s
Sep  5 23:30:43.778: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Running", Reason="", readiness=true. Elapsed: 54.609814497s
Sep  5 23:30:45.795: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Running", Reason="", readiness=true. Elapsed: 56.625957588s
Sep  5 23:30:47.809: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Running", Reason="", readiness=true. Elapsed: 58.640767924s
Sep  5 23:30:49.826: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Running", Reason="", readiness=true. Elapsed: 1m0.657906191s
Sep  5 23:30:51.841: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Running", Reason="", readiness=true. Elapsed: 1m2.672817791s
Sep  5 23:30:53.864: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Running", Reason="", readiness=true. Elapsed: 1m4.695159441s
Sep  5 23:30:55.882: INFO: Pod "pod-subpath-test-projected-qd8k": Phase="Succeeded", Reason="", readiness=false. Elapsed: 1m6.713943432s
STEP: Saw pod success
Sep  5 23:30:55.883: INFO: Pod "pod-subpath-test-projected-qd8k" satisfied condition "Succeeded or Failed"
Sep  5 23:30:55.903: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-subpath-test-projected-qd8k container test-container-subpath-projected-qd8k: <nil>
STEP: delete the pod
Sep  5 23:31:03.117: INFO: Waiting for pod pod-subpath-test-projected-qd8k to disappear
Sep  5 23:31:03.140: INFO: Pod pod-subpath-test-projected-qd8k still exists
Sep  5 23:31:05.140: INFO: Waiting for pod pod-subpath-test-projected-qd8k to disappear
Sep  5 23:31:05.158: INFO: Pod pod-subpath-test-projected-qd8k still exists
Sep  5 23:31:07.140: INFO: Waiting for pod pod-subpath-test-projected-qd8k to disappear
Sep  5 23:31:07.164: INFO: Pod pod-subpath-test-projected-qd8k still exists
Sep  5 23:31:09.140: INFO: Waiting for pod pod-subpath-test-projected-qd8k to disappear
Sep  5 23:31:09.157: INFO: Pod pod-subpath-test-projected-qd8k no longer exists
STEP: Deleting pod pod-subpath-test-projected-qd8k
Sep  5 23:31:09.157: INFO: Deleting pod "pod-subpath-test-projected-qd8k" in namespace "subpath-3943"
[AfterEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:31:09.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3943" for this suite.

• [SLOW TEST:80.968 seconds]
[sig-storage] Subpath
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":296,"completed":145,"skipped":2591,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:31:09.486: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-9965
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 23:31:09.960: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Creating first CR 
Sep  5 23:31:11.379: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-06T06:31:11Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-06T06:31:11Z]] name:name1 resourceVersion:151417 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:60dee230-dad2-48d2-87dc-7951ae2fafbe] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Sep  5 23:31:21.406: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-06T06:31:21Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-06T06:31:21Z]] name:name2 resourceVersion:151559 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:fc87ad24-42b1-4a7a-87b0-e312b71053a7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Sep  5 23:31:31.449: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-06T06:31:11Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-06T06:31:31Z]] name:name1 resourceVersion:151652 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:60dee230-dad2-48d2-87dc-7951ae2fafbe] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Sep  5 23:31:41.490: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-06T06:31:21Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-06T06:31:41Z]] name:name2 resourceVersion:151742 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:fc87ad24-42b1-4a7a-87b0-e312b71053a7] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Sep  5 23:31:51.533: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-06T06:31:11Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-06T06:31:31Z]] name:name1 resourceVersion:151840 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:60dee230-dad2-48d2-87dc-7951ae2fafbe] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Sep  5 23:32:01.589: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-06T06:31:21Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-06T06:31:41Z]] name:name2 resourceVersion:151936 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:fc87ad24-42b1-4a7a-87b0-e312b71053a7] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:32:12.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-9965" for this suite.

• [SLOW TEST:63.228 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":296,"completed":146,"skipped":2616,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should test the lifecycle of a ReplicationController [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:32:12.714: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3586
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should test the lifecycle of a ReplicationController [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ReplicationController
STEP: waiting for RC to be added
STEP: waiting for available Replicas
STEP: patching ReplicationController
STEP: waiting for RC to be modified
STEP: patching ReplicationController status
STEP: waiting for RC to be modified
STEP: waiting for available Replicas
STEP: fetching ReplicationController status
STEP: patching ReplicationController scale
STEP: waiting for RC to be modified
STEP: waiting for ReplicationController's scale to be the max amount
STEP: fetching ReplicationController; ensuring that it's patched
STEP: updating ReplicationController status
STEP: waiting for RC to be modified
STEP: listing all ReplicationControllers
STEP: checking that ReplicationController has expected values
STEP: deleting ReplicationControllers by collection
STEP: waiting for ReplicationController to have a DELETED watchEvent
[AfterEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:33:12.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3586" for this suite.

• [SLOW TEST:59.950 seconds]
[sig-apps] ReplicationController
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","total":296,"completed":147,"skipped":2627,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Docker Containers
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:33:12.665: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7145
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test override all
Sep  5 23:33:13.115: INFO: Waiting up to 5m0s for pod "client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac" in namespace "containers-7145" to be "Succeeded or Failed"
Sep  5 23:33:13.163: INFO: Pod "client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac": Phase="Pending", Reason="", readiness=false. Elapsed: 47.353439ms
Sep  5 23:33:15.171: INFO: Pod "client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055979089s
Sep  5 23:33:17.183: INFO: Pod "client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067984025s
Sep  5 23:33:19.199: INFO: Pod "client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084130891s
Sep  5 23:33:21.213: INFO: Pod "client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac": Phase="Pending", Reason="", readiness=false. Elapsed: 8.097484005s
Sep  5 23:33:23.224: INFO: Pod "client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac": Phase="Pending", Reason="", readiness=false. Elapsed: 10.108337608s
Sep  5 23:33:25.236: INFO: Pod "client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac": Phase="Pending", Reason="", readiness=false. Elapsed: 12.120367085s
Sep  5 23:33:27.248: INFO: Pod "client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac": Phase="Pending", Reason="", readiness=false. Elapsed: 14.132982797s
Sep  5 23:33:29.262: INFO: Pod "client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac": Phase="Pending", Reason="", readiness=false. Elapsed: 16.146487994s
Sep  5 23:33:31.279: INFO: Pod "client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac": Phase="Pending", Reason="", readiness=false. Elapsed: 18.164129756s
Sep  5 23:33:33.293: INFO: Pod "client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac": Phase="Pending", Reason="", readiness=false. Elapsed: 20.178113532s
Sep  5 23:33:35.306: INFO: Pod "client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac": Phase="Pending", Reason="", readiness=false. Elapsed: 22.190605822s
Sep  5 23:33:37.318: INFO: Pod "client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac": Phase="Pending", Reason="", readiness=false. Elapsed: 24.202699465s
Sep  5 23:33:39.330: INFO: Pod "client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac": Phase="Pending", Reason="", readiness=false. Elapsed: 26.215052933s
Sep  5 23:33:41.348: INFO: Pod "client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac": Phase="Pending", Reason="", readiness=false. Elapsed: 28.233090121s
Sep  5 23:33:43.360: INFO: Pod "client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac": Phase="Pending", Reason="", readiness=false. Elapsed: 30.244958991s
Sep  5 23:33:45.394: INFO: Pod "client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.278575543s
STEP: Saw pod success
Sep  5 23:33:45.394: INFO: Pod "client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac" satisfied condition "Succeeded or Failed"
Sep  5 23:33:45.408: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac container agnhost-container: <nil>
STEP: delete the pod
Sep  5 23:33:45.510: INFO: Waiting for pod client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac to disappear
Sep  5 23:33:45.523: INFO: Pod client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac still exists
Sep  5 23:33:47.524: INFO: Waiting for pod client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac to disappear
Sep  5 23:33:47.567: INFO: Pod client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac still exists
Sep  5 23:33:49.525: INFO: Waiting for pod client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac to disappear
Sep  5 23:33:49.571: INFO: Pod client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac still exists
Sep  5 23:33:51.525: INFO: Waiting for pod client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac to disappear
Sep  5 23:33:51.546: INFO: Pod client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac still exists
Sep  5 23:33:53.525: INFO: Waiting for pod client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac to disappear
Sep  5 23:33:53.548: INFO: Pod client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac still exists
Sep  5 23:33:55.525: INFO: Waiting for pod client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac to disappear
Sep  5 23:33:55.539: INFO: Pod client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac still exists
Sep  5 23:33:57.524: INFO: Waiting for pod client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac to disappear
Sep  5 23:33:57.543: INFO: Pod client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac still exists
Sep  5 23:33:59.526: INFO: Waiting for pod client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac to disappear
Sep  5 23:33:59.540: INFO: Pod client-containers-df00b8f2-aa94-4983-8d6b-25ea8ac58bac no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:33:59.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7145" for this suite.

• [SLOW TEST:47.504 seconds]
[k8s.io] Docker Containers
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":296,"completed":148,"skipped":2650,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:34:00.169: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4419
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-8e34cc7d-b8fe-4130-8739-6e1d4432cfd2
STEP: Creating a pod to test consume configMaps
Sep  5 23:34:00.900: INFO: Waiting up to 5m0s for pod "pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090" in namespace "configmap-4419" to be "Succeeded or Failed"
Sep  5 23:34:00.921: INFO: Pod "pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090": Phase="Pending", Reason="", readiness=false. Elapsed: 20.648653ms
Sep  5 23:34:02.931: INFO: Pod "pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030296885s
Sep  5 23:34:04.949: INFO: Pod "pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048895823s
Sep  5 23:34:06.978: INFO: Pod "pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090": Phase="Pending", Reason="", readiness=false. Elapsed: 6.077275035s
Sep  5 23:34:08.995: INFO: Pod "pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094255798s
Sep  5 23:34:11.006: INFO: Pod "pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090": Phase="Pending", Reason="", readiness=false. Elapsed: 10.105376915s
Sep  5 23:34:13.022: INFO: Pod "pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090": Phase="Pending", Reason="", readiness=false. Elapsed: 12.121704107s
Sep  5 23:34:15.057: INFO: Pod "pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090": Phase="Pending", Reason="", readiness=false. Elapsed: 14.15689654s
Sep  5 23:34:17.094: INFO: Pod "pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090": Phase="Pending", Reason="", readiness=false. Elapsed: 16.193653136s
Sep  5 23:34:19.123: INFO: Pod "pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090": Phase="Pending", Reason="", readiness=false. Elapsed: 18.222458207s
Sep  5 23:34:21.161: INFO: Pod "pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090": Phase="Pending", Reason="", readiness=false. Elapsed: 20.261068718s
Sep  5 23:34:23.170: INFO: Pod "pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090": Phase="Pending", Reason="", readiness=false. Elapsed: 22.269208704s
Sep  5 23:34:25.181: INFO: Pod "pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090": Phase="Pending", Reason="", readiness=false. Elapsed: 24.281058922s
Sep  5 23:34:27.195: INFO: Pod "pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090": Phase="Pending", Reason="", readiness=false. Elapsed: 26.294567209s
Sep  5 23:34:29.217: INFO: Pod "pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090": Phase="Pending", Reason="", readiness=false. Elapsed: 28.316912075s
Sep  5 23:34:31.235: INFO: Pod "pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.335143445s
STEP: Saw pod success
Sep  5 23:34:31.236: INFO: Pod "pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090" satisfied condition "Succeeded or Failed"
Sep  5 23:34:31.244: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 container agnhost-container: <nil>
STEP: delete the pod
Sep  5 23:34:31.313: INFO: Waiting for pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 to disappear
Sep  5 23:34:31.335: INFO: Pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 still exists
Sep  5 23:34:33.335: INFO: Waiting for pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 to disappear
Sep  5 23:34:33.347: INFO: Pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 still exists
Sep  5 23:34:35.335: INFO: Waiting for pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 to disappear
Sep  5 23:34:35.359: INFO: Pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 still exists
Sep  5 23:34:37.336: INFO: Waiting for pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 to disappear
Sep  5 23:34:37.351: INFO: Pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 still exists
Sep  5 23:34:39.336: INFO: Waiting for pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 to disappear
Sep  5 23:34:39.347: INFO: Pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 still exists
Sep  5 23:34:41.337: INFO: Waiting for pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 to disappear
Sep  5 23:34:41.358: INFO: Pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 still exists
Sep  5 23:34:43.336: INFO: Waiting for pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 to disappear
Sep  5 23:34:43.347: INFO: Pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 still exists
Sep  5 23:34:45.336: INFO: Waiting for pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 to disappear
Sep  5 23:34:45.351: INFO: Pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 still exists
Sep  5 23:34:47.336: INFO: Waiting for pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 to disappear
Sep  5 23:34:47.350: INFO: Pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 still exists
Sep  5 23:34:49.337: INFO: Waiting for pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 to disappear
Sep  5 23:34:49.353: INFO: Pod pod-configmaps-5b2b79cf-f598-4132-ada4-0d575c5ba090 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:34:49.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4419" for this suite.

• [SLOW TEST:49.433 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":296,"completed":149,"skipped":2650,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:34:49.603: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-16
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-7b172668-6648-4fec-a8b7-34b458e4ea43
STEP: Creating a pod to test consume secrets
Sep  5 23:34:50.088: INFO: Waiting up to 5m0s for pod "pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f" in namespace "secrets-16" to be "Succeeded or Failed"
Sep  5 23:34:50.100: INFO: Pod "pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.940973ms
Sep  5 23:34:52.112: INFO: Pod "pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023875823s
Sep  5 23:34:54.124: INFO: Pod "pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036060757s
Sep  5 23:34:56.149: INFO: Pod "pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061210132s
Sep  5 23:34:58.165: INFO: Pod "pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.076641969s
Sep  5 23:35:00.174: INFO: Pod "pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.085752531s
Sep  5 23:35:02.188: INFO: Pod "pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.100192265s
Sep  5 23:35:04.213: INFO: Pod "pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.124512974s
Sep  5 23:35:06.225: INFO: Pod "pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.137319447s
Sep  5 23:35:08.266: INFO: Pod "pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.178399147s
Sep  5 23:35:10.287: INFO: Pod "pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.198538608s
Sep  5 23:35:12.311: INFO: Pod "pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.222687622s
Sep  5 23:35:14.325: INFO: Pod "pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.236593086s
Sep  5 23:35:16.337: INFO: Pod "pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 26.24929195s
Sep  5 23:35:18.350: INFO: Pod "pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.262232407s
Sep  5 23:35:20.363: INFO: Pod "pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 30.274740678s
Sep  5 23:35:22.378: INFO: Pod "pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.289515137s
STEP: Saw pod success
Sep  5 23:35:22.378: INFO: Pod "pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f" satisfied condition "Succeeded or Failed"
Sep  5 23:35:22.388: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f container secret-env-test: <nil>
STEP: delete the pod
Sep  5 23:35:29.156: INFO: Waiting for pod pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f to disappear
Sep  5 23:35:29.188: INFO: Pod pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f still exists
Sep  5 23:35:31.188: INFO: Waiting for pod pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f to disappear
Sep  5 23:35:31.206: INFO: Pod pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f still exists
Sep  5 23:35:33.190: INFO: Waiting for pod pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f to disappear
Sep  5 23:35:33.202: INFO: Pod pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f still exists
Sep  5 23:35:35.188: INFO: Waiting for pod pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f to disappear
Sep  5 23:35:35.201: INFO: Pod pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f still exists
Sep  5 23:35:37.188: INFO: Waiting for pod pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f to disappear
Sep  5 23:35:37.198: INFO: Pod pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f still exists
Sep  5 23:35:39.189: INFO: Waiting for pod pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f to disappear
Sep  5 23:35:39.200: INFO: Pod pod-secrets-284bced9-0d82-4eec-90be-a789c54c2b9f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:35:39.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-16" for this suite.

• [SLOW TEST:50.779 seconds]
[sig-api-machinery] Secrets
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":296,"completed":150,"skipped":2673,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:35:40.382: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1678
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should get a host IP [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating pod
Sep  5 23:36:10.909: INFO: Pod pod-hostip-d0a04699-f035-4ea3-94cd-135207bd6a74 has hostIP: 10.185.226.233
[AfterEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:36:10.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1678" for this suite.

• [SLOW TEST:30.790 seconds]
[k8s.io] Pods
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should get a host IP [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":296,"completed":151,"skipped":2677,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:36:11.173: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4525
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 23:36:11.557: INFO: >>> kubeConfig: ./kconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:36:12.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4525" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":296,"completed":152,"skipped":2687,"failed":1,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]"]}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:36:12.871: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2746
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 23:36:52.246: FAIL: while waiting for the pod container to fail
Unexpected error:
    <*errors.errorString | 0xc0014195b0>: {
        s: "pod was expected to be pending, but it is in the state: Failed",
    }
    pod was expected to be pending, but it is in the state: Failed
occurred

Full Stack Trace
k8s.io/kubernetes/test/e2e/common.testPodFailSubpath(0xc0009f6c60, 0x1)
	/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:376 +0x112
k8s.io/kubernetes/test/e2e/common.glob..func9.6()
	/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:214 +0x1e5
k8s.io/kubernetes/test/e2e.RunE2ETests(0x48f1d7)
	_output/local/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x6ae
k8s.io/kubernetes/test/e2e.TestE2E(0x408ed9)
	_output/local/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x19
testing.tRunner(0xc001480820, 0x462aaa0)
	/usr/lib/golang/src/testing/testing.go:1259 +0x102
created by testing.(*T).Run
	/usr/lib/golang/src/testing/testing.go:1306 +0x35a
Sep  5 23:36:52.247: INFO: Deleting pod "var-expansion-e7f2b076-acc8-4294-bb4d-58320a1c6e68" in namespace "var-expansion-2746"
Sep  5 23:36:52.310: INFO: Wait up to 5m0s for pod "var-expansion-e7f2b076-acc8-4294-bb4d-58320a1c6e68" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
STEP: Collecting events from namespace "var-expansion-2746".
STEP: Found 12 events.
Sep  5 23:37:12.388: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for var-expansion-e7f2b076-acc8-4294-bb4d-58320a1c6e68: { } Scheduled: Successfully assigned var-expansion-2746/var-expansion-e7f2b076-acc8-4294-bb4d-58320a1c6e68 to sc2-10-185-226-233.eng.vmware.com
Sep  5 23:37:12.388: INFO: At 2021-09-05 23:36:19 -0700 PDT - event for var-expansion-e7f2b076-acc8-4294-bb4d-58320a1c6e68: {image-controller } Image: Image busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v14553 bound successfully
Sep  5 23:37:12.388: INFO: At 2021-09-05 23:36:20 -0700 PDT - event for busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v14553: {image-controller } Status: sc2-10-185-233-127.eng.vmware.com: Image status changed to Resolving
Sep  5 23:37:12.388: INFO: At 2021-09-05 23:36:25 -0700 PDT - event for var-expansion-e7f2b076-acc8-4294-bb4d-58320a1c6e68: {nsx-container-ncp 4216b30cf7711a81a7935dfb22c7b9d2} SuccessfulRealizeNSXResource: Successfully realized NSX resource for Pod
Sep  5 23:37:12.388: INFO: At 2021-09-05 23:36:28 -0700 PDT - event for busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v14553: {image-controller } Resolve: sc2-10-185-233-127.eng.vmware.com: Image resolved to ChainID sha256:23bc2b70b2014dec0ac22f27bb93e9babd08cdd6f1115d0c955b9ff22b382f5a
Sep  5 23:37:12.388: INFO: At 2021-09-05 23:36:29 -0700 PDT - event for busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v14553: {image-controller } Bind: Imagedisk bind failed: Operation cannot be fulfilled on images.imagecontroller.vmware.com "busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v14553": the object has been modified; please apply your changes to the latest version and try again
Sep  5 23:37:12.388: INFO: At 2021-09-05 23:36:29 -0700 PDT - event for busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v14553: {image-controller } Bind: Imagedisk 23bc2b70b2014dec0ac22f27bb93e9babd08cdd6f1115d0c955b9ff22b382f5a-v3965207 successfully bound
Sep  5 23:37:12.388: INFO: At 2021-09-05 23:36:29 -0700 PDT - event for busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v14553: {image-controller } Status: Image status changed to Ready
Sep  5 23:37:12.388: INFO: At 2021-09-05 23:36:38 -0700 PDT - event for var-expansion-e7f2b076-acc8-4294-bb4d-58320a1c6e68: {kubelet sc2-10-185-226-233.eng.vmware.com} Pulling: Waiting for Image var-expansion-2746/busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v14553
Sep  5 23:37:12.388: INFO: At 2021-09-05 23:36:38 -0700 PDT - event for var-expansion-e7f2b076-acc8-4294-bb4d-58320a1c6e68: {kubelet sc2-10-185-226-233.eng.vmware.com} Pulled: Image var-expansion-2746/busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v14553 is ready
Sep  5 23:37:12.388: INFO: At 2021-09-05 23:36:51 -0700 PDT - event for var-expansion-e7f2b076-acc8-4294-bb4d-58320a1c6e68: {kubelet sc2-10-185-226-233.eng.vmware.com} SuccessfulMountVolume: Successfully mounted volume workdir1
Sep  5 23:37:12.388: INFO: At 2021-09-05 23:36:51 -0700 PDT - event for var-expansion-e7f2b076-acc8-4294-bb4d-58320a1c6e68: {kubelet sc2-10-185-226-233.eng.vmware.com} SuccessfulMountVolume: Successfully mounted volume default-token-8nzdw
Sep  5 23:37:12.400: INFO: POD  NODE  PHASE  GRACE  CONDITIONS
Sep  5 23:37:12.400: INFO: 
Sep  5 23:37:12.413: INFO: 
Logging node info for node 421629d5b5512e89fd5a479875bff24c
Sep  5 23:37:12.423: INFO: Node Info: &Node{ObjectMeta:{421629d5b5512e89fd5a479875bff24c   /api/v1/nodes/421629d5b5512e89fd5a479875bff24c 5392ab9e-8d36-4a02-8e7c-d62b5fbca56f 152381 0 2021-09-05 19:56:51 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:421629d5b5512e89fd5a479875bff24c kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node-role.kubernetes.io/master:] map[kubeadm.alpha.kubernetes.io/cri-socket:/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 vmware-system-workload-ip-configured: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubeadm Update v1 2021-09-05 19:57:37 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/control-plane":{},"f:node-role.kubernetes.io/master":{}}}}} {kubectl-annotate Update v1 2021-09-05 20:03:41 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-workload-ip-configured":{}}}}} {kube-controller-manager Update v1 2021-09-05 20:11:03 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:taints":{}}}} {kubelet Update v1 2021-09-05 20:11:08 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-09-05 23:32:40 -0700 PDT,LastTransitionTime:2021-09-05 20:11:07 -0700 PDT,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-09-05 23:32:40 -0700 PDT,LastTransitionTime:2021-09-05 20:11:07 -0700 PDT,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-09-05 23:32:40 -0700 PDT,LastTransitionTime:2021-09-05 20:11:07 -0700 PDT,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-05 23:32:40 -0700 PDT,LastTransitionTime:2021-09-05 20:11:07 -0700 PDT,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:172.26.0.4,},NodeAddress{Type:Hostname,Address:421629d5b5512e89fd5a479875bff24c,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:a84e0f9b757b45c3bda61dd8d7e81c47,SystemUUID:d5291642-51b5-892e-fd5a-479875bff24c,BootID:96557016-0d11-4d1b-9d79-0a2c04c403df,KernelVersion:4.19.198-1.ph3-esx,OSImage:VMware Photon OS/Linux,ContainerRuntimeVersion:containerd://1.4.4,KubeletVersion:v1.20.8+vmware.wcp.1,KubeProxyVersion:v1.20.8+vmware.wcp.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[docker.io/vmware/wcp-authproxy:0.0.11.18508287],SizeBytes:131961954,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.21.0],SizeBytes:126851027,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.20.8],SizeBytes:123038003,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.21.0],SizeBytes:121071415,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.19.12],SizeBytes:120109363,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.20.8],SizeBytes:117573957,},ContainerImage{Names:[localhost:5000/vmware/kubectl-plugin-vsphere@sha256:50972d3c755506a39b060daf001e0fb4d7bfcc47aa059a945573d6962338a8f4 localhost:5000/vmware/kubectl-plugin-vsphere:0.0.11.18508287 docker.io/vmware/kubectl-plugin-vsphere:0.0.11.18508287],SizeBytes:114408041,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.19.12],SizeBytes:112073027,},ContainerImage{Names:[projects.registry.vmware.com/tkg/etcd:v3.4.13_vmware.1],SizeBytes:105113092,},ContainerImage{Names:[docker.io/vmware/wcp-schedext:0.0.11.18508287 docker.io/vmware/wcp-schedext:v1.20.8],SizeBytes:86647081,},ContainerImage{Names:[docker.io/vmware/docker-registry:2.7.0.14110537],SizeBytes:53040781,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.21.0],SizeBytes:51910456,},ContainerImage{Names:[docker.io/vmware/wcp-fip:0.1],SizeBytes:49119461,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.20.8],SizeBytes:48519474,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.19.12],SizeBytes:47737138,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-control-plane-controller@sha256:ae28d5c4008d5ad88db2936d9b45e0248d129245810239bbafe0c66a47f2bf7e localhost:5000/vmware/kubeadm-control-plane-controller:v0.3.17_vmware.1],SizeBytes:36065259,},ContainerImage{Names:[localhost:5000/vmware/kube-proxy:active localhost:5002/vmware/kube-proxy:v1.20.8],SizeBytes:35405060,},ContainerImage{Names:[localhost:5000/vmware/capw-controller@sha256:56496b9339aba7d338f2f864f0bb946e1d57f50bbcffeee26c57c1a7be6e9cff localhost:5000/vmware/capw-controller:1.4.1-28-ge3e1d8b],SizeBytes:34792328,},ContainerImage{Names:[localhost:5000/vmware/cluster-api-controller@sha256:72a217e8de91b2e1adcbcb1266a5854e9683f0f37ea828808374563d2bf41861 localhost:5000/vmware/cluster-api-controller:v0.3.17_vmware.1],SizeBytes:34280507,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-bootstrap-controller@sha256:921fc3887b0cfaf4742eefaf2b6d60160fdc25d2600dc08e9a3389aed6fbdaca localhost:5000/vmware/kubeadm-bootstrap-controller:v0.3.17_vmware.1],SizeBytes:33862350,},ContainerImage{Names:[localhost:5000/vmware/ucs@sha256:0006afae8591e51b3ddfe6d1062caa779c2bee08dc20ee38eb8183614ccbdcd1 localhost:5000/vmware/ucs:1.0-34-g9eb2916],SizeBytes:32101483,},ContainerImage{Names:[localhost:5000/vmware/tmc-agent-installer@sha256:4d885d2a505ccc87baf5fb73958593f0b66ce00732397e935114f923367cd4ec localhost:5000/vmware/tmc-agent-installer:1.0],SizeBytes:30452862,},ContainerImage{Names:[localhost:5000/vmware/kube-rbac-proxy@sha256:ce03b0932d049e82747829680025a5d2e3c2a3f50ad27e761329d35e9e8949d5 localhost:5000/vmware/kube-rbac-proxy:v0.4.1_vmware.1 localhost:5000/vmware/kube-rbac-proxy:0.0.1],SizeBytes:29742750,},ContainerImage{Names:[localhost:5000/vmware/vmop@sha256:aa3d277bcb390735b0110d9bf3f3e63ab48db23437c9ac275ea487a2fa460dd5 localhost:5000/vmware/vmop:1.4-201-g3d42a58],SizeBytes:29542297,},ContainerImage{Names:[localhost:5000/vmware/tkgs-plugin@sha256:7160ccba94dbc3a68fdb31108319ca03163688bc5999c4da1ee20e32723ea612 localhost:5000/vmware/tkgs-plugin:prod],SizeBytes:27042759,},ContainerImage{Names:[localhost:5000/vmware/tkg-controller@sha256:700277052cbfe8be5e7a67e7819bde16ac0afceba352c473906dc3cc572cdaf9 localhost:5000/vmware/tkg-controller:1.4-196-g0e69a9e],SizeBytes:26884265,},ContainerImage{Names:[localhost:5000/vmware.io/fluent-bit@sha256:bc50adf05790809513d94105a169912371b4b5f656775350b1ab65b556ecdc32 localhost:5000/vmware.io/fluent-bit:v1.5.1_vmware.3],SizeBytes:26471308,},ContainerImage{Names:[localhost:5000/vmware/namespace-operator@sha256:fbb1177a60f33312e0415830fd55c5b565628a863a25cb500b2058fc475e8da4 localhost:5000/vmware/namespace-operator:c203fcd],SizeBytes:25804352,},ContainerImage{Names:[localhost:5000/vmware/license-operator@sha256:521924f17d102e95723a580ed084b6fbc15e3f317b982a81fd071718c3e84138 localhost:5000/vmware/license-operator:7f54d05],SizeBytes:25041264,},ContainerImage{Names:[localhost:5000/vmware/coredns@sha256:68ed1b5aea34d19af320d3390278e7719efa19e3d9a42ef12fed0b2b99446c4f localhost:5000/vmware/coredns:v1.20],SizeBytes:12884153,},ContainerImage{Names:[docker.io/vmware/pause:1.19.12 docker.io/vmware/pause:1.20.8 docker.io/vmware/pause:1.21.0],SizeBytes:685866,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Sep  5 23:37:12.423: INFO: 
Logging kubelet events for node 421629d5b5512e89fd5a479875bff24c
Sep  5 23:37:12.431: INFO: 
Logging pods the kubelet thinks is on node 421629d5b5512e89fd5a479875bff24c
Sep  5 23:37:12.512: INFO: coredns-594c6dccdd-6jv2b started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container coredns ready: true, restart count 0
Sep  5 23:37:12.512: INFO: capi-controller-manager-d586f4c8-dnnr6 started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:12.512: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:12.512: INFO: kubectl-plugin-vsphere-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container kubectl-plugin-vsphere ready: true, restart count 4
Sep  5 23:37:12.512: INFO: wcp-authproxy-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container wcp-authproxy ready: true, restart count 0
Sep  5 23:37:12.512: INFO: capw-controller-manager-85b7cbb4bf-bghkv started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:12.512: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:12.512: INFO: wcp-fip-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container wcp-fip ready: true, restart count 0
Sep  5 23:37:12.512: INFO: vmware-system-tkg-controller-manager-575d95fb57-zl7sg started at 2021-09-05 20:03:54 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:12.512: INFO: 	Container manager ready: true, restart count 1
Sep  5 23:37:12.512: INFO: capi-kubeadm-bootstrap-controller-manager-5f5774d559-6cbtc started at 2021-09-05 20:03:54 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:12.512: INFO: 	Container manager ready: true, restart count 1
Sep  5 23:37:12.512: INFO: capi-kubeadm-control-plane-webhook-5f4c87b8b9-cphz4 started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:12.512: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:12.512: INFO: capi-webhook-69769f4c68-xjnm2 started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:12.512: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:12.512: INFO: kube-proxy-cwdbw started at 2021-09-05 20:04:19 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  5 23:37:12.512: INFO: docker-registry-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container docker-registry ready: true, restart count 0
Sep  5 23:37:12.512: INFO: etcd-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container etcd ready: true, restart count 0
Sep  5 23:37:12.512: INFO: kube-controller-manager-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 19:57:05 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container kube-controller-manager ready: true, restart count 2
Sep  5 23:37:12.512: INFO: kube-scheduler-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 19:57:05 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container kube-scheduler ready: true, restart count 3
Sep  5 23:37:12.512: INFO: 	Container wcp-schedext ready: true, restart count 1
Sep  5 23:37:12.512: INFO: fluentbit-rpz54 started at 2021-09-05 19:58:12 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container fluentbit ready: true, restart count 0
Sep  5 23:37:12.512: INFO: capi-kubeadm-bootstrap-webhook-6766c687f9-hjz5w started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:12.512: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:12.512: INFO: kube-apiserver-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 19:57:05 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container kube-apiserver ready: true, restart count 2
Sep  5 23:37:12.512: INFO: vmware-system-tkg-webhook-64fd4868cb-mk6zh started at 2021-09-05 20:03:54 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:12.512: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:12.512: INFO: vmware-system-vmop-controller-manager-6649dd65b-q7bqw started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:12.512: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:12.512: INFO: capi-kubeadm-control-plane-controller-manager-7c8d4b456f-l4t5v started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:12.512: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:12.512: INFO: tkgs-plugin-server-57df5fcfbf-km5pj started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:12.512: INFO: capw-webhook-58b86fb8b-vrzsj started at 2021-09-05 20:03:54 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:12.512: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:12.512: INFO: vmware-system-nsop-controller-manager-7dd8c47dcc-hbv6m started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container manager ready: true, restart count 1
Sep  5 23:37:12.512: INFO: upgrade-compatibility-service-65969fd6bc-l72w9 started at 2021-09-05 20:03:55 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container ucs ready: true, restart count 0
Sep  5 23:37:12.512: INFO: vmware-system-license-operator-controller-manager-5f5b6cddmtkxt started at 2021-09-05 20:03:55 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:12.512: INFO: masterproxy-tkgs-plugin-k5csq started at 2021-09-05 20:04:12 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:12.512: INFO: 	Container nginx ready: true, restart count 0
Sep  5 23:37:13.107: INFO: 
Latency metrics for node 421629d5b5512e89fd5a479875bff24c
Sep  5 23:37:13.107: INFO: 
Logging node info for node 4216b0433daa6b68a0e3d69463d0611d
Sep  5 23:37:13.115: INFO: Node Info: &Node{ObjectMeta:{4216b0433daa6b68a0e3d69463d0611d   /api/v1/nodes/4216b0433daa6b68a0e3d69463d0611d f38a87dd-ca99-4665-a189-88c353c18242 153894 0 2021-09-05 19:55:52 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:4216b0433daa6b68a0e3d69463d0611d kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node-role.kubernetes.io/master:] map[kubeadm.alpha.kubernetes.io/cri-socket:/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 vmware-system-workload-ip-configured: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubeadm Update v1 2021-09-05 19:56:44 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/control-plane":{},"f:node-role.kubernetes.io/master":{}}}}} {kubectl-annotate Update v1 2021-09-05 20:01:13 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-workload-ip-configured":{}}}}} {kube-controller-manager Update v1 2021-09-05 20:08:11 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:taints":{}}}} {kubelet Update v1 2021-09-05 20:08:16 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-09-05 23:34:54 -0700 PDT,LastTransitionTime:2021-09-05 20:08:16 -0700 PDT,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-09-05 23:34:54 -0700 PDT,LastTransitionTime:2021-09-05 20:08:16 -0700 PDT,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-09-05 23:34:54 -0700 PDT,LastTransitionTime:2021-09-05 20:08:16 -0700 PDT,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-05 23:34:54 -0700 PDT,LastTransitionTime:2021-09-05 20:08:16 -0700 PDT,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:172.26.0.3,},NodeAddress{Type:Hostname,Address:4216b0433daa6b68a0e3d69463d0611d,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7611aba96c374b2daf54816748293442,SystemUUID:43b01642-aa3d-686b-a0e3-d69463d0611d,BootID:ebcc64ab-5283-4781-b7ac-d3f663f1dc23,KernelVersion:4.19.198-1.ph3-esx,OSImage:VMware Photon OS/Linux,ContainerRuntimeVersion:containerd://1.4.4,KubeletVersion:v1.20.8+vmware.wcp.1,KubeProxyVersion:v1.20.8+vmware.wcp.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[docker.io/vmware/wcp-authproxy:0.0.11.18508287],SizeBytes:131961954,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.21.0],SizeBytes:126851027,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.20.8],SizeBytes:123038003,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.21.0],SizeBytes:121071415,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.19.12],SizeBytes:120109363,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.20.8],SizeBytes:117573957,},ContainerImage{Names:[localhost:5000/vmware/kubectl-plugin-vsphere@sha256:50972d3c755506a39b060daf001e0fb4d7bfcc47aa059a945573d6962338a8f4 docker.io/vmware/kubectl-plugin-vsphere:0.0.11.18508287 localhost:5000/vmware/kubectl-plugin-vsphere:0.0.11.18508287],SizeBytes:114408041,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.19.12],SizeBytes:112073027,},ContainerImage{Names:[projects.registry.vmware.com/tkg/etcd:v3.4.13_vmware.1],SizeBytes:105113092,},ContainerImage{Names:[docker.io/vmware/wcp-schedext:v1.20.8 docker.io/vmware/wcp-schedext:0.0.11.18508287],SizeBytes:86647081,},ContainerImage{Names:[localhost:5000/vmware/registry-agent@sha256:a02a9f68366f57abdd0833d34f1394e402a519d3fe82ff8c1106c67fc7c80392 localhost:5000/vmware/registry-agent:0.0.11.18508287],SizeBytes:76983788,},ContainerImage{Names:[docker.io/vmware/docker-registry:2.7.0.14110537],SizeBytes:53040781,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.21.0],SizeBytes:51910456,},ContainerImage{Names:[docker.io/vmware/wcp-fip:0.1],SizeBytes:49119461,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.20.8],SizeBytes:48519474,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.19.12],SizeBytes:47737138,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-control-plane-controller@sha256:ae28d5c4008d5ad88db2936d9b45e0248d129245810239bbafe0c66a47f2bf7e localhost:5000/vmware/kubeadm-control-plane-controller:v0.3.17_vmware.1],SizeBytes:36065259,},ContainerImage{Names:[localhost:5002/vmware/kube-proxy:v1.20.8 localhost:5000/vmware/kube-proxy:active],SizeBytes:35405060,},ContainerImage{Names:[localhost:5000/vmware/capw-controller@sha256:56496b9339aba7d338f2f864f0bb946e1d57f50bbcffeee26c57c1a7be6e9cff localhost:5000/vmware/capw-controller:1.4.1-28-ge3e1d8b],SizeBytes:34792328,},ContainerImage{Names:[localhost:5000/vmware/cluster-api-controller@sha256:72a217e8de91b2e1adcbcb1266a5854e9683f0f37ea828808374563d2bf41861 localhost:5000/vmware/cluster-api-controller:v0.3.17_vmware.1],SizeBytes:34280507,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-bootstrap-controller@sha256:921fc3887b0cfaf4742eefaf2b6d60160fdc25d2600dc08e9a3389aed6fbdaca localhost:5000/vmware/kubeadm-bootstrap-controller:v0.3.17_vmware.1],SizeBytes:33862350,},ContainerImage{Names:[localhost:5000/vmware/ucs@sha256:0006afae8591e51b3ddfe6d1062caa779c2bee08dc20ee38eb8183614ccbdcd1 localhost:5000/vmware/ucs:1.0-34-g9eb2916],SizeBytes:32101483,},ContainerImage{Names:[localhost:5000/vmware/tmc-agent-installer@sha256:4d885d2a505ccc87baf5fb73958593f0b66ce00732397e935114f923367cd4ec localhost:5000/vmware/tmc-agent-installer:1.0],SizeBytes:30452862,},ContainerImage{Names:[localhost:5000/vmware/kube-rbac-proxy@sha256:ce03b0932d049e82747829680025a5d2e3c2a3f50ad27e761329d35e9e8949d5 localhost:5000/vmware/kube-rbac-proxy:v0.4.1_vmware.1 localhost:5000/vmware/kube-rbac-proxy:0.0.1],SizeBytes:29742750,},ContainerImage{Names:[localhost:5000/vmware/vmop@sha256:aa3d277bcb390735b0110d9bf3f3e63ab48db23437c9ac275ea487a2fa460dd5 localhost:5000/vmware/vmop:1.4-201-g3d42a58],SizeBytes:29542297,},ContainerImage{Names:[localhost:5000/vmware/tkgs-plugin@sha256:7160ccba94dbc3a68fdb31108319ca03163688bc5999c4da1ee20e32723ea612 localhost:5000/vmware/tkgs-plugin:prod],SizeBytes:27042759,},ContainerImage{Names:[localhost:5000/vmware/tkg-controller@sha256:700277052cbfe8be5e7a67e7819bde16ac0afceba352c473906dc3cc572cdaf9 localhost:5000/vmware/tkg-controller:1.4-196-g0e69a9e],SizeBytes:26884265,},ContainerImage{Names:[localhost:5000/vmware.io/fluent-bit@sha256:bc50adf05790809513d94105a169912371b4b5f656775350b1ab65b556ecdc32 localhost:5000/vmware.io/fluent-bit:v1.5.1_vmware.3],SizeBytes:26471308,},ContainerImage{Names:[localhost:5000/vmware/namespace-operator@sha256:fbb1177a60f33312e0415830fd55c5b565628a863a25cb500b2058fc475e8da4 localhost:5000/vmware/namespace-operator:c203fcd],SizeBytes:25804352,},ContainerImage{Names:[localhost:5000/vmware/license-operator@sha256:521924f17d102e95723a580ed084b6fbc15e3f317b982a81fd071718c3e84138 localhost:5000/vmware/license-operator:7f54d05],SizeBytes:25041264,},ContainerImage{Names:[localhost:5000/vmware/coredns@sha256:68ed1b5aea34d19af320d3390278e7719efa19e3d9a42ef12fed0b2b99446c4f localhost:5000/vmware/coredns:v1.20],SizeBytes:12884153,},ContainerImage{Names:[docker.io/vmware/pause:1.21.0 docker.io/vmware/pause:1.19.12 docker.io/vmware/pause:1.20.8],SizeBytes:685866,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Sep  5 23:37:13.116: INFO: 
Logging kubelet events for node 4216b0433daa6b68a0e3d69463d0611d
Sep  5 23:37:13.123: INFO: 
Logging pods the kubelet thinks is on node 4216b0433daa6b68a0e3d69463d0611d
Sep  5 23:37:13.204: INFO: vmware-system-vmop-controller-manager-6649dd65b-6mvnw started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:13.204: INFO: 	Container manager ready: true, restart count 1
Sep  5 23:37:13.204: INFO: vmware-system-tkg-controller-manager-575d95fb57-k4slt started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:13.204: INFO: 	Container manager ready: true, restart count 1
Sep  5 23:37:13.204: INFO: tkgs-plugin-server-57df5fcfbf-pcxc7 started at 2021-09-05 20:01:25 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:13.204: INFO: capw-webhook-58b86fb8b-l8m95 started at 2021-09-05 20:01:38 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:13.204: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:13.204: INFO: capi-controller-manager-d586f4c8-m66p4 started at 2021-09-05 20:01:38 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:13.204: INFO: 	Container manager ready: true, restart count 1
Sep  5 23:37:13.204: INFO: vmware-system-license-operator-controller-manager-5f5b6cddsqckm started at 2021-09-05 20:01:38 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:13.204: INFO: vmware-registry-controller-manager-75fff77685-9v4wb started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container admin-agent ready: true, restart count 0
Sep  5 23:37:13.204: INFO: 	Container service-agent ready: true, restart count 5
Sep  5 23:37:13.204: INFO: kube-controller-manager-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:56:05 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep  5 23:37:13.204: INFO: kube-apiserver-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:56:05 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container kube-apiserver ready: true, restart count 3
Sep  5 23:37:13.204: INFO: masterproxy-tkgs-plugin-b7p7c started at 2021-09-05 20:01:25 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container nginx ready: true, restart count 0
Sep  5 23:37:13.204: INFO: coredns-594c6dccdd-vp82c started at 2021-09-05 20:01:37 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container coredns ready: true, restart count 0
Sep  5 23:37:13.204: INFO: capi-webhook-69769f4c68-5mxkm started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:13.204: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:13.204: INFO: wcp-authproxy-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:58:14 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container wcp-authproxy ready: true, restart count 0
Sep  5 23:37:13.204: INFO: etcd-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:56:05 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container etcd ready: true, restart count 0
Sep  5 23:37:13.204: INFO: fluentbit-8fbtb started at 2021-09-05 19:57:29 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container fluentbit ready: true, restart count 0
Sep  5 23:37:13.204: INFO: capw-controller-manager-85b7cbb4bf-n2kkl started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:13.204: INFO: 	Container manager ready: true, restart count 3
Sep  5 23:37:13.204: INFO: kube-scheduler-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:56:05 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container kube-scheduler ready: true, restart count 2
Sep  5 23:37:13.204: INFO: 	Container wcp-schedext ready: true, restart count 0
Sep  5 23:37:13.204: INFO: upgrade-compatibility-service-65969fd6bc-5ztpr started at 2021-09-05 20:01:38 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container ucs ready: true, restart count 0
Sep  5 23:37:13.204: INFO: capi-kubeadm-control-plane-webhook-5f4c87b8b9-zr5kj started at 2021-09-05 20:01:38 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:13.204: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:13.204: INFO: docker-registry-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:56:05 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container docker-registry ready: true, restart count 0
Sep  5 23:37:13.204: INFO: kube-proxy-rpd77 started at 2021-09-05 20:01:38 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  5 23:37:13.204: INFO: capi-kubeadm-control-plane-controller-manager-7c8d4b456f-qsw77 started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:13.204: INFO: 	Container manager ready: true, restart count 1
Sep  5 23:37:13.204: INFO: vmware-system-tkg-webhook-64fd4868cb-c7l5v started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:13.204: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:13.204: INFO: capi-kubeadm-bootstrap-webhook-6766c687f9-tfvhj started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:13.204: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:13.204: INFO: kubectl-plugin-vsphere-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:56:05 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container kubectl-plugin-vsphere ready: true, restart count 2
Sep  5 23:37:13.204: INFO: wcp-fip-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 20:01:25 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container wcp-fip ready: true, restart count 0
Sep  5 23:37:13.204: INFO: vmware-system-nsop-controller-manager-7dd8c47dcc-fvwkc started at 2021-09-05 20:01:38 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container manager ready: true, restart count 1
Sep  5 23:37:13.204: INFO: capi-kubeadm-bootstrap-controller-manager-5f5774d559-bdxq9 started at 2021-09-05 20:01:38 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:13.204: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:13.204: INFO: 	Container manager ready: true, restart count 1
Sep  5 23:37:13.869: INFO: 
Latency metrics for node 4216b0433daa6b68a0e3d69463d0611d
Sep  5 23:37:13.870: INFO: 
Logging node info for node 4216b30cf7711a81a7935dfb22c7b9d2
Sep  5 23:37:13.951: INFO: Node Info: &Node{ObjectMeta:{4216b30cf7711a81a7935dfb22c7b9d2   /api/v1/nodes/4216b30cf7711a81a7935dfb22c7b9d2 ca76b1f1-4314-4242-80c1-22a5cf294536 153663 0 2021-09-05 19:45:46 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:4216b30cf7711a81a7935dfb22c7b9d2 kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node-role.kubernetes.io/master:] map[kubeadm.alpha.kubernetes.io/cri-socket:/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 vmware-system-workload-ip-configured: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubelet Update v1 2021-09-05 19:45:46 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}} {kubeadm Update v1 2021-09-05 19:45:51 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/control-plane":{},"f:node-role.kubernetes.io/master":{}}}}} {kube-controller-manager Update v1 2021-09-05 19:45:56 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:taints":{}}}} {kubectl-annotate Update v1 2021-09-05 19:55:19 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-workload-ip-configured":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-09-05 23:34:32 -0700 PDT,LastTransitionTime:2021-09-05 19:45:41 -0700 PDT,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-09-05 23:34:32 -0700 PDT,LastTransitionTime:2021-09-05 19:45:41 -0700 PDT,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-09-05 23:34:32 -0700 PDT,LastTransitionTime:2021-09-05 19:45:41 -0700 PDT,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-05 23:34:32 -0700 PDT,LastTransitionTime:2021-09-05 19:58:42 -0700 PDT,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:172.26.0.2,},NodeAddress{Type:Hostname,Address:4216b30cf7711a81a7935dfb22c7b9d2,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:a54387ba332340588c82ffb9af566e0a,SystemUUID:0cb31642-71f7-811a-a793-5dfb22c7b9d2,BootID:ac168864-03bb-44ca-934a-1f275ba10acf,KernelVersion:4.19.198-1.ph3-esx,OSImage:VMware Photon OS/Linux,ContainerRuntimeVersion:containerd://1.4.4,KubeletVersion:v1.20.8+vmware.wcp.1,KubeProxyVersion:v1.20.8+vmware.wcp.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[localhost:5000/vmware/nsx-ncp-photon@sha256:07d0445341f274674dea40d9449da3304c62615afa544f4023e3a3e21c353c89 localhost:5000/vmware/nsx-ncp-photon:3.2.0.18464816],SizeBytes:456347510,},ContainerImage{Names:[localhost:5000/vmware/vsphere-csi@sha256:5f667055674ed889a9a5a3e52e1f074d2adedb5ccf5fdc9a2cefb56e50257346 localhost:5000/vmware/vsphere-csi:vsphere70u3-3530247],SizeBytes:218324207,},ContainerImage{Names:[localhost:5000/vmware/syncer@sha256:ebb1907ad57ee343efe2a756bd7109021701469875d29008004d2e6abe615e66 localhost:5000/vmware/syncer:vsphere70u3-3530247],SizeBytes:186035682,},ContainerImage{Names:[docker.io/vmware/wcp-authproxy:0.0.11.18508287],SizeBytes:131961954,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.21.0],SizeBytes:126851027,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.20.8],SizeBytes:123038003,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.21.0],SizeBytes:121071415,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.19.12],SizeBytes:120109363,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.20.8],SizeBytes:117573957,},ContainerImage{Names:[localhost:5000/vmware/kubectl-plugin-vsphere@sha256:50972d3c755506a39b060daf001e0fb4d7bfcc47aa059a945573d6962338a8f4 docker.io/vmware/kubectl-plugin-vsphere:0.0.11.18508287 localhost:5000/vmware/kubectl-plugin-vsphere:0.0.11.18508287],SizeBytes:114408041,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.19.12],SizeBytes:112073027,},ContainerImage{Names:[projects.registry.vmware.com/tkg/etcd:v3.4.13_vmware.1],SizeBytes:105113092,},ContainerImage{Names:[localhost:5000/vmware/cert-manager-controller@sha256:ff0caa9f76178dfc2c4398046b4b08472ab90def98d1b8649833d092a1468100 localhost:5000/vmware/cert-manager-controller:v0.15.2_vmware.3],SizeBytes:94027733,},ContainerImage{Names:[localhost:5000/vmware/cert-manager-cainjector@sha256:5137cbc469ee041ef0d1f6d35a914889d50c0a44134967ca407ba7c2f1a7628d localhost:5000/vmware/cert-manager-cainjector:v0.15.2_vmware.3],SizeBytes:88001986,},ContainerImage{Names:[localhost:5000/vmware/cert-manager-webhook@sha256:da40b26c6920331288a4809415c202e2ad3a60392ee3f43f535626024b8a9845 localhost:5000/vmware/cert-manager-webhook:v0.15.2_vmware.3],SizeBytes:87704580,},ContainerImage{Names:[docker.io/vmware/wcp-schedext:0.0.11.18508287 docker.io/vmware/wcp-schedext:v1.20.8],SizeBytes:86647081,},ContainerImage{Names:[localhost:5000/vmware/wcp-appplatform-operator-v1alpha2@sha256:39f94d105b25a4f047caa8234d74645c21ccee2e13279ee4e07fb506e27d7494 localhost:5000/vmware/wcp-appplatform-operator-v1alpha2:5e0ffb0],SizeBytes:72197580,},ContainerImage{Names:[localhost:5000/vmware/psp-operator@sha256:3a831bb95bb73b3bda416fc37e3eda8eb2ed2a54109ec2e54ad6d70932f2813e localhost:5000/vmware/psp-operator:49766149],SizeBytes:59395632,},ContainerImage{Names:[docker.io/vmware/docker-registry:2.7.0.14110537],SizeBytes:53040781,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.21.0],SizeBytes:51910456,},ContainerImage{Names:[docker.io/vmware/wcp-fip:0.1],SizeBytes:49119461,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.20.8],SizeBytes:48519474,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.19.12],SizeBytes:47737138,},ContainerImage{Names:[localhost:5000/vmware/image-controller@sha256:e8bcd8932715a95d86f857fc838d67aed6953eeab4554a3dada45c80a222b2ff localhost:5000/vmware/image-controller:0.0.11.18508287],SizeBytes:38823779,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-control-plane-controller@sha256:ae28d5c4008d5ad88db2936d9b45e0248d129245810239bbafe0c66a47f2bf7e localhost:5000/vmware/kubeadm-control-plane-controller:v0.3.17_vmware.1],SizeBytes:36065259,},ContainerImage{Names:[localhost:5002/vmware/kube-proxy:v1.20.8 localhost:5000/vmware/kube-proxy:active],SizeBytes:35405060,},ContainerImage{Names:[localhost:5000/vmware/capw-controller@sha256:56496b9339aba7d338f2f864f0bb946e1d57f50bbcffeee26c57c1a7be6e9cff localhost:5000/vmware/capw-controller:1.4.1-28-ge3e1d8b],SizeBytes:34792328,},ContainerImage{Names:[localhost:5000/vmware/cluster-api-controller@sha256:72a217e8de91b2e1adcbcb1266a5854e9683f0f37ea828808374563d2bf41861 localhost:5000/vmware/cluster-api-controller:v0.3.17_vmware.1],SizeBytes:34280507,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-bootstrap-controller@sha256:921fc3887b0cfaf4742eefaf2b6d60160fdc25d2600dc08e9a3389aed6fbdaca localhost:5000/vmware/kubeadm-bootstrap-controller:v0.3.17_vmware.1],SizeBytes:33862350,},ContainerImage{Names:[localhost:5000/vmware/ucs@sha256:0006afae8591e51b3ddfe6d1062caa779c2bee08dc20ee38eb8183614ccbdcd1 localhost:5000/vmware/ucs:1.0-34-g9eb2916],SizeBytes:32101483,},ContainerImage{Names:[localhost:5000/vmware/tmc-agent-installer@sha256:4d885d2a505ccc87baf5fb73958593f0b66ce00732397e935114f923367cd4ec localhost:5000/vmware/tmc-agent-installer:1.0],SizeBytes:30452862,},ContainerImage{Names:[localhost:5000/vmware/kube-rbac-proxy@sha256:ce03b0932d049e82747829680025a5d2e3c2a3f50ad27e761329d35e9e8949d5 localhost:5000/vmware/kube-rbac-proxy:v0.4.1_vmware.1 localhost:5000/vmware/kube-rbac-proxy:0.0.1],SizeBytes:29742750,},ContainerImage{Names:[localhost:5000/vmware/vmop@sha256:aa3d277bcb390735b0110d9bf3f3e63ab48db23437c9ac275ea487a2fa460dd5 localhost:5000/vmware/vmop:1.4-201-g3d42a58],SizeBytes:29542297,},ContainerImage{Names:[localhost:5000/vmware/tkgs-plugin@sha256:7160ccba94dbc3a68fdb31108319ca03163688bc5999c4da1ee20e32723ea612 localhost:5000/vmware/tkgs-plugin:prod],SizeBytes:27042759,},ContainerImage{Names:[localhost:5000/vmware/tkg-controller@sha256:700277052cbfe8be5e7a67e7819bde16ac0afceba352c473906dc3cc572cdaf9 localhost:5000/vmware/tkg-controller:1.4-196-g0e69a9e],SizeBytes:26884265,},ContainerImage{Names:[localhost:5000/vmware.io/fluent-bit@sha256:bc50adf05790809513d94105a169912371b4b5f656775350b1ab65b556ecdc32 localhost:5000/vmware.io/fluent-bit:v1.5.1_vmware.3],SizeBytes:26471308,},ContainerImage{Names:[localhost:5000/vmware/namespace-operator@sha256:fbb1177a60f33312e0415830fd55c5b565628a863a25cb500b2058fc475e8da4 localhost:5000/vmware/namespace-operator:c203fcd],SizeBytes:25804352,},ContainerImage{Names:[localhost:5000/vmware/license-operator@sha256:521924f17d102e95723a580ed084b6fbc15e3f317b982a81fd071718c3e84138 localhost:5000/vmware/license-operator:7f54d05],SizeBytes:25041264,},ContainerImage{Names:[localhost:5000/vmware/coredns@sha256:68ed1b5aea34d19af320d3390278e7719efa19e3d9a42ef12fed0b2b99446c4f localhost:5000/vmware/coredns:v1.20],SizeBytes:12884153,},ContainerImage{Names:[localhost:5000/vmware.io/csi-attacher@sha256:ef84a54cab084305c3b6638bcf3616539a1ece71054e519ce72c74a85ed6883c localhost:5000/vmware.io/csi-attacher:v3.2.1_vmware.1],SizeBytes:12847160,},ContainerImage{Names:[localhost:5000/vmware/kubernetes-csi_external-resizer/kubernetes-csi_external-resizer@sha256:eafb68e3367ac1b840b55b115351163cd24dfc1752e402c8d83210ab35d251eb localhost:5000/vmware/kubernetes-csi_external-resizer/kubernetes-csi_external-resizer:v1.2.0_vmware.1],SizeBytes:12843921,},ContainerImage{Names:[localhost:5000/vmware/csi-provisioner/csi-provisioner@sha256:f3783faf0e57904cde3702f7f0fb1f6e959c3160c90d399ea90931acd77abc18 localhost:5000/vmware/csi-provisioner/csi-provisioner:v2.1.0_vmware.4],SizeBytes:12501308,},ContainerImage{Names:[localhost:5000/vmware.io/csi-livenessprobe@sha256:7790ca0da41bfc8cd05c35c9309a36d060f847402147fddc32ca14063f9adc89 localhost:5000/vmware.io/csi-livenessprobe:v2.3.0_vmware.1],SizeBytes:5629758,},ContainerImage{Names:[docker.io/vmware/pause:1.19.12 docker.io/vmware/pause:1.20.8 docker.io/vmware/pause:1.21.0],SizeBytes:685866,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Sep  5 23:37:13.952: INFO: 
Logging kubelet events for node 4216b30cf7711a81a7935dfb22c7b9d2
Sep  5 23:37:13.979: INFO: 
Logging pods the kubelet thinks is on node 4216b30cf7711a81a7935dfb22c7b9d2
Sep  5 23:37:14.040: INFO: capi-kubeadm-bootstrap-controller-manager-5f5774d559-whnw5 started at 2021-09-05 19:50:51 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:14.040: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:14.040: INFO: 	Container manager ready: true, restart count 4
Sep  5 23:37:14.040: INFO: capw-controller-manager-85b7cbb4bf-9kcpx started at 2021-09-05 19:50:53 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:14.040: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:14.040: INFO: 	Container manager ready: true, restart count 3
Sep  5 23:37:14.040: INFO: kube-apiserver-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:58:35 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.040: INFO: 	Container kube-apiserver ready: true, restart count 1
Sep  5 23:37:14.040: INFO: wcp-fip-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:58:35 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.040: INFO: 	Container wcp-fip ready: true, restart count 0
Sep  5 23:37:14.040: INFO: fluentbit-k2ft9 started at 2021-09-05 19:47:14 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.040: INFO: 	Container fluentbit ready: true, restart count 0
Sep  5 23:37:14.040: INFO: coredns-594c6dccdd-d29zk started at 2021-09-05 19:47:13 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.040: INFO: 	Container coredns ready: true, restart count 7
Sep  5 23:37:14.040: INFO: kube-controller-manager-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:44:40 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.040: INFO: 	Container kube-controller-manager ready: true, restart count 2
Sep  5 23:37:14.040: INFO: vmware-system-appplatform-operator-mgr-0 started at 2021-09-05 19:47:14 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.040: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:14.040: INFO: cert-manager-cainjector-69c886766f-glw2p started at 2021-09-05 19:47:17 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.040: INFO: 	Container cert-manager ready: true, restart count 5
Sep  5 23:37:14.040: INFO: tkgs-plugin-server-57df5fcfbf-6gcbl started at 2021-09-05 19:52:48 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.040: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:14.040: INFO: vsphere-csi-controller-7ff5f98858-cthb7 started at 2021-09-05 19:52:54 -0700 PDT (0+6 container statuses recorded)
Sep  5 23:37:14.040: INFO: 	Container csi-attacher ready: true, restart count 4
Sep  5 23:37:14.040: INFO: 	Container csi-provisioner ready: true, restart count 4
Sep  5 23:37:14.040: INFO: 	Container csi-resizer ready: true, restart count 6
Sep  5 23:37:14.040: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  5 23:37:14.040: INFO: 	Container vsphere-csi-controller ready: true, restart count 3
Sep  5 23:37:14.040: INFO: 	Container vsphere-syncer ready: true, restart count 2
Sep  5 23:37:14.040: INFO: tmc-agent-installer-1630910220-mtk7j started at 2021-09-05 23:37:09 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.040: INFO: 	Container tmc-agent-installer ready: false, restart count 0
Sep  5 23:37:14.040: INFO: upgrade-compatibility-service-65969fd6bc-wgqdt started at 2021-09-05 19:47:14 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.040: INFO: 	Container ucs ready: true, restart count 0
Sep  5 23:37:14.040: INFO: cert-manager-799b5bbfdf-52dgq started at 2021-09-05 19:47:17 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.040: INFO: 	Container cert-manager ready: true, restart count 0
Sep  5 23:37:14.040: INFO: cert-manager-webhook-74488f47f-c6nmc started at 2021-09-05 19:47:17 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.040: INFO: 	Container cert-manager ready: true, restart count 0
Sep  5 23:37:14.040: INFO: nsx-ncp-6d7f7bf559-df5sk started at 2021-09-05 19:47:14 -0700 PDT (1+1 container statuses recorded)
Sep  5 23:37:14.040: INFO: 	Init container nsx-ncp-upgrade ready: true, restart count 0
Sep  5 23:37:14.040: INFO: 	Container nsx-ncp ready: true, restart count 6
Sep  5 23:37:14.040: INFO: capi-kubeadm-bootstrap-webhook-6766c687f9-tx5q2 started at 2021-09-05 19:50:51 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:14.040: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:14.040: INFO: 	Container manager ready: true, restart count 1
Sep  5 23:37:14.040: INFO: capi-webhook-69769f4c68-p8dfp started at 2021-09-05 19:50:53 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:14.040: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:14.040: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:14.041: INFO: vmware-system-tkg-controller-manager-575d95fb57-rcjkp started at 2021-09-05 19:51:32 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:14.041: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:14.041: INFO: 	Container manager ready: true, restart count 3
Sep  5 23:37:14.041: INFO: kube-scheduler-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:44:40 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:14.041: INFO: 	Container kube-scheduler ready: true, restart count 6
Sep  5 23:37:14.041: INFO: 	Container wcp-schedext ready: true, restart count 0
Sep  5 23:37:14.041: INFO: kubectl-plugin-vsphere-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:44:40 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.041: INFO: 	Container kubectl-plugin-vsphere ready: true, restart count 3
Sep  5 23:37:14.041: INFO: wcp-authproxy-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:58:35 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.041: INFO: 	Container wcp-authproxy ready: true, restart count 0
Sep  5 23:37:14.041: INFO: vmware-system-license-operator-controller-manager-5f5b6cddq7xzp started at 2021-09-05 19:52:58 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.041: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:14.041: INFO: kube-proxy-9zz2h started at 2021-09-05 19:47:13 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.041: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  5 23:37:14.041: INFO: capi-kubeadm-control-plane-controller-manager-7c8d4b456f-g7vmk started at 2021-09-05 19:50:52 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:14.041: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:14.041: INFO: 	Container manager ready: true, restart count 2
Sep  5 23:37:14.041: INFO: capi-kubeadm-control-plane-webhook-5f4c87b8b9-wjs6z started at 2021-09-05 19:50:52 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:14.041: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:14.041: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:14.041: INFO: vmware-system-psp-operator-mgr-6cc4d85755-w7lf5 started at 2021-09-05 19:51:14 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.041: INFO: 	Container manager ready: true, restart count 8
Sep  5 23:37:14.041: INFO: vmware-system-tkg-webhook-64fd4868cb-8khb4 started at 2021-09-05 19:51:32 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:14.041: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:14.041: INFO: 	Container manager ready: true, restart count 0
Sep  5 23:37:14.041: INFO: image-controller-597bd95bc9-t5pqm started at 2021-09-05 19:47:14 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.041: INFO: 	Container image-controller ready: true, restart count 1
Sep  5 23:37:14.041: INFO: capi-controller-manager-d586f4c8-pjhgc started at 2021-09-05 19:50:51 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:14.041: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:14.041: INFO: 	Container manager ready: true, restart count 3
Sep  5 23:37:14.041: INFO: capw-webhook-58b86fb8b-nxkgl started at 2021-09-05 19:50:54 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:14.041: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:14.041: INFO: 	Container manager ready: true, restart count 1
Sep  5 23:37:14.041: INFO: masterproxy-tkgs-plugin-8x4lc started at 2021-09-05 19:53:10 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.041: INFO: 	Container nginx ready: true, restart count 0
Sep  5 23:37:14.041: INFO: vmware-system-nsop-controller-manager-7dd8c47dcc-22cs2 started at 2021-09-05 19:53:20 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.041: INFO: 	Container manager ready: true, restart count 2
Sep  5 23:37:14.041: INFO: tmc-agent-installer-1630910160-h4cxn started at 2021-09-05 23:36:08 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.041: INFO: 	Container tmc-agent-installer ready: false, restart count 0
Sep  5 23:37:14.041: INFO: etcd-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:44:40 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.041: INFO: 	Container etcd ready: true, restart count 0
Sep  5 23:37:14.041: INFO: docker-registry-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:58:35 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:14.041: INFO: 	Container docker-registry ready: true, restart count 0
Sep  5 23:37:14.041: INFO: vmware-system-vmop-controller-manager-6649dd65b-tzzst started at 2021-09-05 19:51:23 -0700 PDT (0+2 container statuses recorded)
Sep  5 23:37:14.041: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 23:37:14.041: INFO: 	Container manager ready: true, restart count 5
Sep  5 23:37:15.375: INFO: 
Latency metrics for node 4216b30cf7711a81a7935dfb22c7b9d2
Sep  5 23:37:15.376: INFO: 
Logging node info for node sc2-10-185-226-150.eng.vmware.com
Sep  5 23:37:15.389: INFO: Node Info: &Node{ObjectMeta:{sc2-10-185-226-150.eng.vmware.com   /api/v1/nodes/sc2-10-185-226-150.eng.vmware.com d6938cb1-1bbd-4690-b273-7b7e638d12bc 155435 0 2021-09-05 19:58:48 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:CRX kubernetes.io/arch:amd64 kubernetes.io/hostname:sc2-10-185-226-150.eng.vmware.com kubernetes.io/os:CRX node-role.kubernetes.io/agent:agent node.kubernetes.io/role:agent type:virtual-kubelet] map[ncp/transport_node:644f660e-c6b2-4fd5-a4b5-5cccdda12759 node.alpha.kubernetes.io/ttl:0 vmware-system-esxi-node-moid:host-16 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{wcpsvc Update v1 2021-09-05 19:58:49 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-esxi-node-moid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{"f:node-role.kubernetes.io/agent":{}}}}} {spherelet Update v1 2021-09-05 20:02:50 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:ncp/transport_node":{}},"f:labels":{".":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node.kubernetes.io/role":{},"f:type":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:nodeInfo":{"f:architecture":{},"f:kubeletVersion":{},"f:operatingSystem":{}}}}} {kube-controller-manager Update v1 2021-09-05 20:28:56 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}},"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:status":{"f:volumesAttached":{}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{12 0} {<nil>} 12 DecimalSI},memory: {{34354941952 0} {<nil>} 33549748Ki BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Allocatable:ResourceList{cpu: {{9 0} {<nil>} 9 DecimalSI},memory: {{4117757952 0} {<nil>} 3927Mi BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-05 23:37:09 -0700 PDT,LastTransitionTime:2021-09-05 23:37:09 -0700 PDT,Reason:KubeletReady,Message:Spherelet is ready.,},},Addresses:[]NodeAddress{NodeAddress{Type:Hostname,Address:sc2-10-185-226-150.eng.vmware.com,},NodeAddress{Type:InternalIP,Address:10.185.226.150,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:,SystemUUID:,BootID:,KernelVersion:,OSImage:,ContainerRuntimeVersion:,KubeletVersion:v1.20.2-sph-0afe203,KubeProxyVersion:,OperatingSystem:ESXi,Architecture:amd64,},Images:[]ContainerImage{},VolumesInUse:[],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/csi/csi.vsphere.vmware.com^47d4cc26-bb29-41cb-a660-79ed55715f11,DevicePath:,},},Config:nil,},}
Sep  5 23:37:15.389: INFO: 
Logging kubelet events for node sc2-10-185-226-150.eng.vmware.com
Sep  5 23:37:15.398: INFO: 
Logging pods the kubelet thinks is on node sc2-10-185-226-150.eng.vmware.com
Sep  5 23:37:15.442: INFO: schedext-test-affinity-1 started at 2021-09-05 20:22:12 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:15.443: INFO: 	Container hello ready: true, restart count 0
Sep  5 23:37:15.443: INFO: schedext-test-affinity-2 started at 2021-09-05 20:22:42 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:15.443: INFO: 	Container hello ready: true, restart count 0
Sep  5 23:37:15.443: INFO: helloworld started at 2021-09-05 20:32:42 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:15.443: INFO: 	Container hello ready: true, restart count 0
Sep  5 23:37:15.443: INFO: hello-web-6b97664bd5-zznrr started at 2021-09-05 22:45:00 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:15.443: INFO: 	Container hello-app ready: true, restart count 0
Sep  5 23:37:15.443: INFO: wcp-sanity-busybox-6f999d6849-kspp7 started at 2021-09-05 20:17:25 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:15.443: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 23:37:15.443: INFO: schedext-test-node-selector-1 started at 2021-09-05 20:21:19 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:15.443: INFO: 	Container hello ready: true, restart count 0
Sep  5 23:37:15.443: INFO: podwithpersistentvolume started at 2021-09-05 20:29:04 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:15.443: INFO: 	Container hello ready: true, restart count 0
Sep  5 23:37:15.443: INFO: wcp-sanity-busybox-6f999d6849-6bn7v started at 2021-09-05 20:30:32 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:15.443: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 23:37:15.443: INFO: curl-pod started at 2021-09-05 20:38:23 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:15.443: INFO: 	Container curl-container ready: true, restart count 0
Sep  5 23:37:15.443: INFO: test-docker-registry started at 2021-09-05 20:41:08 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:15.443: INFO: 	Container test-docker-registry ready: true, restart count 0
Sep  5 23:37:15.487: INFO: 
Latency metrics for node sc2-10-185-226-150.eng.vmware.com
Sep  5 23:37:15.487: INFO: 
Logging node info for node sc2-10-185-226-233.eng.vmware.com
Sep  5 23:37:15.495: INFO: Node Info: &Node{ObjectMeta:{sc2-10-185-226-233.eng.vmware.com   /api/v1/nodes/sc2-10-185-226-233.eng.vmware.com 99eaa46a-24ac-48ad-852e-eabb653ed037 155453 0 2021-09-05 19:58:37 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:CRX kubernetes.io/arch:amd64 kubernetes.io/hostname:sc2-10-185-226-233.eng.vmware.com kubernetes.io/os:CRX node-role.kubernetes.io/agent:agent node.kubernetes.io/role:agent type:virtual-kubelet] map[ncp/transport_node:97c82e2a-be25-406d-ab4e-f897b55c19b3 node.alpha.kubernetes.io/ttl:0 vmware-system-esxi-node-moid:host-19 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{wcpsvc Update v1 2021-09-05 19:58:37 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-esxi-node-moid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{"f:node-role.kubernetes.io/agent":{}}}}} {spherelet Update v1 2021-09-05 20:02:48 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:ncp/transport_node":{}},"f:labels":{".":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node.kubernetes.io/role":{},"f:type":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:nodeInfo":{"f:architecture":{},"f:kubeletVersion":{},"f:operatingSystem":{}}}}} {kube-controller-manager Update v1 2021-09-05 20:27:52 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}},"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{12 0} {<nil>} 12 DecimalSI},memory: {{34354941952 0} {<nil>} 33549748Ki BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Allocatable:ResourceList{cpu: {{10 0} {<nil>} 10 DecimalSI},memory: {{6102712320 0} {<nil>} 5820Mi BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-05 23:37:12 -0700 PDT,LastTransitionTime:2021-09-05 23:37:12 -0700 PDT,Reason:KubeletReady,Message:Spherelet is ready.,},},Addresses:[]NodeAddress{NodeAddress{Type:Hostname,Address:sc2-10-185-226-233.eng.vmware.com,},NodeAddress{Type:InternalIP,Address:10.185.226.233,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:,SystemUUID:,BootID:,KernelVersion:,OSImage:,ContainerRuntimeVersion:,KubeletVersion:v1.20.2-sph-0afe203,KubeProxyVersion:,OperatingSystem:ESXi,Architecture:amd64,},Images:[]ContainerImage{},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Sep  5 23:37:15.496: INFO: 
Logging kubelet events for node sc2-10-185-226-233.eng.vmware.com
Sep  5 23:37:15.505: INFO: 
Logging pods the kubelet thinks is on node sc2-10-185-226-233.eng.vmware.com
Sep  5 23:37:15.543: INFO: wcp-sanity-busybox-6f999d6849-428rc started at 2021-09-05 22:46:12 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:15.544: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 23:37:15.544: INFO: wcp-sanity-busybox-6f999d6849-tx88v started at 2021-09-05 22:46:15 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:15.544: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 23:37:15.592: INFO: 
Latency metrics for node sc2-10-185-226-233.eng.vmware.com
Sep  5 23:37:15.592: INFO: 
Logging node info for node sc2-10-185-233-127.eng.vmware.com
Sep  5 23:37:15.606: INFO: Node Info: &Node{ObjectMeta:{sc2-10-185-233-127.eng.vmware.com   /api/v1/nodes/sc2-10-185-233-127.eng.vmware.com 9215b2dc-bc25-43aa-b2d8-00a2b7b458d7 155444 0 2021-09-05 19:58:26 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:CRX kubernetes.io/arch:amd64 kubernetes.io/hostname:sc2-10-185-233-127.eng.vmware.com kubernetes.io/os:CRX node-role.kubernetes.io/agent:agent node.kubernetes.io/role:agent type:virtual-kubelet] map[ncp/transport_node:9e6990b2-2258-45d1-81b5-10ca792b1fac node.alpha.kubernetes.io/ttl:0 vmware-system-esxi-node-moid:host-12 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kube-controller-manager Update v1 2021-09-05 19:58:26 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}},"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}}}} {wcpsvc Update v1 2021-09-05 19:58:26 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-esxi-node-moid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{"f:node-role.kubernetes.io/agent":{}}}}} {spherelet Update v1 2021-09-05 20:02:46 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:ncp/transport_node":{}},"f:labels":{".":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node.kubernetes.io/role":{},"f:type":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:nodeInfo":{"f:architecture":{},"f:kubeletVersion":{},"f:operatingSystem":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{12 0} {<nil>} 12 DecimalSI},memory: {{34354941952 0} {<nil>} 33549748Ki BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Allocatable:ResourceList{cpu: {{10 0} {<nil>} 10 DecimalSI},memory: {{5038407680 0} {<nil>} 4805Mi BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-05 23:37:10 -0700 PDT,LastTransitionTime:2021-09-05 23:37:10 -0700 PDT,Reason:KubeletReady,Message:Spherelet is ready.,},},Addresses:[]NodeAddress{NodeAddress{Type:Hostname,Address:sc2-10-185-233-127.eng.vmware.com,},NodeAddress{Type:InternalIP,Address:10.185.233.127,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:,SystemUUID:,BootID:,KernelVersion:,OSImage:,ContainerRuntimeVersion:,KubeletVersion:v1.20.2-sph-0afe203,KubeProxyVersion:,OperatingSystem:ESXi,Architecture:amd64,},Images:[]ContainerImage{},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Sep  5 23:37:15.606: INFO: 
Logging kubelet events for node sc2-10-185-233-127.eng.vmware.com
Sep  5 23:37:15.613: INFO: 
Logging pods the kubelet thinks is on node sc2-10-185-233-127.eng.vmware.com
Sep  5 23:37:15.662: INFO: helloworld started at 2021-09-05 20:18:37 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:15.662: INFO: 	Container hello ready: true, restart count 0
Sep  5 23:37:15.662: INFO: nginx-private started at 2021-09-05 20:19:39 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:15.662: INFO: 	Container nginx-private-container ready: true, restart count 0
Sep  5 23:37:15.662: INFO: schedext-test-affinity-3 started at 2021-09-05 20:22:44 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:15.662: INFO: 	Container hello ready: true, restart count 0
Sep  5 23:37:15.662: INFO: wcp-sanity-busybox-6f999d6849-564rw started at 2021-09-05 20:30:14 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:15.662: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 23:37:15.662: INFO: curl-pod started at 2021-09-05 20:33:45 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:15.662: INFO: 	Container curl-container ready: true, restart count 0
Sep  5 23:37:15.662: INFO: hello-web-2-f779cbdff-dmb55 started at 2021-09-05 20:37:45 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:15.662: INFO: 	Container hello-app ready: true, restart count 0
Sep  5 23:37:15.662: INFO: hello-web-1-6b97664bd5-w4mv7 started at 2021-09-05 22:44:58 -0700 PDT (0+1 container statuses recorded)
Sep  5 23:37:15.662: INFO: 	Container hello-app ready: true, restart count 0
Sep  5 23:37:15.730: INFO: 
Latency metrics for node sc2-10-185-233-127.eng.vmware.com
Sep  5 23:37:15.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2746" for this suite.

• Failure [63.611 seconds]
[k8s.io] Variable Expansion
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance] [It]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629

  Sep  5 23:36:52.246: while waiting for the pod container to fail
  Unexpected error:
      <*errors.errorString | 0xc0014195b0>: {
          s: "pod was expected to be pending, but it is in the state: Failed",
      }
      pod was expected to be pending, but it is in the state: Failed
  occurred

  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:376
------------------------------
{"msg":"FAILED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":296,"completed":152,"skipped":2692,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Networking
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:37:16.483: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-607
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Performing setup for networking test in namespace pod-network-test-607
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  5 23:37:16.973: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  5 23:37:17.169: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:37:19.183: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:37:21.182: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:37:23.184: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:37:25.182: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:37:27.188: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:37:29.194: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:37:31.218: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:37:33.187: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:37:35.187: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:37:37.186: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:37:39.182: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:37:41.183: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:37:43.240: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:37:45.195: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:37:47.182: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 23:37:49.183: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 23:37:51.181: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 23:37:53.198: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 23:37:55.188: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 23:37:57.182: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep  5 23:37:57.194: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep  5 23:37:59.205: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep  5 23:38:01.211: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep  5 23:38:03.216: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep  5 23:38:03.233: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep  5 23:38:27.331: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Sep  5 23:38:27.331: INFO: Breadth first check of 172.26.1.195 on host 10.185.226.150...
Sep  5 23:38:27.339: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.26.1.197:9080/dial?request=hostname&protocol=udp&host=172.26.1.195&port=8081&tries=1'] Namespace:pod-network-test-607 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  5 23:38:27.339: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 23:38:27.780: INFO: Waiting for responses: map[]
Sep  5 23:38:27.780: INFO: reached 172.26.1.195 after 0/1 tries
Sep  5 23:38:27.780: INFO: Breadth first check of 172.26.1.194 on host 10.185.226.233...
Sep  5 23:38:27.799: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.26.1.197:9080/dial?request=hostname&protocol=udp&host=172.26.1.194&port=8081&tries=1'] Namespace:pod-network-test-607 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  5 23:38:27.799: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 23:38:27.960: INFO: Waiting for responses: map[]
Sep  5 23:38:27.960: INFO: reached 172.26.1.194 after 0/1 tries
Sep  5 23:38:27.960: INFO: Breadth first check of 172.26.1.196 on host 10.185.233.127...
Sep  5 23:38:27.972: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.26.1.197:9080/dial?request=hostname&protocol=udp&host=172.26.1.196&port=8081&tries=1'] Namespace:pod-network-test-607 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  5 23:38:27.972: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 23:38:28.150: INFO: Waiting for responses: map[]
Sep  5 23:38:28.150: INFO: reached 172.26.1.196 after 0/1 tries
Sep  5 23:38:28.150: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:38:28.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-607" for this suite.

• [SLOW TEST:71.896 seconds]
[sig-network] Networking
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:27
  Granular Checks: Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:30
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":296,"completed":153,"skipped":2748,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:38:28.380: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2428
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 23:38:30.052: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  5 23:38:32.075: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:38:34.089: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:38:36.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:38:38.094: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:38:40.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:38:42.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:38:44.098: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:38:46.360: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:38:48.147: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:38:50.091: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:38:52.096: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:38:54.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:38:56.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:38:58.096: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:39:00.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:39:02.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:39:04.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:39:06.094: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 38, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 23:39:09.161: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:39:09.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2428" for this suite.
STEP: Destroying namespace "webhook-2428-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:42.644 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":296,"completed":154,"skipped":2754,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:39:11.024: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-113
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:39:43.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-113" for this suite.

• [SLOW TEST:32.946 seconds]
[k8s.io] Kubelet
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when scheduling a read only busybox container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:188
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":155,"skipped":2771,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:39:43.971: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-63
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:39:56.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-63" for this suite.

• [SLOW TEST:12.889 seconds]
[sig-api-machinery] ResourceQuota
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":296,"completed":156,"skipped":2775,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:39:56.860: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3629
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-3e85246e-ee3a-49d2-abd2-e17c27bfa663
STEP: Creating a pod to test consume configMaps
Sep  5 23:39:57.335: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f" in namespace "projected-3629" to be "Succeeded or Failed"
Sep  5 23:39:57.345: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.456338ms
Sep  5 23:39:59.357: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021413532s
Sep  5 23:40:01.389: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053031777s
Sep  5 23:40:03.402: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.066547712s
Sep  5 23:40:05.417: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081243587s
Sep  5 23:40:07.424: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.088640767s
Sep  5 23:40:09.484: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.148150966s
Sep  5 23:40:11.496: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.160181334s
Sep  5 23:40:13.524: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.188305469s
Sep  5 23:40:15.587: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.251429842s
Sep  5 23:40:17.611: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.275288023s
Sep  5 23:40:19.624: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.288064204s
Sep  5 23:40:21.638: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.302829955s
Sep  5 23:40:23.651: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 26.31535867s
Sep  5 23:40:25.659: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.323091044s
Sep  5 23:40:27.671: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 30.335451725s
Sep  5 23:40:29.685: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 32.349099195s
Sep  5 23:40:31.702: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 34.366609586s
Sep  5 23:40:33.713: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 36.377482409s
Sep  5 23:40:35.727: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Pending", Reason="", readiness=false. Elapsed: 38.391812285s
Sep  5 23:40:37.742: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 40.406525246s
STEP: Saw pod success
Sep  5 23:40:37.742: INFO: Pod "pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f" satisfied condition "Succeeded or Failed"
Sep  5 23:40:37.750: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f container agnhost-container: <nil>
STEP: delete the pod
Sep  5 23:40:45.319: INFO: Waiting for pod pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f to disappear
Sep  5 23:40:45.337: INFO: Pod pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f still exists
Sep  5 23:40:47.337: INFO: Waiting for pod pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f to disappear
Sep  5 23:40:47.347: INFO: Pod pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f still exists
Sep  5 23:40:49.337: INFO: Waiting for pod pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f to disappear
Sep  5 23:40:49.383: INFO: Pod pod-projected-configmaps-0b0fc2da-394b-4b01-b9d5-0a824f5a043f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:40:49.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3629" for this suite.

• [SLOW TEST:52.805 seconds]
[sig-storage] Projected configMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":157,"skipped":2776,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:40:49.665: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7122
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:41:04.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7122" for this suite.

• [SLOW TEST:14.931 seconds]
[sig-api-machinery] ResourceQuota
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":296,"completed":158,"skipped":2793,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:41:04.596: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-5764
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [sig-node] PodTemplates
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:41:05.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-5764" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":296,"completed":159,"skipped":2832,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:41:05.368: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-2274
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Sep  5 23:41:05.875: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:41:05.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2274" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":296,"completed":160,"skipped":2863,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:41:06.189: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5109
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5109 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5109;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5109 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5109;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5109.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5109.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5109.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5109.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5109.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5109.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5109.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5109.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5109.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5109.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5109.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5109.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5109.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 100.239.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.239.100_udp@PTR;check="$$(dig +tcp +noall +answer +search 100.239.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.239.100_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5109 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5109;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5109 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5109;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5109.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5109.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5109.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5109.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5109.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5109.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5109.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5109.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5109.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5109.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5109.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5109.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5109.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 100.239.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.239.100_udp@PTR;check="$$(dig +tcp +noall +answer +search 100.239.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.239.100_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 23:41:48.896: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-5109/dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64: the server could not find the requested resource (get pods dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64)
Sep  5 23:41:48.924: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-5109/dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64: the server could not find the requested resource (get pods dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64)
Sep  5 23:41:48.943: INFO: Unable to read wheezy_udp@dns-test-service.dns-5109 from pod dns-5109/dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64: the server could not find the requested resource (get pods dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64)
Sep  5 23:41:48.962: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5109 from pod dns-5109/dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64: the server could not find the requested resource (get pods dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64)
Sep  5 23:41:48.988: INFO: Unable to read wheezy_udp@dns-test-service.dns-5109.svc from pod dns-5109/dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64: the server could not find the requested resource (get pods dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64)
Sep  5 23:41:49.002: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5109.svc from pod dns-5109/dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64: the server could not find the requested resource (get pods dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64)
Sep  5 23:41:49.017: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5109.svc from pod dns-5109/dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64: the server could not find the requested resource (get pods dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64)
Sep  5 23:41:49.028: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5109.svc from pod dns-5109/dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64: the server could not find the requested resource (get pods dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64)
Sep  5 23:41:49.121: INFO: Unable to read jessie_udp@dns-test-service from pod dns-5109/dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64: the server could not find the requested resource (get pods dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64)
Sep  5 23:41:49.134: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-5109/dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64: the server could not find the requested resource (get pods dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64)
Sep  5 23:41:49.150: INFO: Unable to read jessie_udp@dns-test-service.dns-5109 from pod dns-5109/dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64: the server could not find the requested resource (get pods dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64)
Sep  5 23:41:49.168: INFO: Unable to read jessie_tcp@dns-test-service.dns-5109 from pod dns-5109/dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64: the server could not find the requested resource (get pods dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64)
Sep  5 23:41:49.217: INFO: Unable to read jessie_udp@dns-test-service.dns-5109.svc from pod dns-5109/dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64: the server could not find the requested resource (get pods dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64)
Sep  5 23:41:49.235: INFO: Unable to read jessie_tcp@dns-test-service.dns-5109.svc from pod dns-5109/dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64: the server could not find the requested resource (get pods dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64)
Sep  5 23:41:49.249: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5109.svc from pod dns-5109/dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64: the server could not find the requested resource (get pods dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64)
Sep  5 23:41:49.269: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5109.svc from pod dns-5109/dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64: the server could not find the requested resource (get pods dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64)
Sep  5 23:41:49.355: INFO: Lookups using dns-5109/dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-5109 wheezy_tcp@dns-test-service.dns-5109 wheezy_udp@dns-test-service.dns-5109.svc wheezy_tcp@dns-test-service.dns-5109.svc wheezy_udp@_http._tcp.dns-test-service.dns-5109.svc wheezy_tcp@_http._tcp.dns-test-service.dns-5109.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-5109 jessie_tcp@dns-test-service.dns-5109 jessie_udp@dns-test-service.dns-5109.svc jessie_tcp@dns-test-service.dns-5109.svc jessie_udp@_http._tcp.dns-test-service.dns-5109.svc jessie_tcp@_http._tcp.dns-test-service.dns-5109.svc]

Sep  5 23:41:54.846: INFO: DNS probes using dns-5109/dns-test-828c5a9f-4d85-4eb4-9f4f-6cd781cc6d64 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:41:55.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5109" for this suite.

• [SLOW TEST:49.741 seconds]
[sig-network] DNS
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":296,"completed":161,"skipped":2867,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:41:55.930: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-8897
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:41:56.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8897" for this suite.
•{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":296,"completed":162,"skipped":2893,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:41:56.885: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-7912
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:41:57.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7912" for this suite.
•{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":296,"completed":163,"skipped":2898,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:41:57.760: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5252
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 23:41:59.340: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  5 23:42:01.427: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:42:03.451: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:42:05.445: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:42:07.440: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:42:09.443: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:42:11.449: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:42:13.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:42:15.437: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:42:17.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:42:19.442: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:42:21.440: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:42:23.439: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:42:25.443: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:42:27.436: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 41, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 23:42:30.474: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  5 23:42:30.483: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3520-crds.webhook.example.com via the AdmissionRegistration API
Sep  5 23:42:31.304: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:42:35.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5252" for this suite.
STEP: Destroying namespace "webhook-5252-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:39.341 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":296,"completed":164,"skipped":2902,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:42:37.101: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-227
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:42:49.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-227" for this suite.

• [SLOW TEST:13.114 seconds]
[sig-api-machinery] ResourceQuota
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":296,"completed":165,"skipped":2923,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:42:50.215: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9972
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Sep  5 23:43:23.008: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9972 PodName:var-expansion-b44a5e81-e4a2-4282-bf49-fabdfddc98ae ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  5 23:43:23.008: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: test for file in mounted path
Sep  5 23:43:23.217: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9972 PodName:var-expansion-b44a5e81-e4a2-4282-bf49-fabdfddc98ae ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  5 23:43:23.217: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: updating the annotation value
Sep  5 23:43:23.900: INFO: Successfully updated pod "var-expansion-b44a5e81-e4a2-4282-bf49-fabdfddc98ae"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Sep  5 23:43:23.920: INFO: Deleting pod "var-expansion-b44a5e81-e4a2-4282-bf49-fabdfddc98ae" in namespace "var-expansion-9972"
Sep  5 23:43:23.952: INFO: Wait up to 5m0s for pod "var-expansion-b44a5e81-e4a2-4282-bf49-fabdfddc98ae" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:44:11.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9972" for this suite.

• [SLOW TEST:82.051 seconds]
[k8s.io] Variable Expansion
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":296,"completed":166,"skipped":2937,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:44:12.267: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5037
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-5037
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5037
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5037
Sep  5 23:44:13.060: INFO: Found 0 stateful pods, waiting for 1
Sep  5 23:44:23.084: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 23:44:33.086: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 23:44:43.082: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 23:44:53.090: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep  5 23:44:53.101: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-5037 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 23:44:53.912: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 23:44:53.913: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 23:44:53.913: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  5 23:44:53.922: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  5 23:45:03.954: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 23:45:03.954: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 23:45:04.023: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999419s
Sep  5 23:45:05.040: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.980104382s
Sep  5 23:45:06.055: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.963226953s
Sep  5 23:45:07.065: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.94855758s
Sep  5 23:45:08.077: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.938883527s
Sep  5 23:45:09.092: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.927022255s
Sep  5 23:45:10.106: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.910436289s
Sep  5 23:45:11.122: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.898065181s
Sep  5 23:45:12.137: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.881755193s
Sep  5 23:45:13.149: INFO: Verifying statefulset ss doesn't scale past 1 for another 866.349433ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5037
Sep  5 23:45:14.165: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-5037 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:45:14.426: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  5 23:45:14.426: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  5 23:45:14.426: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  5 23:45:14.439: INFO: Found 1 stateful pods, waiting for 3
Sep  5 23:45:24.463: INFO: Found 2 stateful pods, waiting for 3
Sep  5 23:45:34.490: INFO: Found 2 stateful pods, waiting for 3
Sep  5 23:45:44.464: INFO: Found 2 stateful pods, waiting for 3
Sep  5 23:45:54.467: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:45:54.467: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:45:54.467: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 23:46:04.451: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:46:04.451: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:46:04.451: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 23:46:14.480: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:46:14.480: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:46:14.480: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep  5 23:46:14.497: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-5037 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 23:46:14.883: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 23:46:14.884: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 23:46:14.884: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  5 23:46:14.884: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-5037 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 23:46:15.273: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 23:46:15.273: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 23:46:15.273: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  5 23:46:15.273: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-5037 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 23:46:15.580: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 23:46:15.580: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 23:46:15.580: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  5 23:46:15.580: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 23:46:15.591: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep  5 23:46:25.608: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 23:46:25.608: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 23:46:25.608: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 23:46:25.648: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999474s
Sep  5 23:46:26.753: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981384289s
Sep  5 23:46:27.762: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.876565117s
Sep  5 23:46:28.771: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.86754402s
Sep  5 23:46:29.788: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.857898021s
Sep  5 23:46:30.810: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.840571489s
Sep  5 23:46:31.826: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.818258015s
Sep  5 23:46:32.836: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.802872879s
Sep  5 23:46:33.848: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.793072159s
Sep  5 23:46:34.863: INFO: Verifying statefulset ss doesn't scale past 3 for another 779.894011ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5037
Sep  5 23:46:35.889: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-5037 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:46:36.296: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  5 23:46:36.296: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  5 23:46:36.296: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  5 23:46:36.296: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-5037 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:46:36.535: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  5 23:46:36.535: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  5 23:46:36.535: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  5 23:46:36.535: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-5037 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:46:36.772: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  5 23:46:36.772: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  5 23:46:36.772: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  5 23:46:36.772: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep  5 23:47:26.819: INFO: Deleting all statefulset in ns statefulset-5037
Sep  5 23:47:26.830: INFO: Scaling statefulset ss to 0
Sep  5 23:47:26.863: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 23:47:26.870: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:47:26.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5037" for this suite.

• [SLOW TEST:195.229 seconds]
[sig-apps] StatefulSet
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":296,"completed":167,"skipped":2946,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:47:27.496: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-6180
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 23:47:29.724: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  5 23:47:31.752: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:47:33.769: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:47:35.774: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:47:37.771: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:47:39.768: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:47:41.776: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:47:43.771: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:47:45.762: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:47:47.777: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:47:49.785: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:47:51.765: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:47:53.771: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:47:55.765: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:47:57.765: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 47, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 23:48:00.809: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:48:01.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6180" for this suite.
STEP: Destroying namespace "webhook-6180-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:35.037 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":296,"completed":168,"skipped":2947,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:48:02.533: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8634
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Sep  5 23:48:03.055: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:51:23.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8634" for this suite.

• [SLOW TEST:201.511 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartAlways pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":296,"completed":169,"skipped":2959,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:51:24.045: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9051
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep  5 23:51:24.742: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9051 /api/v1/namespaces/watch-9051/configmaps/e2e-watch-test-resource-version 057a160a-4f49-4513-886e-dced52fba395 165378 0 2021-09-05 23:51:24 -0700 PDT <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-09-05 23:51:24 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  5 23:51:24.742: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9051 /api/v1/namespaces/watch-9051/configmaps/e2e-watch-test-resource-version 057a160a-4f49-4513-886e-dced52fba395 165379 0 2021-09-05 23:51:24 -0700 PDT <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-09-05 23:51:24 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:51:24.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9051" for this suite.
•{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":296,"completed":170,"skipped":2969,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:51:25.090: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-908
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should support --unix-socket=/path  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Starting the proxy
Sep  5 23:51:25.590: INFO: Asynchronously running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-908 proxy --unix-socket=/tmp/kubectl-proxy-unix821587239/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:51:25.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-908" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":296,"completed":171,"skipped":2970,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:51:25.879: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7621
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7621
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Sep  5 23:51:26.458: INFO: Found 0 stateful pods, waiting for 3
Sep  5 23:51:36.488: INFO: Found 1 stateful pods, waiting for 3
Sep  5 23:51:46.476: INFO: Found 1 stateful pods, waiting for 3
Sep  5 23:51:56.471: INFO: Found 1 stateful pods, waiting for 3
Sep  5 23:52:06.482: INFO: Found 2 stateful pods, waiting for 3
Sep  5 23:52:16.490: INFO: Found 2 stateful pods, waiting for 3
Sep  5 23:52:26.488: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:52:26.488: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:52:26.488: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 23:52:36.475: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:52:36.475: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:52:36.475: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 23:52:46.496: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:52:46.496: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:52:46.496: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 23:52:56.488: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:52:56.488: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:52:56.488: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:52:56.510: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-7621 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 23:52:56.989: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 23:52:56.989: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 23:52:56.989: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from mirror.gcr.io/library/httpd:2.4.38-alpine to mirror.gcr.io/library/httpd:2.4.39-alpine
Sep  5 23:53:07.085: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep  5 23:53:17.206: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-7621 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:53:17.681: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  5 23:53:17.681: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  5 23:53:17.681: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  5 23:53:27.756: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:53:27.756: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:53:27.756: INFO: Waiting for Pod statefulset-7621/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:53:27.756: INFO: Waiting for Pod statefulset-7621/ss2-2 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:53:37.783: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:53:37.783: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:53:37.783: INFO: Waiting for Pod statefulset-7621/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:53:37.783: INFO: Waiting for Pod statefulset-7621/ss2-2 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:53:47.802: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:53:47.802: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:53:47.802: INFO: Waiting for Pod statefulset-7621/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:53:57.777: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:53:57.777: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:53:57.777: INFO: Waiting for Pod statefulset-7621/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:54:07.796: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:54:07.796: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:54:07.796: INFO: Waiting for Pod statefulset-7621/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:54:17.788: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:54:17.788: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:54:17.788: INFO: Waiting for Pod statefulset-7621/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:54:27.797: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:54:27.797: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:54:27.797: INFO: Waiting for Pod statefulset-7621/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:54:37.774: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:54:37.774: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:54:37.774: INFO: Waiting for Pod statefulset-7621/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:54:47.774: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:54:47.774: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:54:47.774: INFO: Waiting for Pod statefulset-7621/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:54:57.780: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:54:57.781: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:55:07.779: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:55:07.779: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:55:17.797: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:55:17.797: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:55:27.795: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:55:27.795: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 23:55:37.783: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:55:47.790: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
STEP: Rolling back to a previous revision
Sep  5 23:55:57.788: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-7621 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 23:55:58.788: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 23:55:58.788: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 23:55:58.788: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  5 23:56:08.882: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep  5 23:56:18.950: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=statefulset-7621 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:56:19.544: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  5 23:56:19.544: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  5 23:56:19.544: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  5 23:56:29.643: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:56:29.643: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 23:56:29.643: INFO: Waiting for Pod statefulset-7621/ss2-1 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 23:56:29.643: INFO: Waiting for Pod statefulset-7621/ss2-2 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 23:56:39.672: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:56:39.672: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 23:56:39.672: INFO: Waiting for Pod statefulset-7621/ss2-1 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 23:56:39.672: INFO: Waiting for Pod statefulset-7621/ss2-2 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 23:56:49.695: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:56:49.695: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 23:56:49.695: INFO: Waiting for Pod statefulset-7621/ss2-1 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 23:56:59.702: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:56:59.702: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 23:56:59.702: INFO: Waiting for Pod statefulset-7621/ss2-1 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 23:57:09.686: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:57:09.686: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 23:57:09.686: INFO: Waiting for Pod statefulset-7621/ss2-1 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 23:57:19.666: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:57:19.666: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 23:57:19.666: INFO: Waiting for Pod statefulset-7621/ss2-1 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 23:57:29.678: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:57:29.678: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 23:57:39.668: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:57:39.668: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 23:57:49.683: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:57:49.683: INFO: Waiting for Pod statefulset-7621/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 23:57:59.665: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:58:09.664: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
Sep  5 23:58:19.677: INFO: Waiting for StatefulSet statefulset-7621/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep  5 23:58:29.663: INFO: Deleting all statefulset in ns statefulset-7621
Sep  5 23:58:29.669: INFO: Scaling statefulset ss2 to 0
Sep  5 23:59:09.736: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 23:59:09.751: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:59:09.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7621" for this suite.

• [SLOW TEST:464.392 seconds]
[sig-apps] StatefulSet
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform rolling updates and roll backs of template modifications [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":296,"completed":172,"skipped":2983,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:59:10.271: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5086
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating secret secrets-5086/secret-test-e540415d-d960-407c-b564-47c225124ecd
STEP: Creating a pod to test consume secrets
Sep  5 23:59:10.920: INFO: Waiting up to 5m0s for pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab" in namespace "secrets-5086" to be "Succeeded or Failed"
Sep  5 23:59:10.934: INFO: Pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab": Phase="Pending", Reason="", readiness=false. Elapsed: 13.327068ms
Sep  5 23:59:12.952: INFO: Pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03172175s
Sep  5 23:59:15.003: INFO: Pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.08283344s
Sep  5 23:59:17.020: INFO: Pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.099908534s
Sep  5 23:59:19.031: INFO: Pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab": Phase="Pending", Reason="", readiness=false. Elapsed: 8.110600644s
Sep  5 23:59:21.049: INFO: Pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab": Phase="Pending", Reason="", readiness=false. Elapsed: 10.128332897s
Sep  5 23:59:23.201: INFO: Pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab": Phase="Pending", Reason="", readiness=false. Elapsed: 12.280428176s
Sep  5 23:59:25.216: INFO: Pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab": Phase="Pending", Reason="", readiness=false. Elapsed: 14.295814456s
Sep  5 23:59:27.233: INFO: Pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab": Phase="Pending", Reason="", readiness=false. Elapsed: 16.312348675s
Sep  5 23:59:29.243: INFO: Pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab": Phase="Pending", Reason="", readiness=false. Elapsed: 18.323016655s
Sep  5 23:59:31.262: INFO: Pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab": Phase="Pending", Reason="", readiness=false. Elapsed: 20.341613474s
Sep  5 23:59:33.281: INFO: Pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab": Phase="Pending", Reason="", readiness=false. Elapsed: 22.360921805s
Sep  5 23:59:35.322: INFO: Pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab": Phase="Pending", Reason="", readiness=false. Elapsed: 24.401263614s
Sep  5 23:59:37.333: INFO: Pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab": Phase="Pending", Reason="", readiness=false. Elapsed: 26.412262412s
Sep  5 23:59:39.346: INFO: Pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab": Phase="Pending", Reason="", readiness=false. Elapsed: 28.425831083s
Sep  5 23:59:41.385: INFO: Pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab": Phase="Pending", Reason="", readiness=false. Elapsed: 30.464996382s
Sep  5 23:59:43.399: INFO: Pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab": Phase="Pending", Reason="", readiness=false. Elapsed: 32.479104454s
Sep  5 23:59:45.411: INFO: Pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.490309042s
STEP: Saw pod success
Sep  5 23:59:45.411: INFO: Pod "pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab" satisfied condition "Succeeded or Failed"
Sep  5 23:59:45.421: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab container env-test: <nil>
STEP: delete the pod
Sep  5 23:59:45.505: INFO: Waiting for pod pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab to disappear
Sep  5 23:59:45.520: INFO: Pod pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab still exists
Sep  5 23:59:47.520: INFO: Waiting for pod pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab to disappear
Sep  5 23:59:47.541: INFO: Pod pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab still exists
Sep  5 23:59:49.520: INFO: Waiting for pod pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab to disappear
Sep  5 23:59:49.546: INFO: Pod pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab still exists
Sep  5 23:59:51.520: INFO: Waiting for pod pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab to disappear
Sep  5 23:59:51.532: INFO: Pod pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab still exists
Sep  5 23:59:53.521: INFO: Waiting for pod pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab to disappear
Sep  5 23:59:53.534: INFO: Pod pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab still exists
Sep  5 23:59:55.520: INFO: Waiting for pod pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab to disappear
Sep  5 23:59:55.538: INFO: Pod pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab still exists
Sep  5 23:59:57.520: INFO: Waiting for pod pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab to disappear
Sep  5 23:59:57.547: INFO: Pod pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab still exists
Sep  5 23:59:59.522: INFO: Waiting for pod pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab to disappear
Sep  5 23:59:59.529: INFO: Pod pod-configmaps-4b071f52-08de-4849-8f7e-48ca04f507ab no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:59:59.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5086" for this suite.

• [SLOW TEST:49.812 seconds]
[sig-api-machinery] Secrets
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should be consumable via the environment [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":296,"completed":173,"skipped":3002,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:00:00.084: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8694
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-9c31f4be-123d-425f-bbca-4161d7892ccf in namespace container-probe-8694
Sep  6 00:00:44.975: INFO: Started pod liveness-9c31f4be-123d-425f-bbca-4161d7892ccf in namespace container-probe-8694
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 00:00:44.982: INFO: Initial restart count of pod liveness-9c31f4be-123d-425f-bbca-4161d7892ccf is 0
Sep  6 00:01:01.213: INFO: Restart count of pod container-probe-8694/liveness-9c31f4be-123d-425f-bbca-4161d7892ccf is now 1 (16.231520309s elapsed)
Sep  6 00:01:17.368: INFO: Restart count of pod container-probe-8694/liveness-9c31f4be-123d-425f-bbca-4161d7892ccf is now 2 (32.38610012s elapsed)
Sep  6 00:01:31.472: INFO: Restart count of pod container-probe-8694/liveness-9c31f4be-123d-425f-bbca-4161d7892ccf is now 3 (46.490324186s elapsed)
Sep  6 00:01:47.611: INFO: Restart count of pod container-probe-8694/liveness-9c31f4be-123d-425f-bbca-4161d7892ccf is now 4 (1m2.629284752s elapsed)
Sep  6 00:02:01.803: INFO: Restart count of pod container-probe-8694/liveness-9c31f4be-123d-425f-bbca-4161d7892ccf is now 5 (1m16.820733342s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:02:01.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8694" for this suite.

• [SLOW TEST:122.532 seconds]
[k8s.io] Probing container
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":296,"completed":174,"skipped":3027,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:02:02.616: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6147
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of events
Sep  6 00:02:03.134: INFO: created test-event-1
Sep  6 00:02:03.157: INFO: created test-event-2
Sep  6 00:02:03.177: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Sep  6 00:02:03.191: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Sep  6 00:02:03.380: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:02:03.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6147" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":296,"completed":175,"skipped":3066,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:02:04.000: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4318
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  6 00:03:06.836: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 00:03:06.843: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 00:03:08.844: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 00:03:08.862: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 00:03:10.844: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 00:03:10.854: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 00:03:12.844: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 00:03:12.863: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 00:03:14.844: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 00:03:14.857: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 00:03:16.844: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 00:03:16.859: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 00:03:18.845: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 00:03:18.875: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 00:03:20.844: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 00:03:20.856: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 00:03:22.843: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 00:03:22.863: INFO: Pod pod-with-prestop-http-hook still exists
Sep  6 00:03:24.843: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  6 00:03:24.869: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:03:24.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4318" for this suite.

• [SLOW TEST:81.298 seconds]
[k8s.io] Container Lifecycle Hook
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":296,"completed":176,"skipped":3088,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:03:25.299: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9001
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-f7004fdf-fd65-446c-90a9-a30e1ba1e966
STEP: Creating a pod to test consume secrets
Sep  6 00:03:26.043: INFO: Waiting up to 5m0s for pod "pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985" in namespace "secrets-9001" to be "Succeeded or Failed"
Sep  6 00:03:26.059: INFO: Pod "pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985": Phase="Pending", Reason="", readiness=false. Elapsed: 16.385034ms
Sep  6 00:03:28.070: INFO: Pod "pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027178146s
Sep  6 00:03:30.085: INFO: Pod "pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042580876s
Sep  6 00:03:32.101: INFO: Pod "pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05780234s
Sep  6 00:03:34.121: INFO: Pod "pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985": Phase="Pending", Reason="", readiness=false. Elapsed: 8.077933152s
Sep  6 00:03:36.134: INFO: Pod "pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985": Phase="Pending", Reason="", readiness=false. Elapsed: 10.091594237s
Sep  6 00:03:38.151: INFO: Pod "pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985": Phase="Pending", Reason="", readiness=false. Elapsed: 12.10800834s
Sep  6 00:03:40.162: INFO: Pod "pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985": Phase="Pending", Reason="", readiness=false. Elapsed: 14.119345084s
Sep  6 00:03:42.184: INFO: Pod "pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985": Phase="Pending", Reason="", readiness=false. Elapsed: 16.140962415s
Sep  6 00:03:44.198: INFO: Pod "pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985": Phase="Pending", Reason="", readiness=false. Elapsed: 18.154787016s
Sep  6 00:03:46.210: INFO: Pod "pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985": Phase="Pending", Reason="", readiness=false. Elapsed: 20.16743298s
Sep  6 00:03:48.227: INFO: Pod "pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985": Phase="Pending", Reason="", readiness=false. Elapsed: 22.184588805s
Sep  6 00:03:50.236: INFO: Pod "pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985": Phase="Pending", Reason="", readiness=false. Elapsed: 24.193184728s
Sep  6 00:03:52.249: INFO: Pod "pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985": Phase="Running", Reason="", readiness=true. Elapsed: 26.205644691s
Sep  6 00:03:54.265: INFO: Pod "pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985": Phase="Running", Reason="", readiness=true. Elapsed: 28.222200167s
Sep  6 00:03:56.287: INFO: Pod "pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985": Phase="Running", Reason="", readiness=true. Elapsed: 30.244444648s
Sep  6 00:03:58.302: INFO: Pod "pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.258816679s
STEP: Saw pod success
Sep  6 00:03:58.302: INFO: Pod "pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985" satisfied condition "Succeeded or Failed"
Sep  6 00:03:58.310: INFO: Trying to get logs from node sc2-10-185-226-150.eng.vmware.com pod pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 00:04:05.167: INFO: Waiting for pod pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985 to disappear
Sep  6 00:04:05.179: INFO: Pod pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985 still exists
Sep  6 00:04:07.179: INFO: Waiting for pod pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985 to disappear
Sep  6 00:04:07.194: INFO: Pod pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985 still exists
Sep  6 00:04:09.179: INFO: Waiting for pod pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985 to disappear
Sep  6 00:04:09.194: INFO: Pod pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985 still exists
Sep  6 00:04:11.179: INFO: Waiting for pod pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985 to disappear
Sep  6 00:04:11.194: INFO: Pod pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985 still exists
Sep  6 00:04:13.179: INFO: Waiting for pod pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985 to disappear
Sep  6 00:04:13.190: INFO: Pod pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985 still exists
Sep  6 00:04:15.180: INFO: Waiting for pod pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985 to disappear
Sep  6 00:04:15.193: INFO: Pod pod-secrets-bf060837-103c-4b83-b41a-7bd312a8f985 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:04:15.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9001" for this suite.

• [SLOW TEST:50.148 seconds]
[sig-storage] Secrets
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":177,"skipped":3110,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:04:15.447: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-955
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-955.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-955.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-955.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-955.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-955.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-955.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-955.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-955.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-955.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-955.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-955.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-955.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-955.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 4.176.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.176.4_udp@PTR;check="$$(dig +tcp +noall +answer +search 4.176.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.176.4_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-955.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-955.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-955.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-955.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-955.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-955.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-955.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-955.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-955.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-955.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-955.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-955.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-955.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 4.176.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.176.4_udp@PTR;check="$$(dig +tcp +noall +answer +search 4.176.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.176.4_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 00:04:52.142: INFO: Unable to read wheezy_udp@dns-test-service.dns-955.svc.cluster.local from pod dns-955/dns-test-4a43c1bc-f654-4d0c-97a0-8724632712a5: the server could not find the requested resource (get pods dns-test-4a43c1bc-f654-4d0c-97a0-8724632712a5)
Sep  6 00:04:52.160: INFO: Unable to read wheezy_tcp@dns-test-service.dns-955.svc.cluster.local from pod dns-955/dns-test-4a43c1bc-f654-4d0c-97a0-8724632712a5: the server could not find the requested resource (get pods dns-test-4a43c1bc-f654-4d0c-97a0-8724632712a5)
Sep  6 00:04:52.173: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-955.svc.cluster.local from pod dns-955/dns-test-4a43c1bc-f654-4d0c-97a0-8724632712a5: the server could not find the requested resource (get pods dns-test-4a43c1bc-f654-4d0c-97a0-8724632712a5)
Sep  6 00:04:52.186: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-955.svc.cluster.local from pod dns-955/dns-test-4a43c1bc-f654-4d0c-97a0-8724632712a5: the server could not find the requested resource (get pods dns-test-4a43c1bc-f654-4d0c-97a0-8724632712a5)
Sep  6 00:04:52.300: INFO: Unable to read jessie_udp@dns-test-service.dns-955.svc.cluster.local from pod dns-955/dns-test-4a43c1bc-f654-4d0c-97a0-8724632712a5: the server could not find the requested resource (get pods dns-test-4a43c1bc-f654-4d0c-97a0-8724632712a5)
Sep  6 00:04:52.310: INFO: Unable to read jessie_tcp@dns-test-service.dns-955.svc.cluster.local from pod dns-955/dns-test-4a43c1bc-f654-4d0c-97a0-8724632712a5: the server could not find the requested resource (get pods dns-test-4a43c1bc-f654-4d0c-97a0-8724632712a5)
Sep  6 00:04:52.328: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-955.svc.cluster.local from pod dns-955/dns-test-4a43c1bc-f654-4d0c-97a0-8724632712a5: the server could not find the requested resource (get pods dns-test-4a43c1bc-f654-4d0c-97a0-8724632712a5)
Sep  6 00:04:52.344: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-955.svc.cluster.local from pod dns-955/dns-test-4a43c1bc-f654-4d0c-97a0-8724632712a5: the server could not find the requested resource (get pods dns-test-4a43c1bc-f654-4d0c-97a0-8724632712a5)
Sep  6 00:04:52.440: INFO: Lookups using dns-955/dns-test-4a43c1bc-f654-4d0c-97a0-8724632712a5 failed for: [wheezy_udp@dns-test-service.dns-955.svc.cluster.local wheezy_tcp@dns-test-service.dns-955.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-955.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-955.svc.cluster.local jessie_udp@dns-test-service.dns-955.svc.cluster.local jessie_tcp@dns-test-service.dns-955.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-955.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-955.svc.cluster.local]

Sep  6 00:04:57.692: INFO: DNS probes using dns-955/dns-test-4a43c1bc-f654-4d0c-97a0-8724632712a5 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:04:58.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-955" for this suite.

• [SLOW TEST:42.956 seconds]
[sig-network] DNS
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":296,"completed":178,"skipped":3129,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:04:58.403: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3363
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-496ef428-4783-4ad8-a99f-f6fa6de907bd
STEP: Creating a pod to test consume configMaps
Sep  6 00:04:58.991: INFO: Waiting up to 5m0s for pod "pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5" in namespace "configmap-3363" to be "Succeeded or Failed"
Sep  6 00:04:59.009: INFO: Pod "pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.226529ms
Sep  6 00:05:01.029: INFO: Pod "pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038626781s
Sep  6 00:05:03.044: INFO: Pod "pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053467599s
Sep  6 00:05:05.073: INFO: Pod "pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.082495096s
Sep  6 00:05:07.087: INFO: Pod "pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.096821948s
Sep  6 00:05:09.100: INFO: Pod "pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.109241676s
Sep  6 00:05:11.112: INFO: Pod "pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.121168626s
Sep  6 00:05:13.144: INFO: Pod "pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.153707866s
Sep  6 00:05:15.177: INFO: Pod "pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.186750694s
Sep  6 00:05:17.195: INFO: Pod "pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.204486678s
Sep  6 00:05:19.213: INFO: Pod "pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.222010365s
Sep  6 00:05:21.233: INFO: Pod "pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.241870601s
Sep  6 00:05:23.245: INFO: Pod "pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.254849059s
Sep  6 00:05:25.259: INFO: Pod "pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.267867636s
Sep  6 00:05:27.274: INFO: Pod "pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.283306303s
Sep  6 00:05:29.289: INFO: Pod "pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.298671737s
STEP: Saw pod success
Sep  6 00:05:29.289: INFO: Pod "pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5" satisfied condition "Succeeded or Failed"
Sep  6 00:05:29.296: INFO: Trying to get logs from node sc2-10-185-226-150.eng.vmware.com pod pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5 container agnhost-container: <nil>
STEP: delete the pod
Sep  6 00:05:29.351: INFO: Waiting for pod pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5 to disappear
Sep  6 00:05:29.370: INFO: Pod pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5 still exists
Sep  6 00:05:31.372: INFO: Waiting for pod pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5 to disappear
Sep  6 00:05:31.382: INFO: Pod pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5 still exists
Sep  6 00:05:33.372: INFO: Waiting for pod pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5 to disappear
Sep  6 00:05:33.383: INFO: Pod pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5 still exists
Sep  6 00:05:35.371: INFO: Waiting for pod pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5 to disappear
Sep  6 00:05:35.382: INFO: Pod pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5 still exists
Sep  6 00:05:37.372: INFO: Waiting for pod pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5 to disappear
Sep  6 00:05:37.382: INFO: Pod pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5 still exists
Sep  6 00:05:39.371: INFO: Waiting for pod pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5 to disappear
Sep  6 00:05:39.382: INFO: Pod pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5 still exists
Sep  6 00:05:41.372: INFO: Waiting for pod pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5 to disappear
Sep  6 00:05:41.381: INFO: Pod pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5 still exists
Sep  6 00:05:43.372: INFO: Waiting for pod pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5 to disappear
Sep  6 00:05:43.387: INFO: Pod pod-configmaps-f335d78d-6ad6-47a2-be9e-8d179b153bb5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:05:43.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3363" for this suite.

• [SLOW TEST:45.293 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":296,"completed":179,"skipped":3138,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:05:43.697: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6106
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl can dry-run update Pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: running the image mirror.gcr.io/library/httpd:2.4.38-alpine
Sep  6 00:05:44.359: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-6106 run e2e-test-httpd-pod --image=mirror.gcr.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod'
Sep  6 00:05:44.531: INFO: stderr: ""
Sep  6 00:05:44.531: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Sep  6 00:05:44.531: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-6106 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "mirror.gcr.io/library/busybox:1.29"}]}} --dry-run=server'
Sep  6 00:05:45.315: INFO: stderr: ""
Sep  6 00:05:45.315: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image mirror.gcr.io/library/httpd:2.4.38-alpine
Sep  6 00:05:45.325: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-6106 delete pods e2e-test-httpd-pod'
Sep  6 00:05:45.499: INFO: stderr: ""
Sep  6 00:05:45.499: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:05:45.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6106" for this suite.
•{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":296,"completed":180,"skipped":3138,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:05:45.718: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3640
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  6 00:05:46.167: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:06:12.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3640" for this suite.

• [SLOW TEST:27.003 seconds]
[k8s.io] Pods
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":296,"completed":181,"skipped":3154,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:06:12.723: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9769
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  6 00:06:13.331: INFO: Waiting up to 5m0s for pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6" in namespace "emptydir-9769" to be "Succeeded or Failed"
Sep  6 00:06:13.341: INFO: Pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.595065ms
Sep  6 00:06:15.352: INFO: Pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02051086s
Sep  6 00:06:17.370: INFO: Pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038360718s
Sep  6 00:06:19.383: INFO: Pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052149917s
Sep  6 00:06:21.396: INFO: Pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.064932291s
Sep  6 00:06:23.408: INFO: Pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.076337985s
Sep  6 00:06:25.430: INFO: Pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.098986005s
Sep  6 00:06:27.448: INFO: Pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.116483372s
Sep  6 00:06:29.463: INFO: Pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6": Phase="Pending", Reason="", readiness=false. Elapsed: 16.13183073s
Sep  6 00:06:31.481: INFO: Pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.150153364s
Sep  6 00:06:33.497: INFO: Pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6": Phase="Pending", Reason="", readiness=false. Elapsed: 20.165257701s
Sep  6 00:06:35.523: INFO: Pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6": Phase="Pending", Reason="", readiness=false. Elapsed: 22.191977707s
Sep  6 00:06:37.536: INFO: Pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6": Phase="Pending", Reason="", readiness=false. Elapsed: 24.204623551s
Sep  6 00:06:39.570: INFO: Pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6": Phase="Pending", Reason="", readiness=false. Elapsed: 26.238229586s
Sep  6 00:06:41.582: INFO: Pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6": Phase="Pending", Reason="", readiness=false. Elapsed: 28.250261288s
Sep  6 00:06:43.618: INFO: Pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6": Phase="Running", Reason="", readiness=true. Elapsed: 30.286703414s
Sep  6 00:06:45.636: INFO: Pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6": Phase="Running", Reason="", readiness=true. Elapsed: 32.3049069s
Sep  6 00:06:47.649: INFO: Pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.317868888s
STEP: Saw pod success
Sep  6 00:06:47.649: INFO: Pod "pod-2b1a8b4b-056d-4256-978c-4558327aa9a6" satisfied condition "Succeeded or Failed"
Sep  6 00:06:47.658: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-2b1a8b4b-056d-4256-978c-4558327aa9a6 container test-container: <nil>
STEP: delete the pod
Sep  6 00:06:47.778: INFO: Waiting for pod pod-2b1a8b4b-056d-4256-978c-4558327aa9a6 to disappear
Sep  6 00:06:47.793: INFO: Pod pod-2b1a8b4b-056d-4256-978c-4558327aa9a6 still exists
Sep  6 00:06:49.793: INFO: Waiting for pod pod-2b1a8b4b-056d-4256-978c-4558327aa9a6 to disappear
Sep  6 00:06:49.812: INFO: Pod pod-2b1a8b4b-056d-4256-978c-4558327aa9a6 still exists
Sep  6 00:06:51.793: INFO: Waiting for pod pod-2b1a8b4b-056d-4256-978c-4558327aa9a6 to disappear
Sep  6 00:06:51.813: INFO: Pod pod-2b1a8b4b-056d-4256-978c-4558327aa9a6 still exists
Sep  6 00:06:53.793: INFO: Waiting for pod pod-2b1a8b4b-056d-4256-978c-4558327aa9a6 to disappear
Sep  6 00:06:53.811: INFO: Pod pod-2b1a8b4b-056d-4256-978c-4558327aa9a6 still exists
Sep  6 00:06:55.794: INFO: Waiting for pod pod-2b1a8b4b-056d-4256-978c-4558327aa9a6 to disappear
Sep  6 00:06:55.810: INFO: Pod pod-2b1a8b4b-056d-4256-978c-4558327aa9a6 still exists
Sep  6 00:06:57.793: INFO: Waiting for pod pod-2b1a8b4b-056d-4256-978c-4558327aa9a6 to disappear
Sep  6 00:06:57.806: INFO: Pod pod-2b1a8b4b-056d-4256-978c-4558327aa9a6 still exists
Sep  6 00:06:59.793: INFO: Waiting for pod pod-2b1a8b4b-056d-4256-978c-4558327aa9a6 to disappear
Sep  6 00:06:59.804: INFO: Pod pod-2b1a8b4b-056d-4256-978c-4558327aa9a6 still exists
Sep  6 00:07:01.793: INFO: Waiting for pod pod-2b1a8b4b-056d-4256-978c-4558327aa9a6 to disappear
Sep  6 00:07:01.813: INFO: Pod pod-2b1a8b4b-056d-4256-978c-4558327aa9a6 still exists
Sep  6 00:07:03.793: INFO: Waiting for pod pod-2b1a8b4b-056d-4256-978c-4558327aa9a6 to disappear
Sep  6 00:07:03.875: INFO: Pod pod-2b1a8b4b-056d-4256-978c-4558327aa9a6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:07:03.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9769" for this suite.

• [SLOW TEST:51.461 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":182,"skipped":3209,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:07:04.183: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7134
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should create services for rc  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating Agnhost RC
Sep  6 00:07:04.746: INFO: namespace kubectl-7134
Sep  6 00:07:04.746: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-7134 create -f -'
Sep  6 00:07:05.677: INFO: stderr: ""
Sep  6 00:07:05.677: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep  6 00:07:06.692: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:06.692: INFO: Found 0 / 1
Sep  6 00:07:07.700: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:07.700: INFO: Found 0 / 1
Sep  6 00:07:08.686: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:08.686: INFO: Found 0 / 1
Sep  6 00:07:09.707: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:09.707: INFO: Found 0 / 1
Sep  6 00:07:10.686: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:10.686: INFO: Found 0 / 1
Sep  6 00:07:11.691: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:11.691: INFO: Found 0 / 1
Sep  6 00:07:12.707: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:12.707: INFO: Found 0 / 1
Sep  6 00:07:13.701: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:13.701: INFO: Found 0 / 1
Sep  6 00:07:14.716: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:14.716: INFO: Found 0 / 1
Sep  6 00:07:15.689: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:15.689: INFO: Found 0 / 1
Sep  6 00:07:16.701: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:16.701: INFO: Found 0 / 1
Sep  6 00:07:17.685: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:17.685: INFO: Found 0 / 1
Sep  6 00:07:18.688: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:18.688: INFO: Found 0 / 1
Sep  6 00:07:19.686: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:19.686: INFO: Found 0 / 1
Sep  6 00:07:20.687: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:20.687: INFO: Found 0 / 1
Sep  6 00:07:21.692: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:21.692: INFO: Found 0 / 1
Sep  6 00:07:22.703: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:22.703: INFO: Found 0 / 1
Sep  6 00:07:23.687: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:23.687: INFO: Found 0 / 1
Sep  6 00:07:24.688: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:24.688: INFO: Found 0 / 1
Sep  6 00:07:25.692: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:25.692: INFO: Found 0 / 1
Sep  6 00:07:26.687: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:26.687: INFO: Found 0 / 1
Sep  6 00:07:27.687: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:27.687: INFO: Found 0 / 1
Sep  6 00:07:28.691: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:28.691: INFO: Found 0 / 1
Sep  6 00:07:29.699: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:29.699: INFO: Found 0 / 1
Sep  6 00:07:30.685: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:30.685: INFO: Found 0 / 1
Sep  6 00:07:31.694: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:31.694: INFO: Found 0 / 1
Sep  6 00:07:32.696: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:32.696: INFO: Found 1 / 1
Sep  6 00:07:32.696: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  6 00:07:32.718: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:07:32.718: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 00:07:32.718: INFO: wait on agnhost-primary startup in kubectl-7134 
Sep  6 00:07:32.718: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-7134 logs agnhost-primary-t8z2s agnhost-primary'
Sep  6 00:07:32.885: INFO: stderr: ""
Sep  6 00:07:32.885: INFO: stdout: "Paused\n"
STEP: exposing RC
Sep  6 00:07:32.885: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-7134 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Sep  6 00:07:33.114: INFO: stderr: ""
Sep  6 00:07:33.114: INFO: stdout: "service/rm2 exposed\n"
Sep  6 00:07:33.148: INFO: Service rm2 in namespace kubectl-7134 found.
STEP: exposing service
Sep  6 00:07:35.217: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-7134 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Sep  6 00:07:35.397: INFO: stderr: ""
Sep  6 00:07:35.397: INFO: stdout: "service/rm3 exposed\n"
Sep  6 00:07:35.442: INFO: Service rm3 in namespace kubectl-7134 found.
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:07:37.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7134" for this suite.

• [SLOW TEST:33.517 seconds]
[sig-cli] Kubectl client
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1229
    should create services for rc  [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":296,"completed":183,"skipped":3211,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:07:37.701: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8429
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  6 00:07:38.224: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 00:07:40.243: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 00:07:42.259: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 00:07:44.255: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 00:07:46.255: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 00:07:48.241: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 00:07:50.238: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 00:07:52.248: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 00:07:54.244: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 00:07:56.249: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 00:07:58.242: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 00:08:00.239: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 00:08:02.241: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 00:08:04.298: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 00:08:06.244: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Pending, waiting for it to be Running (with Ready = true)
Sep  6 00:08:08.247: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Running (Ready = false)
Sep  6 00:08:10.245: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Running (Ready = false)
Sep  6 00:08:12.235: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Running (Ready = false)
Sep  6 00:08:14.239: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Running (Ready = false)
Sep  6 00:08:16.236: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Running (Ready = false)
Sep  6 00:08:18.246: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Running (Ready = false)
Sep  6 00:08:20.239: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Running (Ready = false)
Sep  6 00:08:22.239: INFO: The status of Pod test-webserver-7165bf3a-10dc-441b-a72a-4208088fad60 is Running (Ready = true)
Sep  6 00:08:22.247: INFO: Container started at 2021-09-06 00:08:03 -0700 PDT, pod became ready at 2021-09-06 00:08:22 -0700 PDT
[AfterEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:08:22.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8429" for this suite.

• [SLOW TEST:45.079 seconds]
[k8s.io] Probing container
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":296,"completed":184,"skipped":3217,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:08:22.780: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3653
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-7wp2
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 00:08:23.309: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-7wp2" in namespace "subpath-3653" to be "Succeeded or Failed"
Sep  6 00:08:23.329: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.020715ms
Sep  6 00:08:25.349: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03928714s
Sep  6 00:08:27.365: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055735292s
Sep  6 00:08:29.383: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073213527s
Sep  6 00:08:31.404: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094216209s
Sep  6 00:08:33.424: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.114080912s
Sep  6 00:08:35.444: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.134212321s
Sep  6 00:08:37.458: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.14906155s
Sep  6 00:08:39.477: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.167543032s
Sep  6 00:08:41.492: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.182191155s
Sep  6 00:08:43.531: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.221977905s
Sep  6 00:08:45.542: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.232431822s
Sep  6 00:08:47.555: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.245347338s
Sep  6 00:08:49.608: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Pending", Reason="", readiness=false. Elapsed: 26.298676563s
Sep  6 00:08:51.680: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Pending", Reason="", readiness=false. Elapsed: 28.370776317s
Sep  6 00:08:53.697: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Running", Reason="", readiness=true. Elapsed: 30.387344361s
Sep  6 00:08:55.714: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Running", Reason="", readiness=true. Elapsed: 32.404599464s
Sep  6 00:08:57.731: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Running", Reason="", readiness=true. Elapsed: 34.42164416s
Sep  6 00:08:59.744: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Running", Reason="", readiness=true. Elapsed: 36.434283281s
Sep  6 00:09:01.754: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Running", Reason="", readiness=true. Elapsed: 38.444660884s
Sep  6 00:09:03.767: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Running", Reason="", readiness=true. Elapsed: 40.457994888s
Sep  6 00:09:05.780: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Running", Reason="", readiness=true. Elapsed: 42.470391674s
Sep  6 00:09:07.792: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Running", Reason="", readiness=true. Elapsed: 44.482844306s
Sep  6 00:09:09.815: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Running", Reason="", readiness=true. Elapsed: 46.505101369s
Sep  6 00:09:11.823: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Running", Reason="", readiness=true. Elapsed: 48.513516078s
Sep  6 00:09:13.836: INFO: Pod "pod-subpath-test-configmap-7wp2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 50.52701401s
STEP: Saw pod success
Sep  6 00:09:13.836: INFO: Pod "pod-subpath-test-configmap-7wp2" satisfied condition "Succeeded or Failed"
Sep  6 00:09:13.843: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-subpath-test-configmap-7wp2 container test-container-subpath-configmap-7wp2: <nil>
STEP: delete the pod
Sep  6 00:09:21.484: INFO: Waiting for pod pod-subpath-test-configmap-7wp2 to disappear
Sep  6 00:09:21.506: INFO: Pod pod-subpath-test-configmap-7wp2 still exists
Sep  6 00:09:23.508: INFO: Waiting for pod pod-subpath-test-configmap-7wp2 to disappear
Sep  6 00:09:23.536: INFO: Pod pod-subpath-test-configmap-7wp2 still exists
Sep  6 00:09:25.507: INFO: Waiting for pod pod-subpath-test-configmap-7wp2 to disappear
Sep  6 00:09:25.520: INFO: Pod pod-subpath-test-configmap-7wp2 still exists
Sep  6 00:09:27.507: INFO: Waiting for pod pod-subpath-test-configmap-7wp2 to disappear
Sep  6 00:09:27.524: INFO: Pod pod-subpath-test-configmap-7wp2 still exists
Sep  6 00:09:29.508: INFO: Waiting for pod pod-subpath-test-configmap-7wp2 to disappear
Sep  6 00:09:29.528: INFO: Pod pod-subpath-test-configmap-7wp2 still exists
Sep  6 00:09:31.507: INFO: Waiting for pod pod-subpath-test-configmap-7wp2 to disappear
Sep  6 00:09:31.520: INFO: Pod pod-subpath-test-configmap-7wp2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-7wp2
Sep  6 00:09:31.520: INFO: Deleting pod "pod-subpath-test-configmap-7wp2" in namespace "subpath-3653"
[AfterEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:09:31.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3653" for this suite.

• [SLOW TEST:69.304 seconds]
[sig-storage] Subpath
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":296,"completed":185,"skipped":3241,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:09:32.084: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6467
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-ef3e0d0c-92cd-4958-bc01-a05e36cded52
STEP: Creating a pod to test consume configMaps
Sep  6 00:09:32.601: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12" in namespace "projected-6467" to be "Succeeded or Failed"
Sep  6 00:09:32.613: INFO: Pod "pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12": Phase="Pending", Reason="", readiness=false. Elapsed: 11.817167ms
Sep  6 00:09:34.633: INFO: Pod "pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032206161s
Sep  6 00:09:36.648: INFO: Pod "pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047307293s
Sep  6 00:09:38.663: INFO: Pod "pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061778896s
Sep  6 00:09:40.689: INFO: Pod "pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12": Phase="Pending", Reason="", readiness=false. Elapsed: 8.088189409s
Sep  6 00:09:42.697: INFO: Pod "pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12": Phase="Pending", Reason="", readiness=false. Elapsed: 10.095736512s
Sep  6 00:09:44.710: INFO: Pod "pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12": Phase="Pending", Reason="", readiness=false. Elapsed: 12.108517846s
Sep  6 00:09:46.741: INFO: Pod "pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12": Phase="Pending", Reason="", readiness=false. Elapsed: 14.140111724s
Sep  6 00:09:48.756: INFO: Pod "pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12": Phase="Pending", Reason="", readiness=false. Elapsed: 16.155361563s
Sep  6 00:09:50.809: INFO: Pod "pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12": Phase="Pending", Reason="", readiness=false. Elapsed: 18.208428196s
Sep  6 00:09:52.831: INFO: Pod "pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12": Phase="Pending", Reason="", readiness=false. Elapsed: 20.229596756s
Sep  6 00:09:54.847: INFO: Pod "pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12": Phase="Pending", Reason="", readiness=false. Elapsed: 22.245727594s
Sep  6 00:09:56.897: INFO: Pod "pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12": Phase="Pending", Reason="", readiness=false. Elapsed: 24.296080074s
Sep  6 00:09:58.911: INFO: Pod "pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12": Phase="Pending", Reason="", readiness=false. Elapsed: 26.309575764s
Sep  6 00:10:00.932: INFO: Pod "pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12": Phase="Pending", Reason="", readiness=false. Elapsed: 28.33088677s
Sep  6 00:10:02.964: INFO: Pod "pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.363213996s
STEP: Saw pod success
Sep  6 00:10:02.964: INFO: Pod "pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12" satisfied condition "Succeeded or Failed"
Sep  6 00:10:03.008: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 container agnhost-container: <nil>
STEP: delete the pod
Sep  6 00:10:03.077: INFO: Waiting for pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 to disappear
Sep  6 00:10:03.098: INFO: Pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 still exists
Sep  6 00:10:05.099: INFO: Waiting for pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 to disappear
Sep  6 00:10:05.114: INFO: Pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 still exists
Sep  6 00:10:07.100: INFO: Waiting for pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 to disappear
Sep  6 00:10:07.120: INFO: Pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 still exists
Sep  6 00:10:09.099: INFO: Waiting for pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 to disappear
Sep  6 00:10:09.189: INFO: Pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 still exists
Sep  6 00:10:11.099: INFO: Waiting for pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 to disappear
Sep  6 00:10:11.150: INFO: Pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 still exists
Sep  6 00:10:13.099: INFO: Waiting for pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 to disappear
Sep  6 00:10:13.127: INFO: Pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 still exists
Sep  6 00:10:15.099: INFO: Waiting for pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 to disappear
Sep  6 00:10:15.116: INFO: Pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 still exists
Sep  6 00:10:17.099: INFO: Waiting for pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 to disappear
Sep  6 00:10:17.111: INFO: Pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 still exists
Sep  6 00:10:19.100: INFO: Waiting for pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 to disappear
Sep  6 00:10:19.124: INFO: Pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 still exists
Sep  6 00:10:21.098: INFO: Waiting for pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 to disappear
Sep  6 00:10:21.110: INFO: Pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 still exists
Sep  6 00:10:23.099: INFO: Waiting for pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 to disappear
Sep  6 00:10:23.112: INFO: Pod pod-projected-configmaps-0f7c6888-b7e7-4d2b-bf8d-452e7131fd12 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:10:23.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6467" for this suite.

• [SLOW TEST:51.273 seconds]
[sig-storage] Projected configMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":296,"completed":186,"skipped":3241,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:10:23.358: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6486
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with secret that has name secret-emptykey-test-c72285ba-ab2f-4651-bd96-9763df8ad2ad
[AfterEach] [sig-api-machinery] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:10:23.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6486" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":296,"completed":187,"skipped":3253,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:10:24.635: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9895
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Sep  6 00:11:37.418: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:11:37.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9895" for this suite.

• [SLOW TEST:73.028 seconds]
[sig-api-machinery] Garbage collector
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":296,"completed":188,"skipped":3263,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:11:37.663: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3234
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  6 00:11:38.106: INFO: Waiting up to 5m0s for pod "pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e" in namespace "emptydir-3234" to be "Succeeded or Failed"
Sep  6 00:11:38.126: INFO: Pod "pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 19.306054ms
Sep  6 00:11:40.139: INFO: Pod "pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032688063s
Sep  6 00:11:42.154: INFO: Pod "pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048038321s
Sep  6 00:11:44.174: INFO: Pod "pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.067092696s
Sep  6 00:11:46.186: INFO: Pod "pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.079920858s
Sep  6 00:11:48.196: INFO: Pod "pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.089510895s
Sep  6 00:11:50.209: INFO: Pod "pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.102558709s
Sep  6 00:11:52.218: INFO: Pod "pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.111636147s
Sep  6 00:11:54.237: INFO: Pod "pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.130618423s
Sep  6 00:11:56.255: INFO: Pod "pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.148814698s
Sep  6 00:11:58.268: INFO: Pod "pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.161516077s
Sep  6 00:12:00.286: INFO: Pod "pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.179742501s
Sep  6 00:12:02.297: INFO: Pod "pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 24.19089855s
Sep  6 00:12:04.311: INFO: Pod "pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 26.204935722s
Sep  6 00:12:06.334: INFO: Pod "pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e": Phase="Pending", Reason="", readiness=false. Elapsed: 28.227584267s
Sep  6 00:12:08.358: INFO: Pod "pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.25134746s
STEP: Saw pod success
Sep  6 00:12:08.358: INFO: Pod "pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e" satisfied condition "Succeeded or Failed"
Sep  6 00:12:08.364: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e container test-container: <nil>
STEP: delete the pod
Sep  6 00:12:08.445: INFO: Waiting for pod pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e to disappear
Sep  6 00:12:08.461: INFO: Pod pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e still exists
Sep  6 00:12:10.462: INFO: Waiting for pod pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e to disappear
Sep  6 00:12:10.474: INFO: Pod pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e still exists
Sep  6 00:12:12.462: INFO: Waiting for pod pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e to disappear
Sep  6 00:12:12.470: INFO: Pod pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e still exists
Sep  6 00:12:14.464: INFO: Waiting for pod pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e to disappear
Sep  6 00:12:14.482: INFO: Pod pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e still exists
Sep  6 00:12:16.463: INFO: Waiting for pod pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e to disappear
Sep  6 00:12:16.474: INFO: Pod pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e still exists
Sep  6 00:12:18.462: INFO: Waiting for pod pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e to disappear
Sep  6 00:12:18.473: INFO: Pod pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e still exists
Sep  6 00:12:20.462: INFO: Waiting for pod pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e to disappear
Sep  6 00:12:20.481: INFO: Pod pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e still exists
Sep  6 00:12:22.462: INFO: Waiting for pod pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e to disappear
Sep  6 00:12:22.471: INFO: Pod pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e still exists
Sep  6 00:12:24.462: INFO: Waiting for pod pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e to disappear
Sep  6 00:12:24.480: INFO: Pod pod-3a846d5a-cb96-4212-815f-fbd0e0251a8e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:12:24.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3234" for this suite.

• [SLOW TEST:47.110 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":189,"skipped":3269,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] PodTemplates
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:12:24.773: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-6781
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Create set of pod templates
Sep  6 00:12:25.517: INFO: created test-podtemplate-1
Sep  6 00:12:25.536: INFO: created test-podtemplate-2
Sep  6 00:12:25.550: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Sep  6 00:12:25.569: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Sep  6 00:12:25.640: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:12:25.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6781" for this suite.
•{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":296,"completed":190,"skipped":3281,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] LimitRange
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:12:25.869: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-469
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Sep  6 00:12:26.416: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Sep  6 00:12:26.462: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  6 00:12:26.462: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Sep  6 00:12:26.550: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  6 00:12:26.551: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Sep  6 00:12:26.615: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep  6 00:12:26.615: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Sep  6 00:12:33.854: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:12:33.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-469" for this suite.

• [SLOW TEST:8.294 seconds]
[sig-scheduling] LimitRange
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":296,"completed":191,"skipped":3299,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:12:34.163: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3414
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Sep  6 00:12:34.606: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 00:12:34.638: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 00:12:34.686: INFO: 
Logging pods the apiserver thinks is on node sc2-10-185-226-150.eng.vmware.com before test
Sep  6 00:12:34.806: INFO: podwithpersistentvolume from storage-policy-test started at 2021-09-05 20:29:04 -0700 PDT (1 container statuses recorded)
Sep  6 00:12:34.806: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:12:34.806: INFO: hello-web-6b97664bd5-zznrr from test-cluster-ip-service started at 2021-09-05 22:45:00 -0700 PDT (1 container statuses recorded)
Sep  6 00:12:34.806: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 00:12:34.806: INFO: wcp-sanity-busybox-6f999d6849-kspp7 from test-dataprovider-podvms-ns started at 2021-09-05 20:17:25 -0700 PDT (1 container statuses recorded)
Sep  6 00:12:34.806: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:12:34.806: INFO: curl-pod from test-network-policy started at 2021-09-05 20:38:23 -0700 PDT (1 container statuses recorded)
Sep  6 00:12:34.806: INFO: 	Container curl-container ready: true, restart count 0
Sep  6 00:12:34.806: INFO: schedext-test-node-selector-1 from test-node-selector started at 2021-09-05 20:21:19 -0700 PDT (1 container statuses recorded)
Sep  6 00:12:34.806: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:12:34.806: INFO: schedext-test-affinity-1 from test-pod-affinity started at 2021-09-05 20:22:12 -0700 PDT (1 container statuses recorded)
Sep  6 00:12:34.806: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:12:34.806: INFO: schedext-test-affinity-2 from test-pod-affinity started at 2021-09-05 20:22:42 -0700 PDT (1 container statuses recorded)
Sep  6 00:12:34.806: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:12:34.806: INFO: test-docker-registry from test-private-image-registry-ns started at 2021-09-05 20:41:08 -0700 PDT (1 container statuses recorded)
Sep  6 00:12:34.806: INFO: 	Container test-docker-registry ready: true, restart count 0
Sep  6 00:12:34.806: INFO: helloworld from test-telemetry started at 2021-09-05 20:32:42 -0700 PDT (1 container statuses recorded)
Sep  6 00:12:34.806: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:12:34.806: INFO: wcp-sanity-busybox-6f999d6849-6bn7v from test-update-workload-ns started at 2021-09-05 20:30:32 -0700 PDT (1 container statuses recorded)
Sep  6 00:12:34.806: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:12:34.806: INFO: 
Logging pods the apiserver thinks is on node sc2-10-185-226-233.eng.vmware.com before test
Sep  6 00:12:34.893: INFO: pfpod from limitrange-469 started at <nil> (0 container statuses recorded)
Sep  6 00:12:34.893: INFO: pfpod2 from limitrange-469 started at <nil> (0 container statuses recorded)
Sep  6 00:12:34.893: INFO: pod-no-resources from limitrange-469 started at <nil> (0 container statuses recorded)
Sep  6 00:12:34.893: INFO: pod-partial-resources from limitrange-469 started at <nil> (0 container statuses recorded)
Sep  6 00:12:34.893: INFO: wcp-sanity-busybox-6f999d6849-tx88v from test-dataprovider-podvms-ns started at 2021-09-05 22:46:15 -0700 PDT (1 container statuses recorded)
Sep  6 00:12:34.893: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:12:34.893: INFO: wcp-sanity-busybox-6f999d6849-428rc from test-update-workload-ns started at 2021-09-05 22:46:12 -0700 PDT (1 container statuses recorded)
Sep  6 00:12:34.893: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:12:34.893: INFO: 
Logging pods the apiserver thinks is on node sc2-10-185-233-127.eng.vmware.com before test
Sep  6 00:12:35.035: INFO: curl-pod from test-cluster-ip-service started at 2021-09-05 20:33:45 -0700 PDT (1 container statuses recorded)
Sep  6 00:12:35.035: INFO: 	Container curl-container ready: true, restart count 0
Sep  6 00:12:35.035: INFO: helloworld from test-exec-ns started at 2021-09-05 20:18:37 -0700 PDT (1 container statuses recorded)
Sep  6 00:12:35.035: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:12:35.035: INFO: nginx-private from test-image-pull-secrets-ns started at 2021-09-05 20:19:39 -0700 PDT (1 container statuses recorded)
Sep  6 00:12:35.035: INFO: 	Container nginx-private-container ready: true, restart count 0
Sep  6 00:12:35.035: INFO: hello-web-1-6b97664bd5-w4mv7 from test-network-policy started at 2021-09-05 22:44:58 -0700 PDT (1 container statuses recorded)
Sep  6 00:12:35.035: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 00:12:35.035: INFO: hello-web-2-f779cbdff-dmb55 from test-network-policy started at 2021-09-05 20:37:45 -0700 PDT (1 container statuses recorded)
Sep  6 00:12:35.035: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 00:12:35.035: INFO: schedext-test-affinity-3 from test-pod-affinity started at 2021-09-05 20:22:44 -0700 PDT (1 container statuses recorded)
Sep  6 00:12:35.035: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:12:35.035: INFO: wcp-sanity-busybox-6f999d6849-564rw from test-update-workload-ns started at 2021-09-05 20:30:14 -0700 PDT (1 container statuses recorded)
Sep  6 00:12:35.035: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16a22a2565aca0b4], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match Pod's node affinity.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16a22a25688dab38], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match Pod's node affinity.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:12:36.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3414" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83
•{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":296,"completed":192,"skipped":3313,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:12:36.433: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-939
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:130
[It] should rollback without unnecessary restarts [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  6 00:12:37.519: INFO: Create a RollingUpdate DaemonSet
Sep  6 00:12:37.604: INFO: Check that daemon pods launch on every node of the cluster
Sep  6 00:12:37.631: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:37.631: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:37.631: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:37.660: INFO: Number of nodes with available pods: 0
Sep  6 00:12:37.660: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:38.676: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:38.677: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:38.677: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:38.685: INFO: Number of nodes with available pods: 0
Sep  6 00:12:38.685: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:39.687: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:39.687: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:39.687: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:39.705: INFO: Number of nodes with available pods: 0
Sep  6 00:12:39.705: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:40.678: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:40.678: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:40.678: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:40.689: INFO: Number of nodes with available pods: 0
Sep  6 00:12:40.689: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:41.680: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:41.680: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:41.680: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:41.687: INFO: Number of nodes with available pods: 0
Sep  6 00:12:41.687: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:42.681: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:42.682: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:42.682: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:42.689: INFO: Number of nodes with available pods: 0
Sep  6 00:12:42.689: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:43.678: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:43.678: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:43.678: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:43.693: INFO: Number of nodes with available pods: 0
Sep  6 00:12:43.693: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:44.678: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:44.678: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:44.678: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:44.693: INFO: Number of nodes with available pods: 0
Sep  6 00:12:44.693: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:45.672: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:45.672: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:45.672: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:45.680: INFO: Number of nodes with available pods: 0
Sep  6 00:12:45.680: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:46.679: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:46.679: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:46.680: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:46.692: INFO: Number of nodes with available pods: 0
Sep  6 00:12:46.692: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:47.675: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:47.675: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:47.675: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:47.696: INFO: Number of nodes with available pods: 0
Sep  6 00:12:47.696: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:48.676: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:48.676: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:48.676: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:48.685: INFO: Number of nodes with available pods: 0
Sep  6 00:12:48.685: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:49.696: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:49.696: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:49.696: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:49.705: INFO: Number of nodes with available pods: 0
Sep  6 00:12:49.705: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:50.703: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:50.703: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:50.703: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:50.728: INFO: Number of nodes with available pods: 0
Sep  6 00:12:50.728: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:51.675: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:51.676: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:51.676: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:51.687: INFO: Number of nodes with available pods: 0
Sep  6 00:12:51.687: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:52.690: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:52.690: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:52.690: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:52.706: INFO: Number of nodes with available pods: 0
Sep  6 00:12:52.706: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:53.675: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:53.675: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:53.675: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:53.698: INFO: Number of nodes with available pods: 0
Sep  6 00:12:53.698: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:54.674: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:54.674: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:54.674: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:54.688: INFO: Number of nodes with available pods: 0
Sep  6 00:12:54.688: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:55.694: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:55.694: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:55.694: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:55.703: INFO: Number of nodes with available pods: 0
Sep  6 00:12:55.703: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:56.676: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:56.676: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:56.676: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:56.690: INFO: Number of nodes with available pods: 0
Sep  6 00:12:56.690: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:57.673: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:57.673: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:57.673: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:57.682: INFO: Number of nodes with available pods: 0
Sep  6 00:12:57.682: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:58.689: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:58.689: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:58.689: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:58.713: INFO: Number of nodes with available pods: 0
Sep  6 00:12:58.713: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:12:59.734: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:59.734: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:59.734: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:12:59.752: INFO: Number of nodes with available pods: 0
Sep  6 00:12:59.752: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:13:00.675: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:00.675: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:00.675: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:00.684: INFO: Number of nodes with available pods: 0
Sep  6 00:13:00.684: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:13:01.696: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:01.697: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:01.697: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:01.708: INFO: Number of nodes with available pods: 0
Sep  6 00:13:01.708: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:13:02.678: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:02.679: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:02.679: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:02.689: INFO: Number of nodes with available pods: 0
Sep  6 00:13:02.689: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:13:03.725: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:03.725: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:03.725: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:03.764: INFO: Number of nodes with available pods: 0
Sep  6 00:13:03.764: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:13:04.672: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:04.672: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:04.672: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:04.683: INFO: Number of nodes with available pods: 0
Sep  6 00:13:04.683: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:13:05.796: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:05.796: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:05.796: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:05.821: INFO: Number of nodes with available pods: 0
Sep  6 00:13:05.821: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:13:06.673: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:06.674: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:06.674: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:06.681: INFO: Number of nodes with available pods: 0
Sep  6 00:13:06.681: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:13:07.674: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:07.675: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:07.675: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:07.682: INFO: Number of nodes with available pods: 0
Sep  6 00:13:07.682: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:13:08.676: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:08.676: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:08.676: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:08.685: INFO: Number of nodes with available pods: 0
Sep  6 00:13:08.685: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:13:09.679: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:09.679: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:09.679: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:09.688: INFO: Number of nodes with available pods: 1
Sep  6 00:13:09.688: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:13:10.687: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:10.687: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:10.687: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:10.710: INFO: Number of nodes with available pods: 1
Sep  6 00:13:10.710: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:13:11.673: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:11.673: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:11.673: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:11.683: INFO: Number of nodes with available pods: 2
Sep  6 00:13:11.683: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:13:12.678: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:12.678: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:12.678: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:12.685: INFO: Number of nodes with available pods: 2
Sep  6 00:13:12.685: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:13:13.673: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:13.673: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:13.673: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:13.679: INFO: Number of nodes with available pods: 3
Sep  6 00:13:13.679: INFO: Number of running nodes: 3, number of available pods: 3
Sep  6 00:13:13.679: INFO: Update the DaemonSet to trigger a rollout
Sep  6 00:13:13.728: INFO: Updating DaemonSet daemon-set
Sep  6 00:13:27.786: INFO: Roll back the DaemonSet before rollout is complete
Sep  6 00:13:27.816: INFO: Updating DaemonSet daemon-set
Sep  6 00:13:27.816: INFO: Make sure DaemonSet rollback is complete
Sep  6 00:13:27.835: INFO: Wrong image for pod: daemon-set-bz94z. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep  6 00:13:27.835: INFO: Pod daemon-set-bz94z is not available
Sep  6 00:13:27.851: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:27.851: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:27.851: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:28.867: INFO: Pod daemon-set-j9dcd is not available
Sep  6 00:13:28.882: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:28.882: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:13:28.882: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:96
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-939, will wait for the garbage collector to delete the pods
Sep  6 00:13:29.016: INFO: Deleting DaemonSet.extensions daemon-set took: 51.506155ms
Sep  6 00:13:29.116: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.212278ms
Sep  6 00:13:47.046: INFO: Number of nodes with available pods: 0
Sep  6 00:13:47.046: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 00:13:47.057: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-939/daemonsets","resourceVersion":"180426"},"items":null}

Sep  6 00:13:47.069: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-939/pods","resourceVersion":"180426"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:13:47.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-939" for this suite.

• [SLOW TEST:71.237 seconds]
[sig-apps] Daemon set [Serial]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":296,"completed":193,"skipped":3317,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:13:47.670: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9308
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  6 00:13:48.583: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep  6 00:13:48.644: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  6 00:13:53.666: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  6 00:14:19.793: INFO: Creating deployment "test-rolling-update-deployment"
Sep  6 00:14:20.121: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep  6 00:14:20.176: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep  6 00:14:22.208: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep  6 00:14:22.216: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:14:24.237: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:14:26.234: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:14:28.234: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:14:30.266: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:14:32.237: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:14:34.237: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:14:36.230: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:14:38.229: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:14:40.228: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:14:42.229: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:14:44.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:14:46.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:14:48.234: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 14, 20, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-6b6bf9df46\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:14:50.230: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Sep  6 00:14:50.260: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9308 /apis/apps/v1/namespaces/deployment-9308/deployments/test-rolling-update-deployment d4b72995-e430-4af5-8d68-ee369259715a 181175 1 2021-09-06 00:14:19 -0700 PDT <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-09-06 00:14:19 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-06 00:14:48 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d20f48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-09-06 00:14:20 -0700 PDT,LastTransitionTime:2021-09-06 00:14:20 -0700 PDT,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-6b6bf9df46" has successfully progressed.,LastUpdateTime:2021-09-06 00:14:48 -0700 PDT,LastTransitionTime:2021-09-06 00:14:20 -0700 PDT,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  6 00:14:50.268: INFO: New ReplicaSet "test-rolling-update-deployment-6b6bf9df46" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46  deployment-9308 /apis/apps/v1/namespaces/deployment-9308/replicasets/test-rolling-update-deployment-6b6bf9df46 f6d80f50-16b6-44e3-9c12-28b9f1b2092c 181164 1 2021-09-06 00:14:20 -0700 PDT <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d4b72995-e430-4af5-8d68-ee369259715a 0xc002d213d7 0xc002d213d8}] []  [{kube-controller-manager Update apps/v1 2021-09-06 00:14:48 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4b72995-e430-4af5-8d68-ee369259715a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 6b6bf9df46,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002d21468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  6 00:14:50.268: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep  6 00:14:50.268: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9308 /apis/apps/v1/namespaces/deployment-9308/replicasets/test-rolling-update-controller 1057d2e8-65e8-4cc2-8206-a7ac0d31ef7a 181174 2 2021-09-06 00:13:48 -0700 PDT <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d4b72995-e430-4af5-8d68-ee369259715a 0xc002d212c7 0xc002d212c8}] []  [{e2e.test Update apps/v1 2021-09-06 00:13:48 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-06 00:14:48 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d4b72995-e430-4af5-8d68-ee369259715a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002d21368 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 00:14:50.276: INFO: Pod "test-rolling-update-controller-6fn2q" is available:
&Pod{ObjectMeta:{test-rolling-update-controller-6fn2q test-rolling-update-controller- deployment-9308 /api/v1/namespaces/deployment-9308/pods/test-rolling-update-controller-6fn2q 91e58ef9-0eaf-4a1d-9ae4-a74c8acff803 181169 0 2021-09-06 00:13:48 -0700 PDT 2021-09-06 00:14:48 -0700 PDT 0xc0031a84d8 map[name:sample-pod pod:httpd] map[attachment_id:26d4e8b6-5814-46eb-beea-230e1048bac7 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:00:09 vlan:None vmware-system-ephemeral-disk-uuid:6000C29e-c53d-430a-11a5-d327603a0289 vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v16561"} vmware-system-vm-moid:vm-1168:3f0e15ce-7291-45c7-8a72-39d27d06e017 vmware-system-vm-uuid:50168a71-1ab3-91b8-9b9f-c0c1c941b040] [{apps/v1 ReplicaSet test-rolling-update-controller 1057d2e8-65e8-4cc2-8206-a7ac0d31ef7a 0xc0031a8547 0xc0031a8548}] [lifecycle-controller/system.vmware.com]  [{image-controller Update v1 2021-09-06 00:13:48 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {kube-controller-manager Update v1 2021-09-06 00:13:48 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1057d2e8-65e8-4cc2-8206-a7ac0d31ef7a\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-df5sk Update v1 2021-09-06 00:13:55 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-06 00:14:10 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:14:18 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-b2pbb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-b2pbb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-b2pbb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-233.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:13:48 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:14:17 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:14:17 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:14:17 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.185.226.233,PodIP:172.26.1.210,StartTime:2021-09-06 00:14:13 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:14:14 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v16561,ContainerID:ac911be9-3cef-4611-a189-1dcc7c717a73,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.210,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:14:50.276: INFO: Pod "test-rolling-update-deployment-6b6bf9df46-s5699" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-6b6bf9df46-s5699 test-rolling-update-deployment-6b6bf9df46- deployment-9308 /api/v1/namespaces/deployment-9308/pods/test-rolling-update-deployment-6b6bf9df46-s5699 1b84456e-81d2-42fa-b244-5ca1f28aa8a4 181160 0 2021-09-06 00:14:20 -0700 PDT <nil> <nil> map[name:sample-pod pod-template-hash:6b6bf9df46] map[attachment_id:01815972-3ee6-4500-ae3e-bc919a5f7497 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:00:03 vlan:None vmware-system-ephemeral-disk-uuid:6000C294-9f7b-dfd2-97b2-b8fd0dd93eab vmware-system-image-references:{"agnhost":"agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v32754"} vmware-system-vm-moid:vm-1170:3f0e15ce-7291-45c7-8a72-39d27d06e017 vmware-system-vm-uuid:50165ef3-4756-4c38-a7a4-da97e231d6ee] [{apps/v1 ReplicaSet test-rolling-update-deployment-6b6bf9df46 f6d80f50-16b6-44e3-9c12-28b9f1b2092c 0xc0031a888f 0xc0031a88a0}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:14:20 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f6d80f50-16b6-44e3-9c12-28b9f1b2092c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-06 00:14:21 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {nsx-ncp-6d7f7bf559-df5sk Update v1 2021-09-06 00:14:22 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-06 00:14:37 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:14:48 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.211\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-b2pbb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-b2pbb,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-b2pbb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-233.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:14:20 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:14:49 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:14:49 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:14:49 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.185.226.233,PodIP:172.26.1.211,StartTime:2021-09-06 00:14:44 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:14:45 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v32754,ContainerID:9e88d3cc-73d5-4b61-99bb-37268801bf9b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:14:50.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9308" for this suite.

• [SLOW TEST:62.835 seconds]
[sig-apps] Deployment
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":296,"completed":194,"skipped":3347,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:14:50.506: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-641
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-641, will wait for the garbage collector to delete the pods
Sep  6 00:15:19.129: INFO: Deleting Job.batch foo took: 38.020137ms
Sep  6 00:15:19.229: INFO: Terminating Job.batch foo pods took: 100.152575ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:16:07.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-641" for this suite.

• [SLOW TEST:77.726 seconds]
[sig-apps] Job
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":296,"completed":195,"skipped":3356,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:16:08.232: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-3637
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Sep  6 00:16:41.298: INFO: Successfully updated pod "adopt-release-lhdrx"
STEP: Checking that the Job readopts the Pod
Sep  6 00:16:41.298: INFO: Waiting up to 15m0s for pod "adopt-release-lhdrx" in namespace "job-3637" to be "adopted"
Sep  6 00:16:41.316: INFO: Pod "adopt-release-lhdrx": Phase="Running", Reason="", readiness=true. Elapsed: 18.36251ms
Sep  6 00:16:43.326: INFO: Pod "adopt-release-lhdrx": Phase="Running", Reason="", readiness=true. Elapsed: 2.028659129s
Sep  6 00:16:43.327: INFO: Pod "adopt-release-lhdrx" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Sep  6 00:16:43.859: INFO: Successfully updated pod "adopt-release-lhdrx"
STEP: Checking that the Job releases the Pod
Sep  6 00:16:43.859: INFO: Waiting up to 15m0s for pod "adopt-release-lhdrx" in namespace "job-3637" to be "released"
Sep  6 00:16:43.875: INFO: Pod "adopt-release-lhdrx": Phase="Running", Reason="", readiness=true. Elapsed: 16.359229ms
Sep  6 00:16:45.887: INFO: Pod "adopt-release-lhdrx": Phase="Running", Reason="", readiness=true. Elapsed: 2.028207914s
Sep  6 00:16:45.887: INFO: Pod "adopt-release-lhdrx" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:16:45.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3637" for this suite.

• [SLOW TEST:37.915 seconds]
[sig-apps] Job
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":296,"completed":196,"skipped":3358,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:16:46.148: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8823
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8823.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8823.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 00:17:20.831: INFO: DNS probes using dns-8823/dns-test-e43e6149-2709-421e-8742-9802503a4075 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:17:20.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8823" for this suite.

• [SLOW TEST:34.958 seconds]
[sig-network] DNS
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":296,"completed":197,"skipped":3405,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:17:21.106: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2790
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Sep  6 00:17:21.657: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  6 00:17:35.772: INFO: >>> kubeConfig: ./kconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:18:08.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2790" for this suite.

• [SLOW TEST:47.934 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":296,"completed":198,"skipped":3440,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:18:09.041: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7206
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 00:18:10.682: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  6 00:18:12.725: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:18:14.750: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:18:16.748: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:18:18.738: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:18:20.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:18:22.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:18:24.772: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:18:26.740: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:18:28.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:18:30.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:18:32.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:18:34.739: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:18:36.738: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:18:38.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:18:40.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:18:42.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 18, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 00:18:45.814: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:18:47.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7206" for this suite.
STEP: Destroying namespace "webhook-7206-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:38.933 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":296,"completed":199,"skipped":3520,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:18:47.974: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-469
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:18:56.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-469" for this suite.

• [SLOW TEST:8.858 seconds]
[sig-api-machinery] ResourceQuota
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":296,"completed":200,"skipped":3531,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:18:56.833: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-439
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep  6 00:18:57.586: INFO: Waiting up to 5m0s for pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d" in namespace "downward-api-439" to be "Succeeded or Failed"
Sep  6 00:18:57.648: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Pending", Reason="", readiness=false. Elapsed: 61.610199ms
Sep  6 00:18:59.665: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078778193s
Sep  6 00:19:01.715: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.129472777s
Sep  6 00:19:03.730: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.143578756s
Sep  6 00:19:05.762: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.176028814s
Sep  6 00:19:07.785: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.199563187s
Sep  6 00:19:09.804: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.218002612s
Sep  6 00:19:11.838: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.252455849s
Sep  6 00:19:13.853: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.267201092s
Sep  6 00:19:15.886: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.299954194s
Sep  6 00:19:17.910: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Pending", Reason="", readiness=false. Elapsed: 20.324424893s
Sep  6 00:19:19.925: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Pending", Reason="", readiness=false. Elapsed: 22.339337383s
Sep  6 00:19:21.988: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Pending", Reason="", readiness=false. Elapsed: 24.402520815s
Sep  6 00:19:24.009: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Pending", Reason="", readiness=false. Elapsed: 26.422721521s
Sep  6 00:19:26.022: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Pending", Reason="", readiness=false. Elapsed: 28.436309683s
Sep  6 00:19:28.129: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Pending", Reason="", readiness=false. Elapsed: 30.543529798s
Sep  6 00:19:30.159: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Running", Reason="", readiness=true. Elapsed: 32.573432014s
Sep  6 00:19:32.204: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Running", Reason="", readiness=true. Elapsed: 34.618428525s
Sep  6 00:19:34.223: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Running", Reason="", readiness=true. Elapsed: 36.636723683s
Sep  6 00:19:36.248: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 38.662447399s
STEP: Saw pod success
Sep  6 00:19:36.248: INFO: Pod "downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d" satisfied condition "Succeeded or Failed"
Sep  6 00:19:36.260: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d container client-container: <nil>
STEP: delete the pod
Sep  6 00:19:36.374: INFO: Waiting for pod downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d to disappear
Sep  6 00:19:36.392: INFO: Pod downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d still exists
Sep  6 00:19:38.393: INFO: Waiting for pod downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d to disappear
Sep  6 00:19:38.409: INFO: Pod downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d still exists
Sep  6 00:19:40.392: INFO: Waiting for pod downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d to disappear
Sep  6 00:19:40.406: INFO: Pod downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d still exists
Sep  6 00:19:42.392: INFO: Waiting for pod downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d to disappear
Sep  6 00:19:42.421: INFO: Pod downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d still exists
Sep  6 00:19:44.393: INFO: Waiting for pod downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d to disappear
Sep  6 00:19:44.439: INFO: Pod downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d still exists
Sep  6 00:19:46.392: INFO: Waiting for pod downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d to disappear
Sep  6 00:19:46.413: INFO: Pod downwardapi-volume-942f83b9-92cb-427e-9353-4f6f77f3488d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:19:46.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-439" for this suite.

• [SLOW TEST:49.841 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":201,"skipped":3549,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:19:46.675: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5031
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep  6 00:19:47.393: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a" in namespace "projected-5031" to be "Succeeded or Failed"
Sep  6 00:19:47.432: INFO: Pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a": Phase="Pending", Reason="", readiness=false. Elapsed: 38.85374ms
Sep  6 00:19:49.447: INFO: Pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053845933s
Sep  6 00:19:51.470: INFO: Pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.076554305s
Sep  6 00:19:53.484: INFO: Pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.09150095s
Sep  6 00:19:55.497: INFO: Pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.104507928s
Sep  6 00:19:57.504: INFO: Pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.111144657s
Sep  6 00:19:59.536: INFO: Pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.143041091s
Sep  6 00:20:01.561: INFO: Pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.167855051s
Sep  6 00:20:03.581: INFO: Pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.188049841s
Sep  6 00:20:05.607: INFO: Pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.213633666s
Sep  6 00:20:07.626: INFO: Pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.232586575s
Sep  6 00:20:09.657: INFO: Pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.26361362s
Sep  6 00:20:11.681: INFO: Pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a": Phase="Pending", Reason="", readiness=false. Elapsed: 24.287736127s
Sep  6 00:20:13.691: INFO: Pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a": Phase="Pending", Reason="", readiness=false. Elapsed: 26.298223906s
Sep  6 00:20:15.710: INFO: Pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a": Phase="Running", Reason="", readiness=true. Elapsed: 28.31738864s
Sep  6 00:20:17.724: INFO: Pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a": Phase="Running", Reason="", readiness=true. Elapsed: 30.331361048s
Sep  6 00:20:19.739: INFO: Pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a": Phase="Running", Reason="", readiness=true. Elapsed: 32.346510052s
Sep  6 00:20:21.768: INFO: Pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.374675915s
STEP: Saw pod success
Sep  6 00:20:21.768: INFO: Pod "downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a" satisfied condition "Succeeded or Failed"
Sep  6 00:20:21.774: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a container client-container: <nil>
STEP: delete the pod
Sep  6 00:20:27.089: INFO: Waiting for pod downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a to disappear
Sep  6 00:20:27.111: INFO: Pod downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a still exists
Sep  6 00:20:29.111: INFO: Waiting for pod downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a to disappear
Sep  6 00:20:29.122: INFO: Pod downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a still exists
Sep  6 00:20:31.111: INFO: Waiting for pod downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a to disappear
Sep  6 00:20:31.129: INFO: Pod downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a still exists
Sep  6 00:20:33.111: INFO: Waiting for pod downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a to disappear
Sep  6 00:20:33.132: INFO: Pod downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a still exists
Sep  6 00:20:35.112: INFO: Waiting for pod downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a to disappear
Sep  6 00:20:35.122: INFO: Pod downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a still exists
Sep  6 00:20:37.112: INFO: Waiting for pod downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a to disappear
Sep  6 00:20:37.124: INFO: Pod downwardapi-volume-6d31c079-a6a8-4606-a245-5a504b65ff4a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:20:37.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5031" for this suite.

• [SLOW TEST:50.724 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":202,"skipped":3609,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:20:37.399: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-906
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 00:20:39.194: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  6 00:20:41.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:20:43.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:20:45.239: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:20:47.244: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:20:49.242: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:20:51.236: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:20:53.236: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:20:55.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:20:57.243: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:20:59.257: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:21:01.240: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:21:03.236: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:21:05.233: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:21:07.242: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:21:09.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 20, 39, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 00:21:12.363: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:21:25.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-906" for this suite.
STEP: Destroying namespace "webhook-906-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:48.487 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":296,"completed":203,"skipped":3620,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:21:25.888: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4323
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep  6 00:21:26.340: INFO: Waiting up to 5m0s for pod "downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0" in namespace "downward-api-4323" to be "Succeeded or Failed"
Sep  6 00:21:26.359: INFO: Pod "downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.985399ms
Sep  6 00:21:28.367: INFO: Pod "downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026889606s
Sep  6 00:21:30.391: INFO: Pod "downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050930219s
Sep  6 00:21:32.412: INFO: Pod "downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.071377825s
Sep  6 00:21:34.435: INFO: Pod "downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094804421s
Sep  6 00:21:36.450: INFO: Pod "downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.109495961s
Sep  6 00:21:38.463: INFO: Pod "downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.122050058s
Sep  6 00:21:40.483: INFO: Pod "downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.142725774s
Sep  6 00:21:42.496: INFO: Pod "downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.155960809s
Sep  6 00:21:44.511: INFO: Pod "downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.170479352s
Sep  6 00:21:46.527: INFO: Pod "downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.186119472s
Sep  6 00:21:48.542: INFO: Pod "downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.20152791s
Sep  6 00:21:50.560: INFO: Pod "downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.219814632s
Sep  6 00:21:52.575: INFO: Pod "downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.234655388s
STEP: Saw pod success
Sep  6 00:21:52.575: INFO: Pod "downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0" satisfied condition "Succeeded or Failed"
Sep  6 00:21:52.587: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0 container client-container: <nil>
STEP: delete the pod
Sep  6 00:21:58.101: INFO: Waiting for pod downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0 to disappear
Sep  6 00:21:58.110: INFO: Pod downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0 still exists
Sep  6 00:22:00.111: INFO: Waiting for pod downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0 to disappear
Sep  6 00:22:00.124: INFO: Pod downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0 still exists
Sep  6 00:22:02.111: INFO: Waiting for pod downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0 to disappear
Sep  6 00:22:02.132: INFO: Pod downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0 still exists
Sep  6 00:22:04.112: INFO: Waiting for pod downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0 to disappear
Sep  6 00:22:04.122: INFO: Pod downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0 still exists
Sep  6 00:22:06.111: INFO: Waiting for pod downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0 to disappear
Sep  6 00:22:06.123: INFO: Pod downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0 still exists
Sep  6 00:22:08.111: INFO: Waiting for pod downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0 to disappear
Sep  6 00:22:08.124: INFO: Pod downwardapi-volume-745be1ad-e3e6-452d-957c-1925391287a0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:22:08.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4323" for this suite.

• [SLOW TEST:42.695 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's cpu request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":296,"completed":204,"skipped":3648,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Events
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:22:08.584: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-1313
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:22:09.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1313" for this suite.
•{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":296,"completed":205,"skipped":3671,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:22:09.391: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7581
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  6 00:22:09.835: INFO: Creating ReplicaSet my-hostname-basic-c6b0230b-d8cd-4cad-bfa9-a075076844cc
Sep  6 00:22:09.880: INFO: Pod name my-hostname-basic-c6b0230b-d8cd-4cad-bfa9-a075076844cc: Found 0 pods out of 1
Sep  6 00:22:14.890: INFO: Pod name my-hostname-basic-c6b0230b-d8cd-4cad-bfa9-a075076844cc: Found 1 pods out of 1
Sep  6 00:22:14.890: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-c6b0230b-d8cd-4cad-bfa9-a075076844cc" is running
Sep  6 00:22:36.926: INFO: Pod "my-hostname-basic-c6b0230b-d8cd-4cad-bfa9-a075076844cc-ktqvm" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-06 00:22:10 -0700 PDT Reason: Message:}])
Sep  6 00:22:36.927: INFO: Trying to dial the pod
Sep  6 00:22:42.032: INFO: Controller my-hostname-basic-c6b0230b-d8cd-4cad-bfa9-a075076844cc: Got expected result from replica 1 [my-hostname-basic-c6b0230b-d8cd-4cad-bfa9-a075076844cc-ktqvm]: "my-hostname-basic-c6b0230b-d8cd-4cad-bfa9-a075076844cc-ktqvm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:22:42.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7581" for this suite.

• [SLOW TEST:33.394 seconds]
[sig-apps] ReplicaSet
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":296,"completed":206,"skipped":3717,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:22:42.786: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4236
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Sep  6 00:22:43.358: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:23:11.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4236" for this suite.

• [SLOW TEST:28.684 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should invoke init containers on a RestartNever pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":296,"completed":207,"skipped":3748,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:23:11.471: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7782
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Sep  6 00:23:11.883: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 00:23:11.919: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 00:23:11.932: INFO: 
Logging pods the apiserver thinks is on node sc2-10-185-226-150.eng.vmware.com before test
Sep  6 00:23:11.985: INFO: podwithpersistentvolume from storage-policy-test started at 2021-09-05 20:29:04 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:11.985: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:23:11.985: INFO: hello-web-6b97664bd5-zznrr from test-cluster-ip-service started at 2021-09-05 22:45:00 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:11.985: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 00:23:11.985: INFO: wcp-sanity-busybox-6f999d6849-kspp7 from test-dataprovider-podvms-ns started at 2021-09-05 20:17:25 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:11.985: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:23:11.985: INFO: curl-pod from test-network-policy started at 2021-09-05 20:38:23 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:11.985: INFO: 	Container curl-container ready: true, restart count 0
Sep  6 00:23:11.985: INFO: schedext-test-node-selector-1 from test-node-selector started at 2021-09-05 20:21:19 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:11.985: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:23:11.985: INFO: schedext-test-affinity-1 from test-pod-affinity started at 2021-09-05 20:22:12 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:11.985: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:23:11.985: INFO: schedext-test-affinity-2 from test-pod-affinity started at 2021-09-05 20:22:42 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:11.985: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:23:11.985: INFO: test-docker-registry from test-private-image-registry-ns started at 2021-09-05 20:41:08 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:11.985: INFO: 	Container test-docker-registry ready: true, restart count 0
Sep  6 00:23:11.985: INFO: helloworld from test-telemetry started at 2021-09-05 20:32:42 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:11.985: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:23:11.985: INFO: wcp-sanity-busybox-6f999d6849-6bn7v from test-update-workload-ns started at 2021-09-05 20:30:32 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:11.985: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:23:11.985: INFO: 
Logging pods the apiserver thinks is on node sc2-10-185-226-233.eng.vmware.com before test
Sep  6 00:23:12.013: INFO: pod-init-b0939c63-b1e5-4e3f-9f97-72a0674dd5f7 from init-container-4236 started at 2021-09-06 00:23:07 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:12.013: INFO: 	Container run1 ready: true, restart count 0
Sep  6 00:23:12.013: INFO: wcp-sanity-busybox-6f999d6849-tx88v from test-dataprovider-podvms-ns started at 2021-09-05 22:46:15 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:12.013: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:23:12.013: INFO: wcp-sanity-busybox-6f999d6849-428rc from test-update-workload-ns started at 2021-09-05 22:46:12 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:12.013: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:23:12.013: INFO: 
Logging pods the apiserver thinks is on node sc2-10-185-233-127.eng.vmware.com before test
Sep  6 00:23:12.047: INFO: curl-pod from test-cluster-ip-service started at 2021-09-05 20:33:45 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:12.047: INFO: 	Container curl-container ready: true, restart count 0
Sep  6 00:23:12.047: INFO: helloworld from test-exec-ns started at 2021-09-05 20:18:37 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:12.047: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:23:12.047: INFO: nginx-private from test-image-pull-secrets-ns started at 2021-09-05 20:19:39 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:12.047: INFO: 	Container nginx-private-container ready: true, restart count 0
Sep  6 00:23:12.047: INFO: hello-web-1-6b97664bd5-w4mv7 from test-network-policy started at 2021-09-05 22:44:58 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:12.047: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 00:23:12.047: INFO: hello-web-2-f779cbdff-dmb55 from test-network-policy started at 2021-09-05 20:37:45 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:12.047: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 00:23:12.047: INFO: schedext-test-affinity-3 from test-pod-affinity started at 2021-09-05 20:22:44 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:12.047: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:23:12.047: INFO: wcp-sanity-busybox-6f999d6849-564rw from test-update-workload-ns started at 2021-09-05 20:30:14 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:12.047: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: verifying the node has the label node sc2-10-185-226-150.eng.vmware.com
STEP: verifying the node has the label node sc2-10-185-226-233.eng.vmware.com
STEP: verifying the node has the label node sc2-10-185-233-127.eng.vmware.com
Sep  6 00:23:12.293: INFO: Pod podwithpersistentvolume requesting resource cpu=0m on Node sc2-10-185-226-150.eng.vmware.com
Sep  6 00:23:12.293: INFO: Pod curl-pod requesting resource cpu=0m on Node sc2-10-185-233-127.eng.vmware.com
Sep  6 00:23:12.293: INFO: Pod hello-web-6b97664bd5-zznrr requesting resource cpu=0m on Node sc2-10-185-226-150.eng.vmware.com
Sep  6 00:23:12.293: INFO: Pod wcp-sanity-busybox-6f999d6849-kspp7 requesting resource cpu=0m on Node sc2-10-185-226-150.eng.vmware.com
Sep  6 00:23:12.293: INFO: Pod wcp-sanity-busybox-6f999d6849-tx88v requesting resource cpu=0m on Node sc2-10-185-226-233.eng.vmware.com
Sep  6 00:23:12.293: INFO: Pod helloworld requesting resource cpu=0m on Node sc2-10-185-233-127.eng.vmware.com
Sep  6 00:23:12.293: INFO: Pod nginx-private requesting resource cpu=0m on Node sc2-10-185-233-127.eng.vmware.com
Sep  6 00:23:12.293: INFO: Pod curl-pod requesting resource cpu=0m on Node sc2-10-185-226-150.eng.vmware.com
Sep  6 00:23:12.293: INFO: Pod hello-web-1-6b97664bd5-w4mv7 requesting resource cpu=0m on Node sc2-10-185-233-127.eng.vmware.com
Sep  6 00:23:12.293: INFO: Pod hello-web-2-f779cbdff-dmb55 requesting resource cpu=0m on Node sc2-10-185-233-127.eng.vmware.com
Sep  6 00:23:12.293: INFO: Pod schedext-test-node-selector-1 requesting resource cpu=0m on Node sc2-10-185-226-150.eng.vmware.com
Sep  6 00:23:12.293: INFO: Pod schedext-test-affinity-1 requesting resource cpu=500m on Node sc2-10-185-226-150.eng.vmware.com
Sep  6 00:23:12.293: INFO: Pod schedext-test-affinity-2 requesting resource cpu=500m on Node sc2-10-185-226-150.eng.vmware.com
Sep  6 00:23:12.293: INFO: Pod schedext-test-affinity-3 requesting resource cpu=500m on Node sc2-10-185-233-127.eng.vmware.com
Sep  6 00:23:12.293: INFO: Pod test-docker-registry requesting resource cpu=0m on Node sc2-10-185-226-150.eng.vmware.com
Sep  6 00:23:12.293: INFO: Pod helloworld requesting resource cpu=0m on Node sc2-10-185-226-150.eng.vmware.com
Sep  6 00:23:12.293: INFO: Pod wcp-sanity-busybox-6f999d6849-428rc requesting resource cpu=0m on Node sc2-10-185-226-233.eng.vmware.com
Sep  6 00:23:12.293: INFO: Pod wcp-sanity-busybox-6f999d6849-564rw requesting resource cpu=0m on Node sc2-10-185-233-127.eng.vmware.com
Sep  6 00:23:12.293: INFO: Pod wcp-sanity-busybox-6f999d6849-6bn7v requesting resource cpu=0m on Node sc2-10-185-226-150.eng.vmware.com
STEP: Starting Pods to consume most of the cluster CPU.
Sep  6 00:23:12.293: INFO: Creating a pod which consumes cpu=5600m on Node sc2-10-185-226-150.eng.vmware.com
Sep  6 00:23:12.331: INFO: Creating a pod which consumes cpu=7000m on Node sc2-10-185-226-233.eng.vmware.com
Sep  6 00:23:12.365: INFO: Creating a pod which consumes cpu=6650m on Node sc2-10-185-233-127.eng.vmware.com
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24e2b1b3-9390-4438-8d99-106481d4ccef.1630913000], Reason = [SuccessfulRealizeNSXResource], Message = [Successfully realized NSX resource for Pod]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24e2b1b3-9390-4438-8d99-106481d4ccef.16a22ab9c540c147], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7782/filler-pod-24e2b1b3-9390-4438-8d99-106481d4ccef to sc2-10-185-226-150.eng.vmware.com]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24e2b1b3-9390-4438-8d99-106481d4ccef.16a22ab9cca67446], Reason = [Image], Message = [Image pause-54467452d21a00c28aa4cce37d0850173b528ca6-v8939 bound successfully]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24e2b1b3-9390-4438-8d99-106481d4ccef.16a22abdb43b7590], Reason = [Pulling], Message = [Waiting for Image sched-pred-7782/pause-54467452d21a00c28aa4cce37d0850173b528ca6-v8939]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24e2b1b3-9390-4438-8d99-106481d4ccef.16a22abdb440b168], Reason = [Pulled], Message = [Image sched-pred-7782/pause-54467452d21a00c28aa4cce37d0850173b528ca6-v8939 is ready]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24e2b1b3-9390-4438-8d99-106481d4ccef.16a22abef843a748], Reason = [SuccessfulMountVolume], Message = [Successfully mounted volume default-token-66gml]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24e2b1b3-9390-4438-8d99-106481d4ccef.16a22abef8444b58], Reason = [Created], Message = [Created container filler-pod-24e2b1b3-9390-4438-8d99-106481d4ccef]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-24e2b1b3-9390-4438-8d99-106481d4ccef.16a22ac0257058a0], Reason = [Started], Message = [Started container filler-pod-24e2b1b3-9390-4438-8d99-106481d4ccef]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3f15589c-977b-4bf0-b5bb-d73bcd89fba3.1630913001], Reason = [SuccessfulRealizeNSXResource], Message = [Successfully realized NSX resource for Pod]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3f15589c-977b-4bf0-b5bb-d73bcd89fba3.16a22ab9caba6870], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7782/filler-pod-3f15589c-977b-4bf0-b5bb-d73bcd89fba3 to sc2-10-185-226-233.eng.vmware.com]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3f15589c-977b-4bf0-b5bb-d73bcd89fba3.16a22abc46459c42], Reason = [Image], Message = [Image pause-54467452d21a00c28aa4cce37d0850173b528ca6-v8939 bound successfully]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3f15589c-977b-4bf0-b5bb-d73bcd89fba3.16a22abdc1d339e0], Reason = [Pulling], Message = [Waiting for Image sched-pred-7782/pause-54467452d21a00c28aa4cce37d0850173b528ca6-v8939]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3f15589c-977b-4bf0-b5bb-d73bcd89fba3.16a22abdc1d6a0f8], Reason = [Pulled], Message = [Image sched-pred-7782/pause-54467452d21a00c28aa4cce37d0850173b528ca6-v8939 is ready]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3f15589c-977b-4bf0-b5bb-d73bcd89fba3.16a22ac04e008c18], Reason = [SuccessfulMountVolume], Message = [Successfully mounted volume default-token-66gml]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3f15589c-977b-4bf0-b5bb-d73bcd89fba3.16a22ac04e021e70], Reason = [Created], Message = [Created container filler-pod-3f15589c-977b-4bf0-b5bb-d73bcd89fba3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3f15589c-977b-4bf0-b5bb-d73bcd89fba3.16a22ac04e029788], Reason = [Started], Message = [Started container filler-pod-3f15589c-977b-4bf0-b5bb-d73bcd89fba3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c20e93e-c701-4f7b-9b28-a90c89bf00e7.1630913000], Reason = [SuccessfulRealizeNSXResource], Message = [Successfully realized NSX resource for Pod]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c20e93e-c701-4f7b-9b28-a90c89bf00e7.16a22ab9d198aa91], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7782/filler-pod-8c20e93e-c701-4f7b-9b28-a90c89bf00e7 to sc2-10-185-233-127.eng.vmware.com]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c20e93e-c701-4f7b-9b28-a90c89bf00e7.16a22abcb7f4ebba], Reason = [Image], Message = [Image pause-54467452d21a00c28aa4cce37d0850173b528ca6-v8939 bound successfully]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c20e93e-c701-4f7b-9b28-a90c89bf00e7.16a22abe4efa3c60], Reason = [Pulling], Message = [Waiting for Image sched-pred-7782/pause-54467452d21a00c28aa4cce37d0850173b528ca6-v8939]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c20e93e-c701-4f7b-9b28-a90c89bf00e7.16a22abe4f035480], Reason = [Pulled], Message = [Image sched-pred-7782/pause-54467452d21a00c28aa4cce37d0850173b528ca6-v8939 is ready]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c20e93e-c701-4f7b-9b28-a90c89bf00e7.16a22abfa431b108], Reason = [SuccessfulMountVolume], Message = [Successfully mounted volume default-token-66gml]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c20e93e-c701-4f7b-9b28-a90c89bf00e7.16a22abfa4323da8], Reason = [Created], Message = [Created container filler-pod-8c20e93e-c701-4f7b-9b28-a90c89bf00e7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8c20e93e-c701-4f7b-9b28-a90c89bf00e7.16a22abfa4348b80], Reason = [Started], Message = [Started container filler-pod-8c20e93e-c701-4f7b-9b28-a90c89bf00e7]
STEP: Considering event: 
Type = [Normal], Name = [pause-54467452d21a00c28aa4cce37d0850173b528ca6-v8939.16a22aba3aa43ad0], Reason = [Status], Message = [sc2-10-185-226-150.eng.vmware.com: Image status changed to Resolving]
STEP: Considering event: 
Type = [Normal], Name = [pause-54467452d21a00c28aa4cce37d0850173b528ca6-v8939.16a22abbf614dff8], Reason = [Resolve], Message = [sc2-10-185-226-150.eng.vmware.com: Image resolved to ChainID sha256:ba0dae6243cc9fa2890df40a625721fdbea5c94ca6da897acdd814d710149770]
STEP: Considering event: 
Type = [Warning], Name = [pause-54467452d21a00c28aa4cce37d0850173b528ca6-v8939.16a22abc3bfbbeb6], Reason = [Bind], Message = [Imagedisk bind failed: Operation cannot be fulfilled on images.imagecontroller.vmware.com "pause-54467452d21a00c28aa4cce37d0850173b528ca6-v8939": the object has been modified; please apply your changes to the latest version and try again]
STEP: Considering event: 
Type = [Normal], Name = [pause-54467452d21a00c28aa4cce37d0850173b528ca6-v8939.16a22abc40c9b381], Reason = [Bind], Message = [Imagedisk ba0dae6243cc9fa2890df40a625721fdbea5c94ca6da897acdd814d710149770-v38546479 successfully bound]
STEP: Considering event: 
Type = [Normal], Name = [pause-54467452d21a00c28aa4cce37d0850173b528ca6-v8939.16a22abc42995af7], Reason = [Status], Message = [Image status changed to Ready]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16a22ac0cceaa378], Reason = [FailedScheduling], Message = [Insufficient resources.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16a22ac0d8c59bb1], Reason = [FailedScheduling], Message = [Insufficient resources.]
STEP: removing the label node off the node sc2-10-185-226-150.eng.vmware.com
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node sc2-10-185-226-233.eng.vmware.com
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node sc2-10-185-233-127.eng.vmware.com
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:23:43.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7782" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:32.397 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":296,"completed":208,"skipped":3751,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:23:43.868: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2530
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:130
[It] should retry creating failed daemon pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  6 00:23:44.642: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:44.642: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:44.642: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:44.666: INFO: Number of nodes with available pods: 0
Sep  6 00:23:44.666: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:23:45.678: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:45.678: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:45.678: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:45.687: INFO: Number of nodes with available pods: 0
Sep  6 00:23:45.687: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:23:46.718: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:46.718: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:46.718: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:46.728: INFO: Number of nodes with available pods: 0
Sep  6 00:23:46.728: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:23:47.682: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:47.682: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:47.682: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:47.692: INFO: Number of nodes with available pods: 0
Sep  6 00:23:47.692: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:23:48.678: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:48.678: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:48.678: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:48.685: INFO: Number of nodes with available pods: 0
Sep  6 00:23:48.685: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:23:49.680: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:49.681: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:49.681: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:49.689: INFO: Number of nodes with available pods: 0
Sep  6 00:23:49.689: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:23:50.783: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:50.783: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:50.783: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:50.883: INFO: Number of nodes with available pods: 0
Sep  6 00:23:50.883: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:23:51.740: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:51.740: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:51.740: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:51.767: INFO: Number of nodes with available pods: 0
Sep  6 00:23:51.767: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:23:52.680: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:52.680: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:52.680: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:52.688: INFO: Number of nodes with available pods: 0
Sep  6 00:23:52.688: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:23:53.679: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:53.679: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:53.679: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:53.687: INFO: Number of nodes with available pods: 0
Sep  6 00:23:53.687: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:23:54.686: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:54.686: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:54.686: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:54.695: INFO: Number of nodes with available pods: 0
Sep  6 00:23:54.695: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:23:55.706: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:55.706: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:55.706: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:55.713: INFO: Number of nodes with available pods: 0
Sep  6 00:23:55.713: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:23:56.683: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:56.683: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:56.683: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:56.696: INFO: Number of nodes with available pods: 0
Sep  6 00:23:56.696: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:23:57.684: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:57.684: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:57.684: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:57.692: INFO: Number of nodes with available pods: 0
Sep  6 00:23:57.692: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:23:58.691: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:58.691: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:58.691: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:58.702: INFO: Number of nodes with available pods: 0
Sep  6 00:23:58.702: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:23:59.697: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:59.697: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:59.697: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:23:59.727: INFO: Number of nodes with available pods: 0
Sep  6 00:23:59.727: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:00.678: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:00.678: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:00.679: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:00.734: INFO: Number of nodes with available pods: 0
Sep  6 00:24:00.734: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:01.678: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:01.678: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:01.678: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:01.688: INFO: Number of nodes with available pods: 0
Sep  6 00:24:01.688: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:02.681: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:02.681: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:02.681: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:02.693: INFO: Number of nodes with available pods: 0
Sep  6 00:24:02.693: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:03.681: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:03.681: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:03.681: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:03.690: INFO: Number of nodes with available pods: 0
Sep  6 00:24:03.690: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:04.679: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:04.679: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:04.679: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:04.689: INFO: Number of nodes with available pods: 0
Sep  6 00:24:04.689: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:05.706: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:05.706: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:05.706: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:05.723: INFO: Number of nodes with available pods: 0
Sep  6 00:24:05.723: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:06.681: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:06.682: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:06.682: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:06.691: INFO: Number of nodes with available pods: 0
Sep  6 00:24:06.691: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:07.683: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:07.683: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:07.683: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:07.700: INFO: Number of nodes with available pods: 0
Sep  6 00:24:07.700: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:08.683: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:08.683: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:08.683: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:08.701: INFO: Number of nodes with available pods: 0
Sep  6 00:24:08.701: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:09.681: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:09.681: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:09.681: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:09.700: INFO: Number of nodes with available pods: 0
Sep  6 00:24:09.700: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:10.681: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:10.681: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:10.681: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:10.688: INFO: Number of nodes with available pods: 0
Sep  6 00:24:10.688: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:11.683: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:11.683: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:11.683: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:11.691: INFO: Number of nodes with available pods: 0
Sep  6 00:24:11.691: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:12.680: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:12.680: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:12.680: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:12.693: INFO: Number of nodes with available pods: 0
Sep  6 00:24:12.693: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:13.711: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:13.711: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:13.711: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:13.722: INFO: Number of nodes with available pods: 0
Sep  6 00:24:13.722: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:14.717: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:14.718: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:14.718: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:14.751: INFO: Number of nodes with available pods: 0
Sep  6 00:24:14.751: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:15.685: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:15.685: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:15.685: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:15.710: INFO: Number of nodes with available pods: 0
Sep  6 00:24:15.710: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:16.678: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:16.678: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:16.678: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:16.689: INFO: Number of nodes with available pods: 2
Sep  6 00:24:16.689: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:17.698: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:17.698: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:17.698: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:17.714: INFO: Number of nodes with available pods: 2
Sep  6 00:24:17.714: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:18.679: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:18.679: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:18.679: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:18.691: INFO: Number of nodes with available pods: 3
Sep  6 00:24:18.691: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep  6 00:24:18.803: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:18.803: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:18.803: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:18.823: INFO: Number of nodes with available pods: 2
Sep  6 00:24:18.823: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:19.845: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:19.845: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:19.845: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:19.875: INFO: Number of nodes with available pods: 2
Sep  6 00:24:19.875: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:20.849: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:20.849: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:20.849: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:20.869: INFO: Number of nodes with available pods: 2
Sep  6 00:24:20.869: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:21.837: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:21.837: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:21.837: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:21.844: INFO: Number of nodes with available pods: 2
Sep  6 00:24:21.844: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:22.838: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:22.838: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:22.838: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:22.846: INFO: Number of nodes with available pods: 2
Sep  6 00:24:22.846: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:23.865: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:23.865: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:23.865: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:23.880: INFO: Number of nodes with available pods: 2
Sep  6 00:24:23.880: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:24.834: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:24.834: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:24.835: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:24.841: INFO: Number of nodes with available pods: 2
Sep  6 00:24:24.841: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:25.869: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:25.869: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:25.869: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:25.895: INFO: Number of nodes with available pods: 2
Sep  6 00:24:25.895: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:26.846: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:26.846: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:26.846: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:26.857: INFO: Number of nodes with available pods: 2
Sep  6 00:24:26.857: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:27.838: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:27.839: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:27.839: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:27.847: INFO: Number of nodes with available pods: 2
Sep  6 00:24:27.847: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:28.839: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:28.839: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:28.839: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:28.851: INFO: Number of nodes with available pods: 2
Sep  6 00:24:28.851: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:29.837: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:29.837: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:29.837: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:29.846: INFO: Number of nodes with available pods: 2
Sep  6 00:24:29.846: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:30.837: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:30.837: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:30.837: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:30.847: INFO: Number of nodes with available pods: 2
Sep  6 00:24:30.847: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:31.838: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:31.838: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:31.838: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:31.846: INFO: Number of nodes with available pods: 2
Sep  6 00:24:31.846: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:32.837: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:32.837: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:32.838: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:32.859: INFO: Number of nodes with available pods: 2
Sep  6 00:24:32.859: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:33.860: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:33.860: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:33.860: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:33.874: INFO: Number of nodes with available pods: 2
Sep  6 00:24:33.874: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:34.837: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:34.837: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:34.838: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:34.845: INFO: Number of nodes with available pods: 2
Sep  6 00:24:34.845: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:35.836: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:35.836: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:35.836: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:35.845: INFO: Number of nodes with available pods: 2
Sep  6 00:24:35.845: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:36.842: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:36.842: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:36.842: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:36.863: INFO: Number of nodes with available pods: 2
Sep  6 00:24:36.863: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:37.837: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:37.837: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:37.837: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:37.844: INFO: Number of nodes with available pods: 2
Sep  6 00:24:37.844: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:38.847: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:38.847: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:38.847: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:38.858: INFO: Number of nodes with available pods: 2
Sep  6 00:24:38.859: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:39.839: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:39.839: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:39.839: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:39.865: INFO: Number of nodes with available pods: 2
Sep  6 00:24:39.865: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:40.838: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:40.838: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:40.838: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:40.846: INFO: Number of nodes with available pods: 2
Sep  6 00:24:40.846: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:41.836: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:41.836: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:41.836: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:41.846: INFO: Number of nodes with available pods: 2
Sep  6 00:24:41.846: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:42.839: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:42.839: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:42.839: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:42.851: INFO: Number of nodes with available pods: 2
Sep  6 00:24:42.851: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:43.840: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:43.840: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:43.840: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:43.848: INFO: Number of nodes with available pods: 2
Sep  6 00:24:43.848: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:44.845: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:44.845: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:44.845: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:44.859: INFO: Number of nodes with available pods: 2
Sep  6 00:24:44.859: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:45.838: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:45.838: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:45.838: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:45.845: INFO: Number of nodes with available pods: 2
Sep  6 00:24:45.845: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:46.835: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:46.835: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:46.835: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:46.844: INFO: Number of nodes with available pods: 2
Sep  6 00:24:46.844: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:47.836: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:47.836: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:47.836: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:47.845: INFO: Number of nodes with available pods: 2
Sep  6 00:24:47.845: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:48.839: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:48.839: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:48.839: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:48.845: INFO: Number of nodes with available pods: 2
Sep  6 00:24:48.845: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:49.835: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:49.835: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:49.835: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:49.844: INFO: Number of nodes with available pods: 2
Sep  6 00:24:49.844: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:50.836: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:50.836: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:50.836: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:50.845: INFO: Number of nodes with available pods: 2
Sep  6 00:24:50.845: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:51.835: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:51.835: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:51.835: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:51.844: INFO: Number of nodes with available pods: 2
Sep  6 00:24:51.844: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:52.842: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:52.842: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:52.843: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:52.856: INFO: Number of nodes with available pods: 2
Sep  6 00:24:52.857: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:53.842: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:53.842: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:53.842: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:53.850: INFO: Number of nodes with available pods: 2
Sep  6 00:24:53.850: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:54.835: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:54.835: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:54.835: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:54.845: INFO: Number of nodes with available pods: 2
Sep  6 00:24:54.845: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:55.841: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:55.841: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:55.841: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:55.849: INFO: Number of nodes with available pods: 2
Sep  6 00:24:55.849: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:56.834: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:56.834: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:56.834: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:56.846: INFO: Number of nodes with available pods: 2
Sep  6 00:24:56.846: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:57.872: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:57.872: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:57.872: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:57.898: INFO: Number of nodes with available pods: 2
Sep  6 00:24:57.898: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:58.835: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:58.835: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:58.835: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:58.844: INFO: Number of nodes with available pods: 2
Sep  6 00:24:58.844: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:24:59.839: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:59.839: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:59.839: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:24:59.849: INFO: Number of nodes with available pods: 2
Sep  6 00:24:59.849: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:25:00.840: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:25:00.840: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:25:00.840: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:25:00.847: INFO: Number of nodes with available pods: 2
Sep  6 00:25:00.847: INFO: Node sc2-10-185-226-233.eng.vmware.com is running more than one daemon pod
Sep  6 00:25:01.838: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:25:01.838: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:25:01.838: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:25:01.845: INFO: Number of nodes with available pods: 3
Sep  6 00:25:01.845: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:96
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2530, will wait for the garbage collector to delete the pods
Sep  6 00:25:01.955: INFO: Deleting DaemonSet.extensions daemon-set took: 26.807665ms
Sep  6 00:25:04.156: INFO: Terminating DaemonSet.extensions daemon-set pods took: 2.200858061s
Sep  6 00:25:22.680: INFO: Number of nodes with available pods: 0
Sep  6 00:25:22.680: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 00:25:22.685: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2530/daemonsets","resourceVersion":"188832"},"items":null}

Sep  6 00:25:22.691: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2530/pods","resourceVersion":"188832"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:25:22.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2530" for this suite.

• [SLOW TEST:99.605 seconds]
[sig-apps] Daemon set [Serial]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":296,"completed":209,"skipped":3755,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-apps] Deployment 
  should run the lifecycle of a Deployment [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:25:23.473: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1202
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] should run the lifecycle of a Deployment [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Deployment
STEP: waiting for Deployment to be created
STEP: waiting for all Replicas to be Ready
Sep  6 00:25:24.095: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  6 00:25:24.095: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  6 00:25:24.174: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  6 00:25:24.174: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  6 00:25:24.249: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  6 00:25:24.249: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  6 00:25:24.334: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  6 00:25:24.334: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Sep  6 00:25:52.103: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep  6 00:25:52.103: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Sep  6 00:25:52.157: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment
Sep  6 00:25:52.230: INFO: observed event type ADDED
STEP: waiting for Replicas to scale
Sep  6 00:25:52.239: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 0
Sep  6 00:25:52.239: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 0
Sep  6 00:25:52.239: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 0
Sep  6 00:25:52.239: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 0
Sep  6 00:25:52.240: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 0
Sep  6 00:25:52.240: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 0
Sep  6 00:25:52.240: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 0
Sep  6 00:25:52.240: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 0
Sep  6 00:25:52.240: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1
Sep  6 00:25:52.240: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1
Sep  6 00:25:52.240: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 2
Sep  6 00:25:52.240: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 2
Sep  6 00:25:52.240: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 2
Sep  6 00:25:52.240: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 2
Sep  6 00:25:52.302: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 2
Sep  6 00:25:52.302: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 2
Sep  6 00:25:52.396: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 2
Sep  6 00:25:52.396: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 2
Sep  6 00:25:52.462: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1
STEP: listing Deployments
Sep  6 00:25:52.526: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment
Sep  6 00:25:52.614: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1
STEP: fetching the DeploymentStatus
Sep  6 00:25:52.665: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1 and labels map[test-deployment:patched test-deployment-static:true]
Sep  6 00:25:52.665: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  6 00:25:52.766: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  6 00:25:52.869: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  6 00:25:52.970: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  6 00:25:53.086: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  6 00:25:53.135: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  6 00:25:53.236: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Sep  6 00:25:53.336: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus
STEP: fetching the DeploymentStatus
Sep  6 00:26:32.545: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1
Sep  6 00:26:32.545: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1
Sep  6 00:26:32.546: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1
Sep  6 00:26:32.546: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1
Sep  6 00:26:32.546: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1
Sep  6 00:26:32.546: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1
Sep  6 00:26:32.546: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1
Sep  6 00:26:32.546: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1
Sep  6 00:26:32.546: INFO: observed Deployment test-deployment in namespace deployment-1202 with ReadyReplicas 1
STEP: deleting the Deployment
Sep  6 00:26:32.725: INFO: observed event type MODIFIED
Sep  6 00:26:32.725: INFO: observed event type MODIFIED
Sep  6 00:26:32.725: INFO: observed event type MODIFIED
Sep  6 00:26:32.725: INFO: observed event type MODIFIED
Sep  6 00:26:32.726: INFO: observed event type MODIFIED
Sep  6 00:26:32.726: INFO: observed event type MODIFIED
Sep  6 00:26:32.726: INFO: observed event type MODIFIED
Sep  6 00:26:32.726: INFO: observed event type MODIFIED
Sep  6 00:26:32.726: INFO: observed event type MODIFIED
Sep  6 00:26:32.726: INFO: observed event type MODIFIED
Sep  6 00:26:32.726: INFO: observed event type MODIFIED
Sep  6 00:26:32.726: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Sep  6 00:26:32.748: INFO: Log out all the ReplicaSets if there is no deployment created
Sep  6 00:26:32.783: INFO: ReplicaSet "test-deployment-7664fb7d98":
&ReplicaSet{ObjectMeta:{test-deployment-7664fb7d98  deployment-1202 /apis/apps/v1/namespaces/deployment-1202/replicasets/test-deployment-7664fb7d98 0a7ecda2-9859-4304-a78c-41116fbda5ca 189769 3 2021-09-06 00:25:52 -0700 PDT <nil> <nil> map[pod-template-hash:7664fb7d98 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment a3e2a11e-304a-42a3-8a23-f8f65c9865cb 0xc003abf217 0xc003abf218}] []  [{kube-controller-manager Update apps/v1 2021-09-06 00:26:32 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e2a11e-304a-42a3-8a23-f8f65c9865cb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7664fb7d98,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7664fb7d98 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003abf280 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:3,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Sep  6 00:26:32.813: INFO: pod: "test-deployment-7664fb7d98-7fkkb":
&Pod{ObjectMeta:{test-deployment-7664fb7d98-7fkkb test-deployment-7664fb7d98- deployment-1202 /api/v1/namespaces/deployment-1202/pods/test-deployment-7664fb7d98-7fkkb ef78529a-7938-4dc7-a229-7c5e050850cb 189739 0 2021-09-06 00:25:53 -0700 PDT <nil> <nil> map[pod-template-hash:7664fb7d98 test-deployment-static:true] map[attachment_id:9ee71800-b024-4d4b-bf0d-976a2753ca88 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:00:21 vlan:None vmware-system-ephemeral-disk-uuid:6000C292-f604-9cb6-1fcb-e1e087403b0c vmware-system-image-references:{"test-deployment":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v75293"} vmware-system-vm-moid:vm-1252:3f0e15ce-7291-45c7-8a72-39d27d06e017 vmware-system-vm-uuid:50168afa-ac76-8d21-3dce-073a59be1803] [{apps/v1 ReplicaSet test-deployment-7664fb7d98 0a7ecda2-9859-4304-a78c-41116fbda5ca 0xc003abf687 0xc003abf688}] [lifecycle-controller/system.vmware.com]  [{image-controller Update v1 2021-09-06 00:25:53 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {kube-controller-manager Update v1 2021-09-06 00:25:53 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a7ecda2-9859-4304-a78c-41116fbda5ca\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-df5sk Update v1 2021-09-06 00:25:56 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-06 00:26:23 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:26:32 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.214\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5s4g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5s4g,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5s4g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-150.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:25:53 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:26:31 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:26:31 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:26:31 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.185.226.150,PodIP:172.26.1.214,StartTime:2021-09-06 00:26:27 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:26:29 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v75293,ContainerID:4da69bc0-534e-4cd5-969c-331343b37316,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.214,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  6 00:26:32.813: INFO: pod: "test-deployment-7664fb7d98-ggmkb":
&Pod{ObjectMeta:{test-deployment-7664fb7d98-ggmkb test-deployment-7664fb7d98- deployment-1202 /api/v1/namespaces/deployment-1202/pods/test-deployment-7664fb7d98-ggmkb 41a90d02-03fd-4732-b2d9-6e3a7f834451 189771 0 2021-09-06 00:26:32 -0700 PDT <nil> <nil> map[pod-template-hash:7664fb7d98 test-deployment-static:true] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-deployment-7664fb7d98 0a7ecda2-9859-4304-a78c-41116fbda5ca 0xc003abf877 0xc003abf878}] []  [{kube-controller-manager Update v1 2021-09-06 00:26:32 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a7ecda2-9859-4304-a78c-41116fbda5ca\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5s4g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5s4g,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5s4g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-233.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:26:32 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  6 00:26:32.813: INFO: ReplicaSet "test-deployment-7c65d4bcf9":
&ReplicaSet{ObjectMeta:{test-deployment-7c65d4bcf9  deployment-1202 /apis/apps/v1/namespaces/deployment-1202/replicasets/test-deployment-7c65d4bcf9 fb4a7e02-ec61-42bd-acec-108afb2e7bd6 189768 4 2021-09-06 00:25:52 -0700 PDT <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment a3e2a11e-304a-42a3-8a23-f8f65c9865cb 0xc003abf2e7 0xc003abf2e8}] []  [{kube-controller-manager Update apps/v1 2021-09-06 00:26:32 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e2a11e-304a-42a3-8a23-f8f65c9865cb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c65d4bcf9,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/pause:3.2 [/bin/sleep 100000] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003abf368 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Sep  6 00:26:32.846: INFO: pod: "test-deployment-7c65d4bcf9-sbjqp":
&Pod{ObjectMeta:{test-deployment-7c65d4bcf9-sbjqp test-deployment-7c65d4bcf9- deployment-1202 /api/v1/namespaces/deployment-1202/pods/test-deployment-7c65d4bcf9-sbjqp b23d8ae3-77e0-4e70-98ba-6881c16436e6 189738 0 2021-09-06 00:25:52 -0700 PDT <nil> <nil> map[pod-template-hash:7c65d4bcf9 test-deployment-static:true] map[attachment_id:ecfeba99-c452-4615-924d-41b8da2a1ae9 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:00:1e vlan:None vmware-system-ephemeral-disk-uuid:6000C295-d7a4-82b3-c811-3e62ad3321eb vmware-system-image-references:{"test-deployment":"pause-54467452d21a00c28aa4cce37d0850173b528ca6-v72990"} vmware-system-vm-moid:vm-1250:3f0e15ce-7291-45c7-8a72-39d27d06e017 vmware-system-vm-uuid:50161d74-8416-c6b2-6476-b94b2724ce0a] [{apps/v1 ReplicaSet test-deployment-7c65d4bcf9 fb4a7e02-ec61-42bd-acec-108afb2e7bd6 0xc002f58707 0xc002f58708}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:25:52 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fb4a7e02-ec61-42bd-acec-108afb2e7bd6\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:command":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-06 00:25:53 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {nsx-ncp-6d7f7bf559-df5sk Update v1 2021-09-06 00:25:55 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-06 00:26:22 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:26:32 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:message":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.213\"}":{".":{},"f:ip":{}}},"f:reason":{},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5s4g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5s4g,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/pause:3.2,Command:[/bin/sleep 100000],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5s4g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-150.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Failed,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:25:52 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:26:32 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:26:32 -0700 PDT,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:26:32 -0700 PDT,Reason:ContainersNotReady,Message:containers with unready status: [test-deployment],},},Message:error running container test-deployment: container_linux.go:370: starting container process caused: exec: "/bin/sleep": stat /bin/sleep: no such file or directory,Reason:error running container test-deployment: container_linux.go:370: starting container process caused: exec: "/bin/sleep": stat /bin/sleep: no such file or directory,HostIP:10.185.226.150,PodIP:172.26.1.213,StartTime:2021-09-06 00:26:28 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:k8s.gcr.io/pause:3.2,ImageID:pause-54467452d21a00c28aa4cce37d0850173b528ca6-v72990,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.213,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Sep  6 00:26:32.846: INFO: ReplicaSet "test-deployment-8b6954bfb":
&ReplicaSet{ObjectMeta:{test-deployment-8b6954bfb  deployment-1202 /apis/apps/v1/namespaces/deployment-1202/replicasets/test-deployment-8b6954bfb 3cb3055f-135e-45b0-98ae-9422bacee4bf 189258 2 2021-09-06 00:25:24 -0700 PDT <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment a3e2a11e-304a-42a3-8a23-f8f65c9865cb 0xc003abf3c7 0xc003abf3c8}] []  [{kube-controller-manager Update apps/v1 2021-09-06 00:25:52 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e2a11e-304a-42a3-8a23-f8f65c9865cb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8b6954bfb,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[] [] []  []} {[] [] [{test-deployment k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003abf430 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}

Sep  6 00:26:32.866: INFO: pod: "test-deployment-8b6954bfb-7mrmf":
&Pod{ObjectMeta:{test-deployment-8b6954bfb-7mrmf test-deployment-8b6954bfb- deployment-1202 /api/v1/namespaces/deployment-1202/pods/test-deployment-8b6954bfb-7mrmf 35a7e858-f287-40c7-bb63-836cf93d54c2 189227 0 2021-09-06 00:25:24 -0700 PDT <nil> <nil> map[pod-template-hash:8b6954bfb test-deployment-static:true] map[attachment_id:96700c2f-d79d-45e8-86f6-0f35670029a8 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:00:0f vlan:None vmware-system-ephemeral-disk-uuid:6000C291-195d-90b5-953e-fd7095794d35 vmware-system-image-references:{"test-deployment":"agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v48900"} vmware-system-vm-moid:vm-1248:3f0e15ce-7291-45c7-8a72-39d27d06e017 vmware-system-vm-uuid:501601b0-51b0-93a9-1550-dbef78c5e58f] [{apps/v1 ReplicaSet test-deployment-8b6954bfb 3cb3055f-135e-45b0-98ae-9422bacee4bf 0xc008a5c847 0xc008a5c848}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:25:24 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3cb3055f-135e-45b0-98ae-9422bacee4bf\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-df5sk Update v1 2021-09-06 00:25:33 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {image-controller Update v1 2021-09-06 00:25:37 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {scheduler-extender Update v1 2021-09-06 00:25:45 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:25:52 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.211\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-g5s4g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-g5s4g,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-g5s4g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-233.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:25:24 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:25:51 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:25:51 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:25:51 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.185.226.233,PodIP:172.26.1.211,StartTime:2021-09-06 00:25:49 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:25:51 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v48900,ContainerID:23e25eeb-2529-47d1-8877-8a9c411a5480,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:26:32.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1202" for this suite.

• [SLOW TEST:70.031 seconds]
[sig-apps] Deployment
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","total":296,"completed":210,"skipped":3756,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:26:33.504: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5304
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 00:26:35.215: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  6 00:26:37.259: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:26:39.278: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:26:41.272: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:26:43.280: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:26:45.271: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:26:47.295: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:26:49.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:26:51.271: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:26:53.270: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:26:55.268: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:26:57.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:26:59.276: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:27:01.272: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:27:03.278: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:27:05.267: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:27:07.270: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:27:09.271: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 26, 35, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 00:27:12.308: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:27:13.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5304" for this suite.
STEP: Destroying namespace "webhook-5304-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:40.808 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":296,"completed":211,"skipped":3763,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:27:14.313: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2037
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should update labels on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Sep  6 00:27:45.396: INFO: Successfully updated pod "labelsupdatede71096d-dfd1-464a-aed1-7718d020132c"
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:27:47.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2037" for this suite.

• [SLOW TEST:33.459 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":296,"completed":212,"skipped":3783,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:27:47.772: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3791
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service in namespace services-3791
STEP: creating service affinity-clusterip in namespace services-3791
STEP: creating replication controller affinity-clusterip in namespace services-3791
Sep  6 00:28:30.547: INFO: Creating new exec pod
Sep  6 00:28:55.613: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=services-3791 exec execpod-affinityfwcg6 -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Sep  6 00:28:56.654: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Sep  6 00:28:56.654: INFO: stdout: ""
Sep  6 00:28:56.655: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=services-3791 exec execpod-affinityfwcg6 -- /bin/sh -x -c nc -zv -t -w 2 172.24.75.7 80'
Sep  6 00:28:56.904: INFO: stderr: "+ nc -zv -t -w 2 172.24.75.7 80\nConnection to 172.24.75.7 80 port [tcp/http] succeeded!\n"
Sep  6 00:28:56.904: INFO: stdout: ""
Sep  6 00:28:56.904: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=services-3791 exec execpod-affinityfwcg6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.24.75.7:80/ ; done'
Sep  6 00:28:57.569: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.75.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.75.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.75.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.75.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.75.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.75.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.75.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.75.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.75.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.75.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.75.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.75.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.75.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.75.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.75.7:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.75.7:80/\n"
Sep  6 00:28:57.569: INFO: stdout: "\naffinity-clusterip-n6kfg\naffinity-clusterip-n6kfg\naffinity-clusterip-n6kfg\naffinity-clusterip-n6kfg\naffinity-clusterip-n6kfg\naffinity-clusterip-n6kfg\naffinity-clusterip-n6kfg\naffinity-clusterip-n6kfg\naffinity-clusterip-n6kfg\naffinity-clusterip-n6kfg\naffinity-clusterip-n6kfg\naffinity-clusterip-n6kfg\naffinity-clusterip-n6kfg\naffinity-clusterip-n6kfg\naffinity-clusterip-n6kfg\naffinity-clusterip-n6kfg"
Sep  6 00:28:57.569: INFO: Received response from host: affinity-clusterip-n6kfg
Sep  6 00:28:57.569: INFO: Received response from host: affinity-clusterip-n6kfg
Sep  6 00:28:57.569: INFO: Received response from host: affinity-clusterip-n6kfg
Sep  6 00:28:57.569: INFO: Received response from host: affinity-clusterip-n6kfg
Sep  6 00:28:57.569: INFO: Received response from host: affinity-clusterip-n6kfg
Sep  6 00:28:57.569: INFO: Received response from host: affinity-clusterip-n6kfg
Sep  6 00:28:57.569: INFO: Received response from host: affinity-clusterip-n6kfg
Sep  6 00:28:57.569: INFO: Received response from host: affinity-clusterip-n6kfg
Sep  6 00:28:57.569: INFO: Received response from host: affinity-clusterip-n6kfg
Sep  6 00:28:57.569: INFO: Received response from host: affinity-clusterip-n6kfg
Sep  6 00:28:57.569: INFO: Received response from host: affinity-clusterip-n6kfg
Sep  6 00:28:57.569: INFO: Received response from host: affinity-clusterip-n6kfg
Sep  6 00:28:57.569: INFO: Received response from host: affinity-clusterip-n6kfg
Sep  6 00:28:57.569: INFO: Received response from host: affinity-clusterip-n6kfg
Sep  6 00:28:57.569: INFO: Received response from host: affinity-clusterip-n6kfg
Sep  6 00:28:57.569: INFO: Received response from host: affinity-clusterip-n6kfg
Sep  6 00:28:57.569: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-3791, will wait for the garbage collector to delete the pods
Sep  6 00:28:57.771: INFO: Deleting ReplicationController affinity-clusterip took: 61.400928ms
Sep  6 00:29:00.072: INFO: Terminating ReplicationController affinity-clusterip pods took: 2.300982229s
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:29:14.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3791" for this suite.
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:87.150 seconds]
[sig-network] Services
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":296,"completed":213,"skipped":3784,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:29:14.922: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5618
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should find a service from listing all namespaces [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching services
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:29:15.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5618" for this suite.
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749
•{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":296,"completed":214,"skipped":3790,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:29:15.576: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3401
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:29:16.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3401" for this suite.
•{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":296,"completed":215,"skipped":3816,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:29:16.395: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-4164
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  6 00:29:16.829: INFO: >>> kubeConfig: ./kconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:29:26.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4164" for this suite.

• [SLOW TEST:10.651 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":296,"completed":216,"skipped":3848,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:29:27.047: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3388
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-configmap-r6gl
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 00:29:28.427: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-r6gl" in namespace "subpath-3388" to be "Succeeded or Failed"
Sep  6 00:29:28.527: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Pending", Reason="", readiness=false. Elapsed: 99.920496ms
Sep  6 00:29:30.579: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.152669257s
Sep  6 00:29:32.618: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Pending", Reason="", readiness=false. Elapsed: 4.191333726s
Sep  6 00:29:34.691: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Pending", Reason="", readiness=false. Elapsed: 6.264375948s
Sep  6 00:29:36.728: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Pending", Reason="", readiness=false. Elapsed: 8.301466209s
Sep  6 00:29:38.812: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Pending", Reason="", readiness=false. Elapsed: 10.385063322s
Sep  6 00:29:40.842: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Pending", Reason="", readiness=false. Elapsed: 12.415009093s
Sep  6 00:29:42.858: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Pending", Reason="", readiness=false. Elapsed: 14.431001319s
Sep  6 00:29:44.874: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Pending", Reason="", readiness=false. Elapsed: 16.447022253s
Sep  6 00:29:46.886: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Pending", Reason="", readiness=false. Elapsed: 18.458961476s
Sep  6 00:29:48.892: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Pending", Reason="", readiness=false. Elapsed: 20.465696759s
Sep  6 00:29:50.904: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Pending", Reason="", readiness=false. Elapsed: 22.477770241s
Sep  6 00:29:52.913: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Pending", Reason="", readiness=false. Elapsed: 24.486169214s
Sep  6 00:29:54.922: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Pending", Reason="", readiness=false. Elapsed: 26.495472355s
Sep  6 00:29:56.934: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Pending", Reason="", readiness=false. Elapsed: 28.507569576s
Sep  6 00:29:58.948: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Running", Reason="", readiness=true. Elapsed: 30.520946241s
Sep  6 00:30:00.961: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Running", Reason="", readiness=true. Elapsed: 32.534638722s
Sep  6 00:30:02.987: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Running", Reason="", readiness=true. Elapsed: 34.560232907s
Sep  6 00:30:04.994: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Running", Reason="", readiness=true. Elapsed: 36.567838072s
Sep  6 00:30:07.011: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Running", Reason="", readiness=true. Elapsed: 38.584506585s
Sep  6 00:30:09.027: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Running", Reason="", readiness=true. Elapsed: 40.600145474s
Sep  6 00:30:11.038: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Running", Reason="", readiness=true. Elapsed: 42.610989463s
Sep  6 00:30:13.052: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Running", Reason="", readiness=true. Elapsed: 44.625514127s
Sep  6 00:30:15.077: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Running", Reason="", readiness=true. Elapsed: 46.650768183s
Sep  6 00:30:17.088: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Running", Reason="", readiness=true. Elapsed: 48.661075286s
Sep  6 00:30:19.098: INFO: Pod "pod-subpath-test-configmap-r6gl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 50.671309182s
STEP: Saw pod success
Sep  6 00:30:19.098: INFO: Pod "pod-subpath-test-configmap-r6gl" satisfied condition "Succeeded or Failed"
Sep  6 00:30:19.104: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-subpath-test-configmap-r6gl container test-container-subpath-configmap-r6gl: <nil>
STEP: delete the pod
Sep  6 00:30:25.309: INFO: Waiting for pod pod-subpath-test-configmap-r6gl to disappear
Sep  6 00:30:25.324: INFO: Pod pod-subpath-test-configmap-r6gl still exists
Sep  6 00:30:27.324: INFO: Waiting for pod pod-subpath-test-configmap-r6gl to disappear
Sep  6 00:30:27.335: INFO: Pod pod-subpath-test-configmap-r6gl still exists
Sep  6 00:30:29.324: INFO: Waiting for pod pod-subpath-test-configmap-r6gl to disappear
Sep  6 00:30:29.342: INFO: Pod pod-subpath-test-configmap-r6gl still exists
Sep  6 00:30:31.325: INFO: Waiting for pod pod-subpath-test-configmap-r6gl to disappear
Sep  6 00:30:31.335: INFO: Pod pod-subpath-test-configmap-r6gl still exists
Sep  6 00:30:33.324: INFO: Waiting for pod pod-subpath-test-configmap-r6gl to disappear
Sep  6 00:30:33.346: INFO: Pod pod-subpath-test-configmap-r6gl still exists
Sep  6 00:30:35.325: INFO: Waiting for pod pod-subpath-test-configmap-r6gl to disappear
Sep  6 00:30:35.336: INFO: Pod pod-subpath-test-configmap-r6gl no longer exists
STEP: Deleting pod pod-subpath-test-configmap-r6gl
Sep  6 00:30:35.336: INFO: Deleting pod "pod-subpath-test-configmap-r6gl" in namespace "subpath-3388"
[AfterEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:30:35.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3388" for this suite.

• [SLOW TEST:68.582 seconds]
[sig-storage] Subpath
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":296,"completed":217,"skipped":3871,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:30:35.630: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7867
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:30:36.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7867" for this suite.
•{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":296,"completed":218,"skipped":3898,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Job
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:30:36.464: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-5005
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:32:00.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5005" for this suite.

• [SLOW TEST:84.760 seconds]
[sig-apps] Job
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":296,"completed":219,"skipped":3901,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:32:01.224: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6145
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:32:13.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6145" for this suite.

• [SLOW TEST:12.892 seconds]
[sig-api-machinery] ResourceQuota
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":296,"completed":220,"skipped":3909,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:32:14.116: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2241
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  6 00:33:10.827: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 00:33:10.838: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 00:33:12.838: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 00:33:12.868: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 00:33:14.839: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 00:33:14.858: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 00:33:16.838: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 00:33:16.854: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 00:33:18.838: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 00:33:18.851: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 00:33:20.839: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 00:33:20.878: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 00:33:22.838: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 00:33:22.860: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 00:33:24.838: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 00:33:24.872: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 00:33:26.838: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 00:33:26.851: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  6 00:33:28.838: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  6 00:33:28.852: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:33:28.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2241" for this suite.

• [SLOW TEST:74.996 seconds]
[k8s.io] Container Lifecycle Hook
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":296,"completed":221,"skipped":3909,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:33:29.112: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2755
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-27a31df9-73e5-4a67-90d8-99d4fe80435c
STEP: Creating a pod to test consume configMaps
Sep  6 00:33:29.726: INFO: Waiting up to 5m0s for pod "pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac" in namespace "configmap-2755" to be "Succeeded or Failed"
Sep  6 00:33:29.753: INFO: Pod "pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac": Phase="Pending", Reason="", readiness=false. Elapsed: 27.204804ms
Sep  6 00:33:31.771: INFO: Pod "pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045219974s
Sep  6 00:33:33.788: INFO: Pod "pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061960193s
Sep  6 00:33:35.808: INFO: Pod "pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac": Phase="Pending", Reason="", readiness=false. Elapsed: 6.08168148s
Sep  6 00:33:37.825: INFO: Pod "pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac": Phase="Pending", Reason="", readiness=false. Elapsed: 8.099256461s
Sep  6 00:33:39.838: INFO: Pod "pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac": Phase="Pending", Reason="", readiness=false. Elapsed: 10.112369843s
Sep  6 00:33:41.854: INFO: Pod "pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac": Phase="Pending", Reason="", readiness=false. Elapsed: 12.127931033s
Sep  6 00:33:43.873: INFO: Pod "pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac": Phase="Pending", Reason="", readiness=false. Elapsed: 14.147174871s
Sep  6 00:33:45.896: INFO: Pod "pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac": Phase="Pending", Reason="", readiness=false. Elapsed: 16.170120412s
Sep  6 00:33:47.907: INFO: Pod "pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac": Phase="Pending", Reason="", readiness=false. Elapsed: 18.180672553s
Sep  6 00:33:49.921: INFO: Pod "pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac": Phase="Pending", Reason="", readiness=false. Elapsed: 20.195625711s
Sep  6 00:33:51.944: INFO: Pod "pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac": Phase="Pending", Reason="", readiness=false. Elapsed: 22.218564348s
Sep  6 00:33:53.965: INFO: Pod "pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac": Phase="Pending", Reason="", readiness=false. Elapsed: 24.23879281s
Sep  6 00:33:55.978: INFO: Pod "pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac": Phase="Pending", Reason="", readiness=false. Elapsed: 26.25211281s
Sep  6 00:33:57.992: INFO: Pod "pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac": Phase="Pending", Reason="", readiness=false. Elapsed: 28.26621381s
Sep  6 00:34:00.012: INFO: Pod "pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.285957863s
STEP: Saw pod success
Sep  6 00:34:00.012: INFO: Pod "pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac" satisfied condition "Succeeded or Failed"
Sep  6 00:34:00.025: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 00:34:00.129: INFO: Waiting for pod pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac to disappear
Sep  6 00:34:00.145: INFO: Pod pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac still exists
Sep  6 00:34:02.145: INFO: Waiting for pod pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac to disappear
Sep  6 00:34:02.163: INFO: Pod pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac still exists
Sep  6 00:34:04.147: INFO: Waiting for pod pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac to disappear
Sep  6 00:34:04.167: INFO: Pod pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac still exists
Sep  6 00:34:06.146: INFO: Waiting for pod pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac to disappear
Sep  6 00:34:06.155: INFO: Pod pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac still exists
Sep  6 00:34:08.146: INFO: Waiting for pod pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac to disappear
Sep  6 00:34:08.164: INFO: Pod pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac still exists
Sep  6 00:34:10.147: INFO: Waiting for pod pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac to disappear
Sep  6 00:34:10.158: INFO: Pod pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac still exists
Sep  6 00:34:12.146: INFO: Waiting for pod pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac to disappear
Sep  6 00:34:12.156: INFO: Pod pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac still exists
Sep  6 00:34:14.146: INFO: Waiting for pod pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac to disappear
Sep  6 00:34:14.168: INFO: Pod pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac still exists
Sep  6 00:34:16.145: INFO: Waiting for pod pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac to disappear
Sep  6 00:34:16.162: INFO: Pod pod-configmaps-e51332a0-fca8-4e8e-a4e1-0e7850917aac no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:34:16.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2755" for this suite.

• [SLOW TEST:47.278 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":296,"completed":222,"skipped":3932,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:34:16.391: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5595
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep  6 00:34:16.861: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91" in namespace "projected-5595" to be "Succeeded or Failed"
Sep  6 00:34:16.874: INFO: Pod "downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91": Phase="Pending", Reason="", readiness=false. Elapsed: 13.012002ms
Sep  6 00:34:18.885: INFO: Pod "downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024108626s
Sep  6 00:34:20.899: INFO: Pod "downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038624843s
Sep  6 00:34:22.931: INFO: Pod "downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070544861s
Sep  6 00:34:24.947: INFO: Pod "downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91": Phase="Pending", Reason="", readiness=false. Elapsed: 8.08574136s
Sep  6 00:34:26.955: INFO: Pod "downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91": Phase="Pending", Reason="", readiness=false. Elapsed: 10.094257519s
Sep  6 00:34:28.972: INFO: Pod "downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91": Phase="Pending", Reason="", readiness=false. Elapsed: 12.111118236s
Sep  6 00:34:30.984: INFO: Pod "downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91": Phase="Pending", Reason="", readiness=false. Elapsed: 14.123289662s
Sep  6 00:34:32.999: INFO: Pod "downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91": Phase="Pending", Reason="", readiness=false. Elapsed: 16.138234515s
Sep  6 00:34:35.017: INFO: Pod "downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91": Phase="Pending", Reason="", readiness=false. Elapsed: 18.156411004s
Sep  6 00:34:37.026: INFO: Pod "downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91": Phase="Pending", Reason="", readiness=false. Elapsed: 20.164804904s
Sep  6 00:34:39.052: INFO: Pod "downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91": Phase="Pending", Reason="", readiness=false. Elapsed: 22.191107233s
Sep  6 00:34:41.067: INFO: Pod "downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91": Phase="Pending", Reason="", readiness=false. Elapsed: 24.20617309s
Sep  6 00:34:43.082: INFO: Pod "downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91": Phase="Pending", Reason="", readiness=false. Elapsed: 26.220773164s
Sep  6 00:34:45.099: INFO: Pod "downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.23823487s
STEP: Saw pod success
Sep  6 00:34:45.099: INFO: Pod "downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91" satisfied condition "Succeeded or Failed"
Sep  6 00:34:45.107: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91 container client-container: <nil>
STEP: delete the pod
Sep  6 00:34:45.175: INFO: Waiting for pod downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91 to disappear
Sep  6 00:34:45.193: INFO: Pod downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91 still exists
Sep  6 00:34:47.194: INFO: Waiting for pod downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91 to disappear
Sep  6 00:34:47.205: INFO: Pod downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91 still exists
Sep  6 00:34:49.195: INFO: Waiting for pod downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91 to disappear
Sep  6 00:34:49.220: INFO: Pod downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91 still exists
Sep  6 00:34:51.194: INFO: Waiting for pod downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91 to disappear
Sep  6 00:34:51.206: INFO: Pod downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91 still exists
Sep  6 00:34:53.195: INFO: Waiting for pod downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91 to disappear
Sep  6 00:34:53.210: INFO: Pod downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91 still exists
Sep  6 00:34:55.195: INFO: Waiting for pod downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91 to disappear
Sep  6 00:34:55.227: INFO: Pod downwardapi-volume-ccea3fe8-0c46-4b5b-8fc6-ce1e93d41d91 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:34:55.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5595" for this suite.

• [SLOW TEST:39.135 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":296,"completed":223,"skipped":4036,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:34:55.526: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5836
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-84d0170a-e834-4c7b-ac2e-6d37842b5418
STEP: Creating a pod to test consume configMaps
Sep  6 00:34:56.052: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08" in namespace "projected-5836" to be "Succeeded or Failed"
Sep  6 00:34:56.069: INFO: Pod "pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08": Phase="Pending", Reason="", readiness=false. Elapsed: 17.253415ms
Sep  6 00:34:58.085: INFO: Pod "pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033240912s
Sep  6 00:35:00.099: INFO: Pod "pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047122901s
Sep  6 00:35:02.123: INFO: Pod "pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070663861s
Sep  6 00:35:04.139: INFO: Pod "pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08": Phase="Pending", Reason="", readiness=false. Elapsed: 8.086685659s
Sep  6 00:35:06.148: INFO: Pod "pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08": Phase="Pending", Reason="", readiness=false. Elapsed: 10.096313007s
Sep  6 00:35:08.164: INFO: Pod "pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08": Phase="Pending", Reason="", readiness=false. Elapsed: 12.111813113s
Sep  6 00:35:10.180: INFO: Pod "pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08": Phase="Pending", Reason="", readiness=false. Elapsed: 14.12849442s
Sep  6 00:35:12.195: INFO: Pod "pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08": Phase="Pending", Reason="", readiness=false. Elapsed: 16.142758791s
Sep  6 00:35:14.220: INFO: Pod "pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08": Phase="Pending", Reason="", readiness=false. Elapsed: 18.168480543s
Sep  6 00:35:16.231: INFO: Pod "pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08": Phase="Pending", Reason="", readiness=false. Elapsed: 20.17914735s
Sep  6 00:35:18.243: INFO: Pod "pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08": Phase="Pending", Reason="", readiness=false. Elapsed: 22.191183858s
Sep  6 00:35:20.276: INFO: Pod "pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08": Phase="Pending", Reason="", readiness=false. Elapsed: 24.224349608s
Sep  6 00:35:22.288: INFO: Pod "pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08": Phase="Pending", Reason="", readiness=false. Elapsed: 26.235662126s
Sep  6 00:35:24.302: INFO: Pod "pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08": Phase="Pending", Reason="", readiness=false. Elapsed: 28.249812138s
Sep  6 00:35:26.317: INFO: Pod "pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.264623403s
STEP: Saw pod success
Sep  6 00:35:26.317: INFO: Pod "pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08" satisfied condition "Succeeded or Failed"
Sep  6 00:35:26.323: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08 container agnhost-container: <nil>
STEP: delete the pod
Sep  6 00:35:26.378: INFO: Waiting for pod pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08 to disappear
Sep  6 00:35:26.394: INFO: Pod pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08 still exists
Sep  6 00:35:28.395: INFO: Waiting for pod pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08 to disappear
Sep  6 00:35:28.408: INFO: Pod pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08 still exists
Sep  6 00:35:30.395: INFO: Waiting for pod pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08 to disappear
Sep  6 00:35:30.407: INFO: Pod pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08 still exists
Sep  6 00:35:32.394: INFO: Waiting for pod pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08 to disappear
Sep  6 00:35:32.407: INFO: Pod pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08 still exists
Sep  6 00:35:34.395: INFO: Waiting for pod pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08 to disappear
Sep  6 00:35:34.405: INFO: Pod pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08 still exists
Sep  6 00:35:36.395: INFO: Waiting for pod pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08 to disappear
Sep  6 00:35:36.403: INFO: Pod pod-projected-configmaps-4940c802-4f0e-4d0f-8ad2-428e51a6dc08 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:35:36.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5836" for this suite.

• [SLOW TEST:41.101 seconds]
[sig-storage] Projected configMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":296,"completed":224,"skipped":4039,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:35:36.627: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2971
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-df65b553-7432-483e-910d-dac40af68f87
STEP: Creating a pod to test consume configMaps
Sep  6 00:35:37.105: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850" in namespace "projected-2971" to be "Succeeded or Failed"
Sep  6 00:35:37.118: INFO: Pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850": Phase="Pending", Reason="", readiness=false. Elapsed: 12.083343ms
Sep  6 00:35:39.132: INFO: Pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026763504s
Sep  6 00:35:41.142: INFO: Pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036544382s
Sep  6 00:35:43.154: INFO: Pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049057926s
Sep  6 00:35:45.184: INFO: Pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850": Phase="Pending", Reason="", readiness=false. Elapsed: 8.078945734s
Sep  6 00:35:47.194: INFO: Pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850": Phase="Pending", Reason="", readiness=false. Elapsed: 10.088445434s
Sep  6 00:35:49.211: INFO: Pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850": Phase="Pending", Reason="", readiness=false. Elapsed: 12.105427089s
Sep  6 00:35:51.235: INFO: Pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850": Phase="Pending", Reason="", readiness=false. Elapsed: 14.129563869s
Sep  6 00:35:53.248: INFO: Pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850": Phase="Pending", Reason="", readiness=false. Elapsed: 16.142400283s
Sep  6 00:35:55.268: INFO: Pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850": Phase="Pending", Reason="", readiness=false. Elapsed: 18.162299063s
Sep  6 00:35:57.279: INFO: Pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850": Phase="Pending", Reason="", readiness=false. Elapsed: 20.173561244s
Sep  6 00:35:59.309: INFO: Pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850": Phase="Pending", Reason="", readiness=false. Elapsed: 22.203969022s
Sep  6 00:36:01.337: INFO: Pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850": Phase="Pending", Reason="", readiness=false. Elapsed: 24.231580245s
Sep  6 00:36:03.355: INFO: Pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850": Phase="Pending", Reason="", readiness=false. Elapsed: 26.249624301s
Sep  6 00:36:05.375: INFO: Pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850": Phase="Running", Reason="", readiness=true. Elapsed: 28.269826598s
Sep  6 00:36:07.391: INFO: Pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850": Phase="Running", Reason="", readiness=true. Elapsed: 30.285218873s
Sep  6 00:36:09.404: INFO: Pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850": Phase="Running", Reason="", readiness=true. Elapsed: 32.298402951s
Sep  6 00:36:11.426: INFO: Pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.320903063s
STEP: Saw pod success
Sep  6 00:36:11.426: INFO: Pod "pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850" satisfied condition "Succeeded or Failed"
Sep  6 00:36:11.433: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850 container agnhost-container: <nil>
STEP: delete the pod
Sep  6 00:36:17.147: INFO: Waiting for pod pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850 to disappear
Sep  6 00:36:17.272: INFO: Pod pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850 still exists
Sep  6 00:36:19.273: INFO: Waiting for pod pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850 to disappear
Sep  6 00:36:19.302: INFO: Pod pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850 still exists
Sep  6 00:36:21.272: INFO: Waiting for pod pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850 to disappear
Sep  6 00:36:21.343: INFO: Pod pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850 still exists
Sep  6 00:36:23.272: INFO: Waiting for pod pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850 to disappear
Sep  6 00:36:23.313: INFO: Pod pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850 still exists
Sep  6 00:36:25.272: INFO: Waiting for pod pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850 to disappear
Sep  6 00:36:25.307: INFO: Pod pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850 still exists
Sep  6 00:36:27.272: INFO: Waiting for pod pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850 to disappear
Sep  6 00:36:27.289: INFO: Pod pod-projected-configmaps-f249fcac-a0d3-4740-b5b6-1dd6d2008850 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:36:27.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2971" for this suite.

• [SLOW TEST:51.014 seconds]
[sig-storage] Projected configMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":296,"completed":225,"skipped":4045,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Lease
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:36:27.642: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-8878
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Lease
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:36:28.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-8878" for this suite.
•{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":296,"completed":226,"skipped":4050,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:36:28.767: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2482
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-2482/configmap-test-6ca4ed98-5b83-4d9e-97e6-f1224dee140d
STEP: Creating a pod to test consume configMaps
Sep  6 00:36:29.503: INFO: Waiting up to 5m0s for pod "pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9" in namespace "configmap-2482" to be "Succeeded or Failed"
Sep  6 00:36:29.531: INFO: Pod "pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9": Phase="Pending", Reason="", readiness=false. Elapsed: 27.433599ms
Sep  6 00:36:31.545: INFO: Pod "pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042168842s
Sep  6 00:36:33.576: INFO: Pod "pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072389283s
Sep  6 00:36:35.589: INFO: Pod "pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.085500237s
Sep  6 00:36:37.599: INFO: Pod "pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.096172185s
Sep  6 00:36:39.626: INFO: Pod "pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.122791419s
Sep  6 00:36:41.640: INFO: Pod "pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.136386643s
Sep  6 00:36:43.650: INFO: Pod "pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.147110184s
Sep  6 00:36:45.661: INFO: Pod "pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.157796849s
Sep  6 00:36:47.678: INFO: Pod "pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.174850217s
Sep  6 00:36:49.760: INFO: Pod "pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.256394948s
Sep  6 00:36:51.800: INFO: Pod "pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9": Phase="Pending", Reason="", readiness=false. Elapsed: 22.297002966s
Sep  6 00:36:53.833: INFO: Pod "pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9": Phase="Pending", Reason="", readiness=false. Elapsed: 24.330169364s
Sep  6 00:36:55.847: INFO: Pod "pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9": Phase="Pending", Reason="", readiness=false. Elapsed: 26.34348792s
Sep  6 00:36:57.860: INFO: Pod "pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.356378877s
Sep  6 00:36:59.897: INFO: Pod "pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9": Phase="Pending", Reason="", readiness=false. Elapsed: 30.39428128s
Sep  6 00:37:01.913: INFO: Pod "pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.409499045s
STEP: Saw pod success
Sep  6 00:37:01.913: INFO: Pod "pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9" satisfied condition "Succeeded or Failed"
Sep  6 00:37:01.920: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9 container env-test: <nil>
STEP: delete the pod
Sep  6 00:37:07.234: INFO: Waiting for pod pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9 to disappear
Sep  6 00:37:07.258: INFO: Pod pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9 still exists
Sep  6 00:37:09.259: INFO: Waiting for pod pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9 to disappear
Sep  6 00:37:09.278: INFO: Pod pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9 still exists
Sep  6 00:37:11.259: INFO: Waiting for pod pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9 to disappear
Sep  6 00:37:11.297: INFO: Pod pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9 still exists
Sep  6 00:37:13.258: INFO: Waiting for pod pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9 to disappear
Sep  6 00:37:13.278: INFO: Pod pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9 still exists
Sep  6 00:37:15.259: INFO: Waiting for pod pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9 to disappear
Sep  6 00:37:15.275: INFO: Pod pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9 still exists
Sep  6 00:37:17.259: INFO: Waiting for pod pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9 to disappear
Sep  6 00:37:17.274: INFO: Pod pod-configmaps-62fdae66-eb58-4211-9ff8-d7c1d71dbbf9 no longer exists
[AfterEach] [sig-node] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:37:17.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2482" for this suite.

• [SLOW TEST:48.743 seconds]
[sig-node] ConfigMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via environment variable [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":296,"completed":227,"skipped":4068,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SS
------------------------------
[sig-node] RuntimeClass 
   should support RuntimeClasses API operations [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] RuntimeClass
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:37:17.511: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename runtimeclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in runtimeclass-9790
STEP: Waiting for a default service account to be provisioned in namespace
[It]  should support RuntimeClasses API operations [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting /apis
STEP: getting /apis/node.k8s.io
STEP: getting /apis/node.k8s.io/v1
STEP: creating
STEP: watching
Sep  6 00:37:18.154: INFO: starting watch
STEP: getting
STEP: listing
STEP: patching
STEP: updating
Sep  6 00:37:18.242: INFO: waiting for watch events with expected annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-node] RuntimeClass
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:37:18.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9790" for this suite.
•{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","total":296,"completed":228,"skipped":4070,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:37:18.554: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5666
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep  6 00:37:19.124: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f" in namespace "projected-5666" to be "Succeeded or Failed"
Sep  6 00:37:19.145: INFO: Pod "downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.846644ms
Sep  6 00:37:21.158: INFO: Pod "downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033910841s
Sep  6 00:37:23.173: INFO: Pod "downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048206958s
Sep  6 00:37:25.209: INFO: Pod "downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084691413s
Sep  6 00:37:27.229: INFO: Pod "downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.104460753s
Sep  6 00:37:29.248: INFO: Pod "downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.123641611s
Sep  6 00:37:31.263: INFO: Pod "downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.13855329s
Sep  6 00:37:33.274: INFO: Pod "downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.149146118s
Sep  6 00:37:35.316: INFO: Pod "downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.191967259s
Sep  6 00:37:37.325: INFO: Pod "downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.200308197s
Sep  6 00:37:39.348: INFO: Pod "downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.223569252s
Sep  6 00:37:41.363: INFO: Pod "downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.238650165s
Sep  6 00:37:43.378: INFO: Pod "downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.254040815s
Sep  6 00:37:45.394: INFO: Pod "downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f": Phase="Pending", Reason="", readiness=false. Elapsed: 26.26939296s
Sep  6 00:37:47.404: INFO: Pod "downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.279246682s
Sep  6 00:37:49.419: INFO: Pod "downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f": Phase="Pending", Reason="", readiness=false. Elapsed: 30.294734428s
Sep  6 00:37:51.435: INFO: Pod "downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.310284274s
STEP: Saw pod success
Sep  6 00:37:51.435: INFO: Pod "downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f" satisfied condition "Succeeded or Failed"
Sep  6 00:37:51.440: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f container client-container: <nil>
STEP: delete the pod
Sep  6 00:37:58.744: INFO: Waiting for pod downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f to disappear
Sep  6 00:37:58.776: INFO: Pod downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f still exists
Sep  6 00:38:00.776: INFO: Waiting for pod downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f to disappear
Sep  6 00:38:00.806: INFO: Pod downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f still exists
Sep  6 00:38:02.776: INFO: Waiting for pod downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f to disappear
Sep  6 00:38:02.789: INFO: Pod downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f still exists
Sep  6 00:38:04.776: INFO: Waiting for pod downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f to disappear
Sep  6 00:38:04.796: INFO: Pod downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f still exists
Sep  6 00:38:06.777: INFO: Waiting for pod downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f to disappear
Sep  6 00:38:06.792: INFO: Pod downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f still exists
Sep  6 00:38:08.776: INFO: Waiting for pod downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f to disappear
Sep  6 00:38:08.788: INFO: Pod downwardapi-volume-6bb9c693-7c00-4f68-898e-34de27d10e4f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:38:08.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5666" for this suite.

• [SLOW TEST:50.464 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":296,"completed":229,"skipped":4089,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-instrumentation] Events API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:38:09.018: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-5433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:38:09.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5433" for this suite.
•{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":296,"completed":230,"skipped":4093,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:38:09.994: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-839
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should update annotations on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating the pod
Sep  6 00:38:41.132: INFO: Successfully updated pod "annotationupdate99477266-a299-48a3-85fb-506e28fb347d"
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:38:43.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-839" for this suite.

• [SLOW TEST:33.489 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":296,"completed":231,"skipped":4101,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] version v1
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:38:43.484: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1334
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-l4pht in namespace proxy-1334
Sep  6 00:39:15.064: INFO: setup took 31.158029616s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep  6 00:39:15.143: INFO: (0) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 78.992434ms)
Sep  6 00:39:15.145: INFO: (0) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 80.696289ms)
Sep  6 00:39:15.156: INFO: (0) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 92.130622ms)
Sep  6 00:39:15.157: INFO: (0) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 92.679313ms)
Sep  6 00:39:15.165: INFO: (0) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 100.833181ms)
Sep  6 00:39:15.166: INFO: (0) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 101.070224ms)
Sep  6 00:39:15.166: INFO: (0) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 101.496177ms)
Sep  6 00:39:15.170: INFO: (0) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 106.148176ms)
Sep  6 00:39:15.173: INFO: (0) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 108.804239ms)
Sep  6 00:39:15.173: INFO: (0) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 109.080828ms)
Sep  6 00:39:15.173: INFO: (0) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 108.738632ms)
Sep  6 00:39:15.178: INFO: (0) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 113.707537ms)
Sep  6 00:39:15.180: INFO: (0) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 115.51947ms)
Sep  6 00:39:15.180: INFO: (0) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 115.589445ms)
Sep  6 00:39:15.180: INFO: (0) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 115.811591ms)
Sep  6 00:39:15.181: INFO: (0) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 117.248783ms)
Sep  6 00:39:15.204: INFO: (1) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 22.102734ms)
Sep  6 00:39:15.207: INFO: (1) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 24.487529ms)
Sep  6 00:39:15.210: INFO: (1) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 27.990709ms)
Sep  6 00:39:15.216: INFO: (1) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 33.49435ms)
Sep  6 00:39:15.216: INFO: (1) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 33.07855ms)
Sep  6 00:39:15.216: INFO: (1) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 33.024493ms)
Sep  6 00:39:15.216: INFO: (1) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 32.753392ms)
Sep  6 00:39:15.216: INFO: (1) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 33.367925ms)
Sep  6 00:39:15.216: INFO: (1) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 33.745306ms)
Sep  6 00:39:15.229: INFO: (1) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 46.92089ms)
Sep  6 00:39:15.233: INFO: (1) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 50.294061ms)
Sep  6 00:39:15.234: INFO: (1) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 52.065157ms)
Sep  6 00:39:15.235: INFO: (1) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 52.213575ms)
Sep  6 00:39:15.236: INFO: (1) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 52.806148ms)
Sep  6 00:39:15.236: INFO: (1) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 52.940817ms)
Sep  6 00:39:15.238: INFO: (1) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 55.469344ms)
Sep  6 00:39:15.272: INFO: (2) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 33.498886ms)
Sep  6 00:39:15.273: INFO: (2) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 34.028511ms)
Sep  6 00:39:15.277: INFO: (2) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 38.61836ms)
Sep  6 00:39:15.300: INFO: (2) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 61.472004ms)
Sep  6 00:39:15.300: INFO: (2) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 61.512333ms)
Sep  6 00:39:15.300: INFO: (2) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 61.675069ms)
Sep  6 00:39:15.300: INFO: (2) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 61.321575ms)
Sep  6 00:39:15.300: INFO: (2) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 61.461947ms)
Sep  6 00:39:15.306: INFO: (2) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 67.368115ms)
Sep  6 00:39:15.311: INFO: (2) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 71.568518ms)
Sep  6 00:39:15.311: INFO: (2) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 71.714394ms)
Sep  6 00:39:15.322: INFO: (2) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 83.284356ms)
Sep  6 00:39:15.324: INFO: (2) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 85.307569ms)
Sep  6 00:39:15.326: INFO: (2) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 86.851026ms)
Sep  6 00:39:15.326: INFO: (2) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 87.13141ms)
Sep  6 00:39:15.326: INFO: (2) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 87.127197ms)
Sep  6 00:39:15.342: INFO: (3) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 16.229405ms)
Sep  6 00:39:15.354: INFO: (3) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 27.518442ms)
Sep  6 00:39:15.362: INFO: (3) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 35.535247ms)
Sep  6 00:39:15.367: INFO: (3) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 40.627464ms)
Sep  6 00:39:15.375: INFO: (3) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 48.337881ms)
Sep  6 00:39:15.375: INFO: (3) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 48.423919ms)
Sep  6 00:39:15.382: INFO: (3) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 55.721946ms)
Sep  6 00:39:15.382: INFO: (3) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 55.805391ms)
Sep  6 00:39:15.384: INFO: (3) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 57.456293ms)
Sep  6 00:39:15.386: INFO: (3) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 59.416966ms)
Sep  6 00:39:15.390: INFO: (3) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 63.715616ms)
Sep  6 00:39:15.390: INFO: (3) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 63.730022ms)
Sep  6 00:39:15.391: INFO: (3) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 64.205813ms)
Sep  6 00:39:15.391: INFO: (3) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 64.70725ms)
Sep  6 00:39:15.398: INFO: (3) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 70.711492ms)
Sep  6 00:39:15.400: INFO: (3) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 73.728995ms)
Sep  6 00:39:15.418: INFO: (4) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 17.511179ms)
Sep  6 00:39:15.418: INFO: (4) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 17.480841ms)
Sep  6 00:39:15.439: INFO: (4) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 38.492319ms)
Sep  6 00:39:15.439: INFO: (4) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 38.376607ms)
Sep  6 00:39:15.439: INFO: (4) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 39.065543ms)
Sep  6 00:39:15.442: INFO: (4) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 40.898721ms)
Sep  6 00:39:15.448: INFO: (4) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 47.227854ms)
Sep  6 00:39:15.449: INFO: (4) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 47.941811ms)
Sep  6 00:39:15.449: INFO: (4) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 48.258939ms)
Sep  6 00:39:15.449: INFO: (4) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 48.352505ms)
Sep  6 00:39:15.449: INFO: (4) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 48.78866ms)
Sep  6 00:39:15.450: INFO: (4) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 49.098211ms)
Sep  6 00:39:15.458: INFO: (4) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 56.987545ms)
Sep  6 00:39:15.458: INFO: (4) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 57.015194ms)
Sep  6 00:39:15.459: INFO: (4) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 58.153382ms)
Sep  6 00:39:15.459: INFO: (4) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 58.296973ms)
Sep  6 00:39:15.489: INFO: (5) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 29.94049ms)
Sep  6 00:39:15.490: INFO: (5) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 30.41148ms)
Sep  6 00:39:15.490: INFO: (5) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 30.985783ms)
Sep  6 00:39:15.493: INFO: (5) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 34.200927ms)
Sep  6 00:39:15.496: INFO: (5) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 36.148856ms)
Sep  6 00:39:15.496: INFO: (5) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 36.535502ms)
Sep  6 00:39:15.496: INFO: (5) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 36.173377ms)
Sep  6 00:39:15.499: INFO: (5) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 39.453085ms)
Sep  6 00:39:15.499: INFO: (5) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 39.673742ms)
Sep  6 00:39:15.503: INFO: (5) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 43.614941ms)
Sep  6 00:39:15.505: INFO: (5) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 44.987365ms)
Sep  6 00:39:15.505: INFO: (5) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 46.077375ms)
Sep  6 00:39:15.507: INFO: (5) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 47.439791ms)
Sep  6 00:39:15.508: INFO: (5) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 48.604128ms)
Sep  6 00:39:15.510: INFO: (5) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 49.958534ms)
Sep  6 00:39:15.514: INFO: (5) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 53.956108ms)
Sep  6 00:39:15.540: INFO: (6) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 26.085878ms)
Sep  6 00:39:15.540: INFO: (6) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 25.599508ms)
Sep  6 00:39:15.540: INFO: (6) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 25.979978ms)
Sep  6 00:39:15.540: INFO: (6) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 25.73829ms)
Sep  6 00:39:15.540: INFO: (6) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 26.049999ms)
Sep  6 00:39:15.540: INFO: (6) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 25.839178ms)
Sep  6 00:39:15.541: INFO: (6) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 27.15543ms)
Sep  6 00:39:15.544: INFO: (6) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 29.710402ms)
Sep  6 00:39:15.544: INFO: (6) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 30.09122ms)
Sep  6 00:39:15.544: INFO: (6) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 30.113254ms)
Sep  6 00:39:15.547: INFO: (6) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 33.05302ms)
Sep  6 00:39:15.549: INFO: (6) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 35.12689ms)
Sep  6 00:39:15.550: INFO: (6) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 35.836399ms)
Sep  6 00:39:15.553: INFO: (6) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 38.629277ms)
Sep  6 00:39:15.553: INFO: (6) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 38.879772ms)
Sep  6 00:39:15.553: INFO: (6) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 38.805831ms)
Sep  6 00:39:15.578: INFO: (7) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 24.749541ms)
Sep  6 00:39:15.582: INFO: (7) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 28.824546ms)
Sep  6 00:39:15.582: INFO: (7) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 28.80205ms)
Sep  6 00:39:15.587: INFO: (7) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 33.656732ms)
Sep  6 00:39:15.587: INFO: (7) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 33.702977ms)
Sep  6 00:39:15.587: INFO: (7) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 33.684872ms)
Sep  6 00:39:15.588: INFO: (7) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 35.108972ms)
Sep  6 00:39:15.589: INFO: (7) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 35.587057ms)
Sep  6 00:39:15.589: INFO: (7) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 35.514166ms)
Sep  6 00:39:15.594: INFO: (7) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 41.046248ms)
Sep  6 00:39:15.595: INFO: (7) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 42.150422ms)
Sep  6 00:39:15.595: INFO: (7) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 42.030926ms)
Sep  6 00:39:15.597: INFO: (7) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 43.339505ms)
Sep  6 00:39:15.597: INFO: (7) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 43.609676ms)
Sep  6 00:39:15.598: INFO: (7) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 44.003833ms)
Sep  6 00:39:15.598: INFO: (7) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 44.051363ms)
Sep  6 00:39:15.618: INFO: (8) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 19.71795ms)
Sep  6 00:39:15.618: INFO: (8) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 19.890176ms)
Sep  6 00:39:15.618: INFO: (8) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 20.393574ms)
Sep  6 00:39:15.619: INFO: (8) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 21.38177ms)
Sep  6 00:39:15.620: INFO: (8) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 21.323942ms)
Sep  6 00:39:15.622: INFO: (8) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 23.504769ms)
Sep  6 00:39:15.622: INFO: (8) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 23.612791ms)
Sep  6 00:39:15.623: INFO: (8) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 24.737999ms)
Sep  6 00:39:15.624: INFO: (8) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 25.37883ms)
Sep  6 00:39:15.624: INFO: (8) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 25.906935ms)
Sep  6 00:39:15.635: INFO: (8) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 36.822229ms)
Sep  6 00:39:15.646: INFO: (8) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 48.011814ms)
Sep  6 00:39:15.646: INFO: (8) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 47.928391ms)
Sep  6 00:39:15.646: INFO: (8) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 48.316958ms)
Sep  6 00:39:15.647: INFO: (8) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 49.165159ms)
Sep  6 00:39:15.648: INFO: (8) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 49.33833ms)
Sep  6 00:39:15.665: INFO: (9) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 17.547333ms)
Sep  6 00:39:15.669: INFO: (9) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 21.089209ms)
Sep  6 00:39:15.672: INFO: (9) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 23.745412ms)
Sep  6 00:39:15.674: INFO: (9) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 24.922231ms)
Sep  6 00:39:15.678: INFO: (9) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 29.455728ms)
Sep  6 00:39:15.678: INFO: (9) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 29.172604ms)
Sep  6 00:39:15.678: INFO: (9) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 29.70882ms)
Sep  6 00:39:15.679: INFO: (9) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 30.806653ms)
Sep  6 00:39:15.679: INFO: (9) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 30.272336ms)
Sep  6 00:39:15.679: INFO: (9) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 30.442988ms)
Sep  6 00:39:15.687: INFO: (9) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 38.212352ms)
Sep  6 00:39:15.688: INFO: (9) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 38.331939ms)
Sep  6 00:39:15.693: INFO: (9) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 44.066587ms)
Sep  6 00:39:15.707: INFO: (9) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 58.107673ms)
Sep  6 00:39:15.707: INFO: (9) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 58.083584ms)
Sep  6 00:39:15.707: INFO: (9) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 58.53483ms)
Sep  6 00:39:15.732: INFO: (10) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 24.76982ms)
Sep  6 00:39:15.733: INFO: (10) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 25.722507ms)
Sep  6 00:39:15.733: INFO: (10) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 25.465821ms)
Sep  6 00:39:15.734: INFO: (10) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 25.920687ms)
Sep  6 00:39:15.734: INFO: (10) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 26.560234ms)
Sep  6 00:39:15.734: INFO: (10) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 26.69718ms)
Sep  6 00:39:15.736: INFO: (10) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 28.590172ms)
Sep  6 00:39:15.736: INFO: (10) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 28.291757ms)
Sep  6 00:39:15.736: INFO: (10) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 28.567131ms)
Sep  6 00:39:15.736: INFO: (10) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 28.401854ms)
Sep  6 00:39:15.746: INFO: (10) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 38.17647ms)
Sep  6 00:39:15.747: INFO: (10) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 39.554204ms)
Sep  6 00:39:15.747: INFO: (10) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 39.558622ms)
Sep  6 00:39:15.749: INFO: (10) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 42.024445ms)
Sep  6 00:39:15.749: INFO: (10) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 41.496889ms)
Sep  6 00:39:15.750: INFO: (10) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 42.614931ms)
Sep  6 00:39:15.776: INFO: (11) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 25.132231ms)
Sep  6 00:39:15.786: INFO: (11) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 35.324286ms)
Sep  6 00:39:15.786: INFO: (11) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 35.104104ms)
Sep  6 00:39:15.787: INFO: (11) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 35.964311ms)
Sep  6 00:39:15.787: INFO: (11) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 36.466238ms)
Sep  6 00:39:15.795: INFO: (11) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 43.829948ms)
Sep  6 00:39:15.795: INFO: (11) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 43.755689ms)
Sep  6 00:39:15.795: INFO: (11) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 44.10532ms)
Sep  6 00:39:15.795: INFO: (11) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 44.200078ms)
Sep  6 00:39:15.795: INFO: (11) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 43.794835ms)
Sep  6 00:39:15.795: INFO: (11) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 44.040696ms)
Sep  6 00:39:15.795: INFO: (11) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 44.404848ms)
Sep  6 00:39:15.795: INFO: (11) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 43.754708ms)
Sep  6 00:39:15.795: INFO: (11) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 44.04069ms)
Sep  6 00:39:15.795: INFO: (11) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 43.83049ms)
Sep  6 00:39:15.795: INFO: (11) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 44.020646ms)
Sep  6 00:39:15.825: INFO: (12) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 29.812175ms)
Sep  6 00:39:15.834: INFO: (12) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 38.592105ms)
Sep  6 00:39:15.834: INFO: (12) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 38.909428ms)
Sep  6 00:39:15.834: INFO: (12) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 39.066206ms)
Sep  6 00:39:15.834: INFO: (12) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 39.326588ms)
Sep  6 00:39:15.835: INFO: (12) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 39.191142ms)
Sep  6 00:39:15.835: INFO: (12) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 39.149863ms)
Sep  6 00:39:15.835: INFO: (12) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 39.495989ms)
Sep  6 00:39:15.835: INFO: (12) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 40.273127ms)
Sep  6 00:39:15.835: INFO: (12) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 39.890006ms)
Sep  6 00:39:15.849: INFO: (12) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 54.108861ms)
Sep  6 00:39:15.851: INFO: (12) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 55.909896ms)
Sep  6 00:39:15.854: INFO: (12) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 58.120897ms)
Sep  6 00:39:15.854: INFO: (12) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 58.058395ms)
Sep  6 00:39:15.854: INFO: (12) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 59.137766ms)
Sep  6 00:39:15.855: INFO: (12) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 59.185225ms)
Sep  6 00:39:15.874: INFO: (13) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 18.67843ms)
Sep  6 00:39:15.880: INFO: (13) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 24.499709ms)
Sep  6 00:39:15.884: INFO: (13) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 28.949138ms)
Sep  6 00:39:15.884: INFO: (13) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 29.231302ms)
Sep  6 00:39:15.889: INFO: (13) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 33.807767ms)
Sep  6 00:39:15.889: INFO: (13) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 34.191447ms)
Sep  6 00:39:15.897: INFO: (13) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 41.905263ms)
Sep  6 00:39:15.897: INFO: (13) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 41.956997ms)
Sep  6 00:39:15.897: INFO: (13) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 41.788202ms)
Sep  6 00:39:15.898: INFO: (13) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 42.136774ms)
Sep  6 00:39:15.898: INFO: (13) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 42.304339ms)
Sep  6 00:39:15.903: INFO: (13) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 47.57597ms)
Sep  6 00:39:15.905: INFO: (13) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 50.413456ms)
Sep  6 00:39:15.909: INFO: (13) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 53.681641ms)
Sep  6 00:39:15.909: INFO: (13) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 53.607214ms)
Sep  6 00:39:15.909: INFO: (13) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 54.124691ms)
Sep  6 00:39:15.950: INFO: (14) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 41.147073ms)
Sep  6 00:39:15.952: INFO: (14) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 42.804683ms)
Sep  6 00:39:15.954: INFO: (14) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 44.143336ms)
Sep  6 00:39:15.983: INFO: (14) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 73.351452ms)
Sep  6 00:39:15.983: INFO: (14) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 73.759884ms)
Sep  6 00:39:15.983: INFO: (14) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 74.008319ms)
Sep  6 00:39:15.985: INFO: (14) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 75.368077ms)
Sep  6 00:39:15.994: INFO: (14) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 84.015752ms)
Sep  6 00:39:15.994: INFO: (14) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 84.380186ms)
Sep  6 00:39:15.994: INFO: (14) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 84.052659ms)
Sep  6 00:39:15.997: INFO: (14) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 87.299603ms)
Sep  6 00:39:15.998: INFO: (14) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 88.184348ms)
Sep  6 00:39:15.999: INFO: (14) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 88.566397ms)
Sep  6 00:39:16.003: INFO: (14) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 92.768926ms)
Sep  6 00:39:16.004: INFO: (14) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 94.146583ms)
Sep  6 00:39:16.016: INFO: (14) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 105.8624ms)
Sep  6 00:39:16.050: INFO: (15) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 34.007262ms)
Sep  6 00:39:16.050: INFO: (15) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 34.221942ms)
Sep  6 00:39:16.050: INFO: (15) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 34.412815ms)
Sep  6 00:39:16.059: INFO: (15) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 42.571144ms)
Sep  6 00:39:16.060: INFO: (15) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 43.822906ms)
Sep  6 00:39:16.060: INFO: (15) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 43.672924ms)
Sep  6 00:39:16.061: INFO: (15) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 44.388167ms)
Sep  6 00:39:16.061: INFO: (15) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 44.634379ms)
Sep  6 00:39:16.061: INFO: (15) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 44.750018ms)
Sep  6 00:39:16.062: INFO: (15) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 44.877436ms)
Sep  6 00:39:16.062: INFO: (15) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 45.389284ms)
Sep  6 00:39:16.064: INFO: (15) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 47.816962ms)
Sep  6 00:39:16.065: INFO: (15) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 48.835689ms)
Sep  6 00:39:16.066: INFO: (15) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 49.027335ms)
Sep  6 00:39:16.068: INFO: (15) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 51.36446ms)
Sep  6 00:39:16.068: INFO: (15) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 51.721075ms)
Sep  6 00:39:16.090: INFO: (16) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 22.12645ms)
Sep  6 00:39:16.094: INFO: (16) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 24.781077ms)
Sep  6 00:39:16.099: INFO: (16) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 29.296769ms)
Sep  6 00:39:16.100: INFO: (16) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 31.217574ms)
Sep  6 00:39:16.101: INFO: (16) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 30.406026ms)
Sep  6 00:39:16.102: INFO: (16) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 32.891919ms)
Sep  6 00:39:16.103: INFO: (16) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 33.383723ms)
Sep  6 00:39:16.110: INFO: (16) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 41.490429ms)
Sep  6 00:39:16.111: INFO: (16) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 41.440755ms)
Sep  6 00:39:16.115: INFO: (16) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 45.615046ms)
Sep  6 00:39:16.117: INFO: (16) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 48.28945ms)
Sep  6 00:39:16.117: INFO: (16) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 48.684235ms)
Sep  6 00:39:16.117: INFO: (16) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 47.470154ms)
Sep  6 00:39:16.125: INFO: (16) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 55.512288ms)
Sep  6 00:39:16.127: INFO: (16) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 56.816641ms)
Sep  6 00:39:16.127: INFO: (16) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 56.438389ms)
Sep  6 00:39:16.152: INFO: (17) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 25.16652ms)
Sep  6 00:39:16.161: INFO: (17) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 33.834552ms)
Sep  6 00:39:16.161: INFO: (17) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 34.151832ms)
Sep  6 00:39:16.163: INFO: (17) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 35.739506ms)
Sep  6 00:39:16.163: INFO: (17) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 35.766017ms)
Sep  6 00:39:16.174: INFO: (17) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 47.399838ms)
Sep  6 00:39:16.176: INFO: (17) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 49.310266ms)
Sep  6 00:39:16.177: INFO: (17) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 49.712351ms)
Sep  6 00:39:16.177: INFO: (17) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 49.609649ms)
Sep  6 00:39:16.177: INFO: (17) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 49.427394ms)
Sep  6 00:39:16.178: INFO: (17) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 51.178731ms)
Sep  6 00:39:16.180: INFO: (17) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 53.020748ms)
Sep  6 00:39:16.180: INFO: (17) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 53.532297ms)
Sep  6 00:39:16.183: INFO: (17) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 56.177975ms)
Sep  6 00:39:16.183: INFO: (17) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 56.036018ms)
Sep  6 00:39:16.188: INFO: (17) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 60.697416ms)
Sep  6 00:39:16.207: INFO: (18) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 19.686238ms)
Sep  6 00:39:16.217: INFO: (18) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 28.433558ms)
Sep  6 00:39:16.220: INFO: (18) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 31.689851ms)
Sep  6 00:39:16.220: INFO: (18) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 31.649327ms)
Sep  6 00:39:16.227: INFO: (18) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 38.854241ms)
Sep  6 00:39:16.228: INFO: (18) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 40.227449ms)
Sep  6 00:39:16.230: INFO: (18) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 42.053857ms)
Sep  6 00:39:16.231: INFO: (18) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 42.906771ms)
Sep  6 00:39:16.231: INFO: (18) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 42.949346ms)
Sep  6 00:39:16.232: INFO: (18) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 43.732381ms)
Sep  6 00:39:16.240: INFO: (18) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 51.816284ms)
Sep  6 00:39:16.249: INFO: (18) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 60.425349ms)
Sep  6 00:39:16.254: INFO: (18) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 65.69488ms)
Sep  6 00:39:16.262: INFO: (18) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 73.943163ms)
Sep  6 00:39:16.268: INFO: (18) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 79.603582ms)
Sep  6 00:39:16.268: INFO: (18) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 79.325664ms)
Sep  6 00:39:16.311: INFO: (19) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc/proxy/rewriteme">test</a> (200; 42.126325ms)
Sep  6 00:39:16.312: INFO: (19) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 43.473871ms)
Sep  6 00:39:16.312: INFO: (19) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:462/proxy/: tls qux (200; 43.292153ms)
Sep  6 00:39:16.312: INFO: (19) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">... (200; 43.595651ms)
Sep  6 00:39:16.313: INFO: (19) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:160/proxy/: foo (200; 44.287559ms)
Sep  6 00:39:16.313: INFO: (19) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:443/proxy/tlsrewritem... (200; 44.802862ms)
Sep  6 00:39:16.315: INFO: (19) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/: <a href="/api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:1080/proxy/rewriteme">test<... (200; 46.206398ms)
Sep  6 00:39:16.315: INFO: (19) /api/v1/namespaces/proxy-1334/pods/http:proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 46.959689ms)
Sep  6 00:39:16.323: INFO: (19) /api/v1/namespaces/proxy-1334/pods/proxy-service-l4pht-lkmcc:162/proxy/: bar (200; 55.018065ms)
Sep  6 00:39:16.323: INFO: (19) /api/v1/namespaces/proxy-1334/pods/https:proxy-service-l4pht-lkmcc:460/proxy/: tls baz (200; 55.149378ms)
Sep  6 00:39:16.358: INFO: (19) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname1/proxy/: foo (200; 89.183043ms)
Sep  6 00:39:16.402: INFO: (19) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname2/proxy/: bar (200; 134.065196ms)
Sep  6 00:39:16.403: INFO: (19) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname1/proxy/: tls baz (200; 134.552468ms)
Sep  6 00:39:16.403: INFO: (19) /api/v1/namespaces/proxy-1334/services/http:proxy-service-l4pht:portname1/proxy/: foo (200; 134.720865ms)
Sep  6 00:39:16.403: INFO: (19) /api/v1/namespaces/proxy-1334/services/proxy-service-l4pht:portname2/proxy/: bar (200; 134.362278ms)
Sep  6 00:39:16.403: INFO: (19) /api/v1/namespaces/proxy-1334/services/https:proxy-service-l4pht:tlsportname2/proxy/: tls qux (200; 134.418866ms)
STEP: deleting ReplicationController proxy-service-l4pht in namespace proxy-1334, will wait for the garbage collector to delete the pods
Sep  6 00:39:16.524: INFO: Deleting ReplicationController proxy-service-l4pht took: 47.639858ms
Sep  6 00:39:16.624: INFO: Terminating ReplicationController proxy-service-l4pht pods took: 100.244718ms
[AfterEach] version v1
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:39:28.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1334" for this suite.

• [SLOW TEST:45.052 seconds]
[sig-network] Proxy
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":296,"completed":232,"skipped":4104,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:39:28.536: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5154
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's command
Sep  6 00:39:29.126: INFO: Waiting up to 5m0s for pod "var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66" in namespace "var-expansion-5154" to be "Succeeded or Failed"
Sep  6 00:39:29.144: INFO: Pod "var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66": Phase="Pending", Reason="", readiness=false. Elapsed: 18.047034ms
Sep  6 00:39:31.181: INFO: Pod "var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055211627s
Sep  6 00:39:33.194: INFO: Pod "var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067744829s
Sep  6 00:39:35.208: INFO: Pod "var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66": Phase="Pending", Reason="", readiness=false. Elapsed: 6.082007031s
Sep  6 00:39:37.253: INFO: Pod "var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66": Phase="Pending", Reason="", readiness=false. Elapsed: 8.127637826s
Sep  6 00:39:39.264: INFO: Pod "var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66": Phase="Pending", Reason="", readiness=false. Elapsed: 10.13805314s
Sep  6 00:39:41.276: INFO: Pod "var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66": Phase="Pending", Reason="", readiness=false. Elapsed: 12.150667201s
Sep  6 00:39:43.297: INFO: Pod "var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66": Phase="Pending", Reason="", readiness=false. Elapsed: 14.171676728s
Sep  6 00:39:45.309: INFO: Pod "var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66": Phase="Pending", Reason="", readiness=false. Elapsed: 16.183177102s
Sep  6 00:39:47.322: INFO: Pod "var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66": Phase="Pending", Reason="", readiness=false. Elapsed: 18.196175638s
Sep  6 00:39:49.337: INFO: Pod "var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66": Phase="Pending", Reason="", readiness=false. Elapsed: 20.211116545s
Sep  6 00:39:51.365: INFO: Pod "var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66": Phase="Pending", Reason="", readiness=false. Elapsed: 22.238851091s
Sep  6 00:39:53.381: INFO: Pod "var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66": Phase="Pending", Reason="", readiness=false. Elapsed: 24.255010305s
Sep  6 00:39:55.390: INFO: Pod "var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.264664028s
STEP: Saw pod success
Sep  6 00:39:55.390: INFO: Pod "var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66" satisfied condition "Succeeded or Failed"
Sep  6 00:39:55.398: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66 container dapi-container: <nil>
STEP: delete the pod
Sep  6 00:39:55.496: INFO: Waiting for pod var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66 to disappear
Sep  6 00:39:55.514: INFO: Pod var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66 still exists
Sep  6 00:39:57.514: INFO: Waiting for pod var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66 to disappear
Sep  6 00:39:57.526: INFO: Pod var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66 still exists
Sep  6 00:39:59.515: INFO: Waiting for pod var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66 to disappear
Sep  6 00:39:59.531: INFO: Pod var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66 still exists
Sep  6 00:40:01.515: INFO: Waiting for pod var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66 to disappear
Sep  6 00:40:01.527: INFO: Pod var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66 still exists
Sep  6 00:40:03.516: INFO: Waiting for pod var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66 to disappear
Sep  6 00:40:03.527: INFO: Pod var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66 still exists
Sep  6 00:40:05.515: INFO: Waiting for pod var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66 to disappear
Sep  6 00:40:05.526: INFO: Pod var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66 still exists
Sep  6 00:40:07.516: INFO: Waiting for pod var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66 to disappear
Sep  6 00:40:07.549: INFO: Pod var-expansion-c2e556f4-d760-417a-99b9-f94226bfef66 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:40:07.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5154" for this suite.

• [SLOW TEST:39.280 seconds]
[k8s.io] Variable Expansion
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":296,"completed":233,"skipped":4112,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:40:07.816: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2601
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  6 00:40:35.798: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:40:35.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2601" for this suite.

• [SLOW TEST:28.249 seconds]
[k8s.io] Container Runtime
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  blackbox test
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":296,"completed":234,"skipped":4123,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:40:36.065: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7299
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep  6 00:40:36.990: INFO: Waiting up to 5m0s for pod "pod-d5b803b6-4d05-4aed-a204-ece482bfca3e" in namespace "emptydir-7299" to be "Succeeded or Failed"
Sep  6 00:40:37.001: INFO: Pod "pod-d5b803b6-4d05-4aed-a204-ece482bfca3e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.647503ms
Sep  6 00:40:39.010: INFO: Pod "pod-d5b803b6-4d05-4aed-a204-ece482bfca3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020175362s
Sep  6 00:40:41.029: INFO: Pod "pod-d5b803b6-4d05-4aed-a204-ece482bfca3e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039811636s
Sep  6 00:40:43.046: INFO: Pod "pod-d5b803b6-4d05-4aed-a204-ece482bfca3e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056025767s
Sep  6 00:40:45.058: INFO: Pod "pod-d5b803b6-4d05-4aed-a204-ece482bfca3e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.068061308s
Sep  6 00:40:47.067: INFO: Pod "pod-d5b803b6-4d05-4aed-a204-ece482bfca3e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.076930389s
Sep  6 00:40:49.077: INFO: Pod "pod-d5b803b6-4d05-4aed-a204-ece482bfca3e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.087210974s
Sep  6 00:40:51.098: INFO: Pod "pod-d5b803b6-4d05-4aed-a204-ece482bfca3e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.108369075s
Sep  6 00:40:53.111: INFO: Pod "pod-d5b803b6-4d05-4aed-a204-ece482bfca3e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.121872264s
Sep  6 00:40:55.127: INFO: Pod "pod-d5b803b6-4d05-4aed-a204-ece482bfca3e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.137699944s
Sep  6 00:40:57.138: INFO: Pod "pod-d5b803b6-4d05-4aed-a204-ece482bfca3e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.148760922s
Sep  6 00:40:59.151: INFO: Pod "pod-d5b803b6-4d05-4aed-a204-ece482bfca3e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.161515808s
Sep  6 00:41:01.167: INFO: Pod "pod-d5b803b6-4d05-4aed-a204-ece482bfca3e": Phase="Running", Reason="", readiness=true. Elapsed: 24.177418351s
Sep  6 00:41:03.196: INFO: Pod "pod-d5b803b6-4d05-4aed-a204-ece482bfca3e": Phase="Running", Reason="", readiness=true. Elapsed: 26.206338347s
Sep  6 00:41:05.209: INFO: Pod "pod-d5b803b6-4d05-4aed-a204-ece482bfca3e": Phase="Running", Reason="", readiness=true. Elapsed: 28.218889775s
Sep  6 00:41:07.220: INFO: Pod "pod-d5b803b6-4d05-4aed-a204-ece482bfca3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.230300021s
STEP: Saw pod success
Sep  6 00:41:07.220: INFO: Pod "pod-d5b803b6-4d05-4aed-a204-ece482bfca3e" satisfied condition "Succeeded or Failed"
Sep  6 00:41:07.225: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-d5b803b6-4d05-4aed-a204-ece482bfca3e container test-container: <nil>
STEP: delete the pod
Sep  6 00:41:14.378: INFO: Waiting for pod pod-d5b803b6-4d05-4aed-a204-ece482bfca3e to disappear
Sep  6 00:41:14.391: INFO: Pod pod-d5b803b6-4d05-4aed-a204-ece482bfca3e still exists
Sep  6 00:41:16.391: INFO: Waiting for pod pod-d5b803b6-4d05-4aed-a204-ece482bfca3e to disappear
Sep  6 00:41:16.405: INFO: Pod pod-d5b803b6-4d05-4aed-a204-ece482bfca3e still exists
Sep  6 00:41:18.391: INFO: Waiting for pod pod-d5b803b6-4d05-4aed-a204-ece482bfca3e to disappear
Sep  6 00:41:18.398: INFO: Pod pod-d5b803b6-4d05-4aed-a204-ece482bfca3e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:41:18.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7299" for this suite.

• [SLOW TEST:43.386 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":235,"skipped":4169,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:41:19.452: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2019
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep  6 00:41:19.935: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2" in namespace "downward-api-2019" to be "Succeeded or Failed"
Sep  6 00:41:19.947: INFO: Pod "downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 11.684566ms
Sep  6 00:41:21.958: INFO: Pod "downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022378298s
Sep  6 00:41:23.970: INFO: Pod "downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034520622s
Sep  6 00:41:25.982: INFO: Pod "downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046619318s
Sep  6 00:41:27.991: INFO: Pod "downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.05544826s
Sep  6 00:41:30.005: INFO: Pod "downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.070011389s
Sep  6 00:41:32.019: INFO: Pod "downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.083845116s
Sep  6 00:41:34.037: INFO: Pod "downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.101385628s
Sep  6 00:41:36.047: INFO: Pod "downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.111397873s
Sep  6 00:41:38.059: INFO: Pod "downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.124232692s
Sep  6 00:41:40.079: INFO: Pod "downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.143633698s
Sep  6 00:41:42.092: INFO: Pod "downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.157262955s
Sep  6 00:41:44.105: INFO: Pod "downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.170026219s
Sep  6 00:41:46.118: INFO: Pod "downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 26.182922702s
Sep  6 00:41:48.144: INFO: Pod "downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 28.209132063s
Sep  6 00:41:50.156: INFO: Pod "downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2": Phase="Pending", Reason="", readiness=false. Elapsed: 30.220781368s
Sep  6 00:41:52.170: INFO: Pod "downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.235196312s
STEP: Saw pod success
Sep  6 00:41:52.170: INFO: Pod "downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2" satisfied condition "Succeeded or Failed"
Sep  6 00:41:52.175: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2 container client-container: <nil>
STEP: delete the pod
Sep  6 00:41:58.014: INFO: Waiting for pod downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2 to disappear
Sep  6 00:41:58.032: INFO: Pod downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2 still exists
Sep  6 00:42:00.033: INFO: Waiting for pod downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2 to disappear
Sep  6 00:42:00.043: INFO: Pod downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2 still exists
Sep  6 00:42:02.033: INFO: Waiting for pod downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2 to disappear
Sep  6 00:42:02.089: INFO: Pod downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2 still exists
Sep  6 00:42:04.034: INFO: Waiting for pod downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2 to disappear
Sep  6 00:42:04.047: INFO: Pod downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2 still exists
Sep  6 00:42:06.033: INFO: Waiting for pod downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2 to disappear
Sep  6 00:42:06.046: INFO: Pod downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2 still exists
Sep  6 00:42:08.034: INFO: Waiting for pod downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2 to disappear
Sep  6 00:42:08.045: INFO: Pod downwardapi-volume-a8fe7bba-4205-4a77-bac2-25b9d29aa5c2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:42:08.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2019" for this suite.

• [SLOW TEST:49.338 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":296,"completed":236,"skipped":4185,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:42:08.791: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-7114
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test substitution in container's args
Sep  6 00:42:10.011: INFO: Waiting up to 5m0s for pod "var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001" in namespace "var-expansion-7114" to be "Succeeded or Failed"
Sep  6 00:42:10.031: INFO: Pod "var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001": Phase="Pending", Reason="", readiness=false. Elapsed: 20.099204ms
Sep  6 00:42:12.045: INFO: Pod "var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034693156s
Sep  6 00:42:14.075: INFO: Pod "var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063847739s
Sep  6 00:42:16.085: INFO: Pod "var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073905383s
Sep  6 00:42:18.257: INFO: Pod "var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001": Phase="Pending", Reason="", readiness=false. Elapsed: 8.246088915s
Sep  6 00:42:20.267: INFO: Pod "var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001": Phase="Pending", Reason="", readiness=false. Elapsed: 10.256508959s
Sep  6 00:42:22.290: INFO: Pod "var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001": Phase="Pending", Reason="", readiness=false. Elapsed: 12.279149834s
Sep  6 00:42:24.304: INFO: Pod "var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001": Phase="Pending", Reason="", readiness=false. Elapsed: 14.293198587s
Sep  6 00:42:26.318: INFO: Pod "var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001": Phase="Pending", Reason="", readiness=false. Elapsed: 16.306855127s
Sep  6 00:42:28.327: INFO: Pod "var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001": Phase="Pending", Reason="", readiness=false. Elapsed: 18.316312624s
Sep  6 00:42:30.341: INFO: Pod "var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001": Phase="Pending", Reason="", readiness=false. Elapsed: 20.330109873s
Sep  6 00:42:32.353: INFO: Pod "var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001": Phase="Pending", Reason="", readiness=false. Elapsed: 22.341887371s
Sep  6 00:42:34.367: INFO: Pod "var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001": Phase="Pending", Reason="", readiness=false. Elapsed: 24.355864805s
Sep  6 00:42:36.380: INFO: Pod "var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.368924501s
STEP: Saw pod success
Sep  6 00:42:36.380: INFO: Pod "var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001" satisfied condition "Succeeded or Failed"
Sep  6 00:42:36.392: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001 container dapi-container: <nil>
STEP: delete the pod
Sep  6 00:42:36.467: INFO: Waiting for pod var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001 to disappear
Sep  6 00:42:36.479: INFO: Pod var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001 still exists
Sep  6 00:42:38.481: INFO: Waiting for pod var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001 to disappear
Sep  6 00:42:38.490: INFO: Pod var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001 still exists
Sep  6 00:42:40.480: INFO: Waiting for pod var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001 to disappear
Sep  6 00:42:40.495: INFO: Pod var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001 still exists
Sep  6 00:42:42.480: INFO: Waiting for pod var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001 to disappear
Sep  6 00:42:42.491: INFO: Pod var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001 still exists
Sep  6 00:42:44.480: INFO: Waiting for pod var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001 to disappear
Sep  6 00:42:44.491: INFO: Pod var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001 still exists
Sep  6 00:42:46.480: INFO: Waiting for pod var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001 to disappear
Sep  6 00:42:46.494: INFO: Pod var-expansion-08a65e4b-3334-466a-b1c0-fb95ac5d2001 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:42:46.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7114" for this suite.

• [SLOW TEST:37.981 seconds]
[k8s.io] Variable Expansion
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":296,"completed":237,"skipped":4208,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:42:46.772: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7871
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  6 00:42:47.205: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-7871 create -f -'
Sep  6 00:42:48.380: INFO: stderr: ""
Sep  6 00:42:48.380: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Sep  6 00:42:48.380: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-7871 create -f -'
Sep  6 00:42:48.822: INFO: stderr: ""
Sep  6 00:42:48.822: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep  6 00:42:49.841: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:42:49.841: INFO: Found 0 / 1
Sep  6 00:42:50.843: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:42:50.843: INFO: Found 0 / 1
Sep  6 00:42:51.886: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:42:51.886: INFO: Found 0 / 1
Sep  6 00:42:52.838: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:42:52.838: INFO: Found 0 / 1
Sep  6 00:42:53.834: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:42:53.834: INFO: Found 0 / 1
Sep  6 00:42:54.841: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:42:54.841: INFO: Found 0 / 1
Sep  6 00:42:55.840: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:42:55.840: INFO: Found 0 / 1
Sep  6 00:42:56.834: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:42:56.834: INFO: Found 0 / 1
Sep  6 00:42:57.833: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:42:57.833: INFO: Found 0 / 1
Sep  6 00:42:58.837: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:42:58.837: INFO: Found 0 / 1
Sep  6 00:42:59.832: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:42:59.832: INFO: Found 0 / 1
Sep  6 00:43:00.843: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:43:00.843: INFO: Found 0 / 1
Sep  6 00:43:01.834: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:43:01.834: INFO: Found 0 / 1
Sep  6 00:43:02.838: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:43:02.839: INFO: Found 0 / 1
Sep  6 00:43:03.832: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:43:03.832: INFO: Found 0 / 1
Sep  6 00:43:04.838: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:43:04.838: INFO: Found 0 / 1
Sep  6 00:43:05.843: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:43:05.843: INFO: Found 0 / 1
Sep  6 00:43:06.836: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:43:06.836: INFO: Found 0 / 1
Sep  6 00:43:07.844: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:43:07.844: INFO: Found 0 / 1
Sep  6 00:43:08.840: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:43:08.840: INFO: Found 0 / 1
Sep  6 00:43:09.844: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:43:09.844: INFO: Found 0 / 1
Sep  6 00:43:10.844: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:43:10.844: INFO: Found 0 / 1
Sep  6 00:43:11.837: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:43:11.837: INFO: Found 0 / 1
Sep  6 00:43:12.837: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:43:12.837: INFO: Found 0 / 1
Sep  6 00:43:13.849: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:43:13.849: INFO: Found 0 / 1
Sep  6 00:43:14.836: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:43:14.836: INFO: Found 0 / 1
Sep  6 00:43:15.834: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:43:15.834: INFO: Found 0 / 1
Sep  6 00:43:16.836: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:43:16.836: INFO: Found 1 / 1
Sep  6 00:43:16.836: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  6 00:43:16.848: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  6 00:43:16.848: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  6 00:43:16.848: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-7871 describe pod agnhost-primary-sszdg'
Sep  6 00:43:17.006: INFO: stderr: ""
Sep  6 00:43:17.006: INFO: stdout: "Name:         agnhost-primary-sszdg\nNamespace:    kubectl-7871\nPriority:     0\nNode:         sc2-10-185-226-233.eng.vmware.com/10.185.226.233\nStart Time:   Mon, 06 Sep 2021 00:43:13 -0700\nLabels:       app=agnhost\n              role=primary\nAnnotations:  attachment_id: 07c038c7-81f7-4924-ac0b-48b77a5c7f45\n              kubernetes.io/psp: e2e-test-privileged-psp\n              mac: 04:50:56:00:00:0d\n              vlan: None\n              vmware-system-ephemeral-disk-uuid: 6000C29d-ac9a-c2b6-bcb3-82f81002edda\n              vmware-system-image-references: {\"agnhost-primary\":\"agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v1371\"}\n              vmware-system-vm-moid: vm-1366:3f0e15ce-7291-45c7-8a72-39d27d06e017\n              vmware-system-vm-uuid: 5016f4dc-a7ee-f7a1-bc61-eeb4cb0182b3\nStatus:       Running\nIP:           172.26.1.194\nIPs:\n  IP:           172.26.1.194\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   5ebf056e-b331-4739-88d6-ab608d2c2377\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Image ID:       agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v1371\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 06 Sep 2021 00:43:14 -0700\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-4s2t2 (ro)\nConditions:\n  Type              Status\n  PodScheduled      True \n  Initialized       True \n  ContainersReady   True \n  Ready             True \nVolumes:\n  default-token-4s2t2:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-4s2t2\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason                        Age   From               Message\n  ----    ------                        ----  ----               -------\n  Normal  Scheduled                     28s   default-scheduler  Successfully assigned kubectl-7871/agnhost-primary-sszdg to sc2-10-185-226-233.eng.vmware.com\n  Normal  Image                         29s   image-controller   Image agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v1371 bound successfully\n  Normal  SuccessfulRealizeNSXResource  21s   nsx-container-ncp  Successfully realized NSX resource for Pod\n  Normal  Pulling                       10s   kubelet            Waiting for Image kubectl-7871/agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v1371\n  Normal  Pulled                        10s   kubelet            Image kubectl-7871/agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v1371 is ready\n  Normal  Created                       2s    kubelet            Created container agnhost-primary\n  Normal  Started                       2s    kubelet            Started container agnhost-primary\n  Normal  SuccessfulMountVolume         2s    kubelet            Successfully mounted volume default-token-4s2t2\n"
Sep  6 00:43:17.006: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-7871 describe rc agnhost-primary'
Sep  6 00:43:17.191: INFO: stderr: ""
Sep  6 00:43:17.191: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7871\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.21\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  29s   replication-controller  Created pod: agnhost-primary-sszdg\n"
Sep  6 00:43:17.192: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-7871 describe service agnhost-primary'
Sep  6 00:43:17.334: INFO: stderr: ""
Sep  6 00:43:17.334: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7871\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                172.24.61.251\nIPs:               172.24.61.251\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.26.1.194:6379\nSession Affinity:  None\nEvents:\n  Type    Reason                        Age   From               Message\n  ----    ------                        ----  ----               -------\n  Normal  SuccessfulRealizeNSXResource  27s   nsx-container-ncp  Successful to process DLB endpoint resource\n"
Sep  6 00:43:17.349: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-7871 describe node 421629d5b5512e89fd5a479875bff24c'
Sep  6 00:43:17.626: INFO: stderr: ""
Sep  6 00:43:17.626: INFO: stdout: "Name:               421629d5b5512e89fd5a479875bff24c\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=421629d5b5512e89fd5a479875bff24c\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    vmware-system-workload-ip-configured: \n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 05 Sep 2021 19:56:51 -0700\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  421629d5b5512e89fd5a479875bff24c\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 06 Sep 2021 00:43:08 -0700\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 06 Sep 2021 00:43:13 -0700   Sun, 05 Sep 2021 20:11:07 -0700   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 06 Sep 2021 00:43:13 -0700   Sun, 05 Sep 2021 20:11:07 -0700   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 06 Sep 2021 00:43:13 -0700   Sun, 05 Sep 2021 20:11:07 -0700   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 06 Sep 2021 00:43:13 -0700   Sun, 05 Sep 2021 20:11:07 -0700   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.26.0.4\n  Hostname:    421629d5b5512e89fd5a479875bff24c\nCapacity:\n  cpu:                2\n  ephemeral-storage:  32881404Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8136988Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  32881404Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8136988Ki\n  pods:               110\nSystem Info:\n  Machine ID:                     a84e0f9b757b45c3bda61dd8d7e81c47\n  System UUID:                    d5291642-51b5-892e-fd5a-479875bff24c\n  Boot ID:                        96557016-0d11-4d1b-9d79-0a2c04c403df\n  Kernel Version:                 4.19.198-1.ph3-esx\n  OS Image:                       VMware Photon OS/Linux\n  Operating System:               linux\n  Architecture:                   amd64\n  Container Runtime Version:      containerd://1.4.4\n  Kubelet Version:                v1.20.8+vmware.wcp.1\n  Kube-Proxy Version:             v1.20.8+vmware.wcp.1\nNon-terminated Pods:              (27 in total)\n  Namespace                       Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                       ----                                                               ------------  ----------  ---------------  -------------  ---\n  kube-system                     coredns-594c6dccdd-6jv2b                                           100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     4h56m\n  kube-system                     docker-registry-421629d5b5512e89fd5a479875bff24c                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h45m\n  kube-system                     etcd-421629d5b5512e89fd5a479875bff24c                              100m (5%)     0 (0%)      100Mi (1%)       0 (0%)         4h44m\n  kube-system                     kube-apiserver-421629d5b5512e89fd5a479875bff24c                    250m (12%)    0 (0%)      0 (0%)           0 (0%)         4h39m\n  kube-system                     kube-controller-manager-421629d5b5512e89fd5a479875bff24c           200m (10%)    0 (0%)      0 (0%)           0 (0%)         4h44m\n  kube-system                     kube-proxy-cwdbw                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h38m\n  kube-system                     kube-scheduler-421629d5b5512e89fd5a479875bff24c                    100m (5%)     0 (0%)      0 (0%)           0 (0%)         4h44m\n  kube-system                     kubectl-plugin-vsphere-421629d5b5512e89fd5a479875bff24c            0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h45m\n  kube-system                     wcp-authproxy-421629d5b5512e89fd5a479875bff24c                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h43m\n  kube-system                     wcp-fip-421629d5b5512e89fd5a479875bff24c                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         4h44m\n  vmware-system-capw              capi-controller-manager-d586f4c8-dnnr6                             0 (0%)        0 (0%)      150Mi (1%)       1800Mi (22%)   4h52m\n  vmware-system-capw              capi-kubeadm-bootstrap-controller-manager-5f5774d559-6cbtc         0 (0%)        0 (0%)      150Mi (1%)       1200Mi (15%)   4h52m\n  vmware-system-capw              capi-kubeadm-bootstrap-webhook-6766c687f9-hjz5w                    0 (0%)        0 (0%)      150Mi (1%)       1200Mi (15%)   4h52m\n  vmware-system-capw              capi-kubeadm-control-plane-controller-manager-7c8d4b456f-l4t5v     0 (0%)        0 (0%)      150Mi (1%)       1200Mi (15%)   4h52m\n  vmware-system-capw              capi-kubeadm-control-plane-webhook-5f4c87b8b9-cphz4                0 (0%)        0 (0%)      150Mi (1%)       1200Mi (15%)   4h52m\n  vmware-system-capw              capi-webhook-69769f4c68-xjnm2                                      0 (0%)        0 (0%)      150Mi (1%)       1200Mi (15%)   4h52m\n  vmware-system-capw              capw-controller-manager-85b7cbb4bf-bghkv                           10m (0%)      100m (5%)   150Mi (1%)       800Mi (10%)    4h52m\n  vmware-system-capw              capw-webhook-58b86fb8b-vrzsj                                       10m (0%)      100m (5%)   150Mi (1%)       800Mi (10%)    4h52m\n  vmware-system-license-operator  vmware-system-license-operator-controller-manager-5f5b6cddmtkxt    100m (5%)     150m (7%)   20Mi (0%)        300Mi (3%)     4h50m\n  vmware-system-logging           fluentbit-rpz54                                                    100m (5%)     500m (25%)  0 (0%)           0 (0%)         4h45m\n  vmware-system-nsop              vmware-system-nsop-controller-manager-7dd8c47dcc-hbv6m             20m (1%)      100m (5%)   20Mi (0%)        300Mi (3%)     4h49m\n  vmware-system-tkg               masterproxy-tkgs-plugin-k5csq                                      10m (0%)      250m (12%)  75Mi (0%)        200Mi (2%)     4h39m\n  vmware-system-tkg               tkgs-plugin-server-57df5fcfbf-km5pj                                100m (5%)     800m (40%)  256Mi (3%)       512Mi (6%)     4h50m\n  vmware-system-tkg               vmware-system-tkg-controller-manager-575d95fb57-zl7sg              110m (5%)     400m (20%)  120Mi (1%)       460Mi (5%)     4h51m\n  vmware-system-tkg               vmware-system-tkg-webhook-64fd4868cb-mk6zh                         20m (1%)      400m (20%)  90Mi (1%)        120Mi (1%)     4h51m\n  vmware-system-ucs               upgrade-compatibility-service-65969fd6bc-l72w9                     100m (5%)     1 (50%)     20Mi (0%)        600Mi (7%)     4h56m\n  vmware-system-vmop              vmware-system-vmop-controller-manager-6649dd65b-q7bqw              100m (5%)     250m (12%)  75Mi (0%)        400Mi (5%)     4h51m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests      Limits\n  --------           --------      ------\n  cpu                1430m (71%)   4050m (202%)\n  memory             2046Mi (25%)  12462Mi (156%)\n  ephemeral-storage  0 (0%)        0 (0%)\n  hugepages-1Gi      0 (0%)        0 (0%)\n  hugepages-2Mi      0 (0%)        0 (0%)\nEvents:              <none>\n"
Sep  6 00:43:17.626: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-7871 describe namespace kubectl-7871'
Sep  6 00:43:17.766: INFO: stderr: ""
Sep  6 00:43:17.766: INFO: stdout: "Name:         kubectl-7871\nLabels:       e2e-framework=kubectl\n              e2e-run=854bdb38-70c3-48cb-a727-f932682d0ef7\n              vSphereClusterID=domain-c9\nAnnotations:  ls_id-0: 16940472-e82f-46fb-b4a2-e669acc3a1ca\n              ncp/extpoolid: domain-c9:3f0e15ce-7291-45c7-8a72-39d27d06e017-ippool-192-168-124-1-192-168-124-254\n              ncp/router_id: t1_0b5e4f99-1f58-42b0-aaff-b0e5be23fc4d_rtr\n              ncp/snat_ip: 192.168.124.33\n              ncp/subnet-0: 172.26.1.192/28\n              vmware-system-namespace-owner-count: 1\n              vmware-system-resource-pool: resgroup-1364\n              vmware-system-resource-pool-cpu-limit: 23.8660\n              vmware-system-resource-pool-memory-limit: 5000Mi\n              vmware-system-self-service-namespace: true\n              vmware-system-vm-folder: group-v1365\nStatus:       Active\n\nResource Quotas\n  Name:                                                                   kubectl-7871\n  Resource                                                                Used  Hard\n  --------                                                                ---   ---\n  requests.storage                                                        0     10000Mi\n  Name:                                                                   kubectl-7871-storagequota\n  Resource                                                                Used  Hard\n  --------                                                                ---   ---\n  wcpglobal-storage-profile.storageclass.storage.k8s.io/requests.storage  0     9223372036854775807\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:43:17.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7871" for this suite.

• [SLOW TEST:31.326 seconds]
[sig-cli] Kubectl client
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1090
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":296,"completed":238,"skipped":4208,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:43:18.098: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9629
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support proportional scaling [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  6 00:43:18.617: INFO: Creating deployment "webserver-deployment"
Sep  6 00:43:18.691: INFO: Waiting for observed generation 1
Sep  6 00:43:20.753: INFO: Waiting for all required pods to come up
Sep  6 00:43:20.775: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep  6 00:44:22.874: INFO: Waiting for deployment "webserver-deployment" to complete
Sep  6 00:44:22.906: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep  6 00:44:22.974: INFO: Updating deployment webserver-deployment
Sep  6 00:44:22.974: INFO: Waiting for observed generation 2
Sep  6 00:44:25.019: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep  6 00:44:25.037: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep  6 00:44:25.064: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  6 00:44:25.129: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep  6 00:44:25.129: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep  6 00:44:25.142: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  6 00:44:25.158: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep  6 00:44:25.158: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep  6 00:44:25.205: INFO: Updating deployment webserver-deployment
Sep  6 00:44:25.205: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep  6 00:44:25.230: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep  6 00:44:25.252: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Sep  6 00:44:27.396: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9629 /apis/apps/v1/namespaces/deployment-9629/deployments/webserver-deployment 35514281-0a6a-4fcc-9f66-20d4aa5ff964 202936 3 2021-09-06 00:43:18 -0700 PDT <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-09-06 00:43:18 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-06 00:44:23 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a44ce38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-09-06 00:44:25 -0700 PDT,LastTransitionTime:2021-09-06 00:44:25 -0700 PDT,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-09-06 00:44:26 -0700 PDT,LastTransitionTime:2021-09-06 00:43:18 -0700 PDT,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep  6 00:44:27.443: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-9629 /apis/apps/v1/namespaces/deployment-9629/replicasets/webserver-deployment-795d758f88 4f1f9f5f-04e4-4794-9dfa-bff834f638c2 202930 3 2021-09-06 00:44:23 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 35514281-0a6a-4fcc-9f66-20d4aa5ff964 0xc005c18117 0xc005c18118}] []  [{kube-controller-manager Update apps/v1 2021-09-06 00:44:23 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35514281-0a6a-4fcc-9f66-20d4aa5ff964\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005c181a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 00:44:27.443: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep  6 00:44:27.444: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-867f44f6fb  deployment-9629 /apis/apps/v1/namespaces/deployment-9629/replicasets/webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 202934 3 2021-09-06 00:43:18 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 35514281-0a6a-4fcc-9f66-20d4aa5ff964 0xc005c18227 0xc005c18228}] []  [{kube-controller-manager Update apps/v1 2021-09-06 00:44:03 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35514281-0a6a-4fcc-9f66-20d4aa5ff964\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 867f44f6fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005c18298 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep  6 00:44:27.477: INFO: Pod "webserver-deployment-795d758f88-5vpsj" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-5vpsj webserver-deployment-795d758f88- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-795d758f88-5vpsj f1b45971-62c9-4d3a-94ac-ab05dac49f15 202972 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4f1f9f5f-04e4-4794-9dfa-bff834f638c2 0xc002f58217 0xc002f58218}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1f9f5f-04e4-4794-9dfa-bff834f638c2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-233.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:27 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.477: INFO: Pod "webserver-deployment-795d758f88-6vxwp" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-6vxwp webserver-deployment-795d758f88- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-795d758f88-6vxwp d67e788d-8739-451c-9062-f3207b3cef89 202860 0 2021-09-06 00:44:23 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp vmware-system-image-references:{"httpd":"webserver-ea8d19c56b2978338502181157e293b3d96a2b18-v58548"}] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4f1f9f5f-04e4-4794-9dfa-bff834f638c2 0xc002f58340 0xc002f58341}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:23 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1f9f5f-04e4-4794-9dfa-bff834f638c2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-06 00:44:24 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-150.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:23 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.477: INFO: Pod "webserver-deployment-795d758f88-8jnvt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-8jnvt webserver-deployment-795d758f88- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-795d758f88-8jnvt 4aa23dae-a444-40a2-a0e5-bcaa427550f4 202925 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4f1f9f5f-04e4-4794-9dfa-bff834f638c2 0xc002f584b0 0xc002f584b1}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1f9f5f-04e4-4794-9dfa-bff834f638c2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-233-127.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:26 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.478: INFO: Pod "webserver-deployment-795d758f88-94l4j" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-94l4j webserver-deployment-795d758f88- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-795d758f88-94l4j d16d9e8d-c8f3-4742-b261-618ae11a53b8 202923 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4f1f9f5f-04e4-4794-9dfa-bff834f638c2 0xc002f585e0 0xc002f585e1}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1f9f5f-04e4-4794-9dfa-bff834f638c2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.478: INFO: Pod "webserver-deployment-795d758f88-f9cfv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-f9cfv webserver-deployment-795d758f88- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-795d758f88-f9cfv 71c164c8-0d13-43b7-a380-480c66b8f59c 202859 0 2021-09-06 00:44:23 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp vmware-system-image-references:{"httpd":"webserver-ea8d19c56b2978338502181157e293b3d96a2b18-v5443"}] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4f1f9f5f-04e4-4794-9dfa-bff834f638c2 0xc002f58700 0xc002f58701}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:23 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1f9f5f-04e4-4794-9dfa-bff834f638c2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-06 00:44:24 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-233-127.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:23 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.478: INFO: Pod "webserver-deployment-795d758f88-h257r" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-h257r webserver-deployment-795d758f88- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-795d758f88-h257r 88e4b7d7-00b5-469a-a4ec-f7ed614f8ae9 202858 0 2021-09-06 00:44:23 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp vmware-system-image-references:{"httpd":"webserver-ea8d19c56b2978338502181157e293b3d96a2b18-v25414"}] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4f1f9f5f-04e4-4794-9dfa-bff834f638c2 0xc002f58850 0xc002f58851}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:23 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1f9f5f-04e4-4794-9dfa-bff834f638c2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-06 00:44:24 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-150.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:23 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.478: INFO: Pod "webserver-deployment-795d758f88-kvwmf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kvwmf webserver-deployment-795d758f88- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-795d758f88-kvwmf 1dfa746a-5e1f-466b-9649-32035993f0e4 202861 0 2021-09-06 00:44:23 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp vmware-system-image-references:{"httpd":"webserver-ea8d19c56b2978338502181157e293b3d96a2b18-v45729"}] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4f1f9f5f-04e4-4794-9dfa-bff834f638c2 0xc002f589a0 0xc002f589a1}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:23 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1f9f5f-04e4-4794-9dfa-bff834f638c2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-06 00:44:24 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-233.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:23 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.478: INFO: Pod "webserver-deployment-795d758f88-lb4l4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-lb4l4 webserver-deployment-795d758f88- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-795d758f88-lb4l4 789969d7-bc1b-4a4f-b48f-eb5a2b8f4710 202915 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4f1f9f5f-04e4-4794-9dfa-bff834f638c2 0xc002f58af0 0xc002f58af1}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1f9f5f-04e4-4794-9dfa-bff834f638c2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.478: INFO: Pod "webserver-deployment-795d758f88-m7qf7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-m7qf7 webserver-deployment-795d758f88- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-795d758f88-m7qf7 e0c91055-51ed-44cd-9d0d-fb3ff6c9ce5c 202905 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4f1f9f5f-04e4-4794-9dfa-bff834f638c2 0xc002f58c20 0xc002f58c21}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1f9f5f-04e4-4794-9dfa-bff834f638c2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.479: INFO: Pod "webserver-deployment-795d758f88-pd4pt" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-pd4pt webserver-deployment-795d758f88- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-795d758f88-pd4pt 9006f21d-9b79-4357-947a-20cf44f129de 202910 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4f1f9f5f-04e4-4794-9dfa-bff834f638c2 0xc002f58d40 0xc002f58d41}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1f9f5f-04e4-4794-9dfa-bff834f638c2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.479: INFO: Pod "webserver-deployment-795d758f88-rckw5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rckw5 webserver-deployment-795d758f88- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-795d758f88-rckw5 875724d6-9f54-48fe-9eac-5d3ea88745ff 202867 0 2021-09-06 00:44:23 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp vmware-system-image-references:{"httpd":"webserver-ea8d19c56b2978338502181157e293b3d96a2b18-v78129"}] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4f1f9f5f-04e4-4794-9dfa-bff834f638c2 0xc002f58e60 0xc002f58e61}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:23 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1f9f5f-04e4-4794-9dfa-bff834f638c2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-06 00:44:24 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-233.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:23 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.479: INFO: Pod "webserver-deployment-795d758f88-s8nf7" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-s8nf7 webserver-deployment-795d758f88- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-795d758f88-s8nf7 0ef2e2be-c992-4db4-92b0-15e4a8ee860f 202906 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4f1f9f5f-04e4-4794-9dfa-bff834f638c2 0xc002f58fb0 0xc002f58fb1}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1f9f5f-04e4-4794-9dfa-bff834f638c2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.479: INFO: Pod "webserver-deployment-795d758f88-t94cx" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-t94cx webserver-deployment-795d758f88- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-795d758f88-t94cx 9b81f552-5f2c-469e-80a9-6d2b26ed9fa3 202970 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 4f1f9f5f-04e4-4794-9dfa-bff834f638c2 0xc002f590d0 0xc002f590d1}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f1f9f5f-04e4-4794-9dfa-bff834f638c2\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-150.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:27 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.479: INFO: Pod "webserver-deployment-867f44f6fb-4pc8w" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-4pc8w webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-4pc8w 0821e286-495e-4259-9d12-6dc02dbc33a3 202527 0 2021-09-06 00:43:19 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:4c407ea2-5070-46b2-aca4-548647b29b18 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:00:29 vlan:None vmware-system-ephemeral-disk-uuid:6000C29e-f0a1-1d24-2ee4-739e0970e6bd vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v51297"} vmware-system-vm-moid:vm-1376:3f0e15ce-7291-45c7-8a72-39d27d06e017 vmware-system-vm-uuid:50162519-cd73-4908-af74-8c23a3bc21f1] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc002f59227 0xc002f59228}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:43:19 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-df5sk Update v1 2021-09-06 00:43:30 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {image-controller Update v1 2021-09-06 00:43:36 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {scheduler-extender Update v1 2021-09-06 00:43:52 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:44:03 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-150.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:43:20 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:03 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:03 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:03 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.185.226.150,PodIP:172.26.1.218,StartTime:2021-09-06 00:44:00 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:44:01 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v51297,ContainerID:81ddc9eb-354f-4764-9f32-97a6d7b60c27,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.218,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.479: INFO: Pod "webserver-deployment-867f44f6fb-55mvm" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-55mvm webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-55mvm dc763e44-d27c-423e-b9a4-ead07bb46390 202965 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc002f59407 0xc002f59408}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-233-127.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:27 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.480: INFO: Pod "webserver-deployment-867f44f6fb-5l576" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-5l576 webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-5l576 976aecf2-450b-4d77-8050-6292966c83a7 202962 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc002f59520 0xc002f59521}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-233.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:26 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.480: INFO: Pod "webserver-deployment-867f44f6fb-752gr" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-752gr webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-752gr f869f91b-dc12-49dc-beac-f82195cda670 202919 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc002f59640 0xc002f59641}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-233.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:25 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.480: INFO: Pod "webserver-deployment-867f44f6fb-94xvv" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-94xvv webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-94xvv 46be3ea4-4aca-4908-8470-b59ff664425c 202917 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc002f59790 0xc002f59791}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.480: INFO: Pod "webserver-deployment-867f44f6fb-997x2" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-997x2 webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-997x2 5ebbba01-7062-4ac2-aa96-4169b702cf3e 202752 0 2021-09-06 00:43:18 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:7f9c34bf-c3f8-4b3f-8ab8-241a1056c652 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:00:20 vlan:None vmware-system-ephemeral-disk-uuid:6000C299-96f4-bd67-abbe-13bbb47788c2 vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v75362"} vmware-system-vm-moid:vm-1374:3f0e15ce-7291-45c7-8a72-39d27d06e017 vmware-system-vm-uuid:50161427-2bfb-b712-1aa9-e6ed61e2ae8b] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc002f598b7 0xc002f598b8}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:43:18 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-06 00:43:19 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {nsx-ncp-6d7f7bf559-df5sk Update v1 2021-09-06 00:43:29 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-06 00:44:09 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:44:21 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.213\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-233.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:43:19 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:21 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:21 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:21 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.185.226.233,PodIP:172.26.1.213,StartTime:2021-09-06 00:44:19 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:44:21 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v75362,ContainerID:b3e6f6a3-38e8-4f9d-bf81-b659c74806b0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.213,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.481: INFO: Pod "webserver-deployment-867f44f6fb-dnwdj" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-dnwdj webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-dnwdj bd463fef-997c-440e-b8af-b68f9785a238 202567 0 2021-09-06 00:43:18 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:47ab1236-1c76-4017-a0a0-755f10a7d436 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:00:26 vlan:None vmware-system-ephemeral-disk-uuid:6000C29d-061e-dac9-650e-4575c35a0a56 vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v51297"} vmware-system-vm-moid:vm-1375:3f0e15ce-7291-45c7-8a72-39d27d06e017 vmware-system-vm-uuid:5016dee4-e2da-31ab-e3e6-108b350462f8] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc002f59acf 0xc002f59ae0}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:43:18 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-df5sk Update v1 2021-09-06 00:43:30 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {image-controller Update v1 2021-09-06 00:43:36 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {scheduler-extender Update v1 2021-09-06 00:43:52 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:44:05 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-233-127.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:43:19 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:06 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:06 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:06 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.185.233.127,PodIP:172.26.1.216,StartTime:2021-09-06 00:44:02 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:44:05 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v51297,ContainerID:9c187d31-aedd-45ab-b9e3-f92daa0b7d6b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.481: INFO: Pod "webserver-deployment-867f44f6fb-dx6wj" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-dx6wj webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-dx6wj 0e8d0835-32c9-4522-abd3-04f78fc6eed9 202918 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc002f59cd7 0xc002f59cd8}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.481: INFO: Pod "webserver-deployment-867f44f6fb-dxsdz" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-dxsdz webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-dxsdz da1d58c0-3c08-40aa-b28e-b783edc2927a 202914 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc002f59de0 0xc002f59de1}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.481: INFO: Pod "webserver-deployment-867f44f6fb-gc9sk" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-gc9sk webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-gc9sk e2ee5a5a-afb9-4439-a29e-a95cf7ebb963 202958 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc002f59ef0 0xc002f59ef1}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-150.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:26 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.481: INFO: Pod "webserver-deployment-867f44f6fb-km9kw" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-km9kw webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-km9kw 8fdfc8a5-3c14-4c37-afaf-12f9e3ca269b 202528 0 2021-09-06 00:43:19 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:e45f160f-ea6e-44e1-a294-069d3e8daded kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:00:22 vlan:None vmware-system-ephemeral-disk-uuid:6000C29e-2665-3c5c-4ead-c793afcbc2b0 vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v51297"} vmware-system-vm-moid:vm-1373:3f0e15ce-7291-45c7-8a72-39d27d06e017 vmware-system-vm-uuid:501673d4-eeab-e7f0-5641-e42f2b48d1cf] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc0085f8027 0xc0085f8028}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:43:18 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-df5sk Update v1 2021-09-06 00:43:30 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {image-controller Update v1 2021-09-06 00:43:36 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {scheduler-extender Update v1 2021-09-06 00:43:52 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:44:03 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.215\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-150.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:43:19 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:04 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:04 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:04 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.185.226.150,PodIP:172.26.1.215,StartTime:2021-09-06 00:44:01 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:44:03 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v51297,ContainerID:820c29a9-597f-46c8-b620-af51e5e33149,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.481: INFO: Pod "webserver-deployment-867f44f6fb-kmfrz" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-kmfrz webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-kmfrz 9108be83-63d0-4139-9e59-ba6a94e12437 202807 0 2021-09-06 00:43:18 -0700 PDT 2021-09-06 00:44:23 -0700 PDT 0xc0085f81f8 map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:2ca049e4-2732-45e4-9d9d-5fd0437a55be kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:00:1e vlan:None vmware-system-ephemeral-disk-uuid:6000C29f-2cd7-ecad-4180-3692b6b4832a vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v62142"} vmware-system-vm-moid:vm-1370:3f0e15ce-7291-45c7-8a72-39d27d06e017 vmware-system-vm-uuid:5016632b-7718-fe58-07b1-7744c7e31fd8] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc0085f8247 0xc0085f8248}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:43:18 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-06 00:43:19 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {nsx-ncp-6d7f7bf559-df5sk Update v1 2021-09-06 00:43:30 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-06 00:44:09 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:44:21 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.211\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-233.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:43:19 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:22 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:22 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:22 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.185.226.233,PodIP:172.26.1.211,StartTime:2021-09-06 00:44:19 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:44:20 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v62142,ContainerID:7131ad2e-6e12-494d-87f7-2182a631d71b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.482: INFO: Pod "webserver-deployment-867f44f6fb-mbqpl" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-mbqpl webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-mbqpl c13b5d11-8e16-474d-b56f-93f484951e87 202955 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v51297"}] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc0085f8427 0xc0085f8428}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-06 00:44:26 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-233-127.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:25 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.482: INFO: Pod "webserver-deployment-867f44f6fb-pdqxw" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-pdqxw webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-pdqxw 6bb89802-c95b-404d-a66e-a9663007a129 202913 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc0085f8560 0xc0085f8561}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.482: INFO: Pod "webserver-deployment-867f44f6fb-qgj22" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-qgj22 webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-qgj22 60cc84ab-8e23-4402-9d7e-353e20e31dd3 202566 0 2021-09-06 00:43:19 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:24a886e3-994d-4de5-abc8-36432cd25550 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:00:2a vlan:None vmware-system-ephemeral-disk-uuid:6000C29d-9ead-d0e1-7429-0dcca4fd4301 vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v62142"} vmware-system-vm-moid:vm-1377:3f0e15ce-7291-45c7-8a72-39d27d06e017 vmware-system-vm-uuid:50163135-b1fa-5d33-007e-5f8033c4ee67] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc0085f8697 0xc0085f8698}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:43:19 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-df5sk Update v1 2021-09-06 00:43:31 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {image-controller Update v1 2021-09-06 00:43:37 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {scheduler-extender Update v1 2021-09-06 00:43:52 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:44:05 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-233-127.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:43:20 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:06 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:06 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:06 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.185.233.127,PodIP:172.26.1.219,StartTime:2021-09-06 00:44:01 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:44:04 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v62142,ContainerID:abb19097-d423-446f-9160-9be863c0f7f0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.219,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.482: INFO: Pod "webserver-deployment-867f44f6fb-qqwz4" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-qqwz4 webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-qqwz4 9beab4b9-e0d7-431a-ae7e-c7dd88e6819a 202808 0 2021-09-06 00:43:19 -0700 PDT 2021-09-06 00:44:23 -0700 PDT 0xc0085f8868 map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:8b52dec0-bf47-4b6f-82ae-f6e78cb8f972 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:00:27 vlan:None vmware-system-ephemeral-disk-uuid:6000C299-61cd-ebc9-0172-30aace440ea2 vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v51297"} vmware-system-vm-moid:vm-1379:3f0e15ce-7291-45c7-8a72-39d27d06e017 vmware-system-vm-uuid:5016a72a-1fc5-1721-3cc6-298b03d5bf26] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc0085f88b7 0xc0085f88b8}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:43:19 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-df5sk Update v1 2021-09-06 00:43:30 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {image-controller Update v1 2021-09-06 00:43:37 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {scheduler-extender Update v1 2021-09-06 00:44:09 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:44:21 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.217\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-233.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:43:19 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:22 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:22 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:22 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.185.226.233,PodIP:172.26.1.217,StartTime:2021-09-06 00:44:20 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:44:21 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v51297,ContainerID:6b046bfd-d1b4-46b6-8a12-7ba1be271e04,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.217,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.482: INFO: Pod "webserver-deployment-867f44f6fb-rh9qv" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-rh9qv webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-rh9qv 2683bc20-7d78-4784-a0f9-a8eaf0433cc8 202974 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v75362"}] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc0085f8a97 0xc0085f8a98}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-06 00:44:27 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-150.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:25 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.482: INFO: Pod "webserver-deployment-867f44f6fb-t82sm" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-t82sm webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-t82sm 23e4056c-dbb7-4316-a7f7-e291b000b526 202751 0 2021-09-06 00:43:18 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:a9a2cec6-2917-4e8a-978e-cbbeebc9563c kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:00:1c vlan:None vmware-system-ephemeral-disk-uuid:6000C296-3abe-2ae4-1131-91773a4479fe vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v51297"} vmware-system-vm-moid:vm-1371:3f0e15ce-7291-45c7-8a72-39d27d06e017 vmware-system-vm-uuid:5016ca79-635f-c9a8-34b6-f7fb36f741a0] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc0085f8c07 0xc0085f8c08}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:43:18 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-06 00:43:19 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {nsx-ncp-6d7f7bf559-df5sk Update v1 2021-09-06 00:43:28 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-06 00:44:09 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:44:21 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-233.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:43:19 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:21 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:21 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:21 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.185.226.233,PodIP:172.26.1.210,StartTime:2021-09-06 00:44:17 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:44:19 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v51297,ContainerID:e0535954-4c7e-4b1a-beef-ad1f43a61edc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.210,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.482: INFO: Pod "webserver-deployment-867f44f6fb-wcgv2" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-wcgv2 webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-wcgv2 3fbdff1c-1e6d-435b-a7d5-6fa5b3aba391 202916 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc0085f8df7 0xc0085f8df8}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.483: INFO: Pod "webserver-deployment-867f44f6fb-x7k89" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-x7k89 webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-x7k89 8469fd50-c625-4d25-9308-823adc7391dd 202902 0 2021-09-06 00:44:25 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc0085f8f00 0xc0085f8f01}] []  [{kube-controller-manager Update v1 2021-09-06 00:44:25 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.483: INFO: Pod "webserver-deployment-867f44f6fb-xnhqh" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-xnhqh webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-xnhqh 4de71817-b41b-49b7-8fbd-43f4d62f7a95 202763 0 2021-09-06 00:43:18 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:b9d2dbc3-2feb-4cb7-b543-e69687edfb61 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:00:1f vlan:None vmware-system-ephemeral-disk-uuid:6000C292-311a-09ba-5cd0-05bbde657f22 vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v50513"} vmware-system-vm-moid:vm-1372:3f0e15ce-7291-45c7-8a72-39d27d06e017 vmware-system-vm-uuid:50165ac7-37da-8c82-c8f1-d9a8df2ec101] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc0085f9027 0xc0085f9028}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:43:18 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-06 00:43:19 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {nsx-ncp-6d7f7bf559-df5sk Update v1 2021-09-06 00:43:29 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-06 00:44:09 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:44:21 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.212\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-233.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:43:19 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:21 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:21 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:21 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.185.226.233,PodIP:172.26.1.212,StartTime:2021-09-06 00:44:18 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:44:19 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v50513,ContainerID:41a13b2c-8b19-46e6-8161-681c41d66afb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.212,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:44:27.483: INFO: Pod "webserver-deployment-867f44f6fb-zm8hr" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-zm8hr webserver-deployment-867f44f6fb- deployment-9629 /api/v1/namespaces/deployment-9629/pods/webserver-deployment-867f44f6fb-zm8hr 3cdf8223-e4e3-47ea-8687-c997bc40a632 202755 0 2021-09-06 00:43:18 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:eebc66c5-e1b7-4db4-a9b8-e1539bc55244 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:00:21 vlan:None vmware-system-ephemeral-disk-uuid:6000C295-2196-9a0c-82e7-4f54f1d3a94f vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v41097"} vmware-system-vm-moid:vm-1378:3f0e15ce-7291-45c7-8a72-39d27d06e017 vmware-system-vm-uuid:501612dc-b644-635b-3251-4fad862c9e6c] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 39a89dcb-d2c2-4eb4-9ae1-ccb198f76712 0xc0085f921f 0xc0085f9230}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:43:18 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39a89dcb-d2c2-4eb4-9ae1-ccb198f76712\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-06 00:43:19 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {nsx-ncp-6d7f7bf559-df5sk Update v1 2021-09-06 00:43:30 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-06 00:44:09 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:44:21 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.214\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-fj4gw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-fj4gw,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-fj4gw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-233.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:43:19 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:21 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:21 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:44:21 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.185.226.233,PodIP:172.26.1.214,StartTime:2021-09-06 00:44:16 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:44:17 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v41097,ContainerID:27988b57-dd12-4693-89df-f5c0016bbed2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.214,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:44:27.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9629" for this suite.

• [SLOW TEST:69.932 seconds]
[sig-apps] Deployment
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":296,"completed":239,"skipped":4233,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:44:28.031: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1204
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-1df83f5e-baa4-4ad3-a3e4-cdfc5a855a23
STEP: Creating a pod to test consume configMaps
Sep  6 00:44:29.133: INFO: Waiting up to 5m0s for pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5" in namespace "configmap-1204" to be "Succeeded or Failed"
Sep  6 00:44:29.164: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 31.097913ms
Sep  6 00:44:31.192: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.059558579s
Sep  6 00:44:33.244: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.110993969s
Sep  6 00:44:35.275: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.142260383s
Sep  6 00:44:37.320: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.18755384s
Sep  6 00:44:39.368: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.235459923s
Sep  6 00:44:41.403: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.270458502s
Sep  6 00:44:43.428: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.29515986s
Sep  6 00:44:45.454: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.321536665s
Sep  6 00:44:47.487: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.354519677s
Sep  6 00:44:49.951: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.818108071s
Sep  6 00:44:51.991: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.858628715s
Sep  6 00:44:54.025: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.892740841s
Sep  6 00:44:56.043: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.910495998s
Sep  6 00:44:58.072: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.939673452s
Sep  6 00:45:00.092: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.959518091s
Sep  6 00:45:02.134: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 33.001626552s
Sep  6 00:45:04.211: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 35.078036431s
Sep  6 00:45:06.241: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 37.108309824s
Sep  6 00:45:08.261: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 39.127996116s
Sep  6 00:45:10.298: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 41.165831984s
Sep  6 00:45:12.326: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 43.193713932s
Sep  6 00:45:14.348: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 45.215054005s
Sep  6 00:45:16.362: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 47.229494034s
Sep  6 00:45:18.387: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 49.254244821s
Sep  6 00:45:20.397: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 51.26472209s
Sep  6 00:45:22.413: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 53.280230906s
Sep  6 00:45:24.426: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 55.293068877s
Sep  6 00:45:26.440: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 57.30726464s
Sep  6 00:45:28.453: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 59.320360083s
Sep  6 00:45:30.464: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m1.331817133s
Sep  6 00:45:32.482: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m3.34915019s
Sep  6 00:45:34.495: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m5.362517808s
Sep  6 00:45:36.511: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m7.378081794s
Sep  6 00:45:38.520: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m9.387264702s
Sep  6 00:45:40.545: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m11.412892112s
Sep  6 00:45:42.566: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 1m13.43378877s
STEP: Saw pod success
Sep  6 00:45:42.566: INFO: Pod "pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5" satisfied condition "Succeeded or Failed"
Sep  6 00:45:42.576: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5 container agnhost-container: <nil>
STEP: delete the pod
Sep  6 00:45:42.677: INFO: Waiting for pod pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5 to disappear
Sep  6 00:45:42.686: INFO: Pod pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5 still exists
Sep  6 00:45:44.687: INFO: Waiting for pod pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5 to disappear
Sep  6 00:45:44.714: INFO: Pod pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5 still exists
Sep  6 00:45:46.686: INFO: Waiting for pod pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5 to disappear
Sep  6 00:45:46.710: INFO: Pod pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5 still exists
Sep  6 00:45:48.687: INFO: Waiting for pod pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5 to disappear
Sep  6 00:45:48.700: INFO: Pod pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5 still exists
Sep  6 00:45:50.686: INFO: Waiting for pod pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5 to disappear
Sep  6 00:45:50.700: INFO: Pod pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5 still exists
Sep  6 00:45:52.686: INFO: Waiting for pod pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5 to disappear
Sep  6 00:45:52.703: INFO: Pod pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5 still exists
Sep  6 00:45:54.687: INFO: Waiting for pod pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5 to disappear
Sep  6 00:45:54.706: INFO: Pod pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5 still exists
Sep  6 00:45:56.686: INFO: Waiting for pod pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5 to disappear
Sep  6 00:45:56.738: INFO: Pod pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5 still exists
Sep  6 00:45:58.686: INFO: Waiting for pod pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5 to disappear
Sep  6 00:45:58.703: INFO: Pod pod-configmaps-83a94ef7-fafa-42dd-8c29-7e69255cd4d5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:45:58.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1204" for this suite.

• [SLOW TEST:91.147 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":296,"completed":240,"skipped":4251,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:45:59.177: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-1287
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5041
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5227
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:46:11.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1287" for this suite.
STEP: Destroying namespace "nsdeletetest-5041" for this suite.
Sep  6 00:46:12.153: INFO: Namespace nsdeletetest-5041 was already deleted
STEP: Destroying namespace "nsdeletetest-5227" for this suite.

• [SLOW TEST:13.229 seconds]
[sig-api-machinery] Namespaces [Serial]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":296,"completed":241,"skipped":4251,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:46:12.407: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1556
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:150
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:46:13.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1556" for this suite.
•{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":296,"completed":242,"skipped":4286,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:46:13.943: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5139
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep  6 00:46:14.650: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f" in namespace "projected-5139" to be "Succeeded or Failed"
Sep  6 00:46:14.675: INFO: Pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.073688ms
Sep  6 00:46:16.704: INFO: Pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053396234s
Sep  6 00:46:18.735: INFO: Pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.084800382s
Sep  6 00:46:20.748: INFO: Pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.097264384s
Sep  6 00:46:22.773: INFO: Pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.122579319s
Sep  6 00:46:24.795: INFO: Pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.144994812s
Sep  6 00:46:26.812: INFO: Pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.161991935s
Sep  6 00:46:28.829: INFO: Pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.178789211s
Sep  6 00:46:30.847: INFO: Pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.196596765s
Sep  6 00:46:32.865: INFO: Pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.214990611s
Sep  6 00:46:34.887: INFO: Pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.236803092s
Sep  6 00:46:36.902: INFO: Pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.251479965s
Sep  6 00:46:38.918: INFO: Pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.267180919s
Sep  6 00:46:40.931: INFO: Pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f": Phase="Pending", Reason="", readiness=false. Elapsed: 26.280712714s
Sep  6 00:46:42.943: INFO: Pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.292076104s
Sep  6 00:46:44.954: INFO: Pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f": Phase="Pending", Reason="", readiness=false. Elapsed: 30.303457935s
Sep  6 00:46:46.973: INFO: Pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f": Phase="Pending", Reason="", readiness=false. Elapsed: 32.32262667s
Sep  6 00:46:49.007: INFO: Pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.356058587s
STEP: Saw pod success
Sep  6 00:46:49.007: INFO: Pod "downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f" satisfied condition "Succeeded or Failed"
Sep  6 00:46:49.023: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f container client-container: <nil>
STEP: delete the pod
Sep  6 00:46:54.520: INFO: Waiting for pod downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f to disappear
Sep  6 00:46:54.547: INFO: Pod downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f still exists
Sep  6 00:46:56.548: INFO: Waiting for pod downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f to disappear
Sep  6 00:46:56.561: INFO: Pod downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f still exists
Sep  6 00:46:58.548: INFO: Waiting for pod downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f to disappear
Sep  6 00:46:58.560: INFO: Pod downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f still exists
Sep  6 00:47:00.548: INFO: Waiting for pod downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f to disappear
Sep  6 00:47:00.561: INFO: Pod downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f still exists
Sep  6 00:47:02.548: INFO: Waiting for pod downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f to disappear
Sep  6 00:47:02.559: INFO: Pod downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f still exists
Sep  6 00:47:04.548: INFO: Waiting for pod downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f to disappear
Sep  6 00:47:04.563: INFO: Pod downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f still exists
Sep  6 00:47:06.547: INFO: Waiting for pod downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f to disappear
Sep  6 00:47:06.567: INFO: Pod downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f still exists
Sep  6 00:47:08.548: INFO: Waiting for pod downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f to disappear
Sep  6 00:47:08.561: INFO: Pod downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f still exists
Sep  6 00:47:10.548: INFO: Waiting for pod downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f to disappear
Sep  6 00:47:10.561: INFO: Pod downwardapi-volume-a43f87cf-3151-4622-af3d-ad1c968c784f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:47:10.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5139" for this suite.

• [SLOW TEST:56.861 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":296,"completed":243,"skipped":4303,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:47:10.805: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1214
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Sep  6 00:47:11.258: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 00:47:11.294: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 00:47:11.326: INFO: 
Logging pods the apiserver thinks is on node sc2-10-185-226-150.eng.vmware.com before test
Sep  6 00:47:11.380: INFO: podwithpersistentvolume from storage-policy-test started at 2021-09-05 20:29:04 -0700 PDT (1 container statuses recorded)
Sep  6 00:47:11.381: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:47:11.381: INFO: hello-web-6b97664bd5-zznrr from test-cluster-ip-service started at 2021-09-05 22:45:00 -0700 PDT (1 container statuses recorded)
Sep  6 00:47:11.381: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 00:47:11.381: INFO: wcp-sanity-busybox-6f999d6849-kspp7 from test-dataprovider-podvms-ns started at 2021-09-05 20:17:25 -0700 PDT (1 container statuses recorded)
Sep  6 00:47:11.381: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:47:11.381: INFO: curl-pod from test-network-policy started at 2021-09-05 20:38:23 -0700 PDT (1 container statuses recorded)
Sep  6 00:47:11.381: INFO: 	Container curl-container ready: true, restart count 0
Sep  6 00:47:11.381: INFO: schedext-test-node-selector-1 from test-node-selector started at 2021-09-05 20:21:19 -0700 PDT (1 container statuses recorded)
Sep  6 00:47:11.381: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:47:11.381: INFO: schedext-test-affinity-1 from test-pod-affinity started at 2021-09-05 20:22:12 -0700 PDT (1 container statuses recorded)
Sep  6 00:47:11.381: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:47:11.381: INFO: schedext-test-affinity-2 from test-pod-affinity started at 2021-09-05 20:22:42 -0700 PDT (1 container statuses recorded)
Sep  6 00:47:11.381: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:47:11.381: INFO: test-docker-registry from test-private-image-registry-ns started at 2021-09-05 20:41:08 -0700 PDT (1 container statuses recorded)
Sep  6 00:47:11.381: INFO: 	Container test-docker-registry ready: true, restart count 0
Sep  6 00:47:11.381: INFO: helloworld from test-telemetry started at 2021-09-05 20:32:42 -0700 PDT (1 container statuses recorded)
Sep  6 00:47:11.381: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:47:11.381: INFO: wcp-sanity-busybox-6f999d6849-6bn7v from test-update-workload-ns started at 2021-09-05 20:30:32 -0700 PDT (1 container statuses recorded)
Sep  6 00:47:11.381: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:47:11.381: INFO: 
Logging pods the apiserver thinks is on node sc2-10-185-226-233.eng.vmware.com before test
Sep  6 00:47:11.407: INFO: wcp-sanity-busybox-6f999d6849-tx88v from test-dataprovider-podvms-ns started at 2021-09-05 22:46:15 -0700 PDT (1 container statuses recorded)
Sep  6 00:47:11.407: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:47:11.407: INFO: wcp-sanity-busybox-6f999d6849-428rc from test-update-workload-ns started at 2021-09-05 22:46:12 -0700 PDT (1 container statuses recorded)
Sep  6 00:47:11.407: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:47:11.407: INFO: 
Logging pods the apiserver thinks is on node sc2-10-185-233-127.eng.vmware.com before test
Sep  6 00:47:11.443: INFO: curl-pod from test-cluster-ip-service started at 2021-09-05 20:33:45 -0700 PDT (1 container statuses recorded)
Sep  6 00:47:11.443: INFO: 	Container curl-container ready: true, restart count 0
Sep  6 00:47:11.443: INFO: helloworld from test-exec-ns started at 2021-09-05 20:18:37 -0700 PDT (1 container statuses recorded)
Sep  6 00:47:11.443: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:47:11.443: INFO: nginx-private from test-image-pull-secrets-ns started at 2021-09-05 20:19:39 -0700 PDT (1 container statuses recorded)
Sep  6 00:47:11.443: INFO: 	Container nginx-private-container ready: true, restart count 0
Sep  6 00:47:11.443: INFO: hello-web-1-6b97664bd5-w4mv7 from test-network-policy started at 2021-09-05 22:44:58 -0700 PDT (1 container statuses recorded)
Sep  6 00:47:11.443: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 00:47:11.443: INFO: hello-web-2-f779cbdff-dmb55 from test-network-policy started at 2021-09-05 20:37:45 -0700 PDT (1 container statuses recorded)
Sep  6 00:47:11.443: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 00:47:11.443: INFO: schedext-test-affinity-3 from test-pod-affinity started at 2021-09-05 20:22:44 -0700 PDT (1 container statuses recorded)
Sep  6 00:47:11.443: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:47:11.443: INFO: wcp-sanity-busybox-6f999d6849-564rw from test-update-workload-ns started at 2021-09-05 20:30:14 -0700 PDT (1 container statuses recorded)
Sep  6 00:47:11.443: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-192b4f4f-22a6-4e73-ac40-4af90e3c5fa6 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.185.226.233 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-192b4f4f-22a6-4e73-ac40-4af90e3c5fa6 off the node sc2-10-185-226-233.eng.vmware.com
STEP: verifying the node doesn't have the label kubernetes.io/e2e-192b4f4f-22a6-4e73-ac40-4af90e3c5fa6
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:53:13.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1214" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:363.730 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":296,"completed":244,"skipped":4324,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:53:14.535: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7165
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:187
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a Pod with a static label
STEP: watching for Pod to be ready
Sep  6 00:53:15.268: INFO: observed Pod pod-test in namespace pods-7165 in phase Pending conditions []
Sep  6 00:53:15.401: INFO: observed Pod pod-test in namespace pods-7165 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-06 00:53:15 -0700 PDT  }]
Sep  6 00:53:15.498: INFO: observed Pod pod-test in namespace pods-7165 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-06 00:53:15 -0700 PDT  }]
Sep  6 00:53:25.843: INFO: observed Pod pod-test in namespace pods-7165 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-06 00:53:15 -0700 PDT  }]
Sep  6 00:53:34.022: INFO: observed Pod pod-test in namespace pods-7165 in phase Pending conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-06 00:53:15 -0700 PDT  }]
STEP: patching the Pod with a new Label and updated data
Sep  6 00:53:43.727: INFO: observed event type ADDED
STEP: getting the Pod and ensuring that it's patched
STEP: getting the PodStatus
STEP: replacing the Pod's status Ready condition to False
STEP: check the Pod again to ensure its Ready conditions are False
STEP: deleting the Pod via a Collection with a LabelSelector
STEP: watching for the Pod to be deleted
Sep  6 00:53:43.841: INFO: observed event type ADDED
Sep  6 00:53:43.841: INFO: observed event type MODIFIED
Sep  6 00:53:43.841: INFO: observed event type MODIFIED
Sep  6 00:53:43.841: INFO: observed event type MODIFIED
Sep  6 00:53:43.841: INFO: observed event type MODIFIED
Sep  6 00:53:43.842: INFO: observed event type MODIFIED
Sep  6 00:53:43.842: INFO: observed event type MODIFIED
Sep  6 00:53:43.842: INFO: observed event type MODIFIED
Sep  6 00:53:43.842: INFO: observed event type MODIFIED
[AfterEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:53:58.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7165" for this suite.

• [SLOW TEST:43.762 seconds]
[k8s.io] Pods
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should run through the lifecycle of Pods and PodStatus [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","total":296,"completed":245,"skipped":4363,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:53:58.297: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-550
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ae5c322c-cfad-4d69-aa12-8adcccd83008
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-ae5c322c-cfad-4d69-aa12-8adcccd83008
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:54:35.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-550" for this suite.

• [SLOW TEST:37.145 seconds]
[sig-storage] Projected configMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":296,"completed":246,"skipped":4378,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:54:35.442: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-818
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep  6 00:55:04.407: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl exec --namespace=svcaccounts-818 pod-service-account-27455097-558d-4634-9f68-2e66fddb0093 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep  6 00:55:05.271: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl exec --namespace=svcaccounts-818 pod-service-account-27455097-558d-4634-9f68-2e66fddb0093 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep  6 00:55:05.511: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl exec --namespace=svcaccounts-818 pod-service-account-27455097-558d-4634-9f68-2e66fddb0093 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:55:05.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-818" for this suite.

• [SLOW TEST:30.638 seconds]
[sig-auth] ServiceAccounts
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":296,"completed":247,"skipped":4379,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:55:06.081: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7111
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 00:55:07.663: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  6 00:55:09.691: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:55:11.704: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:55:13.718: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:55:15.710: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:55:17.701: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:55:19.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:55:21.704: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:55:23.714: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:55:25.705: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:55:27.708: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:55:29.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:55:31.703: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:55:33.707: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:55:35.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:55:37.708: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 55, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 00:55:40.744: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  6 00:55:40.758: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2394-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:55:42.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7111" for this suite.
STEP: Destroying namespace "webhook-7111-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:40.113 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":296,"completed":248,"skipped":4442,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:55:46.194: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3554
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  6 00:56:51.776: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 00:56:51.788: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 00:56:53.790: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 00:56:53.807: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 00:56:55.788: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 00:56:55.816: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 00:56:57.789: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 00:56:57.810: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 00:56:59.789: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 00:56:59.806: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 00:57:01.789: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 00:57:01.797: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 00:57:03.790: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 00:57:03.802: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 00:57:05.788: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 00:57:05.804: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 00:57:07.789: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 00:57:07.808: INFO: Pod pod-with-poststart-http-hook still exists
Sep  6 00:57:09.788: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  6 00:57:09.829: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:57:09.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3554" for this suite.

• [SLOW TEST:84.223 seconds]
[k8s.io] Container Lifecycle Hook
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":296,"completed":249,"skipped":4450,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:57:10.417: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-854
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-volume-map-1bca4cb0-e559-4e64-b446-9ed6a3b01d01
STEP: Creating a pod to test consume configMaps
Sep  6 00:57:11.004: INFO: Waiting up to 5m0s for pod "pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864" in namespace "configmap-854" to be "Succeeded or Failed"
Sep  6 00:57:11.018: INFO: Pod "pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864": Phase="Pending", Reason="", readiness=false. Elapsed: 13.285684ms
Sep  6 00:57:13.049: INFO: Pod "pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044658229s
Sep  6 00:57:15.062: INFO: Pod "pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05740213s
Sep  6 00:57:17.082: INFO: Pod "pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864": Phase="Pending", Reason="", readiness=false. Elapsed: 6.077317073s
Sep  6 00:57:19.099: INFO: Pod "pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864": Phase="Pending", Reason="", readiness=false. Elapsed: 8.094281533s
Sep  6 00:57:21.135: INFO: Pod "pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864": Phase="Pending", Reason="", readiness=false. Elapsed: 10.130732032s
Sep  6 00:57:23.152: INFO: Pod "pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864": Phase="Pending", Reason="", readiness=false. Elapsed: 12.147540198s
Sep  6 00:57:25.164: INFO: Pod "pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864": Phase="Pending", Reason="", readiness=false. Elapsed: 14.160021969s
Sep  6 00:57:27.178: INFO: Pod "pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864": Phase="Pending", Reason="", readiness=false. Elapsed: 16.17306271s
Sep  6 00:57:29.199: INFO: Pod "pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864": Phase="Pending", Reason="", readiness=false. Elapsed: 18.194322155s
Sep  6 00:57:31.223: INFO: Pod "pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864": Phase="Pending", Reason="", readiness=false. Elapsed: 20.218767041s
Sep  6 00:57:33.235: INFO: Pod "pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864": Phase="Pending", Reason="", readiness=false. Elapsed: 22.230595443s
Sep  6 00:57:35.247: INFO: Pod "pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864": Phase="Pending", Reason="", readiness=false. Elapsed: 24.242322142s
Sep  6 00:57:37.261: INFO: Pod "pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864": Phase="Pending", Reason="", readiness=false. Elapsed: 26.256041397s
Sep  6 00:57:39.279: INFO: Pod "pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864": Phase="Pending", Reason="", readiness=false. Elapsed: 28.274468758s
Sep  6 00:57:41.298: INFO: Pod "pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.293814666s
STEP: Saw pod success
Sep  6 00:57:41.298: INFO: Pod "pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864" satisfied condition "Succeeded or Failed"
Sep  6 00:57:41.315: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 container agnhost-container: <nil>
STEP: delete the pod
Sep  6 00:57:41.413: INFO: Waiting for pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 to disappear
Sep  6 00:57:41.439: INFO: Pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 still exists
Sep  6 00:57:43.440: INFO: Waiting for pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 to disappear
Sep  6 00:57:43.452: INFO: Pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 still exists
Sep  6 00:57:45.440: INFO: Waiting for pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 to disappear
Sep  6 00:57:45.455: INFO: Pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 still exists
Sep  6 00:57:47.440: INFO: Waiting for pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 to disappear
Sep  6 00:57:47.451: INFO: Pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 still exists
Sep  6 00:57:49.440: INFO: Waiting for pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 to disappear
Sep  6 00:57:49.451: INFO: Pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 still exists
Sep  6 00:57:51.440: INFO: Waiting for pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 to disappear
Sep  6 00:57:51.456: INFO: Pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 still exists
Sep  6 00:57:53.440: INFO: Waiting for pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 to disappear
Sep  6 00:57:53.449: INFO: Pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 still exists
Sep  6 00:57:55.440: INFO: Waiting for pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 to disappear
Sep  6 00:57:55.460: INFO: Pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 still exists
Sep  6 00:57:57.440: INFO: Waiting for pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 to disappear
Sep  6 00:57:57.451: INFO: Pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 still exists
Sep  6 00:57:59.440: INFO: Waiting for pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 to disappear
Sep  6 00:57:59.452: INFO: Pod pod-configmaps-3331f27e-8ba9-47fd-9c6d-a0ebaf4c8864 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:57:59.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-854" for this suite.

• [SLOW TEST:49.245 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":250,"skipped":4486,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:57:59.663: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1007
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:58:30.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1007" for this suite.

• [SLOW TEST:31.060 seconds]
[k8s.io] Kubelet
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when scheduling a busybox Pod with hostAliases
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:137
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":251,"skipped":4508,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:58:30.723: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-6006
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:59:01.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6006" for this suite.

• [SLOW TEST:31.233 seconds]
[sig-storage] EmptyDir wrapper volumes
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":296,"completed":252,"skipped":4572,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:59:01.957: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-3059
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Sep  6 00:59:02.498: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  6 01:00:02.712: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  6 01:00:02.734: INFO: Starting informer...
STEP: Starting pod...
Sep  6 01:00:02.999: INFO: Pod is running on sc2-10-185-226-233.eng.vmware.com. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Sep  6 01:00:03.104: INFO: Pod wasn't evicted. Proceeding
Sep  6 01:00:03.104: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Sep  6 01:01:18.271: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:01:18.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-3059" for this suite.

• [SLOW TEST:136.764 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  removing taint cancels eviction [Disruptive] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":296,"completed":253,"skipped":4584,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:01:18.721: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9561
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep  6 01:01:19.412: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9561 /api/v1/namespaces/watch-9561/configmaps/e2e-watch-test-configmap-a 33a8e03b-e46d-48a1-a58a-e6e20dcaa470 214389 0 2021-09-06 01:01:19 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-06 01:01:19 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 01:01:19.412: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9561 /api/v1/namespaces/watch-9561/configmaps/e2e-watch-test-configmap-a 33a8e03b-e46d-48a1-a58a-e6e20dcaa470 214389 0 2021-09-06 01:01:19 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-06 01:01:19 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep  6 01:01:29.459: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9561 /api/v1/namespaces/watch-9561/configmaps/e2e-watch-test-configmap-a 33a8e03b-e46d-48a1-a58a-e6e20dcaa470 214523 0 2021-09-06 01:01:19 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-06 01:01:29 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 01:01:29.459: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9561 /api/v1/namespaces/watch-9561/configmaps/e2e-watch-test-configmap-a 33a8e03b-e46d-48a1-a58a-e6e20dcaa470 214523 0 2021-09-06 01:01:19 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-06 01:01:29 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep  6 01:01:39.504: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9561 /api/v1/namespaces/watch-9561/configmaps/e2e-watch-test-configmap-a 33a8e03b-e46d-48a1-a58a-e6e20dcaa470 214619 0 2021-09-06 01:01:19 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-06 01:01:29 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 01:01:39.507: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9561 /api/v1/namespaces/watch-9561/configmaps/e2e-watch-test-configmap-a 33a8e03b-e46d-48a1-a58a-e6e20dcaa470 214619 0 2021-09-06 01:01:19 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-06 01:01:29 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep  6 01:01:49.563: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9561 /api/v1/namespaces/watch-9561/configmaps/e2e-watch-test-configmap-a 33a8e03b-e46d-48a1-a58a-e6e20dcaa470 214716 0 2021-09-06 01:01:19 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-06 01:01:29 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 01:01:49.563: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9561 /api/v1/namespaces/watch-9561/configmaps/e2e-watch-test-configmap-a 33a8e03b-e46d-48a1-a58a-e6e20dcaa470 214716 0 2021-09-06 01:01:19 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-06 01:01:29 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep  6 01:01:59.601: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9561 /api/v1/namespaces/watch-9561/configmaps/e2e-watch-test-configmap-b 710b74fd-e55d-41ac-aba7-e4bb88d3f080 214806 0 2021-09-06 01:01:59 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-06 01:01:59 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 01:01:59.601: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9561 /api/v1/namespaces/watch-9561/configmaps/e2e-watch-test-configmap-b 710b74fd-e55d-41ac-aba7-e4bb88d3f080 214806 0 2021-09-06 01:01:59 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-06 01:01:59 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep  6 01:02:09.658: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9561 /api/v1/namespaces/watch-9561/configmaps/e2e-watch-test-configmap-b 710b74fd-e55d-41ac-aba7-e4bb88d3f080 214915 0 2021-09-06 01:01:59 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-06 01:01:59 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 01:02:09.658: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9561 /api/v1/namespaces/watch-9561/configmaps/e2e-watch-test-configmap-b 710b74fd-e55d-41ac-aba7-e4bb88d3f080 214915 0 2021-09-06 01:01:59 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-06 01:01:59 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:02:19.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9561" for this suite.

• [SLOW TEST:61.205 seconds]
[sig-api-machinery] Watchers
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":296,"completed":254,"skipped":4595,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:02:19.926: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9263
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:247
[BeforeEach] Kubectl label
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1314
STEP: creating the pod
Sep  6 01:02:20.420: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-9263 create -f -'
Sep  6 01:02:21.105: INFO: stderr: ""
Sep  6 01:02:21.105: INFO: stdout: "pod/pause created\n"
Sep  6 01:02:21.105: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep  6 01:02:21.105: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9263" to be "running and ready"
Sep  6 01:02:21.114: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 9.674691ms
Sep  6 01:02:23.178: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073281967s
Sep  6 01:02:25.196: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.091634373s
Sep  6 01:02:27.211: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.106530617s
Sep  6 01:02:29.227: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.122050387s
Sep  6 01:02:31.240: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 10.135205191s
Sep  6 01:02:33.251: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.146590626s
Sep  6 01:02:35.266: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 14.16163864s
Sep  6 01:02:37.279: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 16.174102323s
Sep  6 01:02:39.295: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 18.190646946s
Sep  6 01:02:41.311: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 20.206621716s
Sep  6 01:02:43.321: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 22.216006134s
Sep  6 01:02:45.333: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 24.22785706s
Sep  6 01:02:47.346: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 26.241046657s
Sep  6 01:02:47.346: INFO: Pod "pause" satisfied condition "running and ready"
Sep  6 01:02:47.346: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: adding the label testing-label with value testing-label-value to a pod
Sep  6 01:02:47.346: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-9263 label pods pause testing-label=testing-label-value'
Sep  6 01:02:47.505: INFO: stderr: ""
Sep  6 01:02:47.505: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep  6 01:02:47.506: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-9263 get pod pause -L testing-label'
Sep  6 01:02:47.625: INFO: stderr: ""
Sep  6 01:02:47.625: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          26s   testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep  6 01:02:47.625: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-9263 label pods pause testing-label-'
Sep  6 01:02:47.793: INFO: stderr: ""
Sep  6 01:02:47.793: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep  6 01:02:47.793: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-9263 get pod pause -L testing-label'
Sep  6 01:02:47.938: INFO: stderr: ""
Sep  6 01:02:47.938: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          26s   \n"
[AfterEach] Kubectl label
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1320
STEP: using delete to clean up resources
Sep  6 01:02:47.938: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-9263 delete --grace-period=0 --force -f -'
Sep  6 01:02:57.433: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 01:02:57.433: INFO: stdout: "pod \"pause\" force deleted\n"
Sep  6 01:02:57.433: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-9263 get rc,svc -l name=pause --no-headers'
Sep  6 01:02:57.568: INFO: stderr: "No resources found in kubectl-9263 namespace.\n"
Sep  6 01:02:57.568: INFO: stdout: ""
Sep  6 01:02:57.568: INFO: Running '/home/worker/workspace/conformance-nsx-1.20/bin/kubectl --server=https://10.185.230.78:6443 --kubeconfig=./kconfig.yaml --namespace=kubectl-9263 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 01:02:57.697: INFO: stderr: ""
Sep  6 01:02:57.697: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:02:57.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9263" for this suite.

• [SLOW TEST:38.101 seconds]
[sig-cli] Kubectl client
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1312
    should update the label on a resource  [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":296,"completed":255,"skipped":4637,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] Discovery
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:02:58.027: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-4468
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  6 01:02:58.790: INFO: Checking APIGroup: apiregistration.k8s.io
Sep  6 01:02:58.793: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Sep  6 01:02:58.793: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Sep  6 01:02:58.793: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Sep  6 01:02:58.793: INFO: Checking APIGroup: apps
Sep  6 01:02:58.797: INFO: PreferredVersion.GroupVersion: apps/v1
Sep  6 01:02:58.797: INFO: Versions found [{apps/v1 v1}]
Sep  6 01:02:58.797: INFO: apps/v1 matches apps/v1
Sep  6 01:02:58.797: INFO: Checking APIGroup: events.k8s.io
Sep  6 01:02:58.799: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Sep  6 01:02:58.799: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Sep  6 01:02:58.800: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Sep  6 01:02:58.800: INFO: Checking APIGroup: authentication.k8s.io
Sep  6 01:02:58.804: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Sep  6 01:02:58.804: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Sep  6 01:02:58.804: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Sep  6 01:02:58.804: INFO: Checking APIGroup: authorization.k8s.io
Sep  6 01:02:58.811: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Sep  6 01:02:58.811: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Sep  6 01:02:58.811: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Sep  6 01:02:58.811: INFO: Checking APIGroup: autoscaling
Sep  6 01:02:58.815: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Sep  6 01:02:58.815: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Sep  6 01:02:58.815: INFO: autoscaling/v1 matches autoscaling/v1
Sep  6 01:02:58.815: INFO: Checking APIGroup: batch
Sep  6 01:02:58.817: INFO: PreferredVersion.GroupVersion: batch/v1
Sep  6 01:02:58.818: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Sep  6 01:02:58.818: INFO: batch/v1 matches batch/v1
Sep  6 01:02:58.818: INFO: Checking APIGroup: certificates.k8s.io
Sep  6 01:02:58.821: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Sep  6 01:02:58.821: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Sep  6 01:02:58.821: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Sep  6 01:02:58.821: INFO: Checking APIGroup: networking.k8s.io
Sep  6 01:02:58.823: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Sep  6 01:02:58.823: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Sep  6 01:02:58.823: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Sep  6 01:02:58.823: INFO: Checking APIGroup: extensions
Sep  6 01:02:58.826: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Sep  6 01:02:58.826: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Sep  6 01:02:58.826: INFO: extensions/v1beta1 matches extensions/v1beta1
Sep  6 01:02:58.826: INFO: Checking APIGroup: policy
Sep  6 01:02:58.829: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Sep  6 01:02:58.830: INFO: Versions found [{policy/v1beta1 v1beta1}]
Sep  6 01:02:58.830: INFO: policy/v1beta1 matches policy/v1beta1
Sep  6 01:02:58.830: INFO: Checking APIGroup: rbac.authorization.k8s.io
Sep  6 01:02:58.832: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Sep  6 01:02:58.832: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Sep  6 01:02:58.832: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Sep  6 01:02:58.832: INFO: Checking APIGroup: storage.k8s.io
Sep  6 01:02:58.834: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Sep  6 01:02:58.834: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Sep  6 01:02:58.834: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Sep  6 01:02:58.834: INFO: Checking APIGroup: admissionregistration.k8s.io
Sep  6 01:02:58.838: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Sep  6 01:02:58.838: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Sep  6 01:02:58.838: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Sep  6 01:02:58.838: INFO: Checking APIGroup: apiextensions.k8s.io
Sep  6 01:02:58.841: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Sep  6 01:02:58.841: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Sep  6 01:02:58.841: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Sep  6 01:02:58.841: INFO: Checking APIGroup: scheduling.k8s.io
Sep  6 01:02:58.844: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Sep  6 01:02:58.844: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Sep  6 01:02:58.844: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Sep  6 01:02:58.844: INFO: Checking APIGroup: coordination.k8s.io
Sep  6 01:02:58.860: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Sep  6 01:02:58.860: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Sep  6 01:02:58.860: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Sep  6 01:02:58.860: INFO: Checking APIGroup: node.k8s.io
Sep  6 01:02:58.862: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Sep  6 01:02:58.863: INFO: Versions found [{node.k8s.io/v1 v1} {node.k8s.io/v1beta1 v1beta1}]
Sep  6 01:02:58.863: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Sep  6 01:02:58.863: INFO: Checking APIGroup: discovery.k8s.io
Sep  6 01:02:58.865: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Sep  6 01:02:58.865: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Sep  6 01:02:58.865: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Sep  6 01:02:58.865: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Sep  6 01:02:58.867: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta1
Sep  6 01:02:58.867: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Sep  6 01:02:58.867: INFO: flowcontrol.apiserver.k8s.io/v1beta1 matches flowcontrol.apiserver.k8s.io/v1beta1
Sep  6 01:02:58.867: INFO: Checking APIGroup: crd.projectcalico.org
Sep  6 01:02:58.870: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Sep  6 01:02:58.870: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Sep  6 01:02:58.870: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Sep  6 01:02:58.870: INFO: Checking APIGroup: imagecontroller.vmware.com
Sep  6 01:02:58.874: INFO: PreferredVersion.GroupVersion: imagecontroller.vmware.com/v1
Sep  6 01:02:58.874: INFO: Versions found [{imagecontroller.vmware.com/v1 v1}]
Sep  6 01:02:58.874: INFO: imagecontroller.vmware.com/v1 matches imagecontroller.vmware.com/v1
Sep  6 01:02:58.874: INFO: Checking APIGroup: k8s.cni.cncf.io
Sep  6 01:02:58.876: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Sep  6 01:02:58.876: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Sep  6 01:02:58.876: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Sep  6 01:02:58.876: INFO: Checking APIGroup: nsx.vmware.com
Sep  6 01:02:58.878: INFO: PreferredVersion.GroupVersion: nsx.vmware.com/v1
Sep  6 01:02:58.878: INFO: Versions found [{nsx.vmware.com/v1 v1} {nsx.vmware.com/v1alpha1 v1alpha1}]
Sep  6 01:02:58.878: INFO: nsx.vmware.com/v1 matches nsx.vmware.com/v1
Sep  6 01:02:58.878: INFO: Checking APIGroup: appplatform.wcp.vmware.com
Sep  6 01:02:58.883: INFO: PreferredVersion.GroupVersion: appplatform.wcp.vmware.com/v1beta1
Sep  6 01:02:58.883: INFO: Versions found [{appplatform.wcp.vmware.com/v1beta1 v1beta1} {appplatform.wcp.vmware.com/v1alpha2 v1alpha2} {appplatform.wcp.vmware.com/v1alpha1 v1alpha1}]
Sep  6 01:02:58.883: INFO: appplatform.wcp.vmware.com/v1beta1 matches appplatform.wcp.vmware.com/v1beta1
Sep  6 01:02:58.883: INFO: Checking APIGroup: cns.vmware.com
Sep  6 01:02:58.885: INFO: PreferredVersion.GroupVersion: cns.vmware.com/v1alpha1
Sep  6 01:02:58.885: INFO: Versions found [{cns.vmware.com/v1alpha1 v1alpha1}]
Sep  6 01:02:58.885: INFO: cns.vmware.com/v1alpha1 matches cns.vmware.com/v1alpha1
Sep  6 01:02:58.885: INFO: Checking APIGroup: installers.tmc.cloud.vmware.com
Sep  6 01:02:58.887: INFO: PreferredVersion.GroupVersion: installers.tmc.cloud.vmware.com/v1alpha1
Sep  6 01:02:58.887: INFO: Versions found [{installers.tmc.cloud.vmware.com/v1alpha1 v1alpha1}]
Sep  6 01:02:58.887: INFO: installers.tmc.cloud.vmware.com/v1alpha1 matches installers.tmc.cloud.vmware.com/v1alpha1
Sep  6 01:02:58.887: INFO: Checking APIGroup: licenseoperator.vmware.com
Sep  6 01:02:58.889: INFO: PreferredVersion.GroupVersion: licenseoperator.vmware.com/v1alpha1
Sep  6 01:02:58.889: INFO: Versions found [{licenseoperator.vmware.com/v1alpha1 v1alpha1}]
Sep  6 01:02:58.889: INFO: licenseoperator.vmware.com/v1alpha1 matches licenseoperator.vmware.com/v1alpha1
Sep  6 01:02:58.889: INFO: Checking APIGroup: netoperator.vmware.com
Sep  6 01:02:58.892: INFO: PreferredVersion.GroupVersion: netoperator.vmware.com/v1alpha1
Sep  6 01:02:58.892: INFO: Versions found [{netoperator.vmware.com/v1alpha1 v1alpha1}]
Sep  6 01:02:58.892: INFO: netoperator.vmware.com/v1alpha1 matches netoperator.vmware.com/v1alpha1
Sep  6 01:02:58.892: INFO: Checking APIGroup: registryagent.vmware.com
Sep  6 01:02:58.895: INFO: PreferredVersion.GroupVersion: registryagent.vmware.com/v1alpha1
Sep  6 01:02:58.895: INFO: Versions found [{registryagent.vmware.com/v1alpha1 v1alpha1}]
Sep  6 01:02:58.895: INFO: registryagent.vmware.com/v1alpha1 matches registryagent.vmware.com/v1alpha1
Sep  6 01:02:58.895: INFO: Checking APIGroup: run.tanzu.vmware.com
Sep  6 01:02:58.897: INFO: PreferredVersion.GroupVersion: run.tanzu.vmware.com/v1alpha2
Sep  6 01:02:58.897: INFO: Versions found [{run.tanzu.vmware.com/v1alpha2 v1alpha2} {run.tanzu.vmware.com/v1alpha1 v1alpha1}]
Sep  6 01:02:58.897: INFO: run.tanzu.vmware.com/v1alpha2 matches run.tanzu.vmware.com/v1alpha2
Sep  6 01:02:58.897: INFO: Checking APIGroup: topology.tanzu.vmware.com
Sep  6 01:02:58.902: INFO: PreferredVersion.GroupVersion: topology.tanzu.vmware.com/v1alpha1
Sep  6 01:02:58.902: INFO: Versions found [{topology.tanzu.vmware.com/v1alpha1 v1alpha1}]
Sep  6 01:02:58.902: INFO: topology.tanzu.vmware.com/v1alpha1 matches topology.tanzu.vmware.com/v1alpha1
Sep  6 01:02:58.902: INFO: Checking APIGroup: vmoperator.vmware.com
Sep  6 01:02:58.904: INFO: PreferredVersion.GroupVersion: vmoperator.vmware.com/v1alpha1
Sep  6 01:02:58.904: INFO: Versions found [{vmoperator.vmware.com/v1alpha1 v1alpha1}]
Sep  6 01:02:58.904: INFO: vmoperator.vmware.com/v1alpha1 matches vmoperator.vmware.com/v1alpha1
Sep  6 01:02:58.904: INFO: Checking APIGroup: vmware.com
Sep  6 01:02:58.909: INFO: PreferredVersion.GroupVersion: vmware.com/v1alpha1
Sep  6 01:02:58.909: INFO: Versions found [{vmware.com/v1alpha1 v1alpha1}]
Sep  6 01:02:58.909: INFO: vmware.com/v1alpha1 matches vmware.com/v1alpha1
Sep  6 01:02:58.909: INFO: Checking APIGroup: networking.x-k8s.io
Sep  6 01:02:58.911: INFO: PreferredVersion.GroupVersion: networking.x-k8s.io/v1alpha1pre1
Sep  6 01:02:58.911: INFO: Versions found [{networking.x-k8s.io/v1alpha1pre1 v1alpha1pre1}]
Sep  6 01:02:58.911: INFO: networking.x-k8s.io/v1alpha1pre1 matches networking.x-k8s.io/v1alpha1pre1
Sep  6 01:02:58.911: INFO: Checking APIGroup: acme.cert-manager.io
Sep  6 01:02:58.914: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1alpha3
Sep  6 01:02:58.914: INFO: Versions found [{acme.cert-manager.io/v1alpha3 v1alpha3} {acme.cert-manager.io/v1alpha2 v1alpha2}]
Sep  6 01:02:58.915: INFO: acme.cert-manager.io/v1alpha3 matches acme.cert-manager.io/v1alpha3
Sep  6 01:02:58.915: INFO: Checking APIGroup: bootstrap.cluster.x-k8s.io
Sep  6 01:02:58.917: INFO: PreferredVersion.GroupVersion: bootstrap.cluster.x-k8s.io/v1alpha3
Sep  6 01:02:58.917: INFO: Versions found [{bootstrap.cluster.x-k8s.io/v1alpha3 v1alpha3} {bootstrap.cluster.x-k8s.io/v1alpha2 v1alpha2}]
Sep  6 01:02:58.917: INFO: bootstrap.cluster.x-k8s.io/v1alpha3 matches bootstrap.cluster.x-k8s.io/v1alpha3
Sep  6 01:02:58.917: INFO: Checking APIGroup: cert-manager.io
Sep  6 01:02:58.921: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1alpha3
Sep  6 01:02:58.921: INFO: Versions found [{cert-manager.io/v1alpha3 v1alpha3} {cert-manager.io/v1alpha2 v1alpha2}]
Sep  6 01:02:58.921: INFO: cert-manager.io/v1alpha3 matches cert-manager.io/v1alpha3
Sep  6 01:02:58.921: INFO: Checking APIGroup: cluster.x-k8s.io
Sep  6 01:02:58.923: INFO: PreferredVersion.GroupVersion: cluster.x-k8s.io/v1alpha3
Sep  6 01:02:58.923: INFO: Versions found [{cluster.x-k8s.io/v1alpha3 v1alpha3} {cluster.x-k8s.io/v1alpha2 v1alpha2}]
Sep  6 01:02:58.923: INFO: cluster.x-k8s.io/v1alpha3 matches cluster.x-k8s.io/v1alpha3
Sep  6 01:02:58.923: INFO: Checking APIGroup: infrastructure.cluster.vmware.com
Sep  6 01:02:58.926: INFO: PreferredVersion.GroupVersion: infrastructure.cluster.vmware.com/v1alpha3
Sep  6 01:02:58.926: INFO: Versions found [{infrastructure.cluster.vmware.com/v1alpha3 v1alpha3} {infrastructure.cluster.vmware.com/v1alpha2 v1alpha2}]
Sep  6 01:02:58.926: INFO: infrastructure.cluster.vmware.com/v1alpha3 matches infrastructure.cluster.vmware.com/v1alpha3
Sep  6 01:02:58.926: INFO: Checking APIGroup: addons.cluster.x-k8s.io
Sep  6 01:02:58.928: INFO: PreferredVersion.GroupVersion: addons.cluster.x-k8s.io/v1alpha3
Sep  6 01:02:58.928: INFO: Versions found [{addons.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Sep  6 01:02:58.928: INFO: addons.cluster.x-k8s.io/v1alpha3 matches addons.cluster.x-k8s.io/v1alpha3
Sep  6 01:02:58.928: INFO: Checking APIGroup: controlplane.cluster.x-k8s.io
Sep  6 01:02:58.932: INFO: PreferredVersion.GroupVersion: controlplane.cluster.x-k8s.io/v1alpha3
Sep  6 01:02:58.932: INFO: Versions found [{controlplane.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Sep  6 01:02:58.932: INFO: controlplane.cluster.x-k8s.io/v1alpha3 matches controlplane.cluster.x-k8s.io/v1alpha3
Sep  6 01:02:58.932: INFO: Checking APIGroup: exp.cluster.x-k8s.io
Sep  6 01:02:58.935: INFO: PreferredVersion.GroupVersion: exp.cluster.x-k8s.io/v1alpha3
Sep  6 01:02:58.935: INFO: Versions found [{exp.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Sep  6 01:02:58.935: INFO: exp.cluster.x-k8s.io/v1alpha3 matches exp.cluster.x-k8s.io/v1alpha3
Sep  6 01:02:58.935: INFO: Checking APIGroup: psp.wcp.vmware.com
Sep  6 01:02:58.938: INFO: PreferredVersion.GroupVersion: psp.wcp.vmware.com/v1beta1
Sep  6 01:02:58.938: INFO: Versions found [{psp.wcp.vmware.com/v1beta1 v1beta1}]
Sep  6 01:02:58.938: INFO: psp.wcp.vmware.com/v1beta1 matches psp.wcp.vmware.com/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:02:58.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-4468" for this suite.
•{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":296,"completed":256,"skipped":4640,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:02:59.164: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8690
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Sep  6 01:02:59.671: INFO: PodSpec: initContainers in spec.initContainers
Sep  6 01:07:28.386: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-39f27a1e-fb07-48e6-9de7-e794ab5520ec", GenerateName:"", Namespace:"init-container-8690", SelfLink:"/api/v1/namespaces/init-container-8690/pods/pod-init-39f27a1e-fb07-48e6-9de7-e794ab5520ec", UID:"947e5bea-7d4f-4902-adfa-d57754714b55", ResourceVersion:"218163", Generation:0, CreationTimestamp:time.Date(2021, time.September, 6, 1, 2, 59, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"671768396"}, Annotations:map[string]string{"attachment_id":"a6bcd7df-5e41-43ba-8dce-118f34e7e082", "kubernetes.io/psp":"e2e-test-privileged-psp", "mac":"04:50:56:00:00:12", "vlan":"None", "vmware-system-ephemeral-disk-uuid":"6000C293-21a7-9513-781f-36b32ad7bffb", "vmware-system-image-references":"{\"init1\":\"busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v35905\",\"init2\":\"busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v35905\",\"run1\":\"pause-54467452d21a00c28aa4cce37d0850173b528ca6-v60535\"}", "vmware-system-vm-moid":"vm-1487:3f0e15ce-7291-45c7-8a72-39d27d06e017", "vmware-system-vm-uuid":"5016ddff-2a45-6403-9c57-75bd322c4319"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string{"lifecycle-controller/system.vmware.com"}, ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2021, time.September, 6, 1, 2, 59, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0028fe258)}, v1.ManagedFieldsEntry{Manager:"image-controller", Operation:"Update", APIVersion:"v1", Time:time.Date(2021, time.September, 6, 1, 2, 59, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0028fe288)}, v1.ManagedFieldsEntry{Manager:"nsx-ncp-6d7f7bf559-df5sk", Operation:"Update", APIVersion:"v1", Time:time.Date(2021, time.September, 6, 1, 3, 17, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0028fe2b8)}, v1.ManagedFieldsEntry{Manager:"scheduler-extender", Operation:"Update", APIVersion:"v1", Time:time.Date(2021, time.September, 6, 1, 3, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0028fe2e8)}, v1.ManagedFieldsEntry{Manager:"spherelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2021, time.September, 6, 1, 5, 36, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0028fe318)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-4lwpr", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00acca380), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"mirror.gcr.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4lwpr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"mirror.gcr.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4lwpr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4lwpr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0058b6138), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"sc2-10-185-226-233.eng.vmware.com", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0030bc000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0058b61b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0058b61d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0058b61d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0058b61dc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc001f8a350), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2021, time.September, 6, 1, 2, 59, 0, time.Local), Reason:"", Message:""}, v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2021, time.September, 6, 1, 5, 9, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2021, time.September, 6, 1, 5, 9, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2021, time.September, 6, 1, 5, 9, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.185.226.233", PodIP:"172.26.1.194", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.26.1.194"}}, StartTime:time.Date(2021, time.September, 6, 1, 4, 56, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc007f8e020), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0030bc0e0)}, Ready:false, RestartCount:3, Image:"mirror.gcr.io/library/busybox:1.29", ImageID:"busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v35905", ContainerID:"e0f13aed-e589-4996-9123-9efe817fa14a", Started:(*bool)(0xc0058b62b5)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc007f8e080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"mirror.gcr.io/library/busybox:1.29", ImageID:"busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v35905", ContainerID:"", Started:(*bool)(0xc0058b62bb)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc007f8e000), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"pause-54467452d21a00c28aa4cce37d0850173b528ca6-v60535", ContainerID:"", Started:(*bool)(0xc0058b628f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:07:28.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8690" for this suite.

• [SLOW TEST:269.798 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":296,"completed":257,"skipped":4642,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:07:28.963: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1248
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Sep  6 01:07:29.516: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1248 /api/v1/namespaces/dns-1248/pods/test-dns-nameservers 29a42120-c6ce-41e5-adef-b302713d417b 218196 0 2021-09-06 01:07:29 -0700 PDT <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-09-06 01:07:29 -0700 PDT FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-tc7tl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-tc7tl,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-tc7tl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 01:07:29.546: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  6 01:07:31.559: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  6 01:07:33.565: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  6 01:07:35.562: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  6 01:07:37.563: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  6 01:07:39.576: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  6 01:07:41.561: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  6 01:07:43.570: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  6 01:07:45.559: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  6 01:07:47.560: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  6 01:07:49.576: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  6 01:07:51.567: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  6 01:07:53.564: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  6 01:07:55.602: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  6 01:07:57.558: INFO: The status of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Sep  6 01:07:59.556: INFO: The status of Pod test-dns-nameservers is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Sep  6 01:07:59.556: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1248 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 01:07:59.556: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Verifying customized DNS server is configured on pod...
Sep  6 01:07:59.757: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1248 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Sep  6 01:07:59.757: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  6 01:07:59.954: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:08:00.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1248" for this suite.

• [SLOW TEST:33.037 seconds]
[sig-network] DNS
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":296,"completed":258,"skipped":4666,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:08:02.000: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-9195
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep  6 01:08:04.101: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Sep  6 01:08:06.139: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:08:08.149: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:08:10.154: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:08:12.159: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:08:14.151: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:08:16.155: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:08:18.157: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:08:20.163: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:08:22.152: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:08:24.156: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:08:26.148: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:08:28.152: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:08:30.153: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:08:32.156: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:08:34.151: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:08:36.151: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:08:38.151: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 8, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-7d6697c5b7\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 01:08:41.191: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  6 01:08:41.199: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:08:42.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9195" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:41.918 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":296,"completed":259,"skipped":4682,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:08:43.919: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3916
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep  6 01:08:44.889: INFO: Waiting up to 5m0s for pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80" in namespace "downward-api-3916" to be "Succeeded or Failed"
Sep  6 01:08:44.966: INFO: Pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80": Phase="Pending", Reason="", readiness=false. Elapsed: 77.772199ms
Sep  6 01:08:46.988: INFO: Pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09946284s
Sep  6 01:08:49.007: INFO: Pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.118557203s
Sep  6 01:08:51.074: INFO: Pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80": Phase="Pending", Reason="", readiness=false. Elapsed: 6.184986533s
Sep  6 01:08:53.090: INFO: Pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80": Phase="Pending", Reason="", readiness=false. Elapsed: 8.201635092s
Sep  6 01:08:55.103: INFO: Pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80": Phase="Pending", Reason="", readiness=false. Elapsed: 10.214786134s
Sep  6 01:08:57.127: INFO: Pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80": Phase="Pending", Reason="", readiness=false. Elapsed: 12.238226407s
Sep  6 01:08:59.149: INFO: Pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80": Phase="Pending", Reason="", readiness=false. Elapsed: 14.260304159s
Sep  6 01:09:01.172: INFO: Pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80": Phase="Pending", Reason="", readiness=false. Elapsed: 16.283221361s
Sep  6 01:09:03.194: INFO: Pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80": Phase="Pending", Reason="", readiness=false. Elapsed: 18.305060664s
Sep  6 01:09:05.288: INFO: Pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80": Phase="Pending", Reason="", readiness=false. Elapsed: 20.399312025s
Sep  6 01:09:07.314: INFO: Pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80": Phase="Pending", Reason="", readiness=false. Elapsed: 22.42573045s
Sep  6 01:09:09.335: INFO: Pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80": Phase="Pending", Reason="", readiness=false. Elapsed: 24.445995812s
Sep  6 01:09:11.352: INFO: Pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80": Phase="Pending", Reason="", readiness=false. Elapsed: 26.463136071s
Sep  6 01:09:13.365: INFO: Pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80": Phase="Pending", Reason="", readiness=false. Elapsed: 28.476351932s
Sep  6 01:09:15.386: INFO: Pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80": Phase="Pending", Reason="", readiness=false. Elapsed: 30.496942378s
Sep  6 01:09:17.398: INFO: Pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80": Phase="Pending", Reason="", readiness=false. Elapsed: 32.509238558s
Sep  6 01:09:19.411: INFO: Pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.521919478s
STEP: Saw pod success
Sep  6 01:09:19.411: INFO: Pod "downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80" satisfied condition "Succeeded or Failed"
Sep  6 01:09:19.416: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 container client-container: <nil>
STEP: delete the pod
Sep  6 01:09:19.482: INFO: Waiting for pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 to disappear
Sep  6 01:09:19.504: INFO: Pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 still exists
Sep  6 01:09:21.506: INFO: Waiting for pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 to disappear
Sep  6 01:09:21.516: INFO: Pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 still exists
Sep  6 01:09:23.505: INFO: Waiting for pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 to disappear
Sep  6 01:09:23.516: INFO: Pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 still exists
Sep  6 01:09:25.506: INFO: Waiting for pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 to disappear
Sep  6 01:09:25.516: INFO: Pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 still exists
Sep  6 01:09:27.505: INFO: Waiting for pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 to disappear
Sep  6 01:09:27.517: INFO: Pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 still exists
Sep  6 01:09:29.506: INFO: Waiting for pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 to disappear
Sep  6 01:09:29.519: INFO: Pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 still exists
Sep  6 01:09:31.505: INFO: Waiting for pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 to disappear
Sep  6 01:09:31.532: INFO: Pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 still exists
Sep  6 01:09:33.505: INFO: Waiting for pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 to disappear
Sep  6 01:09:33.522: INFO: Pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 still exists
Sep  6 01:09:35.505: INFO: Waiting for pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 to disappear
Sep  6 01:09:35.513: INFO: Pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 still exists
Sep  6 01:09:37.506: INFO: Waiting for pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 to disappear
Sep  6 01:09:37.518: INFO: Pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 still exists
Sep  6 01:09:39.506: INFO: Waiting for pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 to disappear
Sep  6 01:09:39.521: INFO: Pod downwardapi-volume-505d83a2-f217-4536-8d53-92f7645d8a80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:09:39.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3916" for this suite.

• [SLOW TEST:55.825 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":296,"completed":260,"skipped":4690,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:09:39.744: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4269
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-4269
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a new StatefulSet
Sep  6 01:09:40.224: INFO: Found 0 stateful pods, waiting for 3
Sep  6 01:09:50.233: INFO: Found 1 stateful pods, waiting for 3
Sep  6 01:10:00.262: INFO: Found 1 stateful pods, waiting for 3
Sep  6 01:10:10.248: INFO: Found 1 stateful pods, waiting for 3
Sep  6 01:10:20.241: INFO: Found 2 stateful pods, waiting for 3
Sep  6 01:10:30.243: INFO: Found 2 stateful pods, waiting for 3
Sep  6 01:10:40.241: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:10:40.241: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:10:40.241: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  6 01:10:50.253: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:10:50.253: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:10:50.253: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  6 01:11:00.247: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:11:00.247: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:11:00.247: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  6 01:11:10.246: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:11:10.246: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:11:10.246: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from mirror.gcr.io/library/httpd:2.4.38-alpine to mirror.gcr.io/library/httpd:2.4.39-alpine
Sep  6 01:11:10.328: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep  6 01:11:20.437: INFO: Updating stateful set ss2
Sep  6 01:11:20.502: INFO: Waiting for Pod statefulset-4269/ss2-2 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  6 01:11:30.541: INFO: Waiting for Pod statefulset-4269/ss2-2 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
STEP: Restoring Pods to the correct revision when they are deleted
Sep  6 01:11:40.659: INFO: Found 2 stateful pods, waiting for 3
Sep  6 01:11:50.677: INFO: Found 2 stateful pods, waiting for 3
Sep  6 01:12:00.686: INFO: Found 2 stateful pods, waiting for 3
Sep  6 01:12:10.702: INFO: Found 2 stateful pods, waiting for 3
Sep  6 01:12:20.690: INFO: Found 2 stateful pods, waiting for 3
Sep  6 01:12:30.684: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:12:30.684: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:12:30.684: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  6 01:12:40.697: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:12:40.697: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:12:40.697: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  6 01:12:50.688: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:12:50.688: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:12:50.688: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  6 01:13:00.814: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:13:00.814: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:13:00.814: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Failed - Ready=false
Sep  6 01:13:10.685: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:13:10.686: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:13:10.686: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  6 01:13:20.685: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:13:20.685: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:13:20.685: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  6 01:13:30.687: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:13:30.687: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:13:30.687: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  6 01:13:40.708: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:13:40.708: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  6 01:13:40.708: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep  6 01:13:40.770: INFO: Updating stateful set ss2
Sep  6 01:13:40.824: INFO: Waiting for Pod statefulset-4269/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  6 01:13:50.950: INFO: Waiting for Pod statefulset-4269/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  6 01:14:00.918: INFO: Updating stateful set ss2
Sep  6 01:14:00.952: INFO: Waiting for StatefulSet statefulset-4269/ss2 to complete update
Sep  6 01:14:00.952: INFO: Waiting for Pod statefulset-4269/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  6 01:14:10.986: INFO: Waiting for StatefulSet statefulset-4269/ss2 to complete update
Sep  6 01:14:10.986: INFO: Waiting for Pod statefulset-4269/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  6 01:14:21.018: INFO: Waiting for StatefulSet statefulset-4269/ss2 to complete update
Sep  6 01:14:21.018: INFO: Waiting for Pod statefulset-4269/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  6 01:14:30.979: INFO: Waiting for StatefulSet statefulset-4269/ss2 to complete update
Sep  6 01:14:30.979: INFO: Waiting for Pod statefulset-4269/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  6 01:14:40.999: INFO: Waiting for StatefulSet statefulset-4269/ss2 to complete update
Sep  6 01:14:50.978: INFO: Waiting for StatefulSet statefulset-4269/ss2 to complete update
Sep  6 01:15:01.017: INFO: Waiting for StatefulSet statefulset-4269/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep  6 01:15:10.979: INFO: Deleting all statefulset in ns statefulset-4269
Sep  6 01:15:10.986: INFO: Scaling statefulset ss2 to 0
Sep  6 01:16:01.057: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 01:16:01.087: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:16:01.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4269" for this suite.

• [SLOW TEST:382.315 seconds]
[sig-apps] StatefulSet
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":296,"completed":261,"skipped":4707,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:16:02.059: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8538
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  6 01:16:02.919: INFO: Waiting up to 5m0s for pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641" in namespace "emptydir-8538" to be "Succeeded or Failed"
Sep  6 01:16:02.933: INFO: Pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641": Phase="Pending", Reason="", readiness=false. Elapsed: 14.242187ms
Sep  6 01:16:04.942: INFO: Pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02277029s
Sep  6 01:16:06.952: INFO: Pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033405836s
Sep  6 01:16:08.966: INFO: Pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641": Phase="Pending", Reason="", readiness=false. Elapsed: 6.047480173s
Sep  6 01:16:11.002: INFO: Pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641": Phase="Pending", Reason="", readiness=false. Elapsed: 8.083119097s
Sep  6 01:16:13.019: INFO: Pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641": Phase="Pending", Reason="", readiness=false. Elapsed: 10.099928772s
Sep  6 01:16:15.078: INFO: Pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641": Phase="Pending", Reason="", readiness=false. Elapsed: 12.158761622s
Sep  6 01:16:17.090: INFO: Pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641": Phase="Pending", Reason="", readiness=false. Elapsed: 14.170904042s
Sep  6 01:16:19.113: INFO: Pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641": Phase="Pending", Reason="", readiness=false. Elapsed: 16.194403528s
Sep  6 01:16:21.128: INFO: Pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641": Phase="Pending", Reason="", readiness=false. Elapsed: 18.209023008s
Sep  6 01:16:23.146: INFO: Pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641": Phase="Pending", Reason="", readiness=false. Elapsed: 20.227033362s
Sep  6 01:16:25.170: INFO: Pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641": Phase="Pending", Reason="", readiness=false. Elapsed: 22.250842371s
Sep  6 01:16:27.190: INFO: Pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641": Phase="Pending", Reason="", readiness=false. Elapsed: 24.271295284s
Sep  6 01:16:29.208: INFO: Pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641": Phase="Pending", Reason="", readiness=false. Elapsed: 26.28894791s
Sep  6 01:16:31.218: INFO: Pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641": Phase="Running", Reason="", readiness=true. Elapsed: 28.299615906s
Sep  6 01:16:33.238: INFO: Pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641": Phase="Running", Reason="", readiness=true. Elapsed: 30.318851195s
Sep  6 01:16:35.254: INFO: Pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641": Phase="Running", Reason="", readiness=true. Elapsed: 32.335310155s
Sep  6 01:16:37.268: INFO: Pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.349716426s
STEP: Saw pod success
Sep  6 01:16:37.269: INFO: Pod "pod-f0cb32a3-22bb-48ce-bc63-c981833da641" satisfied condition "Succeeded or Failed"
Sep  6 01:16:37.280: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-f0cb32a3-22bb-48ce-bc63-c981833da641 container test-container: <nil>
STEP: delete the pod
Sep  6 01:16:37.379: INFO: Waiting for pod pod-f0cb32a3-22bb-48ce-bc63-c981833da641 to disappear
Sep  6 01:16:37.388: INFO: Pod pod-f0cb32a3-22bb-48ce-bc63-c981833da641 still exists
Sep  6 01:16:39.389: INFO: Waiting for pod pod-f0cb32a3-22bb-48ce-bc63-c981833da641 to disappear
Sep  6 01:16:39.399: INFO: Pod pod-f0cb32a3-22bb-48ce-bc63-c981833da641 still exists
Sep  6 01:16:41.389: INFO: Waiting for pod pod-f0cb32a3-22bb-48ce-bc63-c981833da641 to disappear
Sep  6 01:16:41.403: INFO: Pod pod-f0cb32a3-22bb-48ce-bc63-c981833da641 still exists
Sep  6 01:16:43.389: INFO: Waiting for pod pod-f0cb32a3-22bb-48ce-bc63-c981833da641 to disappear
Sep  6 01:16:43.425: INFO: Pod pod-f0cb32a3-22bb-48ce-bc63-c981833da641 still exists
Sep  6 01:16:45.389: INFO: Waiting for pod pod-f0cb32a3-22bb-48ce-bc63-c981833da641 to disappear
Sep  6 01:16:45.405: INFO: Pod pod-f0cb32a3-22bb-48ce-bc63-c981833da641 still exists
Sep  6 01:16:47.389: INFO: Waiting for pod pod-f0cb32a3-22bb-48ce-bc63-c981833da641 to disappear
Sep  6 01:16:47.396: INFO: Pod pod-f0cb32a3-22bb-48ce-bc63-c981833da641 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:16:47.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8538" for this suite.

• [SLOW TEST:45.787 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:45
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":296,"completed":262,"skipped":4708,"failed":2,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount projected service account token [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:16:47.846: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8473
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount projected service account token [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test service account token: 
Sep  6 01:16:48.344: INFO: Waiting up to 5m0s for pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16" in namespace "svcaccounts-8473" to be "Succeeded or Failed"
Sep  6 01:16:48.360: INFO: Pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16": Phase="Pending", Reason="", readiness=false. Elapsed: 15.389419ms
Sep  6 01:16:50.370: INFO: Pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025331173s
Sep  6 01:16:52.383: INFO: Pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038371147s
Sep  6 01:16:54.402: INFO: Pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16": Phase="Pending", Reason="", readiness=false. Elapsed: 6.057615046s
Sep  6 01:16:56.437: INFO: Pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16": Phase="Pending", Reason="", readiness=false. Elapsed: 8.092651447s
Sep  6 01:16:58.447: INFO: Pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16": Phase="Pending", Reason="", readiness=false. Elapsed: 10.102153049s
Sep  6 01:17:00.481: INFO: Pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16": Phase="Pending", Reason="", readiness=false. Elapsed: 12.136339775s
Sep  6 01:17:02.495: INFO: Pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16": Phase="Pending", Reason="", readiness=false. Elapsed: 14.150435796s
Sep  6 01:17:04.507: INFO: Pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16": Phase="Pending", Reason="", readiness=false. Elapsed: 16.162567501s
Sep  6 01:17:06.518: INFO: Pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16": Phase="Pending", Reason="", readiness=false. Elapsed: 18.173480925s
Sep  6 01:17:08.530: INFO: Pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16": Phase="Pending", Reason="", readiness=false. Elapsed: 20.185224103s
Sep  6 01:17:10.541: INFO: Pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16": Phase="Pending", Reason="", readiness=false. Elapsed: 22.19626982s
Sep  6 01:17:12.560: INFO: Pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16": Phase="Pending", Reason="", readiness=false. Elapsed: 24.215880034s
Sep  6 01:17:14.579: INFO: Pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16": Phase="Pending", Reason="", readiness=false. Elapsed: 26.234243643s
Sep  6 01:17:16.592: INFO: Pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16": Phase="Failed", Reason="ProviderFailed", readiness=false. Elapsed: 28.247534408s
Sep  6 01:17:16.627: INFO: Failed to get logs from node "sc2-10-185-226-233.eng.vmware.com" pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16" container "agnhost-container": an error on the server ("unknown") has prevented the request from succeeding (get pods test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16)
STEP: delete the pod
Sep  6 01:17:16.649: INFO: Waiting for pod test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16 to disappear
Sep  6 01:17:16.675: INFO: Pod test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16 still exists
Sep  6 01:17:18.676: INFO: Waiting for pod test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16 to disappear
Sep  6 01:17:18.688: INFO: Pod test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16 still exists
Sep  6 01:17:20.677: INFO: Waiting for pod test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16 to disappear
Sep  6 01:17:20.693: INFO: Pod test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16 still exists
Sep  6 01:17:22.676: INFO: Waiting for pod test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16 to disappear
Sep  6 01:17:22.705: INFO: Pod test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16 still exists
Sep  6 01:17:24.676: INFO: Waiting for pod test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16 to disappear
Sep  6 01:17:24.691: INFO: Pod test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16 still exists
Sep  6 01:17:26.676: INFO: Waiting for pod test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16 to disappear
Sep  6 01:17:26.687: INFO: Pod test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16 no longer exists
Sep  6 01:17:26.688: FAIL: Unexpected error:
    <*errors.errorString | 0xc002388e50>: {
        s: "expected pod \"test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16\" success: pod \"test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16\" failed with status: {Phase:Failed Conditions:[{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-06 01:16:48 -0700 PDT Reason: Message:} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-06 01:17:14 -0700 PDT Reason:UnknownContainerStatuses Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-06 01:17:14 -0700 PDT Reason:UnknownContainerStatuses Message:} {Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-06 01:17:14 -0700 PDT Reason: Message:}] Message:rpc error: code = Internal desc = Could not run pod: service account token []string(nil)/3600/v1.BoundObjectReference{Kind:\"Pod\", APIVersion:\"v1\", Name:\"test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16\", UID:\"4d01b7df-23c9-4fd2-b522-8b7c1c011146\"} not found Reason:ProviderFailed NominatedNodeName: HostIP: PodIP: PodIPs:[] StartTime:<nil> InitContainerStatuses:[] ContainerStatuses:[] QOSClass:BestEffort EphemeralContainerStatuses:[]}",
    }
    expected pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16" success: pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16" failed with status: {Phase:Failed Conditions:[{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-06 01:16:48 -0700 PDT Reason: Message:} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-06 01:17:14 -0700 PDT Reason:UnknownContainerStatuses Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-06 01:17:14 -0700 PDT Reason:UnknownContainerStatuses Message:} {Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-06 01:17:14 -0700 PDT Reason: Message:}] Message:rpc error: code = Internal desc = Could not run pod: service account token []string(nil)/3600/v1.BoundObjectReference{Kind:"Pod", APIVersion:"v1", Name:"test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16", UID:"4d01b7df-23c9-4fd2-b522-8b7c1c011146"} not found Reason:ProviderFailed NominatedNodeName: HostIP: PodIP: PodIPs:[] StartTime:<nil> InitContainerStatuses:[] ContainerStatuses:[] QOSClass:BestEffort EphemeralContainerStatuses:[]}
occurred

Full Stack Trace
k8s.io/kubernetes/test/e2e/framework.(*Framework).testContainerOutputMatcher(0x4475a53, {0x4470b3b, 0xc00353f1f8}, 0xc000803800, 0x0, {0xc00353f1d8, 0x1, 0x1}, 0x2)
	/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:738 +0x176
k8s.io/kubernetes/test/e2e/framework.(*Framework).TestContainerOutputRegexp(...)
	/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:564
k8s.io/kubernetes/test/e2e/auth.glob..func6.4()
	/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/service_accounts.go:472 +0x41a
k8s.io/kubernetes/test/e2e.RunE2ETests(0x48f1d7)
	_output/local/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x6ae
k8s.io/kubernetes/test/e2e.TestE2E(0x408ed9)
	_output/local/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:144 +0x19
testing.tRunner(0xc001480820, 0x462aaa0)
	/usr/lib/golang/src/testing/testing.go:1259 +0x102
created by testing.(*T).Run
	/usr/lib/golang/src/testing/testing.go:1306 +0x35a
[AfterEach] [sig-auth] ServiceAccounts
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
STEP: Collecting events from namespace "svcaccounts-8473".
STEP: Found 10 events.
Sep  6 01:17:26.702: INFO: At 0001-01-01 00:00:00 +0000 UTC - event for test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16: { } Scheduled: Successfully assigned svcaccounts-8473/test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16 to sc2-10-185-226-233.eng.vmware.com
Sep  6 01:17:26.702: INFO: At 2021-09-06 01:16:48 -0700 PDT - event for test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16: {image-controller } Image: Image agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v60120 bound successfully
Sep  6 01:17:26.702: INFO: At 2021-09-06 01:16:51 -0700 PDT - event for agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v60120: {image-controller } Status: sc2-10-185-233-127.eng.vmware.com: Image status changed to Resolving
Sep  6 01:17:26.702: INFO: At 2021-09-06 01:16:58 -0700 PDT - event for test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16: {nsx-container-ncp 4216b30cf7711a81a7935dfb22c7b9d2} SuccessfulRealizeNSXResource: Successfully realized NSX resource for Pod
Sep  6 01:17:26.702: INFO: At 2021-09-06 01:17:00 -0700 PDT - event for agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v60120: {image-controller } Bind: Imagedisk bind failed: Operation cannot be fulfilled on images.imagecontroller.vmware.com "agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v60120": the object has been modified; please apply your changes to the latest version and try again
Sep  6 01:17:26.702: INFO: At 2021-09-06 01:17:00 -0700 PDT - event for agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v60120: {image-controller } Bind: Imagedisk d6492710f58c40963fae92a842ae782d61c22922bbe23fdb1a7e9e2483ef31b3-v19357092 successfully bound
Sep  6 01:17:26.702: INFO: At 2021-09-06 01:17:00 -0700 PDT - event for agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v60120: {image-controller } Status: Image status changed to Ready
Sep  6 01:17:26.702: INFO: At 2021-09-06 01:17:00 -0700 PDT - event for agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v60120: {image-controller } Resolve: sc2-10-185-233-127.eng.vmware.com: Image resolved to ChainID sha256:d6492710f58c40963fae92a842ae782d61c22922bbe23fdb1a7e9e2483ef31b3
Sep  6 01:17:26.702: INFO: At 2021-09-06 01:17:07 -0700 PDT - event for test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16: {kubelet sc2-10-185-226-233.eng.vmware.com} Pulling: Waiting for Image svcaccounts-8473/agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v60120
Sep  6 01:17:26.702: INFO: At 2021-09-06 01:17:07 -0700 PDT - event for test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16: {kubelet sc2-10-185-226-233.eng.vmware.com} Pulled: Image svcaccounts-8473/agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v60120 is ready
Sep  6 01:17:26.710: INFO: POD  NODE  PHASE  GRACE  CONDITIONS
Sep  6 01:17:26.710: INFO: 
Sep  6 01:17:26.723: INFO: 
Logging node info for node 421629d5b5512e89fd5a479875bff24c
Sep  6 01:17:26.736: INFO: Node Info: &Node{ObjectMeta:{421629d5b5512e89fd5a479875bff24c   /api/v1/nodes/421629d5b5512e89fd5a479875bff24c 5392ab9e-8d36-4a02-8e7c-d62b5fbca56f 222266 0 2021-09-05 19:56:51 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:421629d5b5512e89fd5a479875bff24c kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node-role.kubernetes.io/master:] map[kubeadm.alpha.kubernetes.io/cri-socket:/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 vmware-system-workload-ip-configured: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubeadm Update v1 2021-09-05 19:57:37 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/control-plane":{},"f:node-role.kubernetes.io/master":{}}}}} {kubectl-annotate Update v1 2021-09-05 20:03:41 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-workload-ip-configured":{}}}}} {kube-controller-manager Update v1 2021-09-05 20:11:03 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:taints":{}}}} {kubelet Update v1 2021-09-05 20:11:08 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-09-06 01:13:22 -0700 PDT,LastTransitionTime:2021-09-05 20:11:07 -0700 PDT,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-09-06 01:13:22 -0700 PDT,LastTransitionTime:2021-09-05 20:11:07 -0700 PDT,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-09-06 01:13:22 -0700 PDT,LastTransitionTime:2021-09-05 20:11:07 -0700 PDT,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-06 01:13:22 -0700 PDT,LastTransitionTime:2021-09-05 20:11:07 -0700 PDT,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:172.26.0.4,},NodeAddress{Type:Hostname,Address:421629d5b5512e89fd5a479875bff24c,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:a84e0f9b757b45c3bda61dd8d7e81c47,SystemUUID:d5291642-51b5-892e-fd5a-479875bff24c,BootID:96557016-0d11-4d1b-9d79-0a2c04c403df,KernelVersion:4.19.198-1.ph3-esx,OSImage:VMware Photon OS/Linux,ContainerRuntimeVersion:containerd://1.4.4,KubeletVersion:v1.20.8+vmware.wcp.1,KubeProxyVersion:v1.20.8+vmware.wcp.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[docker.io/vmware/wcp-authproxy:0.0.11.18508287],SizeBytes:131961954,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.21.0],SizeBytes:126851027,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.20.8],SizeBytes:123038003,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.21.0],SizeBytes:121071415,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.19.12],SizeBytes:120109363,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.20.8],SizeBytes:117573957,},ContainerImage{Names:[localhost:5000/vmware/kubectl-plugin-vsphere@sha256:50972d3c755506a39b060daf001e0fb4d7bfcc47aa059a945573d6962338a8f4 localhost:5000/vmware/kubectl-plugin-vsphere:0.0.11.18508287 docker.io/vmware/kubectl-plugin-vsphere:0.0.11.18508287],SizeBytes:114408041,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.19.12],SizeBytes:112073027,},ContainerImage{Names:[projects.registry.vmware.com/tkg/etcd:v3.4.13_vmware.1],SizeBytes:105113092,},ContainerImage{Names:[docker.io/vmware/wcp-schedext:0.0.11.18508287 docker.io/vmware/wcp-schedext:v1.20.8],SizeBytes:86647081,},ContainerImage{Names:[docker.io/vmware/docker-registry:2.7.0.14110537],SizeBytes:53040781,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.21.0],SizeBytes:51910456,},ContainerImage{Names:[docker.io/vmware/wcp-fip:0.1],SizeBytes:49119461,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.20.8],SizeBytes:48519474,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.19.12],SizeBytes:47737138,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-control-plane-controller@sha256:ae28d5c4008d5ad88db2936d9b45e0248d129245810239bbafe0c66a47f2bf7e localhost:5000/vmware/kubeadm-control-plane-controller:v0.3.17_vmware.1],SizeBytes:36065259,},ContainerImage{Names:[localhost:5000/vmware/kube-proxy:active localhost:5002/vmware/kube-proxy:v1.20.8],SizeBytes:35405060,},ContainerImage{Names:[localhost:5000/vmware/capw-controller@sha256:56496b9339aba7d338f2f864f0bb946e1d57f50bbcffeee26c57c1a7be6e9cff localhost:5000/vmware/capw-controller:1.4.1-28-ge3e1d8b],SizeBytes:34792328,},ContainerImage{Names:[localhost:5000/vmware/cluster-api-controller@sha256:72a217e8de91b2e1adcbcb1266a5854e9683f0f37ea828808374563d2bf41861 localhost:5000/vmware/cluster-api-controller:v0.3.17_vmware.1],SizeBytes:34280507,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-bootstrap-controller@sha256:921fc3887b0cfaf4742eefaf2b6d60160fdc25d2600dc08e9a3389aed6fbdaca localhost:5000/vmware/kubeadm-bootstrap-controller:v0.3.17_vmware.1],SizeBytes:33862350,},ContainerImage{Names:[localhost:5000/vmware/ucs@sha256:0006afae8591e51b3ddfe6d1062caa779c2bee08dc20ee38eb8183614ccbdcd1 localhost:5000/vmware/ucs:1.0-34-g9eb2916],SizeBytes:32101483,},ContainerImage{Names:[localhost:5000/vmware/tmc-agent-installer@sha256:4d885d2a505ccc87baf5fb73958593f0b66ce00732397e935114f923367cd4ec localhost:5000/vmware/tmc-agent-installer:1.0],SizeBytes:30452862,},ContainerImage{Names:[localhost:5000/vmware/kube-rbac-proxy@sha256:ce03b0932d049e82747829680025a5d2e3c2a3f50ad27e761329d35e9e8949d5 localhost:5000/vmware/kube-rbac-proxy:v0.4.1_vmware.1 localhost:5000/vmware/kube-rbac-proxy:0.0.1],SizeBytes:29742750,},ContainerImage{Names:[localhost:5000/vmware/vmop@sha256:aa3d277bcb390735b0110d9bf3f3e63ab48db23437c9ac275ea487a2fa460dd5 localhost:5000/vmware/vmop:1.4-201-g3d42a58],SizeBytes:29542297,},ContainerImage{Names:[localhost:5000/vmware/tkgs-plugin@sha256:7160ccba94dbc3a68fdb31108319ca03163688bc5999c4da1ee20e32723ea612 localhost:5000/vmware/tkgs-plugin:prod],SizeBytes:27042759,},ContainerImage{Names:[localhost:5000/vmware/tkg-controller@sha256:700277052cbfe8be5e7a67e7819bde16ac0afceba352c473906dc3cc572cdaf9 localhost:5000/vmware/tkg-controller:1.4-196-g0e69a9e],SizeBytes:26884265,},ContainerImage{Names:[localhost:5000/vmware.io/fluent-bit@sha256:bc50adf05790809513d94105a169912371b4b5f656775350b1ab65b556ecdc32 localhost:5000/vmware.io/fluent-bit:v1.5.1_vmware.3],SizeBytes:26471308,},ContainerImage{Names:[localhost:5000/vmware/namespace-operator@sha256:fbb1177a60f33312e0415830fd55c5b565628a863a25cb500b2058fc475e8da4 localhost:5000/vmware/namespace-operator:c203fcd],SizeBytes:25804352,},ContainerImage{Names:[localhost:5000/vmware/license-operator@sha256:521924f17d102e95723a580ed084b6fbc15e3f317b982a81fd071718c3e84138 localhost:5000/vmware/license-operator:7f54d05],SizeBytes:25041264,},ContainerImage{Names:[localhost:5000/vmware/coredns@sha256:68ed1b5aea34d19af320d3390278e7719efa19e3d9a42ef12fed0b2b99446c4f localhost:5000/vmware/coredns:v1.20],SizeBytes:12884153,},ContainerImage{Names:[docker.io/vmware/pause:1.19.12 docker.io/vmware/pause:1.20.8 docker.io/vmware/pause:1.21.0],SizeBytes:685866,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Sep  6 01:17:26.736: INFO: 
Logging kubelet events for node 421629d5b5512e89fd5a479875bff24c
Sep  6 01:17:26.745: INFO: 
Logging pods the kubelet thinks is on node 421629d5b5512e89fd5a479875bff24c
Sep  6 01:17:26.813: INFO: kube-apiserver-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 19:57:05 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container kube-apiserver ready: true, restart count 2
Sep  6 01:17:26.814: INFO: vmware-system-tkg-webhook-64fd4868cb-mk6zh started at 2021-09-05 20:03:54 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:26.814: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:26.814: INFO: vmware-system-vmop-controller-manager-6649dd65b-q7bqw started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:26.814: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:26.814: INFO: capi-kubeadm-control-plane-controller-manager-7c8d4b456f-l4t5v started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:26.814: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:26.814: INFO: tmc-agent-installer-1630916220-zllcs started at 2021-09-06 01:17:00 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container tmc-agent-installer ready: false, restart count 0
Sep  6 01:17:26.814: INFO: tkgs-plugin-server-57df5fcfbf-km5pj started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:26.814: INFO: capw-webhook-58b86fb8b-vrzsj started at 2021-09-05 20:03:54 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:26.814: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:26.814: INFO: vmware-system-nsop-controller-manager-7dd8c47dcc-hbv6m started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container manager ready: true, restart count 1
Sep  6 01:17:26.814: INFO: upgrade-compatibility-service-65969fd6bc-l72w9 started at 2021-09-05 20:03:55 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container ucs ready: true, restart count 0
Sep  6 01:17:26.814: INFO: vmware-system-license-operator-controller-manager-5f5b6cddmtkxt started at 2021-09-05 20:03:55 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:26.814: INFO: masterproxy-tkgs-plugin-k5csq started at 2021-09-05 20:04:12 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container nginx ready: true, restart count 0
Sep  6 01:17:26.814: INFO: coredns-594c6dccdd-6jv2b started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container coredns ready: true, restart count 0
Sep  6 01:17:26.814: INFO: capi-controller-manager-d586f4c8-dnnr6 started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:26.814: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:26.814: INFO: kubectl-plugin-vsphere-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container kubectl-plugin-vsphere ready: true, restart count 4
Sep  6 01:17:26.814: INFO: wcp-authproxy-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container wcp-authproxy ready: true, restart count 0
Sep  6 01:17:26.814: INFO: capw-controller-manager-85b7cbb4bf-bghkv started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:26.814: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:26.814: INFO: wcp-fip-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container wcp-fip ready: true, restart count 0
Sep  6 01:17:26.814: INFO: vmware-system-tkg-controller-manager-575d95fb57-zl7sg started at 2021-09-05 20:03:54 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:26.814: INFO: 	Container manager ready: true, restart count 1
Sep  6 01:17:26.814: INFO: capi-kubeadm-bootstrap-controller-manager-5f5774d559-6cbtc started at 2021-09-05 20:03:54 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:26.814: INFO: 	Container manager ready: true, restart count 1
Sep  6 01:17:26.814: INFO: capi-kubeadm-control-plane-webhook-5f4c87b8b9-cphz4 started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:26.814: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:26.814: INFO: capi-webhook-69769f4c68-xjnm2 started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:26.814: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:26.814: INFO: kube-proxy-cwdbw started at 2021-09-05 20:04:19 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 01:17:26.814: INFO: docker-registry-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container docker-registry ready: true, restart count 0
Sep  6 01:17:26.814: INFO: etcd-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 20:03:54 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container etcd ready: true, restart count 0
Sep  6 01:17:26.814: INFO: kube-controller-manager-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 19:57:05 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container kube-controller-manager ready: true, restart count 2
Sep  6 01:17:26.814: INFO: kube-scheduler-421629d5b5512e89fd5a479875bff24c started at 2021-09-05 19:57:05 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container kube-scheduler ready: true, restart count 3
Sep  6 01:17:26.814: INFO: 	Container wcp-schedext ready: true, restart count 1
Sep  6 01:17:26.814: INFO: fluentbit-rpz54 started at 2021-09-05 19:58:12 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container fluentbit ready: true, restart count 0
Sep  6 01:17:26.814: INFO: capi-kubeadm-bootstrap-webhook-6766c687f9-hjz5w started at 2021-09-05 20:03:55 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:26.814: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:26.814: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:27.433: INFO: 
Latency metrics for node 421629d5b5512e89fd5a479875bff24c
Sep  6 01:17:27.433: INFO: 
Logging node info for node 4216b0433daa6b68a0e3d69463d0611d
Sep  6 01:17:27.445: INFO: Node Info: &Node{ObjectMeta:{4216b0433daa6b68a0e3d69463d0611d   /api/v1/nodes/4216b0433daa6b68a0e3d69463d0611d f38a87dd-ca99-4665-a189-88c353c18242 223615 0 2021-09-05 19:55:52 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:4216b0433daa6b68a0e3d69463d0611d kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node-role.kubernetes.io/master:] map[kubeadm.alpha.kubernetes.io/cri-socket:/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 vmware-system-workload-ip-configured: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubeadm Update v1 2021-09-05 19:56:44 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/control-plane":{},"f:node-role.kubernetes.io/master":{}}}}} {kubectl-annotate Update v1 2021-09-05 20:01:13 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-workload-ip-configured":{}}}}} {kube-controller-manager Update v1 2021-09-05 20:08:11 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:taints":{}}}} {kubelet Update v1 2021-09-05 20:08:16 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-09-06 01:15:35 -0700 PDT,LastTransitionTime:2021-09-05 20:08:16 -0700 PDT,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-09-06 01:15:35 -0700 PDT,LastTransitionTime:2021-09-05 20:08:16 -0700 PDT,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-09-06 01:15:35 -0700 PDT,LastTransitionTime:2021-09-05 20:08:16 -0700 PDT,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-06 01:15:35 -0700 PDT,LastTransitionTime:2021-09-05 20:08:16 -0700 PDT,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:172.26.0.3,},NodeAddress{Type:Hostname,Address:4216b0433daa6b68a0e3d69463d0611d,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:7611aba96c374b2daf54816748293442,SystemUUID:43b01642-aa3d-686b-a0e3-d69463d0611d,BootID:ebcc64ab-5283-4781-b7ac-d3f663f1dc23,KernelVersion:4.19.198-1.ph3-esx,OSImage:VMware Photon OS/Linux,ContainerRuntimeVersion:containerd://1.4.4,KubeletVersion:v1.20.8+vmware.wcp.1,KubeProxyVersion:v1.20.8+vmware.wcp.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[docker.io/vmware/wcp-authproxy:0.0.11.18508287],SizeBytes:131961954,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.21.0],SizeBytes:126851027,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.20.8],SizeBytes:123038003,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.21.0],SizeBytes:121071415,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.19.12],SizeBytes:120109363,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.20.8],SizeBytes:117573957,},ContainerImage{Names:[localhost:5000/vmware/kubectl-plugin-vsphere@sha256:50972d3c755506a39b060daf001e0fb4d7bfcc47aa059a945573d6962338a8f4 docker.io/vmware/kubectl-plugin-vsphere:0.0.11.18508287 localhost:5000/vmware/kubectl-plugin-vsphere:0.0.11.18508287],SizeBytes:114408041,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.19.12],SizeBytes:112073027,},ContainerImage{Names:[projects.registry.vmware.com/tkg/etcd:v3.4.13_vmware.1],SizeBytes:105113092,},ContainerImage{Names:[docker.io/vmware/wcp-schedext:v1.20.8 docker.io/vmware/wcp-schedext:0.0.11.18508287],SizeBytes:86647081,},ContainerImage{Names:[localhost:5000/vmware/registry-agent@sha256:a02a9f68366f57abdd0833d34f1394e402a519d3fe82ff8c1106c67fc7c80392 localhost:5000/vmware/registry-agent:0.0.11.18508287],SizeBytes:76983788,},ContainerImage{Names:[docker.io/vmware/docker-registry:2.7.0.14110537],SizeBytes:53040781,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.21.0],SizeBytes:51910456,},ContainerImage{Names:[docker.io/vmware/wcp-fip:0.1],SizeBytes:49119461,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.20.8],SizeBytes:48519474,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.19.12],SizeBytes:47737138,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-control-plane-controller@sha256:ae28d5c4008d5ad88db2936d9b45e0248d129245810239bbafe0c66a47f2bf7e localhost:5000/vmware/kubeadm-control-plane-controller:v0.3.17_vmware.1],SizeBytes:36065259,},ContainerImage{Names:[localhost:5002/vmware/kube-proxy:v1.20.8 localhost:5000/vmware/kube-proxy:active],SizeBytes:35405060,},ContainerImage{Names:[localhost:5000/vmware/capw-controller@sha256:56496b9339aba7d338f2f864f0bb946e1d57f50bbcffeee26c57c1a7be6e9cff localhost:5000/vmware/capw-controller:1.4.1-28-ge3e1d8b],SizeBytes:34792328,},ContainerImage{Names:[localhost:5000/vmware/cluster-api-controller@sha256:72a217e8de91b2e1adcbcb1266a5854e9683f0f37ea828808374563d2bf41861 localhost:5000/vmware/cluster-api-controller:v0.3.17_vmware.1],SizeBytes:34280507,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-bootstrap-controller@sha256:921fc3887b0cfaf4742eefaf2b6d60160fdc25d2600dc08e9a3389aed6fbdaca localhost:5000/vmware/kubeadm-bootstrap-controller:v0.3.17_vmware.1],SizeBytes:33862350,},ContainerImage{Names:[localhost:5000/vmware/ucs@sha256:0006afae8591e51b3ddfe6d1062caa779c2bee08dc20ee38eb8183614ccbdcd1 localhost:5000/vmware/ucs:1.0-34-g9eb2916],SizeBytes:32101483,},ContainerImage{Names:[localhost:5000/vmware/tmc-agent-installer@sha256:4d885d2a505ccc87baf5fb73958593f0b66ce00732397e935114f923367cd4ec localhost:5000/vmware/tmc-agent-installer:1.0],SizeBytes:30452862,},ContainerImage{Names:[localhost:5000/vmware/kube-rbac-proxy@sha256:ce03b0932d049e82747829680025a5d2e3c2a3f50ad27e761329d35e9e8949d5 localhost:5000/vmware/kube-rbac-proxy:v0.4.1_vmware.1 localhost:5000/vmware/kube-rbac-proxy:0.0.1],SizeBytes:29742750,},ContainerImage{Names:[localhost:5000/vmware/vmop@sha256:aa3d277bcb390735b0110d9bf3f3e63ab48db23437c9ac275ea487a2fa460dd5 localhost:5000/vmware/vmop:1.4-201-g3d42a58],SizeBytes:29542297,},ContainerImage{Names:[localhost:5000/vmware/tkgs-plugin@sha256:7160ccba94dbc3a68fdb31108319ca03163688bc5999c4da1ee20e32723ea612 localhost:5000/vmware/tkgs-plugin:prod],SizeBytes:27042759,},ContainerImage{Names:[localhost:5000/vmware/tkg-controller@sha256:700277052cbfe8be5e7a67e7819bde16ac0afceba352c473906dc3cc572cdaf9 localhost:5000/vmware/tkg-controller:1.4-196-g0e69a9e],SizeBytes:26884265,},ContainerImage{Names:[localhost:5000/vmware.io/fluent-bit@sha256:bc50adf05790809513d94105a169912371b4b5f656775350b1ab65b556ecdc32 localhost:5000/vmware.io/fluent-bit:v1.5.1_vmware.3],SizeBytes:26471308,},ContainerImage{Names:[localhost:5000/vmware/namespace-operator@sha256:fbb1177a60f33312e0415830fd55c5b565628a863a25cb500b2058fc475e8da4 localhost:5000/vmware/namespace-operator:c203fcd],SizeBytes:25804352,},ContainerImage{Names:[localhost:5000/vmware/license-operator@sha256:521924f17d102e95723a580ed084b6fbc15e3f317b982a81fd071718c3e84138 localhost:5000/vmware/license-operator:7f54d05],SizeBytes:25041264,},ContainerImage{Names:[localhost:5000/vmware/coredns@sha256:68ed1b5aea34d19af320d3390278e7719efa19e3d9a42ef12fed0b2b99446c4f localhost:5000/vmware/coredns:v1.20],SizeBytes:12884153,},ContainerImage{Names:[docker.io/vmware/pause:1.21.0 docker.io/vmware/pause:1.19.12 docker.io/vmware/pause:1.20.8],SizeBytes:685866,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Sep  6 01:17:27.445: INFO: 
Logging kubelet events for node 4216b0433daa6b68a0e3d69463d0611d
Sep  6 01:17:27.472: INFO: 
Logging pods the kubelet thinks is on node 4216b0433daa6b68a0e3d69463d0611d
Sep  6 01:17:27.571: INFO: kubectl-plugin-vsphere-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:56:05 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:27.571: INFO: 	Container kubectl-plugin-vsphere ready: true, restart count 2
Sep  6 01:17:27.571: INFO: wcp-fip-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 20:01:25 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:27.571: INFO: 	Container wcp-fip ready: true, restart count 0
Sep  6 01:17:27.571: INFO: vmware-system-nsop-controller-manager-7dd8c47dcc-fvwkc started at 2021-09-05 20:01:38 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:27.571: INFO: 	Container manager ready: true, restart count 1
Sep  6 01:17:27.571: INFO: capi-kubeadm-bootstrap-controller-manager-5f5774d559-bdxq9 started at 2021-09-05 20:01:38 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:27.571: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:27.571: INFO: 	Container manager ready: true, restart count 1
Sep  6 01:17:27.571: INFO: capi-kubeadm-bootstrap-webhook-6766c687f9-tfvhj started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:27.571: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:27.571: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:27.571: INFO: vmware-system-vmop-controller-manager-6649dd65b-6mvnw started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:27.571: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:27.571: INFO: 	Container manager ready: true, restart count 1
Sep  6 01:17:27.571: INFO: vmware-system-tkg-controller-manager-575d95fb57-k4slt started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:27.571: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:27.571: INFO: 	Container manager ready: true, restart count 1
Sep  6 01:17:27.571: INFO: tkgs-plugin-server-57df5fcfbf-pcxc7 started at 2021-09-05 20:01:25 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:27.571: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:27.571: INFO: capw-webhook-58b86fb8b-l8m95 started at 2021-09-05 20:01:38 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:27.571: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:27.571: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:27.571: INFO: vmware-system-license-operator-controller-manager-5f5b6cddsqckm started at 2021-09-05 20:01:38 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:27.571: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:27.571: INFO: vmware-registry-controller-manager-75fff77685-9v4wb started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:27.571: INFO: 	Container admin-agent ready: true, restart count 0
Sep  6 01:17:27.571: INFO: 	Container service-agent ready: true, restart count 5
Sep  6 01:17:27.571: INFO: kube-controller-manager-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:56:05 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:27.571: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep  6 01:17:27.571: INFO: kube-apiserver-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:56:05 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:27.572: INFO: 	Container kube-apiserver ready: true, restart count 3
Sep  6 01:17:27.572: INFO: masterproxy-tkgs-plugin-b7p7c started at 2021-09-05 20:01:25 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:27.572: INFO: 	Container nginx ready: true, restart count 0
Sep  6 01:17:27.572: INFO: coredns-594c6dccdd-vp82c started at 2021-09-05 20:01:37 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:27.572: INFO: 	Container coredns ready: true, restart count 0
Sep  6 01:17:27.572: INFO: capi-controller-manager-d586f4c8-m66p4 started at 2021-09-05 20:01:38 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:27.572: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:27.572: INFO: 	Container manager ready: true, restart count 1
Sep  6 01:17:27.572: INFO: wcp-authproxy-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:58:14 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:27.572: INFO: 	Container wcp-authproxy ready: true, restart count 0
Sep  6 01:17:27.572: INFO: etcd-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:56:05 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:27.572: INFO: 	Container etcd ready: true, restart count 0
Sep  6 01:17:27.572: INFO: fluentbit-8fbtb started at 2021-09-05 19:57:29 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:27.572: INFO: 	Container fluentbit ready: true, restart count 0
Sep  6 01:17:27.572: INFO: capw-controller-manager-85b7cbb4bf-n2kkl started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:27.572: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:27.572: INFO: 	Container manager ready: true, restart count 3
Sep  6 01:17:27.572: INFO: capi-webhook-69769f4c68-5mxkm started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:27.572: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:27.572: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:27.572: INFO: kube-scheduler-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:56:05 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:27.572: INFO: 	Container kube-scheduler ready: true, restart count 2
Sep  6 01:17:27.572: INFO: 	Container wcp-schedext ready: true, restart count 0
Sep  6 01:17:27.572: INFO: upgrade-compatibility-service-65969fd6bc-5ztpr started at 2021-09-05 20:01:38 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:27.572: INFO: 	Container ucs ready: true, restart count 0
Sep  6 01:17:27.572: INFO: capi-kubeadm-control-plane-webhook-5f4c87b8b9-zr5kj started at 2021-09-05 20:01:38 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:27.572: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:27.572: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:27.572: INFO: docker-registry-4216b0433daa6b68a0e3d69463d0611d started at 2021-09-05 19:56:05 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:27.572: INFO: 	Container docker-registry ready: true, restart count 0
Sep  6 01:17:27.572: INFO: kube-proxy-rpd77 started at 2021-09-05 20:01:38 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:27.572: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 01:17:27.572: INFO: capi-kubeadm-control-plane-controller-manager-7c8d4b456f-qsw77 started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:27.572: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:27.572: INFO: 	Container manager ready: true, restart count 1
Sep  6 01:17:27.572: INFO: vmware-system-tkg-webhook-64fd4868cb-c7l5v started at 2021-09-05 20:01:39 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:27.572: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:27.572: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:28.298: INFO: 
Latency metrics for node 4216b0433daa6b68a0e3d69463d0611d
Sep  6 01:17:28.298: INFO: 
Logging node info for node 4216b30cf7711a81a7935dfb22c7b9d2
Sep  6 01:17:28.313: INFO: Node Info: &Node{ObjectMeta:{4216b30cf7711a81a7935dfb22c7b9d2   /api/v1/nodes/4216b30cf7711a81a7935dfb22c7b9d2 ca76b1f1-4314-4242-80c1-22a5cf294536 223175 0 2021-09-05 19:45:46 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:4216b30cf7711a81a7935dfb22c7b9d2 kubernetes.io/os:linux node-role.kubernetes.io/control-plane: node-role.kubernetes.io/master:] map[kubeadm.alpha.kubernetes.io/cri-socket:/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 vmware-system-workload-ip-configured: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubelet Update v1 2021-09-05 19:45:46 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}} {kubeadm Update v1 2021-09-05 19:45:51 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/control-plane":{},"f:node-role.kubernetes.io/master":{}}}}} {kube-controller-manager Update v1 2021-09-05 19:45:56 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:taints":{}}}} {kubectl-annotate Update v1 2021-09-05 19:55:19 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-workload-ip-configured":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-09-06 01:14:52 -0700 PDT,LastTransitionTime:2021-09-05 19:45:41 -0700 PDT,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-09-06 01:14:52 -0700 PDT,LastTransitionTime:2021-09-05 19:45:41 -0700 PDT,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-09-06 01:14:52 -0700 PDT,LastTransitionTime:2021-09-05 19:45:41 -0700 PDT,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-06 01:14:52 -0700 PDT,LastTransitionTime:2021-09-05 19:58:42 -0700 PDT,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:172.26.0.2,},NodeAddress{Type:Hostname,Address:4216b30cf7711a81a7935dfb22c7b9d2,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:a54387ba332340588c82ffb9af566e0a,SystemUUID:0cb31642-71f7-811a-a793-5dfb22c7b9d2,BootID:ac168864-03bb-44ca-934a-1f275ba10acf,KernelVersion:4.19.198-1.ph3-esx,OSImage:VMware Photon OS/Linux,ContainerRuntimeVersion:containerd://1.4.4,KubeletVersion:v1.20.8+vmware.wcp.1,KubeProxyVersion:v1.20.8+vmware.wcp.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[localhost:5000/vmware/nsx-ncp-photon@sha256:07d0445341f274674dea40d9449da3304c62615afa544f4023e3a3e21c353c89 localhost:5000/vmware/nsx-ncp-photon:3.2.0.18464816],SizeBytes:456347510,},ContainerImage{Names:[localhost:5000/vmware/vsphere-csi@sha256:5f667055674ed889a9a5a3e52e1f074d2adedb5ccf5fdc9a2cefb56e50257346 localhost:5000/vmware/vsphere-csi:vsphere70u3-3530247],SizeBytes:218324207,},ContainerImage{Names:[localhost:5000/vmware/syncer@sha256:ebb1907ad57ee343efe2a756bd7109021701469875d29008004d2e6abe615e66 localhost:5000/vmware/syncer:vsphere70u3-3530247],SizeBytes:186035682,},ContainerImage{Names:[docker.io/vmware/wcp-authproxy:0.0.11.18508287],SizeBytes:131961954,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.21.0],SizeBytes:126851027,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.20.8],SizeBytes:123038003,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.21.0],SizeBytes:121071415,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.19.12],SizeBytes:120109363,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.20.8],SizeBytes:117573957,},ContainerImage{Names:[localhost:5000/vmware/kubectl-plugin-vsphere@sha256:50972d3c755506a39b060daf001e0fb4d7bfcc47aa059a945573d6962338a8f4 docker.io/vmware/kubectl-plugin-vsphere:0.0.11.18508287 localhost:5000/vmware/kubectl-plugin-vsphere:0.0.11.18508287],SizeBytes:114408041,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.19.12],SizeBytes:112073027,},ContainerImage{Names:[projects.registry.vmware.com/tkg/etcd:v3.4.13_vmware.1],SizeBytes:105113092,},ContainerImage{Names:[localhost:5000/vmware/cert-manager-controller@sha256:ff0caa9f76178dfc2c4398046b4b08472ab90def98d1b8649833d092a1468100 localhost:5000/vmware/cert-manager-controller:v0.15.2_vmware.3],SizeBytes:94027733,},ContainerImage{Names:[localhost:5000/vmware/cert-manager-cainjector@sha256:5137cbc469ee041ef0d1f6d35a914889d50c0a44134967ca407ba7c2f1a7628d localhost:5000/vmware/cert-manager-cainjector:v0.15.2_vmware.3],SizeBytes:88001986,},ContainerImage{Names:[localhost:5000/vmware/cert-manager-webhook@sha256:da40b26c6920331288a4809415c202e2ad3a60392ee3f43f535626024b8a9845 localhost:5000/vmware/cert-manager-webhook:v0.15.2_vmware.3],SizeBytes:87704580,},ContainerImage{Names:[docker.io/vmware/wcp-schedext:0.0.11.18508287 docker.io/vmware/wcp-schedext:v1.20.8],SizeBytes:86647081,},ContainerImage{Names:[localhost:5000/vmware/wcp-appplatform-operator-v1alpha2@sha256:39f94d105b25a4f047caa8234d74645c21ccee2e13279ee4e07fb506e27d7494 localhost:5000/vmware/wcp-appplatform-operator-v1alpha2:5e0ffb0],SizeBytes:72197580,},ContainerImage{Names:[localhost:5000/vmware/psp-operator@sha256:3a831bb95bb73b3bda416fc37e3eda8eb2ed2a54109ec2e54ad6d70932f2813e localhost:5000/vmware/psp-operator:49766149],SizeBytes:59395632,},ContainerImage{Names:[docker.io/vmware/docker-registry:2.7.0.14110537],SizeBytes:53040781,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.21.0],SizeBytes:51910456,},ContainerImage{Names:[docker.io/vmware/wcp-fip:0.1],SizeBytes:49119461,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.20.8],SizeBytes:48519474,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.19.12],SizeBytes:47737138,},ContainerImage{Names:[localhost:5000/vmware/image-controller@sha256:e8bcd8932715a95d86f857fc838d67aed6953eeab4554a3dada45c80a222b2ff localhost:5000/vmware/image-controller:0.0.11.18508287],SizeBytes:38823779,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-control-plane-controller@sha256:ae28d5c4008d5ad88db2936d9b45e0248d129245810239bbafe0c66a47f2bf7e localhost:5000/vmware/kubeadm-control-plane-controller:v0.3.17_vmware.1],SizeBytes:36065259,},ContainerImage{Names:[localhost:5002/vmware/kube-proxy:v1.20.8 localhost:5000/vmware/kube-proxy:active],SizeBytes:35405060,},ContainerImage{Names:[localhost:5000/vmware/capw-controller@sha256:56496b9339aba7d338f2f864f0bb946e1d57f50bbcffeee26c57c1a7be6e9cff localhost:5000/vmware/capw-controller:1.4.1-28-ge3e1d8b],SizeBytes:34792328,},ContainerImage{Names:[localhost:5000/vmware/cluster-api-controller@sha256:72a217e8de91b2e1adcbcb1266a5854e9683f0f37ea828808374563d2bf41861 localhost:5000/vmware/cluster-api-controller:v0.3.17_vmware.1],SizeBytes:34280507,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-bootstrap-controller@sha256:921fc3887b0cfaf4742eefaf2b6d60160fdc25d2600dc08e9a3389aed6fbdaca localhost:5000/vmware/kubeadm-bootstrap-controller:v0.3.17_vmware.1],SizeBytes:33862350,},ContainerImage{Names:[localhost:5000/vmware/ucs@sha256:0006afae8591e51b3ddfe6d1062caa779c2bee08dc20ee38eb8183614ccbdcd1 localhost:5000/vmware/ucs:1.0-34-g9eb2916],SizeBytes:32101483,},ContainerImage{Names:[localhost:5000/vmware/tmc-agent-installer@sha256:4d885d2a505ccc87baf5fb73958593f0b66ce00732397e935114f923367cd4ec localhost:5000/vmware/tmc-agent-installer:1.0],SizeBytes:30452862,},ContainerImage{Names:[localhost:5000/vmware/kube-rbac-proxy@sha256:ce03b0932d049e82747829680025a5d2e3c2a3f50ad27e761329d35e9e8949d5 localhost:5000/vmware/kube-rbac-proxy:v0.4.1_vmware.1 localhost:5000/vmware/kube-rbac-proxy:0.0.1],SizeBytes:29742750,},ContainerImage{Names:[localhost:5000/vmware/vmop@sha256:aa3d277bcb390735b0110d9bf3f3e63ab48db23437c9ac275ea487a2fa460dd5 localhost:5000/vmware/vmop:1.4-201-g3d42a58],SizeBytes:29542297,},ContainerImage{Names:[localhost:5000/vmware/tkgs-plugin@sha256:7160ccba94dbc3a68fdb31108319ca03163688bc5999c4da1ee20e32723ea612 localhost:5000/vmware/tkgs-plugin:prod],SizeBytes:27042759,},ContainerImage{Names:[localhost:5000/vmware/tkg-controller@sha256:700277052cbfe8be5e7a67e7819bde16ac0afceba352c473906dc3cc572cdaf9 localhost:5000/vmware/tkg-controller:1.4-196-g0e69a9e],SizeBytes:26884265,},ContainerImage{Names:[localhost:5000/vmware.io/fluent-bit@sha256:bc50adf05790809513d94105a169912371b4b5f656775350b1ab65b556ecdc32 localhost:5000/vmware.io/fluent-bit:v1.5.1_vmware.3],SizeBytes:26471308,},ContainerImage{Names:[localhost:5000/vmware/namespace-operator@sha256:fbb1177a60f33312e0415830fd55c5b565628a863a25cb500b2058fc475e8da4 localhost:5000/vmware/namespace-operator:c203fcd],SizeBytes:25804352,},ContainerImage{Names:[localhost:5000/vmware/license-operator@sha256:521924f17d102e95723a580ed084b6fbc15e3f317b982a81fd071718c3e84138 localhost:5000/vmware/license-operator:7f54d05],SizeBytes:25041264,},ContainerImage{Names:[localhost:5000/vmware/coredns@sha256:68ed1b5aea34d19af320d3390278e7719efa19e3d9a42ef12fed0b2b99446c4f localhost:5000/vmware/coredns:v1.20],SizeBytes:12884153,},ContainerImage{Names:[localhost:5000/vmware.io/csi-attacher@sha256:ef84a54cab084305c3b6638bcf3616539a1ece71054e519ce72c74a85ed6883c localhost:5000/vmware.io/csi-attacher:v3.2.1_vmware.1],SizeBytes:12847160,},ContainerImage{Names:[localhost:5000/vmware/kubernetes-csi_external-resizer/kubernetes-csi_external-resizer@sha256:eafb68e3367ac1b840b55b115351163cd24dfc1752e402c8d83210ab35d251eb localhost:5000/vmware/kubernetes-csi_external-resizer/kubernetes-csi_external-resizer:v1.2.0_vmware.1],SizeBytes:12843921,},ContainerImage{Names:[localhost:5000/vmware/csi-provisioner/csi-provisioner@sha256:f3783faf0e57904cde3702f7f0fb1f6e959c3160c90d399ea90931acd77abc18 localhost:5000/vmware/csi-provisioner/csi-provisioner:v2.1.0_vmware.4],SizeBytes:12501308,},ContainerImage{Names:[localhost:5000/vmware.io/csi-livenessprobe@sha256:7790ca0da41bfc8cd05c35c9309a36d060f847402147fddc32ca14063f9adc89 localhost:5000/vmware.io/csi-livenessprobe:v2.3.0_vmware.1],SizeBytes:5629758,},ContainerImage{Names:[docker.io/vmware/pause:1.19.12 docker.io/vmware/pause:1.20.8 docker.io/vmware/pause:1.21.0],SizeBytes:685866,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Sep  6 01:17:28.313: INFO: 
Logging kubelet events for node 4216b30cf7711a81a7935dfb22c7b9d2
Sep  6 01:17:28.321: INFO: 
Logging pods the kubelet thinks is on node 4216b30cf7711a81a7935dfb22c7b9d2
Sep  6 01:17:28.404: INFO: image-controller-597bd95bc9-t5pqm started at 2021-09-05 19:47:14 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container image-controller ready: true, restart count 1
Sep  6 01:17:28.404: INFO: capi-controller-manager-d586f4c8-pjhgc started at 2021-09-05 19:50:51 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:28.404: INFO: 	Container manager ready: true, restart count 3
Sep  6 01:17:28.404: INFO: capw-webhook-58b86fb8b-nxkgl started at 2021-09-05 19:50:54 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:28.404: INFO: 	Container manager ready: true, restart count 1
Sep  6 01:17:28.404: INFO: vmware-system-psp-operator-mgr-6cc4d85755-w7lf5 started at 2021-09-05 19:51:14 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container manager ready: true, restart count 8
Sep  6 01:17:28.404: INFO: vmware-system-tkg-webhook-64fd4868cb-8khb4 started at 2021-09-05 19:51:32 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:28.404: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:28.404: INFO: etcd-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:44:40 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container etcd ready: true, restart count 0
Sep  6 01:17:28.404: INFO: docker-registry-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:58:35 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container docker-registry ready: true, restart count 0
Sep  6 01:17:28.404: INFO: vmware-system-vmop-controller-manager-6649dd65b-tzzst started at 2021-09-05 19:51:23 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:28.404: INFO: 	Container manager ready: true, restart count 5
Sep  6 01:17:28.404: INFO: masterproxy-tkgs-plugin-8x4lc started at 2021-09-05 19:53:10 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container nginx ready: true, restart count 0
Sep  6 01:17:28.404: INFO: vmware-system-nsop-controller-manager-7dd8c47dcc-22cs2 started at 2021-09-05 19:53:20 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container manager ready: true, restart count 2
Sep  6 01:17:28.404: INFO: kube-apiserver-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:58:35 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container kube-apiserver ready: true, restart count 1
Sep  6 01:17:28.404: INFO: wcp-fip-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:58:35 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container wcp-fip ready: true, restart count 0
Sep  6 01:17:28.404: INFO: fluentbit-k2ft9 started at 2021-09-05 19:47:14 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container fluentbit ready: true, restart count 0
Sep  6 01:17:28.404: INFO: capi-kubeadm-bootstrap-controller-manager-5f5774d559-whnw5 started at 2021-09-05 19:50:51 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:28.404: INFO: 	Container manager ready: true, restart count 4
Sep  6 01:17:28.404: INFO: capw-controller-manager-85b7cbb4bf-9kcpx started at 2021-09-05 19:50:53 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:28.404: INFO: 	Container manager ready: true, restart count 3
Sep  6 01:17:28.404: INFO: coredns-594c6dccdd-d29zk started at 2021-09-05 19:47:13 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container coredns ready: true, restart count 7
Sep  6 01:17:28.404: INFO: kube-controller-manager-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:44:40 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container kube-controller-manager ready: true, restart count 2
Sep  6 01:17:28.404: INFO: vmware-system-appplatform-operator-mgr-0 started at 2021-09-05 19:47:14 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:28.404: INFO: cert-manager-cainjector-69c886766f-glw2p started at 2021-09-05 19:47:17 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container cert-manager ready: true, restart count 6
Sep  6 01:17:28.404: INFO: upgrade-compatibility-service-65969fd6bc-wgqdt started at 2021-09-05 19:47:14 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container ucs ready: true, restart count 0
Sep  6 01:17:28.404: INFO: cert-manager-799b5bbfdf-52dgq started at 2021-09-05 19:47:17 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container cert-manager ready: true, restart count 0
Sep  6 01:17:28.404: INFO: cert-manager-webhook-74488f47f-c6nmc started at 2021-09-05 19:47:17 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container cert-manager ready: true, restart count 0
Sep  6 01:17:28.404: INFO: tkgs-plugin-server-57df5fcfbf-6gcbl started at 2021-09-05 19:52:48 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:28.404: INFO: vsphere-csi-controller-7ff5f98858-cthb7 started at 2021-09-05 19:52:54 -0700 PDT (0+6 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container csi-attacher ready: true, restart count 4
Sep  6 01:17:28.404: INFO: 	Container csi-provisioner ready: true, restart count 4
Sep  6 01:17:28.404: INFO: 	Container csi-resizer ready: true, restart count 6
Sep  6 01:17:28.404: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  6 01:17:28.404: INFO: 	Container vsphere-csi-controller ready: true, restart count 3
Sep  6 01:17:28.404: INFO: 	Container vsphere-syncer ready: true, restart count 3
Sep  6 01:17:28.404: INFO: capi-webhook-69769f4c68-p8dfp started at 2021-09-05 19:50:53 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:28.404: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:28.404: INFO: vmware-system-tkg-controller-manager-575d95fb57-rcjkp started at 2021-09-05 19:51:32 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:28.404: INFO: 	Container manager ready: true, restart count 3
Sep  6 01:17:28.404: INFO: kube-scheduler-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:44:40 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container kube-scheduler ready: true, restart count 6
Sep  6 01:17:28.404: INFO: 	Container wcp-schedext ready: true, restart count 0
Sep  6 01:17:28.404: INFO: kubectl-plugin-vsphere-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:44:40 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container kubectl-plugin-vsphere ready: true, restart count 3
Sep  6 01:17:28.404: INFO: wcp-authproxy-4216b30cf7711a81a7935dfb22c7b9d2 started at 2021-09-05 19:58:35 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container wcp-authproxy ready: true, restart count 0
Sep  6 01:17:28.404: INFO: nsx-ncp-6d7f7bf559-df5sk started at 2021-09-05 19:47:14 -0700 PDT (1+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Init container nsx-ncp-upgrade ready: true, restart count 0
Sep  6 01:17:28.404: INFO: 	Container nsx-ncp ready: true, restart count 6
Sep  6 01:17:28.404: INFO: capi-kubeadm-bootstrap-webhook-6766c687f9-tx5q2 started at 2021-09-05 19:50:51 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:28.404: INFO: 	Container manager ready: true, restart count 1
Sep  6 01:17:28.404: INFO: kube-proxy-9zz2h started at 2021-09-05 19:47:13 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  6 01:17:28.404: INFO: capi-kubeadm-control-plane-controller-manager-7c8d4b456f-g7vmk started at 2021-09-05 19:50:52 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:28.404: INFO: 	Container manager ready: true, restart count 2
Sep  6 01:17:28.404: INFO: capi-kubeadm-control-plane-webhook-5f4c87b8b9-wjs6z started at 2021-09-05 19:50:52 -0700 PDT (0+2 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  6 01:17:28.404: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:28.404: INFO: vmware-system-license-operator-controller-manager-5f5b6cddq7xzp started at 2021-09-05 19:52:58 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:28.404: INFO: 	Container manager ready: true, restart count 0
Sep  6 01:17:29.457: INFO: 
Latency metrics for node 4216b30cf7711a81a7935dfb22c7b9d2
Sep  6 01:17:29.457: INFO: 
Logging node info for node sc2-10-185-226-150.eng.vmware.com
Sep  6 01:17:29.467: INFO: Node Info: &Node{ObjectMeta:{sc2-10-185-226-150.eng.vmware.com   /api/v1/nodes/sc2-10-185-226-150.eng.vmware.com d6938cb1-1bbd-4690-b273-7b7e638d12bc 224999 0 2021-09-05 19:58:48 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:CRX kubernetes.io/arch:amd64 kubernetes.io/hostname:sc2-10-185-226-150.eng.vmware.com kubernetes.io/os:CRX node-role.kubernetes.io/agent:agent node.kubernetes.io/role:agent type:virtual-kubelet] map[ncp/transport_node:644f660e-c6b2-4fd5-a4b5-5cccdda12759 node.alpha.kubernetes.io/ttl:0 vmware-system-esxi-node-moid:host-16 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{wcpsvc Update v1 2021-09-05 19:58:49 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-esxi-node-moid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{"f:node-role.kubernetes.io/agent":{}}}}} {spherelet Update v1 2021-09-05 20:02:50 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:ncp/transport_node":{}},"f:labels":{".":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node.kubernetes.io/role":{},"f:type":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:nodeInfo":{"f:architecture":{},"f:kubeletVersion":{},"f:operatingSystem":{}}}}} {kube-controller-manager Update v1 2021-09-05 20:28:56 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}},"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:status":{"f:volumesAttached":{}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{12 0} {<nil>} 12 DecimalSI},memory: {{34354941952 0} {<nil>} 33549748Ki BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Allocatable:ResourceList{cpu: {{9 0} {<nil>} 9 DecimalSI},memory: {{4109369344 0} {<nil>} 3919Mi BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-06 01:17:26 -0700 PDT,LastTransitionTime:2021-09-06 01:17:26 -0700 PDT,Reason:KubeletReady,Message:Spherelet is ready.,},},Addresses:[]NodeAddress{NodeAddress{Type:Hostname,Address:sc2-10-185-226-150.eng.vmware.com,},NodeAddress{Type:InternalIP,Address:10.185.226.150,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:,SystemUUID:,BootID:,KernelVersion:,OSImage:,ContainerRuntimeVersion:,KubeletVersion:v1.20.2-sph-0afe203,KubeProxyVersion:,OperatingSystem:ESXi,Architecture:amd64,},Images:[]ContainerImage{},VolumesInUse:[],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/csi/csi.vsphere.vmware.com^47d4cc26-bb29-41cb-a660-79ed55715f11,DevicePath:,},},Config:nil,},}
Sep  6 01:17:29.468: INFO: 
Logging kubelet events for node sc2-10-185-226-150.eng.vmware.com
Sep  6 01:17:29.474: INFO: 
Logging pods the kubelet thinks is on node sc2-10-185-226-150.eng.vmware.com
Sep  6 01:17:29.510: INFO: schedext-test-node-selector-1 started at 2021-09-05 20:21:19 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:29.510: INFO: 	Container hello ready: true, restart count 0
Sep  6 01:17:29.510: INFO: podwithpersistentvolume started at 2021-09-05 20:29:04 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:29.510: INFO: 	Container hello ready: true, restart count 0
Sep  6 01:17:29.510: INFO: wcp-sanity-busybox-6f999d6849-6bn7v started at 2021-09-05 20:30:32 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:29.510: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 01:17:29.510: INFO: curl-pod started at 2021-09-05 20:38:23 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:29.510: INFO: 	Container curl-container ready: true, restart count 0
Sep  6 01:17:29.510: INFO: schedext-test-affinity-1 started at 2021-09-05 20:22:12 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:29.510: INFO: 	Container hello ready: true, restart count 0
Sep  6 01:17:29.510: INFO: schedext-test-affinity-2 started at 2021-09-05 20:22:42 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:29.510: INFO: 	Container hello ready: true, restart count 0
Sep  6 01:17:29.510: INFO: helloworld started at 2021-09-05 20:32:42 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:29.510: INFO: 	Container hello ready: true, restart count 0
Sep  6 01:17:29.511: INFO: hello-web-6b97664bd5-zznrr started at 2021-09-05 22:45:00 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:29.511: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 01:17:29.511: INFO: wcp-sanity-busybox-6f999d6849-kspp7 started at 2021-09-05 20:17:25 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:29.511: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 01:17:29.511: INFO: test-docker-registry started at 2021-09-05 20:41:08 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:29.511: INFO: 	Container test-docker-registry ready: true, restart count 0
Sep  6 01:17:29.576: INFO: 
Latency metrics for node sc2-10-185-226-150.eng.vmware.com
Sep  6 01:17:29.576: INFO: 
Logging node info for node sc2-10-185-226-233.eng.vmware.com
Sep  6 01:17:29.590: INFO: Node Info: &Node{ObjectMeta:{sc2-10-185-226-233.eng.vmware.com   /api/v1/nodes/sc2-10-185-226-233.eng.vmware.com 99eaa46a-24ac-48ad-852e-eabb653ed037 224971 0 2021-09-05 19:58:37 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:CRX kubernetes.io/arch:amd64 kubernetes.io/hostname:sc2-10-185-226-233.eng.vmware.com kubernetes.io/os:CRX node-role.kubernetes.io/agent:agent node.kubernetes.io/role:agent type:virtual-kubelet] map[ncp/transport_node:97c82e2a-be25-406d-ab4e-f897b55c19b3 node.alpha.kubernetes.io/ttl:0 vmware-system-esxi-node-moid:host-19 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{wcpsvc Update v1 2021-09-05 19:58:37 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-esxi-node-moid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{"f:node-role.kubernetes.io/agent":{}}}}} {spherelet Update v1 2021-09-05 20:02:48 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:ncp/transport_node":{}},"f:labels":{".":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node.kubernetes.io/role":{},"f:type":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:nodeInfo":{"f:architecture":{},"f:kubeletVersion":{},"f:operatingSystem":{}}}}} {kube-controller-manager Update v1 2021-09-05 20:27:52 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}},"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{12 0} {<nil>} 12 DecimalSI},memory: {{34354941952 0} {<nil>} 33549748Ki BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Allocatable:ResourceList{cpu: {{10 0} {<nil>} 10 DecimalSI},memory: {{6062866432 0} {<nil>} 5782Mi BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-06 01:17:24 -0700 PDT,LastTransitionTime:2021-09-06 01:17:24 -0700 PDT,Reason:KubeletReady,Message:Spherelet is ready.,},},Addresses:[]NodeAddress{NodeAddress{Type:Hostname,Address:sc2-10-185-226-233.eng.vmware.com,},NodeAddress{Type:InternalIP,Address:10.185.226.233,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:,SystemUUID:,BootID:,KernelVersion:,OSImage:,ContainerRuntimeVersion:,KubeletVersion:v1.20.2-sph-0afe203,KubeProxyVersion:,OperatingSystem:ESXi,Architecture:amd64,},Images:[]ContainerImage{},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Sep  6 01:17:29.590: INFO: 
Logging kubelet events for node sc2-10-185-226-233.eng.vmware.com
Sep  6 01:17:29.597: INFO: 
Logging pods the kubelet thinks is on node sc2-10-185-226-233.eng.vmware.com
Sep  6 01:17:29.611: INFO: wcp-sanity-busybox-6f999d6849-qkvhv started at 2021-09-06 01:01:22 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:29.611: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 01:17:29.611: INFO: wcp-sanity-busybox-6f999d6849-dnl4f started at 2021-09-06 01:00:47 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:29.611: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 01:17:29.666: INFO: 
Latency metrics for node sc2-10-185-226-233.eng.vmware.com
Sep  6 01:17:29.666: INFO: 
Logging node info for node sc2-10-185-233-127.eng.vmware.com
Sep  6 01:17:29.678: INFO: Node Info: &Node{ObjectMeta:{sc2-10-185-233-127.eng.vmware.com   /api/v1/nodes/sc2-10-185-233-127.eng.vmware.com 9215b2dc-bc25-43aa-b2d8-00a2b7b458d7 224996 0 2021-09-05 19:58:26 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:CRX kubernetes.io/arch:amd64 kubernetes.io/hostname:sc2-10-185-233-127.eng.vmware.com kubernetes.io/os:CRX node-role.kubernetes.io/agent:agent node.kubernetes.io/role:agent type:virtual-kubelet] map[ncp/transport_node:9e6990b2-2258-45d1-81b5-10ca792b1fac node.alpha.kubernetes.io/ttl:0 vmware-system-esxi-node-moid:host-12 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kube-controller-manager Update v1 2021-09-05 19:58:26 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}},"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}}}} {wcpsvc Update v1 2021-09-05 19:58:26 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-esxi-node-moid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{"f:node-role.kubernetes.io/agent":{}}}}} {spherelet Update v1 2021-09-05 20:02:46 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:ncp/transport_node":{}},"f:labels":{".":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node.kubernetes.io/role":{},"f:type":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:nodeInfo":{"f:architecture":{},"f:kubeletVersion":{},"f:operatingSystem":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{12 0} {<nil>} 12 DecimalSI},memory: {{34354941952 0} {<nil>} 33549748Ki BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Allocatable:ResourceList{cpu: {{10 0} {<nil>} 10 DecimalSI},memory: {{5057282048 0} {<nil>} 4823Mi BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-06 01:17:26 -0700 PDT,LastTransitionTime:2021-09-06 01:17:26 -0700 PDT,Reason:KubeletReady,Message:Spherelet is ready.,},},Addresses:[]NodeAddress{NodeAddress{Type:Hostname,Address:sc2-10-185-233-127.eng.vmware.com,},NodeAddress{Type:InternalIP,Address:10.185.233.127,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:,SystemUUID:,BootID:,KernelVersion:,OSImage:,ContainerRuntimeVersion:,KubeletVersion:v1.20.2-sph-0afe203,KubeProxyVersion:,OperatingSystem:ESXi,Architecture:amd64,},Images:[]ContainerImage{},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Sep  6 01:17:29.678: INFO: 
Logging kubelet events for node sc2-10-185-233-127.eng.vmware.com
Sep  6 01:17:29.690: INFO: 
Logging pods the kubelet thinks is on node sc2-10-185-233-127.eng.vmware.com
Sep  6 01:17:29.724: INFO: wcp-sanity-busybox-6f999d6849-564rw started at 2021-09-05 20:30:14 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:29.724: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 01:17:29.724: INFO: curl-pod started at 2021-09-05 20:33:45 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:29.724: INFO: 	Container curl-container ready: true, restart count 0
Sep  6 01:17:29.724: INFO: nginx-private started at 2021-09-05 20:19:39 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:29.724: INFO: 	Container nginx-private-container ready: true, restart count 0
Sep  6 01:17:29.724: INFO: schedext-test-affinity-3 started at 2021-09-05 20:22:44 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:29.724: INFO: 	Container hello ready: true, restart count 0
Sep  6 01:17:29.724: INFO: hello-web-2-f779cbdff-dmb55 started at 2021-09-05 20:37:45 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:29.724: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 01:17:29.724: INFO: hello-web-1-6b97664bd5-w4mv7 started at 2021-09-05 22:44:58 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:29.725: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 01:17:29.725: INFO: helloworld started at 2021-09-05 20:18:37 -0700 PDT (0+1 container statuses recorded)
Sep  6 01:17:29.725: INFO: 	Container hello ready: true, restart count 0
Sep  6 01:17:29.775: INFO: 
Latency metrics for node sc2-10-185-233-127.eng.vmware.com
Sep  6 01:17:29.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8473" for this suite.

• Failure [42.319 seconds]
[sig-auth] ServiceAccounts
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance] [It]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629

  Sep  6 01:17:26.688: Unexpected error:
      <*errors.errorString | 0xc002388e50>: {
          s: "expected pod \"test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16\" success: pod \"test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16\" failed with status: {Phase:Failed Conditions:[{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-06 01:16:48 -0700 PDT Reason: Message:} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-06 01:17:14 -0700 PDT Reason:UnknownContainerStatuses Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-06 01:17:14 -0700 PDT Reason:UnknownContainerStatuses Message:} {Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-06 01:17:14 -0700 PDT Reason: Message:}] Message:rpc error: code = Internal desc = Could not run pod: service account token []string(nil)/3600/v1.BoundObjectReference{Kind:\"Pod\", APIVersion:\"v1\", Name:\"test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16\", UID:\"4d01b7df-23c9-4fd2-b522-8b7c1c011146\"} not found Reason:ProviderFailed NominatedNodeName: HostIP: PodIP: PodIPs:[] StartTime:<nil> InitContainerStatuses:[] ContainerStatuses:[] QOSClass:BestEffort EphemeralContainerStatuses:[]}",
      }
      expected pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16" success: pod "test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16" failed with status: {Phase:Failed Conditions:[{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-06 01:16:48 -0700 PDT Reason: Message:} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-06 01:17:14 -0700 PDT Reason:UnknownContainerStatuses Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-06 01:17:14 -0700 PDT Reason:UnknownContainerStatuses Message:} {Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-06 01:17:14 -0700 PDT Reason: Message:}] Message:rpc error: code = Internal desc = Could not run pod: service account token []string(nil)/3600/v1.BoundObjectReference{Kind:"Pod", APIVersion:"v1", Name:"test-pod-eeec467c-f8f6-4a87-8778-2cbac449ff16", UID:"4d01b7df-23c9-4fd2-b522-8b7c1c011146"} not found Reason:ProviderFailed NominatedNodeName: HostIP: PodIP: PodIPs:[] StartTime:<nil> InitContainerStatuses:[] ContainerStatuses:[] QOSClass:BestEffort EphemeralContainerStatuses:[]}
  occurred

  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:738
------------------------------
{"msg":"FAILED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","total":296,"completed":262,"skipped":4734,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:17:30.165: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9600
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep  6 01:17:30.824: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f" in namespace "downward-api-9600" to be "Succeeded or Failed"
Sep  6 01:17:30.838: INFO: Pod "downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f": Phase="Pending", Reason="", readiness=false. Elapsed: 13.651466ms
Sep  6 01:17:32.852: INFO: Pod "downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028023285s
Sep  6 01:17:34.865: INFO: Pod "downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041213345s
Sep  6 01:17:36.876: INFO: Pod "downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052242446s
Sep  6 01:17:38.889: INFO: Pod "downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.065027968s
Sep  6 01:17:40.898: INFO: Pod "downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.074132204s
Sep  6 01:17:42.933: INFO: Pod "downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.109071803s
Sep  6 01:17:44.961: INFO: Pod "downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.1375631s
Sep  6 01:17:46.987: INFO: Pod "downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.162753345s
Sep  6 01:17:49.004: INFO: Pod "downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.180370783s
Sep  6 01:17:51.021: INFO: Pod "downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.197306456s
Sep  6 01:17:53.036: INFO: Pod "downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.21173795s
Sep  6 01:17:55.049: INFO: Pod "downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.225493705s
Sep  6 01:17:57.060: INFO: Pod "downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f": Phase="Running", Reason="", readiness=true. Elapsed: 26.236356721s
Sep  6 01:17:59.090: INFO: Pod "downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f": Phase="Running", Reason="", readiness=true. Elapsed: 28.26561759s
Sep  6 01:18:01.118: INFO: Pod "downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f": Phase="Running", Reason="", readiness=true. Elapsed: 30.293806815s
Sep  6 01:18:03.129: INFO: Pod "downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.304802687s
STEP: Saw pod success
Sep  6 01:18:03.129: INFO: Pod "downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f" satisfied condition "Succeeded or Failed"
Sep  6 01:18:03.138: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f container client-container: <nil>
STEP: delete the pod
Sep  6 01:18:09.163: INFO: Waiting for pod downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f to disappear
Sep  6 01:18:09.184: INFO: Pod downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f still exists
Sep  6 01:18:11.186: INFO: Waiting for pod downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f to disappear
Sep  6 01:18:11.199: INFO: Pod downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f still exists
Sep  6 01:18:13.185: INFO: Waiting for pod downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f to disappear
Sep  6 01:18:13.199: INFO: Pod downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f still exists
Sep  6 01:18:15.186: INFO: Waiting for pod downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f to disappear
Sep  6 01:18:15.198: INFO: Pod downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f still exists
Sep  6 01:18:17.185: INFO: Waiting for pod downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f to disappear
Sep  6 01:18:17.193: INFO: Pod downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f still exists
Sep  6 01:18:19.185: INFO: Waiting for pod downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f to disappear
Sep  6 01:18:19.198: INFO: Pod downwardapi-volume-2ef76e13-38fe-4db7-a5df-e65c7af3792f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:18:19.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9600" for this suite.

• [SLOW TEST:49.475 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's cpu limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":296,"completed":263,"skipped":4756,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:18:19.641: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7450
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 01:18:21.451: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  6 01:18:23.490: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:18:25.499: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:18:27.498: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:18:29.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:18:31.503: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:18:33.503: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:18:35.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:18:37.499: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:18:39.504: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:18:41.508: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:18:43.503: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:18:45.501: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:18:47.498: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:18:49.504: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 18, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 01:18:52.568: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:18:52.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7450" for this suite.
STEP: Destroying namespace "webhook-7450-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:33.768 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":296,"completed":264,"skipped":4791,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:18:53.409: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3891
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:41
[It] should provide container's memory request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep  6 01:18:53.971: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4" in namespace "downward-api-3891" to be "Succeeded or Failed"
Sep  6 01:18:53.993: INFO: Pod "downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.580066ms
Sep  6 01:18:56.002: INFO: Pod "downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031171707s
Sep  6 01:18:58.024: INFO: Pod "downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053500492s
Sep  6 01:19:00.039: INFO: Pod "downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.068275577s
Sep  6 01:19:02.059: INFO: Pod "downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.087907675s
Sep  6 01:19:04.075: INFO: Pod "downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.104356156s
Sep  6 01:19:06.090: INFO: Pod "downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.118837738s
Sep  6 01:19:08.099: INFO: Pod "downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.128138922s
Sep  6 01:19:10.112: INFO: Pod "downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.141674258s
Sep  6 01:19:12.149: INFO: Pod "downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.177775593s
Sep  6 01:19:14.161: INFO: Pod "downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.189837308s
Sep  6 01:19:16.170: INFO: Pod "downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.199624282s
Sep  6 01:19:18.182: INFO: Pod "downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4": Phase="Pending", Reason="", readiness=false. Elapsed: 24.211383143s
Sep  6 01:19:20.196: INFO: Pod "downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4": Phase="Pending", Reason="", readiness=false. Elapsed: 26.22567398s
Sep  6 01:19:22.208: INFO: Pod "downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.237496399s
STEP: Saw pod success
Sep  6 01:19:22.208: INFO: Pod "downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4" satisfied condition "Succeeded or Failed"
Sep  6 01:19:22.214: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4 container client-container: <nil>
STEP: delete the pod
Sep  6 01:19:28.713: INFO: Waiting for pod downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4 to disappear
Sep  6 01:19:28.725: INFO: Pod downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4 still exists
Sep  6 01:19:30.726: INFO: Waiting for pod downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4 to disappear
Sep  6 01:19:30.737: INFO: Pod downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4 still exists
Sep  6 01:19:32.726: INFO: Waiting for pod downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4 to disappear
Sep  6 01:19:32.741: INFO: Pod downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4 still exists
Sep  6 01:19:34.726: INFO: Waiting for pod downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4 to disappear
Sep  6 01:19:34.739: INFO: Pod downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4 still exists
Sep  6 01:19:36.725: INFO: Waiting for pod downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4 to disappear
Sep  6 01:19:36.737: INFO: Pod downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4 still exists
Sep  6 01:19:38.727: INFO: Waiting for pod downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4 to disappear
Sep  6 01:19:38.739: INFO: Pod downwardapi-volume-66d6daf5-0903-4bb7-bec2-0031c86560c4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:19:38.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3891" for this suite.

• [SLOW TEST:45.615 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:36
  should provide container's memory request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":296,"completed":265,"skipped":4798,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:19:39.025: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3984
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:41
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward API volume plugin
Sep  6 01:19:39.556: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088" in namespace "projected-3984" to be "Succeeded or Failed"
Sep  6 01:19:39.571: INFO: Pod "downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088": Phase="Pending", Reason="", readiness=false. Elapsed: 14.631607ms
Sep  6 01:19:41.581: INFO: Pod "downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024633224s
Sep  6 01:19:43.604: INFO: Pod "downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048067233s
Sep  6 01:19:45.615: INFO: Pod "downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058570205s
Sep  6 01:19:47.622: INFO: Pod "downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088": Phase="Pending", Reason="", readiness=false. Elapsed: 8.06564668s
Sep  6 01:19:49.632: INFO: Pod "downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088": Phase="Pending", Reason="", readiness=false. Elapsed: 10.076381336s
Sep  6 01:19:51.643: INFO: Pod "downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088": Phase="Pending", Reason="", readiness=false. Elapsed: 12.086703296s
Sep  6 01:19:53.655: INFO: Pod "downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088": Phase="Pending", Reason="", readiness=false. Elapsed: 14.099521389s
Sep  6 01:19:55.674: INFO: Pod "downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088": Phase="Pending", Reason="", readiness=false. Elapsed: 16.118188835s
Sep  6 01:19:57.685: INFO: Pod "downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088": Phase="Pending", Reason="", readiness=false. Elapsed: 18.128929454s
Sep  6 01:19:59.710: INFO: Pod "downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088": Phase="Pending", Reason="", readiness=false. Elapsed: 20.154032774s
Sep  6 01:20:01.731: INFO: Pod "downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088": Phase="Pending", Reason="", readiness=false. Elapsed: 22.174829168s
Sep  6 01:20:03.744: INFO: Pod "downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088": Phase="Pending", Reason="", readiness=false. Elapsed: 24.187848087s
Sep  6 01:20:05.765: INFO: Pod "downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088": Phase="Pending", Reason="", readiness=false. Elapsed: 26.208656493s
Sep  6 01:20:07.776: INFO: Pod "downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088": Phase="Pending", Reason="", readiness=false. Elapsed: 28.219597818s
Sep  6 01:20:09.786: INFO: Pod "downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088": Phase="Pending", Reason="", readiness=false. Elapsed: 30.229871275s
Sep  6 01:20:11.813: INFO: Pod "downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.256873029s
STEP: Saw pod success
Sep  6 01:20:11.813: INFO: Pod "downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088" satisfied condition "Succeeded or Failed"
Sep  6 01:20:11.822: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088 container client-container: <nil>
STEP: delete the pod
Sep  6 01:20:22.540: INFO: Waiting for pod downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088 to disappear
Sep  6 01:20:22.562: INFO: Pod downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088 still exists
Sep  6 01:20:24.562: INFO: Waiting for pod downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088 to disappear
Sep  6 01:20:24.574: INFO: Pod downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088 still exists
Sep  6 01:20:26.562: INFO: Waiting for pod downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088 to disappear
Sep  6 01:20:26.575: INFO: Pod downwardapi-volume-c225b4c3-3f6e-48c0-b0a0-56a226f32088 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:20:26.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3984" for this suite.

• [SLOW TEST:47.798 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":296,"completed":266,"skipped":4835,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:20:26.824: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8508
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:92
Sep  6 01:20:27.244: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 01:20:27.264: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 01:20:27.287: INFO: 
Logging pods the apiserver thinks is on node sc2-10-185-226-150.eng.vmware.com before test
Sep  6 01:20:27.330: INFO: podwithpersistentvolume from storage-policy-test started at 2021-09-05 20:29:04 -0700 PDT (1 container statuses recorded)
Sep  6 01:20:27.330: INFO: 	Container hello ready: true, restart count 0
Sep  6 01:20:27.330: INFO: hello-web-6b97664bd5-zznrr from test-cluster-ip-service started at 2021-09-05 22:45:00 -0700 PDT (1 container statuses recorded)
Sep  6 01:20:27.330: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 01:20:27.330: INFO: wcp-sanity-busybox-6f999d6849-kspp7 from test-dataprovider-podvms-ns started at 2021-09-05 20:17:25 -0700 PDT (1 container statuses recorded)
Sep  6 01:20:27.330: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 01:20:27.330: INFO: curl-pod from test-network-policy started at 2021-09-05 20:38:23 -0700 PDT (1 container statuses recorded)
Sep  6 01:20:27.330: INFO: 	Container curl-container ready: true, restart count 0
Sep  6 01:20:27.330: INFO: schedext-test-node-selector-1 from test-node-selector started at 2021-09-05 20:21:19 -0700 PDT (1 container statuses recorded)
Sep  6 01:20:27.330: INFO: 	Container hello ready: true, restart count 0
Sep  6 01:20:27.330: INFO: schedext-test-affinity-1 from test-pod-affinity started at 2021-09-05 20:22:12 -0700 PDT (1 container statuses recorded)
Sep  6 01:20:27.330: INFO: 	Container hello ready: true, restart count 0
Sep  6 01:20:27.330: INFO: schedext-test-affinity-2 from test-pod-affinity started at 2021-09-05 20:22:42 -0700 PDT (1 container statuses recorded)
Sep  6 01:20:27.330: INFO: 	Container hello ready: true, restart count 0
Sep  6 01:20:27.330: INFO: test-docker-registry from test-private-image-registry-ns started at 2021-09-05 20:41:08 -0700 PDT (1 container statuses recorded)
Sep  6 01:20:27.330: INFO: 	Container test-docker-registry ready: true, restart count 0
Sep  6 01:20:27.330: INFO: helloworld from test-telemetry started at 2021-09-05 20:32:42 -0700 PDT (1 container statuses recorded)
Sep  6 01:20:27.330: INFO: 	Container hello ready: true, restart count 0
Sep  6 01:20:27.330: INFO: wcp-sanity-busybox-6f999d6849-6bn7v from test-update-workload-ns started at 2021-09-05 20:30:32 -0700 PDT (1 container statuses recorded)
Sep  6 01:20:27.330: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 01:20:27.330: INFO: 
Logging pods the apiserver thinks is on node sc2-10-185-226-233.eng.vmware.com before test
Sep  6 01:20:27.370: INFO: wcp-sanity-busybox-6f999d6849-qkvhv from test-dataprovider-podvms-ns started at 2021-09-06 01:01:22 -0700 PDT (1 container statuses recorded)
Sep  6 01:20:27.370: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 01:20:27.370: INFO: wcp-sanity-busybox-6f999d6849-dnl4f from test-update-workload-ns started at 2021-09-06 01:00:47 -0700 PDT (1 container statuses recorded)
Sep  6 01:20:27.370: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 01:20:27.370: INFO: 
Logging pods the apiserver thinks is on node sc2-10-185-233-127.eng.vmware.com before test
Sep  6 01:20:27.414: INFO: curl-pod from test-cluster-ip-service started at 2021-09-05 20:33:45 -0700 PDT (1 container statuses recorded)
Sep  6 01:20:27.414: INFO: 	Container curl-container ready: true, restart count 0
Sep  6 01:20:27.414: INFO: helloworld from test-exec-ns started at 2021-09-05 20:18:37 -0700 PDT (1 container statuses recorded)
Sep  6 01:20:27.414: INFO: 	Container hello ready: true, restart count 0
Sep  6 01:20:27.414: INFO: nginx-private from test-image-pull-secrets-ns started at 2021-09-05 20:19:39 -0700 PDT (1 container statuses recorded)
Sep  6 01:20:27.414: INFO: 	Container nginx-private-container ready: true, restart count 0
Sep  6 01:20:27.414: INFO: hello-web-1-6b97664bd5-w4mv7 from test-network-policy started at 2021-09-05 22:44:58 -0700 PDT (1 container statuses recorded)
Sep  6 01:20:27.414: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 01:20:27.414: INFO: hello-web-2-f779cbdff-dmb55 from test-network-policy started at 2021-09-05 20:37:45 -0700 PDT (1 container statuses recorded)
Sep  6 01:20:27.414: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 01:20:27.414: INFO: schedext-test-affinity-3 from test-pod-affinity started at 2021-09-05 20:22:44 -0700 PDT (1 container statuses recorded)
Sep  6 01:20:27.414: INFO: 	Container hello ready: true, restart count 0
Sep  6 01:20:27.414: INFO: wcp-sanity-busybox-6f999d6849-564rw from test-update-workload-ns started at 2021-09-05 20:30:14 -0700 PDT (1 container statuses recorded)
Sep  6 01:20:27.414: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b5886127-1fe8-4ba2-ac8c-b95947ede3c2 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-b5886127-1fe8-4ba2-ac8c-b95947ede3c2 off the node sc2-10-185-226-233.eng.vmware.com
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b5886127-1fe8-4ba2-ac8c-b95947ede3c2
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:21:23.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8508" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:83

• [SLOW TEST:57.675 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":296,"completed":267,"skipped":4846,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:21:24.499: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9592
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[AfterEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:22:05.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9592" for this suite.

• [SLOW TEST:41.050 seconds]
[k8s.io] Kubelet
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when scheduling a busybox command that always fails in a pod
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:79
    should have an terminated reason [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":296,"completed":268,"skipped":4866,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:22:05.549: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8551
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name configmap-test-upd-99362967-1bd3-493a-b80d-d5a80909455c
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-99362967-1bd3-493a-b80d-d5a80909455c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:23:35.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8551" for this suite.

• [SLOW TEST:90.146 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":296,"completed":269,"skipped":4866,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:23:35.696: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1081
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:85
[It] deployment should support rollover [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  6 01:23:36.348: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep  6 01:23:41.386: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  6 01:24:09.412: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep  6 01:24:11.427: INFO: Creating deployment "test-rollover-deployment"
Sep  6 01:24:11.460: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep  6 01:24:13.510: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep  6 01:24:13.533: INFO: Ensure that both replica sets have 1 created replica
Sep  6 01:24:13.546: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep  6 01:24:13.581: INFO: Updating deployment test-rollover-deployment
Sep  6 01:24:13.581: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep  6 01:24:15.652: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep  6 01:24:15.680: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep  6 01:24:15.706: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:15.706: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:17.753: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:17.753: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:19.741: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:19.741: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:21.732: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:21.732: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:23.736: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:23.736: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:25.727: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:25.727: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:27.731: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:27.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:29.766: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:29.766: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:31.727: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:31.727: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:33.726: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:33.726: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:35.734: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:35.734: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:37.730: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:37.730: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:39.723: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:39.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:41.729: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:41.729: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:43.726: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:43.726: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:45.723: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:45.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:47.726: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:47.726: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:49.742: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:49.742: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 47, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:51.727: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:51.727: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 47, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:53.740: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:53.741: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 47, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:55.727: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:55.727: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 47, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:57.726: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:57.726: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 47, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:24:59.726: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:24:59.726: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 47, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:25:01.740: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:25:01.741: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 47, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:25:03.733: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:25:03.734: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 47, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:25:05.734: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:25:05.734: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 47, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:25:07.725: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 01:25:07.726: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 24, 47, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 24, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-668db69979\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:25:09.748: INFO: 
Sep  6 01:25:09.748: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:79
Sep  6 01:25:09.766: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-1081 /apis/apps/v1/namespaces/deployment-1081/deployments/test-rollover-deployment 89c73649-c864-481f-a933-9447b2223144 230205 2 2021-09-06 01:24:11 -0700 PDT <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-09-06 01:24:13 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-06 01:25:07 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005c18558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-09-06 01:24:11 -0700 PDT,LastTransitionTime:2021-09-06 01:24:11 -0700 PDT,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-668db69979" has successfully progressed.,LastUpdateTime:2021-09-06 01:25:07 -0700 PDT,LastTransitionTime:2021-09-06 01:24:11 -0700 PDT,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  6 01:25:09.774: INFO: New ReplicaSet "test-rollover-deployment-668db69979" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-668db69979  deployment-1081 /apis/apps/v1/namespaces/deployment-1081/replicasets/test-rollover-deployment-668db69979 1d6bf38a-9f55-436b-b4ec-f71417eea4fb 230196 2 2021-09-06 01:24:13 -0700 PDT <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 89c73649-c864-481f-a933-9447b2223144 0xc005c18c17 0xc005c18c18}] []  [{kube-controller-manager Update apps/v1 2021-09-06 01:25:07 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89c73649-c864-481f-a933-9447b2223144\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 668db69979,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.21 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005c18e48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  6 01:25:09.774: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep  6 01:25:09.774: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1081 /apis/apps/v1/namespaces/deployment-1081/replicasets/test-rollover-controller 78a07b1e-327f-4102-899b-3a55949d7fb1 230204 2 2021-09-06 01:23:36 -0700 PDT <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 89c73649-c864-481f-a933-9447b2223144 0xc005c18a07 0xc005c18a08}] []  [{e2e.test Update apps/v1 2021-09-06 01:23:36 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-06 01:25:07 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89c73649-c864-481f-a933-9447b2223144\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005c18aa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 01:25:09.774: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-1081 /apis/apps/v1/namespaces/deployment-1081/replicasets/test-rollover-deployment-78bc8b888c 5370fcbc-9bf7-49ec-961b-55a61707f496 229653 2 2021-09-06 01:24:11 -0700 PDT <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 89c73649-c864-481f-a933-9447b2223144 0xc005c19007 0xc005c19008}] []  [{kube-controller-manager Update apps/v1 2021-09-06 01:24:13 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"89c73649-c864-481f-a933-9447b2223144\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005c19098 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 01:25:09.781: INFO: Pod "test-rollover-controller-8kcqz" is available:
&Pod{ObjectMeta:{test-rollover-controller-8kcqz test-rollover-controller- deployment-1081 /api/v1/namespaces/deployment-1081/pods/test-rollover-controller-8kcqz cb46bb50-d5d2-4e83-aea1-efec1d92cde5 230201 0 2021-09-06 01:23:36 -0700 PDT 2021-09-06 01:25:07 -0700 PDT 0xc005c19528 map[name:rollover-pod pod:httpd] map[attachment_id:d4e5d42f-b220-4d9a-a375-804afa9cb0a2 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:00:07 vlan:None vmware-system-ephemeral-disk-uuid:6000C29a-1480-e7eb-dcfe-7337c7c3a435 vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v4800"} vmware-system-vm-moid:vm-1556:3f0e15ce-7291-45c7-8a72-39d27d06e017 vmware-system-vm-uuid:50169c7a-148a-24bd-b0b9-b4172634147c] [{apps/v1 ReplicaSet test-rollover-controller 78a07b1e-327f-4102-899b-3a55949d7fb1 0xc005c19577 0xc005c19578}] [lifecycle-controller/system.vmware.com]  [{image-controller Update v1 2021-09-06 01:23:36 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {kube-controller-manager Update v1 2021-09-06 01:23:36 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78a07b1e-327f-4102-899b-3a55949d7fb1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-df5sk Update v1 2021-09-06 01:23:47 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-06 01:23:57 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 01:24:07 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sln5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sln5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sln5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-233.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 01:23:36 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 01:24:08 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 01:24:08 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 01:24:08 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.185.226.233,PodIP:172.26.1.210,StartTime:2021-09-06 01:24:05 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 01:24:06 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v4800,ContainerID:5a0087cd-5925-4938-a0a7-139cbde88a9a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.210,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 01:25:09.781: INFO: Pod "test-rollover-deployment-668db69979-v74f4" is available:
&Pod{ObjectMeta:{test-rollover-deployment-668db69979-v74f4 test-rollover-deployment-668db69979- deployment-1081 /api/v1/namespaces/deployment-1081/pods/test-rollover-deployment-668db69979-v74f4 b66c48f4-62b4-408b-8b39-e98880469983 229995 0 2021-09-06 01:24:13 -0700 PDT <nil> <nil> map[name:rollover-pod pod-template-hash:668db69979] map[attachment_id:9f9caff0-774a-4af0-9d19-5c17633b551d kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:00:16 vlan:None vmware-system-ephemeral-disk-uuid:6000C29d-ea82-5d75-f202-ecbe95d90d7e vmware-system-image-references:{"agnhost":"agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v29668"} vmware-system-vm-moid:vm-1559:3f0e15ce-7291-45c7-8a72-39d27d06e017 vmware-system-vm-uuid:50161928-1fd0-adb1-1b8a-1ac708077c6e] [{apps/v1 ReplicaSet test-rollover-deployment-668db69979 1d6bf38a-9f55-436b-b4ec-f71417eea4fb 0xc005c1976f 0xc005c197a0}] [lifecycle-controller/system.vmware.com]  [{image-controller Update v1 2021-09-06 01:24:13 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {kube-controller-manager Update v1 2021-09-06 01:24:13 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1d6bf38a-9f55-436b-b4ec-f71417eea4fb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-df5sk Update v1 2021-09-06 01:24:15 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-06 01:24:40 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 01:24:47 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.212\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-sln5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-sln5p,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-sln5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-10-185-226-233.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 01:24:13 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 01:24:49 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 01:24:49 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 01:24:49 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.185.226.233,PodIP:172.26.1.212,StartTime:2021-09-06 01:24:46 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 01:24:48 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.21,ImageID:agnhost-abbf6d2fdc4b425438a9532ce0fce98fae69b53d-v29668,ContainerID:c9d8e4cc-0cbb-4ed4-9242-797cda9108e3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.212,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:25:09.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1081" for this suite.

• [SLOW TEST:94.358 seconds]
[sig-apps] Deployment
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":296,"completed":270,"skipped":4869,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:25:10.054: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2979
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name cm-test-opt-del-aee09648-d90b-4fe7-87ed-da5d25534eaf
STEP: Creating configMap with name cm-test-opt-upd-505029ca-517b-4e06-a107-1d8008487a7e
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-aee09648-d90b-4fe7-87ed-da5d25534eaf
STEP: Updating configmap cm-test-opt-upd-505029ca-517b-4e06-a107-1d8008487a7e
STEP: Creating configMap with name cm-test-opt-create-15b8f4e2-48b4-4c46-a5d0-77c847c3f738
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:26:35.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2979" for this suite.

• [SLOW TEST:85.686 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":296,"completed":271,"skipped":4889,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:26:35.740: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1595
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Sep  6 01:26:36.266: INFO: Waiting up to 5m0s for pod "downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a" in namespace "downward-api-1595" to be "Succeeded or Failed"
Sep  6 01:26:36.288: INFO: Pod "downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a": Phase="Pending", Reason="", readiness=false. Elapsed: 21.384247ms
Sep  6 01:26:38.339: INFO: Pod "downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072835617s
Sep  6 01:26:40.354: INFO: Pod "downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.087797955s
Sep  6 01:26:42.380: INFO: Pod "downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.113540116s
Sep  6 01:26:44.444: INFO: Pod "downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.17786372s
Sep  6 01:26:46.474: INFO: Pod "downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.20797914s
Sep  6 01:26:48.488: INFO: Pod "downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.222017666s
Sep  6 01:26:50.514: INFO: Pod "downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.247352571s
Sep  6 01:26:52.530: INFO: Pod "downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.264112918s
Sep  6 01:26:54.545: INFO: Pod "downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.278755599s
Sep  6 01:26:56.573: INFO: Pod "downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.3062452s
Sep  6 01:26:58.593: INFO: Pod "downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.32626789s
Sep  6 01:27:00.612: INFO: Pod "downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a": Phase="Pending", Reason="", readiness=false. Elapsed: 24.345166487s
Sep  6 01:27:02.623: INFO: Pod "downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a": Phase="Pending", Reason="", readiness=false. Elapsed: 26.356387635s
Sep  6 01:27:04.644: INFO: Pod "downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.378045439s
STEP: Saw pod success
Sep  6 01:27:04.644: INFO: Pod "downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a" satisfied condition "Succeeded or Failed"
Sep  6 01:27:04.656: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a container dapi-container: <nil>
STEP: delete the pod
Sep  6 01:27:04.719: INFO: Waiting for pod downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a to disappear
Sep  6 01:27:04.736: INFO: Pod downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a still exists
Sep  6 01:27:06.737: INFO: Waiting for pod downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a to disappear
Sep  6 01:27:06.751: INFO: Pod downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a still exists
Sep  6 01:27:08.738: INFO: Waiting for pod downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a to disappear
Sep  6 01:27:08.754: INFO: Pod downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a still exists
Sep  6 01:27:10.738: INFO: Waiting for pod downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a to disappear
Sep  6 01:27:10.756: INFO: Pod downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a still exists
Sep  6 01:27:12.738: INFO: Waiting for pod downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a to disappear
Sep  6 01:27:12.749: INFO: Pod downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a still exists
Sep  6 01:27:14.737: INFO: Waiting for pod downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a to disappear
Sep  6 01:27:14.758: INFO: Pod downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a still exists
Sep  6 01:27:16.737: INFO: Waiting for pod downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a to disappear
Sep  6 01:27:16.749: INFO: Pod downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a still exists
Sep  6 01:27:18.738: INFO: Waiting for pod downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a to disappear
Sep  6 01:27:18.749: INFO: Pod downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a still exists
Sep  6 01:27:20.738: INFO: Waiting for pod downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a to disappear
Sep  6 01:27:20.752: INFO: Pod downward-api-a0719a0a-ada2-4c24-b3df-fb9b0612f94a no longer exists
[AfterEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:27:20.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1595" for this suite.

• [SLOW TEST:45.377 seconds]
[sig-node] Downward API
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":296,"completed":272,"skipped":4895,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:27:21.117: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-242
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a pod to test downward api env vars
Sep  6 01:27:21.603: INFO: Waiting up to 5m0s for pod "downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa" in namespace "downward-api-242" to be "Succeeded or Failed"
Sep  6 01:27:21.615: INFO: Pod "downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa": Phase="Pending", Reason="", readiness=false. Elapsed: 11.984346ms
Sep  6 01:27:23.646: INFO: Pod "downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043093314s
Sep  6 01:27:25.658: INFO: Pod "downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055557184s
Sep  6 01:27:27.668: INFO: Pod "downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065413116s
Sep  6 01:27:29.678: INFO: Pod "downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.075372952s
Sep  6 01:27:31.695: INFO: Pod "downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa": Phase="Pending", Reason="", readiness=false. Elapsed: 10.092607373s
Sep  6 01:27:33.715: INFO: Pod "downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa": Phase="Pending", Reason="", readiness=false. Elapsed: 12.112121613s
Sep  6 01:27:35.731: INFO: Pod "downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa": Phase="Pending", Reason="", readiness=false. Elapsed: 14.128601163s
Sep  6 01:27:37.741: INFO: Pod "downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.138776017s
Sep  6 01:27:39.750: INFO: Pod "downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa": Phase="Pending", Reason="", readiness=false. Elapsed: 18.147449233s
Sep  6 01:27:41.771: INFO: Pod "downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa": Phase="Pending", Reason="", readiness=false. Elapsed: 20.167941917s
Sep  6 01:27:43.790: INFO: Pod "downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa": Phase="Pending", Reason="", readiness=false. Elapsed: 22.187139606s
Sep  6 01:27:45.809: INFO: Pod "downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa": Phase="Pending", Reason="", readiness=false. Elapsed: 24.206613756s
Sep  6 01:27:47.818: INFO: Pod "downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa": Phase="Pending", Reason="", readiness=false. Elapsed: 26.215079394s
Sep  6 01:27:49.830: INFO: Pod "downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.22765704s
STEP: Saw pod success
Sep  6 01:27:49.830: INFO: Pod "downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa" satisfied condition "Succeeded or Failed"
Sep  6 01:27:49.836: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa container dapi-container: <nil>
STEP: delete the pod
Sep  6 01:27:55.406: INFO: Waiting for pod downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa to disappear
Sep  6 01:27:55.440: INFO: Pod downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa still exists
Sep  6 01:27:57.440: INFO: Waiting for pod downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa to disappear
Sep  6 01:27:57.458: INFO: Pod downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa still exists
Sep  6 01:27:59.441: INFO: Waiting for pod downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa to disappear
Sep  6 01:27:59.449: INFO: Pod downward-api-f87fd73a-cf00-4ba0-9793-c6e548338baa no longer exists
[AfterEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:27:59.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-242" for this suite.

• [SLOW TEST:38.617 seconds]
[sig-node] Downward API
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":296,"completed":273,"skipped":4907,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:27:59.735: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4108
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating the pod
Sep  6 01:28:00.229: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:30:59.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4108" for this suite.

• [SLOW TEST:180.352 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":296,"completed":274,"skipped":4982,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:31:00.087: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8659
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod busybox-e6377d03-b6ef-4b07-bdf5-9cf6afedad39 in namespace container-probe-8659
Sep  6 01:31:30.774: INFO: Started pod busybox-e6377d03-b6ef-4b07-bdf5-9cf6afedad39 in namespace container-probe-8659
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 01:31:30.793: INFO: Initial restart count of pod busybox-e6377d03-b6ef-4b07-bdf5-9cf6afedad39 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:35:31.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8659" for this suite.

• [SLOW TEST:271.614 seconds]
[k8s.io] Probing container
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":296,"completed":275,"skipped":4988,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicaSet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:35:31.701: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7959
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep  6 01:36:01.618: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:36:02.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7959" for this suite.

• [SLOW TEST:31.400 seconds]
[sig-apps] ReplicaSet
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":296,"completed":276,"skipped":5014,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:36:03.102: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7925
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:36:33.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7925" for this suite.

• [SLOW TEST:30.932 seconds]
[sig-api-machinery] ResourceQuota
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":296,"completed":277,"skipped":5058,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:36:34.034: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2051
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:53
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod liveness-10549428-cd79-4cba-8fd6-81aef9ee4e7a in namespace container-probe-2051
Sep  6 01:37:06.587: INFO: Started pod liveness-10549428-cd79-4cba-8fd6-81aef9ee4e7a in namespace container-probe-2051
STEP: checking the pod's current state and verifying that restartCount is present
Sep  6 01:37:06.595: INFO: Initial restart count of pod liveness-10549428-cd79-4cba-8fd6-81aef9ee4e7a is 0
Sep  6 01:37:22.723: INFO: Restart count of pod container-probe-2051/liveness-10549428-cd79-4cba-8fd6-81aef9ee4e7a is now 1 (16.127416207s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:37:22.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2051" for this suite.

• [SLOW TEST:48.970 seconds]
[k8s.io] Probing container
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":296,"completed":278,"skipped":5075,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-auth] ServiceAccounts
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:37:23.004: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8979
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:37:23.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8979" for this suite.
•{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":296,"completed":279,"skipped":5085,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:37:23.888: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5314
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 01:37:25.077: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  6 01:37:27.104: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:37:29.143: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:37:31.120: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:37:33.118: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:37:35.114: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:37:37.120: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:37:39.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:37:41.124: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:37:43.115: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:37:45.124: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:37:47.120: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:37:49.119: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:37:51.114: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:37:53.132: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:37:55.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 37, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 01:37:58.185: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:37:58.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5314" for this suite.
STEP: Destroying namespace "webhook-5314-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:35.171 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":296,"completed":280,"skipped":5153,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:37:59.059: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8504
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: set up a multi version CRD
Sep  6 01:37:59.600: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:38:40.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8504" for this suite.

• [SLOW TEST:41.660 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":296,"completed":281,"skipped":5156,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:38:40.720: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-50
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating secret with name secret-test-51e7f166-aa50-4b71-9426-5f3069758e85
STEP: Creating a pod to test consume secrets
Sep  6 01:38:41.214: INFO: Waiting up to 5m0s for pod "pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2" in namespace "secrets-50" to be "Succeeded or Failed"
Sep  6 01:38:41.231: INFO: Pod "pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.048126ms
Sep  6 01:38:43.242: INFO: Pod "pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028602499s
Sep  6 01:38:45.258: INFO: Pod "pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044046479s
Sep  6 01:38:47.267: INFO: Pod "pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053258823s
Sep  6 01:38:49.278: INFO: Pod "pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.064344386s
Sep  6 01:38:51.295: INFO: Pod "pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.081467804s
Sep  6 01:38:53.331: INFO: Pod "pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.117118461s
Sep  6 01:38:55.347: INFO: Pod "pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.133496945s
Sep  6 01:38:57.359: INFO: Pod "pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.145342726s
Sep  6 01:38:59.374: INFO: Pod "pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.160358266s
Sep  6 01:39:01.385: INFO: Pod "pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.171457948s
Sep  6 01:39:03.398: INFO: Pod "pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.184629146s
Sep  6 01:39:05.415: INFO: Pod "pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.201055421s
Sep  6 01:39:07.425: INFO: Pod "pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2": Phase="Pending", Reason="", readiness=false. Elapsed: 26.211638799s
Sep  6 01:39:09.443: INFO: Pod "pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2": Phase="Pending", Reason="", readiness=false. Elapsed: 28.229164375s
Sep  6 01:39:11.458: INFO: Pod "pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.243979541s
STEP: Saw pod success
Sep  6 01:39:11.458: INFO: Pod "pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2" satisfied condition "Succeeded or Failed"
Sep  6 01:39:11.467: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 01:39:11.610: INFO: Waiting for pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 to disappear
Sep  6 01:39:11.616: INFO: Pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 still exists
Sep  6 01:39:13.617: INFO: Waiting for pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 to disappear
Sep  6 01:39:13.630: INFO: Pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 still exists
Sep  6 01:39:15.618: INFO: Waiting for pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 to disappear
Sep  6 01:39:15.630: INFO: Pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 still exists
Sep  6 01:39:17.616: INFO: Waiting for pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 to disappear
Sep  6 01:39:17.628: INFO: Pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 still exists
Sep  6 01:39:19.617: INFO: Waiting for pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 to disappear
Sep  6 01:39:19.631: INFO: Pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 still exists
Sep  6 01:39:21.616: INFO: Waiting for pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 to disappear
Sep  6 01:39:21.645: INFO: Pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 still exists
Sep  6 01:39:23.616: INFO: Waiting for pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 to disappear
Sep  6 01:39:23.630: INFO: Pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 still exists
Sep  6 01:39:25.617: INFO: Waiting for pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 to disappear
Sep  6 01:39:25.633: INFO: Pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 still exists
Sep  6 01:39:27.617: INFO: Waiting for pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 to disappear
Sep  6 01:39:27.633: INFO: Pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 still exists
Sep  6 01:39:29.618: INFO: Waiting for pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 to disappear
Sep  6 01:39:29.633: INFO: Pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 still exists
Sep  6 01:39:31.616: INFO: Waiting for pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 to disappear
Sep  6 01:39:31.635: INFO: Pod pod-secrets-c0e45780-07de-4296-b16e-788942bba3a2 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:39:31.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-50" for this suite.

• [SLOW TEST:51.162 seconds]
[sig-storage] Secrets
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":296,"completed":282,"skipped":5162,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:39:31.882: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2998
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep  6 01:39:32.322: INFO: Pod name pod-release: Found 0 pods out of 1
Sep  6 01:39:37.338: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:39:38.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2998" for this suite.

• [SLOW TEST:6.765 seconds]
[sig-apps] ReplicationController
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":296,"completed":283,"skipped":5175,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:39:38.647: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9359
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:87
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 01:39:40.509: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  6 01:39:42.529: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:39:44.548: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:39:46.545: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:39:48.554: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:39:50.547: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:39:52.539: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:39:54.546: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:39:56.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:39:58.539: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:40:00.546: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:40:02.539: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:40:04.545: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:40:06.570: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:40:08.547: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:40:10.566: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:40:12.539: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:40:14.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:40:16.543: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:40:18.563: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 01:40:20.542: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 1, 39, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-6bd9446d55\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 01:40:23.606: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Sep  6 01:40:23.720: INFO: >>> kubeConfig: ./kconfig.yaml
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:40:23.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9359" for this suite.
STEP: Destroying namespace "webhook-9359-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:102

• [SLOW TEST:45.826 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":296,"completed":284,"skipped":5179,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:40:24.473: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3557
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap with name projected-configmap-test-volume-map-545f99fb-3778-40cd-bfbd-035dac63d577
STEP: Creating a pod to test consume configMaps
Sep  6 01:40:25.003: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42" in namespace "projected-3557" to be "Succeeded or Failed"
Sep  6 01:40:25.032: INFO: Pod "pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42": Phase="Pending", Reason="", readiness=false. Elapsed: 28.053357ms
Sep  6 01:40:27.042: INFO: Pod "pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038598965s
Sep  6 01:40:29.085: INFO: Pod "pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42": Phase="Pending", Reason="", readiness=false. Elapsed: 4.081057447s
Sep  6 01:40:31.101: INFO: Pod "pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42": Phase="Pending", Reason="", readiness=false. Elapsed: 6.097334559s
Sep  6 01:40:33.169: INFO: Pod "pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42": Phase="Pending", Reason="", readiness=false. Elapsed: 8.165230869s
Sep  6 01:40:35.195: INFO: Pod "pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42": Phase="Pending", Reason="", readiness=false. Elapsed: 10.191799035s
Sep  6 01:40:37.217: INFO: Pod "pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42": Phase="Pending", Reason="", readiness=false. Elapsed: 12.213766733s
Sep  6 01:40:39.226: INFO: Pod "pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42": Phase="Pending", Reason="", readiness=false. Elapsed: 14.222430267s
Sep  6 01:40:41.244: INFO: Pod "pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42": Phase="Pending", Reason="", readiness=false. Elapsed: 16.24008076s
Sep  6 01:40:43.270: INFO: Pod "pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42": Phase="Pending", Reason="", readiness=false. Elapsed: 18.266370902s
Sep  6 01:40:45.281: INFO: Pod "pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42": Phase="Pending", Reason="", readiness=false. Elapsed: 20.277877901s
Sep  6 01:40:47.309: INFO: Pod "pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42": Phase="Pending", Reason="", readiness=false. Elapsed: 22.305725649s
Sep  6 01:40:49.320: INFO: Pod "pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42": Phase="Pending", Reason="", readiness=false. Elapsed: 24.316294735s
Sep  6 01:40:51.335: INFO: Pod "pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42": Phase="Pending", Reason="", readiness=false. Elapsed: 26.331721588s
Sep  6 01:40:53.348: INFO: Pod "pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42": Phase="Pending", Reason="", readiness=false. Elapsed: 28.343987433s
Sep  6 01:40:55.354: INFO: Pod "pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42": Phase="Pending", Reason="", readiness=false. Elapsed: 30.350828401s
Sep  6 01:40:57.373: INFO: Pod "pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 32.369084313s
STEP: Saw pod success
Sep  6 01:40:57.373: INFO: Pod "pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42" satisfied condition "Succeeded or Failed"
Sep  6 01:40:57.378: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42 container agnhost-container: <nil>
STEP: delete the pod
Sep  6 01:40:57.454: INFO: Waiting for pod pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42 to disappear
Sep  6 01:40:57.468: INFO: Pod pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42 still exists
Sep  6 01:40:59.469: INFO: Waiting for pod pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42 to disappear
Sep  6 01:40:59.481: INFO: Pod pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42 still exists
Sep  6 01:41:01.469: INFO: Waiting for pod pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42 to disappear
Sep  6 01:41:01.482: INFO: Pod pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42 still exists
Sep  6 01:41:03.469: INFO: Waiting for pod pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42 to disappear
Sep  6 01:41:03.494: INFO: Pod pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42 still exists
Sep  6 01:41:05.470: INFO: Waiting for pod pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42 to disappear
Sep  6 01:41:05.485: INFO: Pod pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42 still exists
Sep  6 01:41:07.469: INFO: Waiting for pod pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42 to disappear
Sep  6 01:41:07.483: INFO: Pod pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42 still exists
Sep  6 01:41:09.469: INFO: Waiting for pod pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42 to disappear
Sep  6 01:41:09.487: INFO: Pod pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42 still exists
Sep  6 01:41:11.469: INFO: Waiting for pod pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42 to disappear
Sep  6 01:41:11.485: INFO: Pod pod-projected-configmaps-ee892189-053c-482a-85a5-b206cdcf0b42 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:41:11.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3557" for this suite.

• [SLOW TEST:47.633 seconds]
[sig-storage] Projected configMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":296,"completed":285,"skipped":5182,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:41:12.107: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-2500
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:130
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
Sep  6 01:41:13.007: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep  6 01:41:13.066: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:13.066: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:13.066: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:13.082: INFO: Number of nodes with available pods: 0
Sep  6 01:41:13.082: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:14.106: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:14.106: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:14.106: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:14.114: INFO: Number of nodes with available pods: 0
Sep  6 01:41:14.114: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:15.110: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:15.111: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:15.111: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:15.126: INFO: Number of nodes with available pods: 0
Sep  6 01:41:15.126: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:16.108: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:16.108: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:16.108: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:16.119: INFO: Number of nodes with available pods: 0
Sep  6 01:41:16.119: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:17.095: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:17.095: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:17.095: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:17.101: INFO: Number of nodes with available pods: 0
Sep  6 01:41:17.102: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:18.112: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:18.112: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:18.112: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:18.135: INFO: Number of nodes with available pods: 0
Sep  6 01:41:18.135: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:19.109: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:19.109: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:19.109: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:19.132: INFO: Number of nodes with available pods: 0
Sep  6 01:41:19.132: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:20.104: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:20.104: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:20.104: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:20.117: INFO: Number of nodes with available pods: 0
Sep  6 01:41:20.117: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:21.095: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:21.095: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:21.095: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:21.106: INFO: Number of nodes with available pods: 0
Sep  6 01:41:21.106: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:22.100: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:22.100: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:22.100: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:22.111: INFO: Number of nodes with available pods: 0
Sep  6 01:41:22.111: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:23.095: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:23.095: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:23.095: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:23.104: INFO: Number of nodes with available pods: 0
Sep  6 01:41:23.104: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:24.205: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:24.205: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:24.205: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:24.223: INFO: Number of nodes with available pods: 0
Sep  6 01:41:24.223: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:25.108: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:25.109: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:25.109: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:25.126: INFO: Number of nodes with available pods: 0
Sep  6 01:41:25.126: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:26.101: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:26.101: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:26.101: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:26.115: INFO: Number of nodes with available pods: 0
Sep  6 01:41:26.115: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:27.125: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:27.125: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:27.125: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:27.145: INFO: Number of nodes with available pods: 0
Sep  6 01:41:27.145: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:28.097: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:28.097: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:28.097: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:28.103: INFO: Number of nodes with available pods: 0
Sep  6 01:41:28.103: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:29.132: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:29.132: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:29.132: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:29.155: INFO: Number of nodes with available pods: 0
Sep  6 01:41:29.155: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:30.100: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:30.100: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:30.100: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:30.114: INFO: Number of nodes with available pods: 0
Sep  6 01:41:30.114: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:31.096: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:31.096: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:31.096: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:31.102: INFO: Number of nodes with available pods: 0
Sep  6 01:41:31.102: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:32.092: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:32.092: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:32.093: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:32.101: INFO: Number of nodes with available pods: 0
Sep  6 01:41:32.101: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:33.098: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:33.098: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:33.098: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:33.108: INFO: Number of nodes with available pods: 0
Sep  6 01:41:33.108: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:34.112: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:34.112: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:34.112: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:34.131: INFO: Number of nodes with available pods: 0
Sep  6 01:41:34.131: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:35.096: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:35.096: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:35.096: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:35.103: INFO: Number of nodes with available pods: 0
Sep  6 01:41:35.103: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:36.093: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:36.093: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:36.093: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:36.100: INFO: Number of nodes with available pods: 0
Sep  6 01:41:36.100: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:37.095: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:37.095: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:37.095: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:37.110: INFO: Number of nodes with available pods: 0
Sep  6 01:41:37.110: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:38.112: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:38.112: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:38.112: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:38.122: INFO: Number of nodes with available pods: 0
Sep  6 01:41:38.122: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:39.135: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:39.136: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:39.136: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:39.150: INFO: Number of nodes with available pods: 0
Sep  6 01:41:39.150: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:40.106: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:40.106: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:40.106: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:40.129: INFO: Number of nodes with available pods: 0
Sep  6 01:41:40.129: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:41.101: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:41.101: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:41.101: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:41.112: INFO: Number of nodes with available pods: 0
Sep  6 01:41:41.112: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:42.102: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:42.102: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:42.102: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:42.123: INFO: Number of nodes with available pods: 0
Sep  6 01:41:42.123: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:43.129: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:43.129: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:43.129: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:43.141: INFO: Number of nodes with available pods: 0
Sep  6 01:41:43.142: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:44.101: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:44.101: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:44.101: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:44.112: INFO: Number of nodes with available pods: 0
Sep  6 01:41:44.112: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:45.105: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:45.105: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:45.105: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:45.115: INFO: Number of nodes with available pods: 0
Sep  6 01:41:45.115: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:46.098: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:46.098: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:46.098: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:46.107: INFO: Number of nodes with available pods: 0
Sep  6 01:41:46.107: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:47.095: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:47.096: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:47.096: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:47.103: INFO: Number of nodes with available pods: 0
Sep  6 01:41:47.103: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:48.100: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:48.100: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:48.100: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:48.114: INFO: Number of nodes with available pods: 0
Sep  6 01:41:48.114: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:49.096: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:49.096: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:49.096: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:49.104: INFO: Number of nodes with available pods: 0
Sep  6 01:41:49.104: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:50.096: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:50.096: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:50.096: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:50.104: INFO: Number of nodes with available pods: 1
Sep  6 01:41:50.104: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:51.101: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:51.101: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:51.101: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:51.109: INFO: Number of nodes with available pods: 1
Sep  6 01:41:51.109: INFO: Node sc2-10-185-226-150.eng.vmware.com is running more than one daemon pod
Sep  6 01:41:52.156: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:52.156: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:52.156: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:52.197: INFO: Number of nodes with available pods: 3
Sep  6 01:41:52.197: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep  6 01:41:52.293: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:52.293: INFO: Wrong image for pod: daemon-set-blbd8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:52.293: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:52.316: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:52.316: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:52.317: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:53.326: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:53.326: INFO: Wrong image for pod: daemon-set-blbd8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:53.326: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:53.333: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:53.333: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:53.333: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:54.332: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:54.332: INFO: Wrong image for pod: daemon-set-blbd8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:54.332: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:54.374: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:54.374: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:54.374: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:55.327: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:55.327: INFO: Wrong image for pod: daemon-set-blbd8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:55.327: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:55.334: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:55.334: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:55.334: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:56.328: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:56.328: INFO: Wrong image for pod: daemon-set-blbd8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:56.328: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:56.336: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:56.336: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:56.336: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:57.328: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:57.328: INFO: Wrong image for pod: daemon-set-blbd8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:57.328: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:57.339: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:57.339: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:57.339: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:58.327: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:58.327: INFO: Wrong image for pod: daemon-set-blbd8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:58.327: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:58.337: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:58.337: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:58.337: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:59.328: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:59.328: INFO: Wrong image for pod: daemon-set-blbd8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:59.328: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:41:59.336: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:59.336: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:41:59.336: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:00.328: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:00.328: INFO: Wrong image for pod: daemon-set-blbd8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:00.328: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:00.338: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:00.338: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:00.338: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:01.329: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:01.329: INFO: Wrong image for pod: daemon-set-blbd8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:01.329: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:01.342: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:01.342: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:01.342: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:02.329: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:02.329: INFO: Wrong image for pod: daemon-set-blbd8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:02.329: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:02.339: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:02.339: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:02.339: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:03.348: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:03.348: INFO: Wrong image for pod: daemon-set-blbd8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:03.348: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:03.357: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:03.357: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:03.357: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:04.328: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:04.328: INFO: Wrong image for pod: daemon-set-blbd8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:04.328: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:04.338: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:04.339: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:04.339: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:05.327: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:05.327: INFO: Wrong image for pod: daemon-set-blbd8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:05.327: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:05.337: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:05.337: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:05.337: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:06.327: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:06.327: INFO: Wrong image for pod: daemon-set-blbd8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:06.327: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:06.337: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:06.337: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:06.337: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:07.331: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:07.331: INFO: Wrong image for pod: daemon-set-blbd8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:07.331: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:07.341: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:07.341: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:07.341: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:08.331: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:08.331: INFO: Wrong image for pod: daemon-set-blbd8. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:08.331: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:08.340: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:08.340: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:08.340: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:09.328: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:09.328: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:09.328: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:09.341: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:09.341: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:09.341: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:10.331: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:10.331: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:10.331: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:10.340: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:10.340: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:10.340: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:11.331: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:11.331: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:11.331: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:11.348: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:11.348: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:11.348: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:12.338: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:12.338: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:12.338: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:12.362: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:12.363: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:12.363: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:13.337: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:13.337: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:13.337: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:13.348: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:13.348: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:13.348: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:14.335: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:14.335: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:14.335: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:14.390: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:14.390: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:14.390: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:15.330: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:15.330: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:15.330: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:15.341: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:15.341: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:15.341: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:16.337: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:16.337: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:16.337: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:16.352: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:16.352: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:16.352: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:17.332: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:17.332: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:17.332: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:17.340: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:17.340: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:17.340: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:18.334: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:18.334: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:18.334: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:18.347: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:18.347: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:18.347: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:19.327: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:19.327: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:19.327: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:19.340: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:19.340: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:19.340: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:20.352: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:20.352: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:20.352: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:20.364: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:20.364: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:20.364: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:21.345: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:21.345: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:21.345: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:21.358: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:21.358: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:21.358: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:22.329: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:22.330: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:22.330: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:22.341: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:22.341: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:22.341: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:23.334: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:23.335: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:23.335: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:23.348: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:23.348: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:23.348: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:24.522: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:24.523: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:24.523: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:25.205: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:25.205: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:25.205: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:25.382: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:25.382: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:25.382: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:25.525: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:25.525: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:25.525: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:26.343: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:26.343: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:26.343: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:26.360: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:26.360: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:26.360: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:27.334: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:27.334: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:27.334: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:27.344: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:27.344: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:27.344: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:28.334: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:28.334: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:28.334: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:28.349: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:28.349: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:28.349: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:29.330: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:29.330: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:29.330: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:29.345: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:29.345: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:29.345: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:30.330: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:30.330: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:30.330: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:30.353: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:30.353: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:30.353: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:31.328: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:31.328: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:31.328: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:31.340: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:31.340: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:31.340: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:32.326: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:32.326: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:32.326: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:32.334: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:32.334: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:32.334: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:33.333: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:33.333: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:33.333: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:33.361: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:33.361: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:33.361: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:34.330: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:34.330: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:34.330: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:34.341: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:34.341: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:34.341: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:35.327: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:35.327: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:35.327: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:35.337: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:35.337: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:35.337: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:36.332: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:36.332: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:36.332: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:36.346: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:36.346: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:36.346: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:37.328: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:37.328: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:37.328: INFO: Pod daemon-set-x2s7n is not available
Sep  6 01:42:37.337: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:37.337: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:37.337: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:38.344: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:38.344: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:38.363: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:38.363: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:38.363: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:39.330: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:39.330: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:39.345: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:39.345: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:39.345: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:40.335: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:40.335: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:40.344: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:40.344: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:40.344: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:41.327: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:41.327: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:41.335: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:41.335: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:41.335: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:42.335: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:42.335: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:42.348: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:42.348: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:42.348: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:43.337: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:43.337: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:43.358: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:43.358: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:43.358: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:44.390: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:44.390: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:44.406: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:44.406: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:44.406: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:45.328: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:45.328: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:45.343: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:45.343: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:45.343: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:46.331: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:46.331: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:46.342: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:46.342: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:46.342: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:47.329: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:47.329: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:47.353: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:47.353: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:47.353: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:48.329: INFO: Wrong image for pod: daemon-set-6s6t2. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:48.329: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:48.340: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:48.340: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:48.340: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:49.345: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:42:49.345: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:49.361: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:49.361: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:49.361: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:50.329: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:42:50.329: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:50.338: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:50.338: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:50.338: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:51.328: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:42:51.328: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:51.339: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:51.340: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:51.340: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:52.354: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:42:52.354: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:52.362: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:52.362: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:52.362: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:53.329: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:42:53.329: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:53.348: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:53.349: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:53.349: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:54.336: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:42:54.336: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:54.353: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:54.353: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:54.353: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:55.328: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:42:55.328: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:55.336: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:55.336: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:55.336: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:56.329: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:42:56.329: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:56.343: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:56.343: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:56.344: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:57.337: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:42:57.337: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:57.348: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:57.348: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:57.348: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:58.332: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:42:58.332: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:58.363: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:58.363: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:58.363: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:59.328: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:42:59.328: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:42:59.337: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:59.337: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:42:59.337: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:00.337: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:43:00.337: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:00.344: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:00.345: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:00.345: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:01.329: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:43:01.329: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:01.339: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:01.339: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:01.339: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:02.340: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:43:02.340: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:02.375: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:02.375: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:02.375: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:03.328: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:43:03.328: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:03.340: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:03.340: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:03.340: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:04.332: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:43:04.332: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:04.341: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:04.341: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:04.341: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:05.329: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:43:05.329: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:05.340: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:05.340: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:05.340: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:06.333: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:43:06.333: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:06.361: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:06.362: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:06.362: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:07.328: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:43:07.328: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:07.341: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:07.341: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:07.341: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:08.342: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:43:08.342: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:08.366: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:08.366: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:08.367: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:09.324: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:43:09.324: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:09.333: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:09.333: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:09.333: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:10.327: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:43:10.327: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:10.335: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:10.335: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:10.335: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:11.331: INFO: Pod daemon-set-hxtvt is not available
Sep  6 01:43:11.331: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:11.338: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:11.338: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:11.338: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:12.326: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:12.336: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:12.336: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:12.336: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:13.329: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:13.338: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:13.338: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:13.338: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:14.336: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:14.374: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:14.374: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:14.374: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:15.331: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:15.342: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:15.342: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:15.342: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:16.329: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:16.337: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:16.337: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:16.337: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:17.332: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:17.345: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:17.346: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:17.346: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:18.329: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:18.345: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:18.345: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:18.345: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:19.328: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:19.340: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:19.340: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:19.340: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:20.328: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:20.336: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:20.337: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:20.337: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:21.329: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:21.337: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:21.337: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:21.337: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:22.329: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:22.336: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:22.336: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:22.336: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:23.333: INFO: Wrong image for pod: daemon-set-mf8bc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.21, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  6 01:43:23.346: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:23.346: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:23.346: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:24.334: INFO: Pod daemon-set-w2lbj is not available
Sep  6 01:43:24.355: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:24.355: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:24.355: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Sep  6 01:43:24.395: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:24.395: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:24.395: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:24.414: INFO: Number of nodes with available pods: 2
Sep  6 01:43:24.414: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:25.430: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:25.430: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:25.430: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:25.439: INFO: Number of nodes with available pods: 2
Sep  6 01:43:25.439: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:26.426: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:26.426: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:26.426: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:26.432: INFO: Number of nodes with available pods: 2
Sep  6 01:43:26.432: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:27.430: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:27.430: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:27.430: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:27.439: INFO: Number of nodes with available pods: 2
Sep  6 01:43:27.439: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:28.511: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:28.511: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:28.511: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:28.520: INFO: Number of nodes with available pods: 2
Sep  6 01:43:28.520: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:29.425: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:29.425: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:29.425: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:29.432: INFO: Number of nodes with available pods: 2
Sep  6 01:43:29.432: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:30.429: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:30.429: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:30.429: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:30.440: INFO: Number of nodes with available pods: 2
Sep  6 01:43:30.440: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:31.425: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:31.425: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:31.425: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:31.431: INFO: Number of nodes with available pods: 2
Sep  6 01:43:31.431: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:32.464: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:32.465: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:32.465: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:32.474: INFO: Number of nodes with available pods: 2
Sep  6 01:43:32.474: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:33.434: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:33.434: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:33.434: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:33.448: INFO: Number of nodes with available pods: 2
Sep  6 01:43:33.448: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:34.428: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:34.428: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:34.428: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:34.434: INFO: Number of nodes with available pods: 2
Sep  6 01:43:34.434: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:35.425: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:35.425: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:35.425: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:35.434: INFO: Number of nodes with available pods: 2
Sep  6 01:43:35.434: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:36.426: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:36.426: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:36.426: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:36.434: INFO: Number of nodes with available pods: 2
Sep  6 01:43:36.434: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:37.425: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:37.425: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:37.425: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:37.436: INFO: Number of nodes with available pods: 2
Sep  6 01:43:37.436: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:38.429: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:38.429: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:38.429: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:38.441: INFO: Number of nodes with available pods: 2
Sep  6 01:43:38.441: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:39.600: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:39.600: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:39.600: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:39.614: INFO: Number of nodes with available pods: 2
Sep  6 01:43:39.614: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:40.426: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:40.426: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:40.426: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:40.434: INFO: Number of nodes with available pods: 2
Sep  6 01:43:40.434: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:41.424: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:41.424: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:41.424: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:41.432: INFO: Number of nodes with available pods: 2
Sep  6 01:43:41.432: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:42.431: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:42.432: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:42.432: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:42.441: INFO: Number of nodes with available pods: 2
Sep  6 01:43:42.441: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:43.430: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:43.430: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:43.430: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:43.441: INFO: Number of nodes with available pods: 2
Sep  6 01:43:43.441: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:44.430: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:44.430: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:44.430: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:44.442: INFO: Number of nodes with available pods: 2
Sep  6 01:43:44.442: INFO: Node sc2-10-185-233-127.eng.vmware.com is running more than one daemon pod
Sep  6 01:43:45.437: INFO: DaemonSet pods can't tolerate node 421629d5b5512e89fd5a479875bff24c with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:45.437: INFO: DaemonSet pods can't tolerate node 4216b0433daa6b68a0e3d69463d0611d with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:45.437: INFO: DaemonSet pods can't tolerate node 4216b30cf7711a81a7935dfb22c7b9d2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 01:43:45.453: INFO: Number of nodes with available pods: 3
Sep  6 01:43:45.453: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:96
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2500, will wait for the garbage collector to delete the pods
Sep  6 01:43:45.597: INFO: Deleting DaemonSet.extensions daemon-set took: 31.06831ms
Sep  6 01:43:47.898: INFO: Terminating DaemonSet.extensions daemon-set pods took: 2.300891728s
Sep  6 01:44:06.729: INFO: Number of nodes with available pods: 0
Sep  6 01:44:06.729: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 01:44:06.740: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2500/daemonsets","resourceVersion":"242694"},"items":null}

Sep  6 01:44:06.747: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2500/pods","resourceVersion":"242695"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:44:06.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2500" for this suite.

• [SLOW TEST:175.775 seconds]
[sig-apps] Daemon set [Serial]
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":296,"completed":286,"skipped":5209,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:44:07.882: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7953
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-7953
[It] should have a working scale subresource [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating statefulset ss in namespace statefulset-7953
Sep  6 01:44:08.970: INFO: Found 0 stateful pods, waiting for 1
Sep  6 01:44:18.988: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep  6 01:44:29.028: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep  6 01:44:38.981: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep  6 01:44:39.034: INFO: Deleting all statefulset in ns statefulset-7953
Sep  6 01:44:39.047: INFO: Scaling statefulset ss to 0
Sep  6 01:44:49.213: INFO: Waiting for statefulset status.replicas updated to 0
Sep  6 01:44:49.219: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:44:49.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7953" for this suite.

• [SLOW TEST:41.670 seconds]
[sig-apps] StatefulSet
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
    should have a working scale subresource [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":296,"completed":287,"skipped":5223,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:44:49.553: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4957
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:45:25.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4957" for this suite.

• [SLOW TEST:35.897 seconds]
[sig-apps] ReplicationController
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":296,"completed":288,"skipped":5236,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:45:25.450: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2532
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating pod pod-subpath-test-secret-ppxx
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 01:45:26.002: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-ppxx" in namespace "subpath-2532" to be "Succeeded or Failed"
Sep  6 01:45:26.013: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Pending", Reason="", readiness=false. Elapsed: 11.069609ms
Sep  6 01:45:28.025: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023636028s
Sep  6 01:45:30.039: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037020011s
Sep  6 01:45:32.059: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Pending", Reason="", readiness=false. Elapsed: 6.057213283s
Sep  6 01:45:34.070: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Pending", Reason="", readiness=false. Elapsed: 8.068590255s
Sep  6 01:45:36.080: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Pending", Reason="", readiness=false. Elapsed: 10.078099764s
Sep  6 01:45:38.108: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Pending", Reason="", readiness=false. Elapsed: 12.106297785s
Sep  6 01:45:40.117: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Pending", Reason="", readiness=false. Elapsed: 14.11554244s
Sep  6 01:45:42.131: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Pending", Reason="", readiness=false. Elapsed: 16.129006369s
Sep  6 01:45:44.141: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Pending", Reason="", readiness=false. Elapsed: 18.139104115s
Sep  6 01:45:46.152: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Pending", Reason="", readiness=false. Elapsed: 20.149964918s
Sep  6 01:45:48.166: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Pending", Reason="", readiness=false. Elapsed: 22.163983649s
Sep  6 01:45:50.183: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Pending", Reason="", readiness=false. Elapsed: 24.181193064s
Sep  6 01:45:52.196: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Pending", Reason="", readiness=false. Elapsed: 26.194027078s
Sep  6 01:45:54.207: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Pending", Reason="", readiness=false. Elapsed: 28.205791911s
Sep  6 01:45:56.221: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Pending", Reason="", readiness=false. Elapsed: 30.219020082s
Sep  6 01:45:58.234: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Running", Reason="", readiness=true. Elapsed: 32.232254461s
Sep  6 01:46:00.246: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Running", Reason="", readiness=true. Elapsed: 34.244633395s
Sep  6 01:46:02.263: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Running", Reason="", readiness=true. Elapsed: 36.260928043s
Sep  6 01:46:04.282: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Running", Reason="", readiness=true. Elapsed: 38.28035613s
Sep  6 01:46:06.293: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Running", Reason="", readiness=true. Elapsed: 40.291160944s
Sep  6 01:46:08.313: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Running", Reason="", readiness=true. Elapsed: 42.311647947s
Sep  6 01:46:10.332: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Running", Reason="", readiness=true. Elapsed: 44.329954288s
Sep  6 01:46:12.365: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Running", Reason="", readiness=true. Elapsed: 46.363628595s
Sep  6 01:46:14.378: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Running", Reason="", readiness=true. Elapsed: 48.376783068s
Sep  6 01:46:16.394: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Running", Reason="", readiness=true. Elapsed: 50.392591838s
Sep  6 01:46:18.408: INFO: Pod "pod-subpath-test-secret-ppxx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 52.406404256s
STEP: Saw pod success
Sep  6 01:46:18.408: INFO: Pod "pod-subpath-test-secret-ppxx" satisfied condition "Succeeded or Failed"
Sep  6 01:46:18.413: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-subpath-test-secret-ppxx container test-container-subpath-secret-ppxx: <nil>
STEP: delete the pod
Sep  6 01:46:18.492: INFO: Waiting for pod pod-subpath-test-secret-ppxx to disappear
Sep  6 01:46:18.513: INFO: Pod pod-subpath-test-secret-ppxx still exists
Sep  6 01:46:20.514: INFO: Waiting for pod pod-subpath-test-secret-ppxx to disappear
Sep  6 01:46:20.527: INFO: Pod pod-subpath-test-secret-ppxx still exists
Sep  6 01:46:22.514: INFO: Waiting for pod pod-subpath-test-secret-ppxx to disappear
Sep  6 01:46:22.529: INFO: Pod pod-subpath-test-secret-ppxx still exists
Sep  6 01:46:24.514: INFO: Waiting for pod pod-subpath-test-secret-ppxx to disappear
Sep  6 01:46:24.525: INFO: Pod pod-subpath-test-secret-ppxx still exists
Sep  6 01:46:26.513: INFO: Waiting for pod pod-subpath-test-secret-ppxx to disappear
Sep  6 01:46:26.525: INFO: Pod pod-subpath-test-secret-ppxx still exists
Sep  6 01:46:28.515: INFO: Waiting for pod pod-subpath-test-secret-ppxx to disappear
Sep  6 01:46:28.527: INFO: Pod pod-subpath-test-secret-ppxx no longer exists
STEP: Deleting pod pod-subpath-test-secret-ppxx
Sep  6 01:46:28.527: INFO: Deleting pod "pod-subpath-test-secret-ppxx" in namespace "subpath-2532"
[AfterEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:46:28.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2532" for this suite.

• [SLOW TEST:63.351 seconds]
[sig-storage] Subpath
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":296,"completed":289,"skipped":5238,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:46:28.801: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3583
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:52
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  6 01:47:19.456: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 01:47:19.469: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 01:47:21.470: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 01:47:21.482: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 01:47:23.469: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 01:47:23.481: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 01:47:25.470: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 01:47:25.483: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 01:47:27.470: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 01:47:27.485: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 01:47:29.469: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 01:47:29.489: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 01:47:31.469: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 01:47:31.485: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  6 01:47:33.469: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  6 01:47:33.478: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:47:33.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3583" for this suite.

• [SLOW TEST:65.128 seconds]
[k8s.io] Container Lifecycle Hook
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:624
  when create a pod with lifecycle hook
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:43
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":296,"completed":290,"skipped":5281,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:47:33.930: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7960
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve a basic endpoint from pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service endpoint-test2 in namespace services-7960
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7960 to expose endpoints map[]
Sep  6 01:47:34.578: INFO: successfully validated that service endpoint-test2 in namespace services-7960 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7960
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7960 to expose endpoints map[pod1:[80]]
Sep  6 01:47:38.676: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]], will retry
Sep  6 01:47:43.676: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]], will retry
Sep  6 01:47:48.668: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]], will retry
Sep  6 01:47:53.661: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]], will retry
Sep  6 01:47:58.661: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]], will retry
Sep  6 01:48:03.698: INFO: successfully validated that service endpoint-test2 in namespace services-7960 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-7960
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7960 to expose endpoints map[pod1:[80] pod2:[80]]
Sep  6 01:48:07.774: INFO: Unexpected endpoints: found map[f10aead6-b30d-4879-9523-2d63c6a59895:[80]], expected map[pod1:[80] pod2:[80]], will retry
Sep  6 01:48:12.782: INFO: Unexpected endpoints: found map[f10aead6-b30d-4879-9523-2d63c6a59895:[80]], expected map[pod1:[80] pod2:[80]], will retry
Sep  6 01:48:17.803: INFO: Unexpected endpoints: found map[f10aead6-b30d-4879-9523-2d63c6a59895:[80]], expected map[pod1:[80] pod2:[80]], will retry
Sep  6 01:48:22.774: INFO: Unexpected endpoints: found map[f10aead6-b30d-4879-9523-2d63c6a59895:[80]], expected map[pod1:[80] pod2:[80]], will retry
Sep  6 01:48:27.776: INFO: Unexpected endpoints: found map[f10aead6-b30d-4879-9523-2d63c6a59895:[80]], expected map[pod1:[80] pod2:[80]], will retry
Sep  6 01:48:28.810: INFO: successfully validated that service endpoint-test2 in namespace services-7960 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-7960
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7960 to expose endpoints map[pod2:[80]]
Sep  6 01:48:28.895: INFO: successfully validated that service endpoint-test2 in namespace services-7960 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-7960
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7960 to expose endpoints map[]
Sep  6 01:48:28.994: INFO: successfully validated that service endpoint-test2 in namespace services-7960 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:48:29.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7960" for this suite.
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:55.440 seconds]
[sig-network] Services
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":296,"completed":291,"skipped":5291,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:48:29.396: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5088
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:745
[It] should serve multiport endpoints from pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: creating service multi-endpoint-test in namespace services-5088
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5088 to expose endpoints map[]
Sep  6 01:48:30.092: INFO: successfully validated that service multi-endpoint-test in namespace services-5088 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5088
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5088 to expose endpoints map[pod1:[100]]
Sep  6 01:48:34.180: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]], will retry
Sep  6 01:48:39.176: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]], will retry
Sep  6 01:48:44.176: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]], will retry
Sep  6 01:48:49.190: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]], will retry
Sep  6 01:48:54.180: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]], will retry
Sep  6 01:48:59.167: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]], will retry
Sep  6 01:49:04.293: INFO: successfully validated that service multi-endpoint-test in namespace services-5088 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5088
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5088 to expose endpoints map[pod1:[100] pod2:[101]]
Sep  6 01:49:08.410: INFO: Unexpected endpoints: found map[12cc1b58-7bf4-4210-861a-5182bc6da54c:[100]], expected map[pod1:[100] pod2:[101]], will retry
Sep  6 01:49:13.426: INFO: Unexpected endpoints: found map[12cc1b58-7bf4-4210-861a-5182bc6da54c:[100]], expected map[pod1:[100] pod2:[101]], will retry
Sep  6 01:49:18.410: INFO: Unexpected endpoints: found map[12cc1b58-7bf4-4210-861a-5182bc6da54c:[100]], expected map[pod1:[100] pod2:[101]], will retry
Sep  6 01:49:23.420: INFO: Unexpected endpoints: found map[12cc1b58-7bf4-4210-861a-5182bc6da54c:[100]], expected map[pod1:[100] pod2:[101]], will retry
Sep  6 01:49:28.411: INFO: Unexpected endpoints: found map[12cc1b58-7bf4-4210-861a-5182bc6da54c:[100]], expected map[pod1:[100] pod2:[101]], will retry
Sep  6 01:49:29.529: INFO: successfully validated that service multi-endpoint-test in namespace services-5088 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-5088
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5088 to expose endpoints map[pod2:[101]]
Sep  6 01:49:29.724: INFO: successfully validated that service multi-endpoint-test in namespace services-5088 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5088
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5088 to expose endpoints map[]
Sep  6 01:49:29.813: INFO: successfully validated that service multi-endpoint-test in namespace services-5088 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:49:29.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5088" for this suite.
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:749

• [SLOW TEST:60.774 seconds]
[sig-network] Services
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":296,"completed":292,"skipped":5311,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
[BeforeEach] [sig-node] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 01:49:30.183: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5712
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
STEP: Creating configMap configmap-5712/configmap-test-091bdc28-5ede-4b27-8158-161c4d71d05a
STEP: Creating a pod to test consume configMaps
Sep  6 01:49:31.294: INFO: Waiting up to 5m0s for pod "pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3" in namespace "configmap-5712" to be "Succeeded or Failed"
Sep  6 01:49:31.310: INFO: Pod "pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.744481ms
Sep  6 01:49:33.328: INFO: Pod "pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033230527s
Sep  6 01:49:35.349: INFO: Pod "pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054061815s
Sep  6 01:49:37.989: INFO: Pod "pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.694695269s
Sep  6 01:49:40.849: INFO: Pod "pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.554697389s
Sep  6 01:49:43.738: INFO: Pod "pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.443087882s
Sep  6 01:49:47.606: INFO: Pod "pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.311215812s
Sep  6 01:49:49.815: INFO: Pod "pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.520378226s
Sep  6 01:49:51.870: INFO: Pod "pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3": Phase="Pending", Reason="", readiness=false. Elapsed: 20.575862195s
Sep  6 01:49:54.243: INFO: Pod "pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3": Phase="Pending", Reason="", readiness=false. Elapsed: 22.948471404s
Sep  6 01:49:56.440: INFO: Pod "pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3": Phase="Pending", Reason="", readiness=false. Elapsed: 25.145961353s
Sep  6 01:49:58.472: INFO: Pod "pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3": Phase="Pending", Reason="", readiness=false. Elapsed: 27.177173701s
Sep  6 01:50:00.599: INFO: Pod "pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 29.304900153s
STEP: Saw pod success
Sep  6 01:50:00.602: INFO: Pod "pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3" satisfied condition "Succeeded or Failed"
Sep  6 01:50:00.667: INFO: Trying to get logs from node sc2-10-185-226-233.eng.vmware.com pod pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3 container env-test: <nil>
STEP: delete the pod
Sep  6 01:50:16.043: INFO: Waiting for pod pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3 to disappear
Sep  6 01:50:16.131: INFO: Pod pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3 still exists
Sep  6 01:50:18.133: INFO: Waiting for pod pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3 to disappear
Sep  6 01:50:22.535: INFO: Pod pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3 still exists
Sep  6 01:50:24.169: INFO: Waiting for pod pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3 to disappear
Sep  6 01:50:24.466: INFO: Pod pod-configmaps-2ecc17cb-6749-4dd9-8912-eb2f21b716d3 no longer exists
[AfterEach] [sig-node] ConfigMap
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 01:50:24.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5712" for this suite.

• [SLOW TEST:54.969 seconds]
[sig-node] ConfigMap
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via the environment [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:629
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":296,"completed":293,"skipped":5333,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSep  6 01:50:25.381: INFO: Running AfterSuite actions on all nodes
Sep  6 01:50:25.405: INFO: Running AfterSuite actions on node 1
Sep  6 01:50:25.405: INFO: Dumping logs locally to: ./e2e.results
Sep  6 01:50:25.422: INFO: Error running cluster/log-dump/log-dump.sh: fork/exec kubernetes/cluster/log-dump/log-dump.sh: no such file or directory

JUnit report was created: /home/worker/workspace/conformance-nsx-1.20/e2e.results/junit_01.xml
{"msg":"Test Suite completed","total":296,"completed":293,"skipped":5371,"failed":3,"failures":["[k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","[k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","[sig-auth] ServiceAccounts should mount projected service account token [Conformance]"]}


Summarizing 3 Failures:

[Fail] [k8s.io] Variable Expansion [It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance] 
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:376

[Fail] [k8s.io] Variable Expansion [It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance] 
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/expansion.go:376

[Fail] [sig-auth] ServiceAccounts [It] should mount projected service account token [Conformance] 
/home/worker/workspace/conformance-nsx-1.20/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/util.go:738

Ran 296 of 5667 Specs in 16261.840 seconds
FAIL! -- 293 Passed | 3 Failed | 0 Pending | 5371 Skipped
--- FAIL: TestE2E (16265.06s)
FAIL
