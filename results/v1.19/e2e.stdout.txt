{"msg":"Test Suite starting","total":291,"completed":0,"skipped":0,"failed":0}
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1630900170 - Will randomize all specs
Will run 291 of 5232 specs

Sep  5 20:49:32.039: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 20:49:32.043: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep  5 20:49:32.065: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep  5 20:49:32.117: INFO: 30 / 30 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep  5 20:49:32.117: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Sep  5 20:49:32.117: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep  5 20:49:32.132: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep  5 20:49:32.132: INFO: e2e test version: v0.0.0-master+$Format:%h$
Sep  5 20:49:32.134: INFO: kube-apiserver version: v1.19.12+vmware.wcp.1
Sep  5 20:49:32.134: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 20:49:32.144: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 20:49:32.144: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
Sep  5 20:49:32.710: INFO: Found PodSecurityPolicies; testing pod creation to see if PodSecurityPolicy is enabled
Sep  5 20:49:32.754: INFO: PSP annotation exists on dry run pod: "wcp-default-psp"; assuming PodSecurityPolicy is enabled
Sep  5 20:49:32.811: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3912
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-8e9642e5-75c7-4996-b569-f290be0d066c
STEP: Creating a pod to test consume configMaps
Sep  5 20:49:33.036: INFO: Waiting up to 5m0s for pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467" in namespace "configmap-3912" to be "Succeeded or Failed"
Sep  5 20:49:33.045: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 9.340806ms
Sep  5 20:49:35.056: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020538874s
Sep  5 20:49:37.069: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032963397s
Sep  5 20:49:39.080: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043963633s
Sep  5 20:49:41.087: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 8.050842517s
Sep  5 20:49:43.099: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 10.063087641s
Sep  5 20:49:45.110: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 12.074607838s
Sep  5 20:49:47.123: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 14.087309987s
Sep  5 20:49:49.134: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 16.09782102s
Sep  5 20:49:51.146: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 18.11015891s
Sep  5 20:49:53.159: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 20.122873401s
Sep  5 20:49:55.173: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 22.136944271s
Sep  5 20:49:57.183: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 24.147207144s
Sep  5 20:49:59.191: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 26.155017171s
Sep  5 20:50:01.343: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 28.307018309s
Sep  5 20:50:03.365: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 30.32911992s
Sep  5 20:50:05.498: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 32.462565751s
Sep  5 20:50:07.511: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 34.47534234s
Sep  5 20:50:09.522: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 36.485642249s
Sep  5 20:50:11.531: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 38.494887109s
Sep  5 20:50:13.548: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 40.511803276s
Sep  5 20:50:15.555: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 42.518962878s
Sep  5 20:50:17.579: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Pending", Reason="", readiness=false. Elapsed: 44.543209846s
Sep  5 20:50:19.594: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Running", Reason="", readiness=true. Elapsed: 46.558593665s
Sep  5 20:50:21.610: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Running", Reason="", readiness=true. Elapsed: 48.574463351s
Sep  5 20:50:23.624: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467": Phase="Succeeded", Reason="", readiness=false. Elapsed: 50.587727235s
STEP: Saw pod success
Sep  5 20:50:23.624: INFO: Pod "pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467" satisfied condition "Succeeded or Failed"
Sep  5 20:50:23.631: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 20:50:23.710: INFO: Waiting for pod pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467 to disappear
Sep  5 20:50:23.734: INFO: Pod pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467 still exists
Sep  5 20:50:25.735: INFO: Waiting for pod pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467 to disappear
Sep  5 20:50:25.743: INFO: Pod pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467 still exists
Sep  5 20:50:27.736: INFO: Waiting for pod pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467 to disappear
Sep  5 20:50:27.747: INFO: Pod pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467 still exists
Sep  5 20:50:29.736: INFO: Waiting for pod pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467 to disappear
Sep  5 20:50:29.744: INFO: Pod pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467 still exists
Sep  5 20:50:31.735: INFO: Waiting for pod pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467 to disappear
Sep  5 20:50:31.742: INFO: Pod pod-configmaps-2fba0876-ea78-4498-a5fe-810c055af467 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 20:50:31.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3912" for this suite.

â€¢ [SLOW TEST:59.913 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":1,"skipped":18,"failed":0}
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 20:50:32.058: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6293
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-fc7e6151-a8c1-4717-bd75-32e964b6da04
STEP: Creating a pod to test consume configMaps
Sep  5 20:50:32.539: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273" in namespace "projected-6293" to be "Succeeded or Failed"
Sep  5 20:50:32.556: INFO: Pod "pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273": Phase="Pending", Reason="", readiness=false. Elapsed: 17.046295ms
Sep  5 20:50:34.573: INFO: Pod "pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03458725s
Sep  5 20:50:36.582: INFO: Pod "pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043393012s
Sep  5 20:50:38.593: INFO: Pod "pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054069478s
Sep  5 20:50:40.602: INFO: Pod "pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273": Phase="Pending", Reason="", readiness=false. Elapsed: 8.063704638s
Sep  5 20:50:42.610: INFO: Pod "pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273": Phase="Pending", Reason="", readiness=false. Elapsed: 10.071117969s
Sep  5 20:50:44.623: INFO: Pod "pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273": Phase="Pending", Reason="", readiness=false. Elapsed: 12.084760836s
Sep  5 20:50:46.632: INFO: Pod "pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273": Phase="Pending", Reason="", readiness=false. Elapsed: 14.093622013s
Sep  5 20:50:48.640: INFO: Pod "pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273": Phase="Pending", Reason="", readiness=false. Elapsed: 16.101177229s
Sep  5 20:50:50.650: INFO: Pod "pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273": Phase="Pending", Reason="", readiness=false. Elapsed: 18.111659926s
Sep  5 20:50:52.657: INFO: Pod "pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273": Phase="Pending", Reason="", readiness=false. Elapsed: 20.117857408s
Sep  5 20:50:54.665: INFO: Pod "pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273": Phase="Pending", Reason="", readiness=false. Elapsed: 22.126739206s
Sep  5 20:50:56.673: INFO: Pod "pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273": Phase="Running", Reason="", readiness=true. Elapsed: 24.133866693s
Sep  5 20:50:58.683: INFO: Pod "pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273": Phase="Running", Reason="", readiness=true. Elapsed: 26.143949846s
Sep  5 20:51:00.692: INFO: Pod "pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273": Phase="Running", Reason="", readiness=true. Elapsed: 28.153764746s
Sep  5 20:51:02.700: INFO: Pod "pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.161721504s
STEP: Saw pod success
Sep  5 20:51:02.700: INFO: Pod "pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273" satisfied condition "Succeeded or Failed"
Sep  5 20:51:02.707: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com pod pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 20:51:07.658: INFO: Waiting for pod pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273 to disappear
Sep  5 20:51:07.668: INFO: Pod pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273 still exists
Sep  5 20:51:09.669: INFO: Waiting for pod pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273 to disappear
Sep  5 20:51:09.676: INFO: Pod pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273 still exists
Sep  5 20:51:11.669: INFO: Waiting for pod pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273 to disappear
Sep  5 20:51:11.678: INFO: Pod pod-projected-configmaps-031e2852-0704-4acf-a778-8a1a2eade273 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 20:51:11.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6293" for this suite.

â€¢ [SLOW TEST:40.300 seconds]
[sig-storage] Projected configMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":291,"completed":2,"skipped":18,"failed":0}
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 20:51:12.358: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9901
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 20:51:13.744: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Sep  5 20:51:15.777: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 20:51:17.794: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 20:51:19.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 20:51:21.783: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 20:51:23.787: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 20:51:25.789: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 20:51:27.786: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 20:51:29.791: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 20:51:31.790: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 20:51:33.791: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 20:51:35.790: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 20, 51, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 20:51:38.820: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 20:51:39.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9901" for this suite.
STEP: Destroying namespace "webhook-9901-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:27.294 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","total":291,"completed":3,"skipped":28,"failed":0}
SSS
------------------------------
[sig-scheduling] LimitRange 
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] LimitRange
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 20:51:39.652: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename limitrange
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in limitrange-3697
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a LimitRange
STEP: Setting up watch
STEP: Submitting a LimitRange
Sep  5 20:51:40.103: INFO: observed the limitRanges list
STEP: Verifying LimitRange creation was observed
STEP: Fetching the LimitRange to ensure it has proper values
Sep  5 20:51:40.114: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  5 20:51:40.114: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements
STEP: Ensuring Pod has resource requirements applied from LimitRange
Sep  5 20:51:40.153: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Sep  5 20:51:40.154: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements
STEP: Ensuring Pod has merged resource requirements applied from LimitRange
Sep  5 20:51:40.180: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Sep  5 20:51:40.180: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources
STEP: Failing to create a Pod with more than max resources
STEP: Updating a LimitRange
STEP: Verifying LimitRange updating is effective
STEP: Creating a Pod with less than former min resources
STEP: Failing to create a Pod with more than max resources
STEP: Deleting a LimitRange
STEP: Verifying the LimitRange was deleted
Sep  5 20:51:47.271: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources
[AfterEach] [sig-scheduling] LimitRange
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 20:51:47.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-3697" for this suite.

â€¢ [SLOW TEST:7.961 seconds]
[sig-scheduling] LimitRange
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","total":291,"completed":4,"skipped":31,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 20:51:47.615: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5244
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-9149fe9c-36b8-4242-98e1-76ba62b41229
STEP: Creating a pod to test consume secrets
Sep  5 20:51:48.506: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586" in namespace "projected-5244" to be "Succeeded or Failed"
Sep  5 20:51:48.525: INFO: Pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586": Phase="Pending", Reason="", readiness=false. Elapsed: 18.250081ms
Sep  5 20:51:50.595: INFO: Pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586": Phase="Pending", Reason="", readiness=false. Elapsed: 2.088305402s
Sep  5 20:51:52.605: INFO: Pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586": Phase="Pending", Reason="", readiness=false. Elapsed: 4.098595384s
Sep  5 20:51:54.616: INFO: Pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586": Phase="Pending", Reason="", readiness=false. Elapsed: 6.110041582s
Sep  5 20:51:56.625: INFO: Pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586": Phase="Pending", Reason="", readiness=false. Elapsed: 8.118106062s
Sep  5 20:51:58.633: INFO: Pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586": Phase="Pending", Reason="", readiness=false. Elapsed: 10.126246051s
Sep  5 20:52:00.672: INFO: Pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586": Phase="Pending", Reason="", readiness=false. Elapsed: 12.165946407s
Sep  5 20:52:02.686: INFO: Pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586": Phase="Pending", Reason="", readiness=false. Elapsed: 14.180053357s
Sep  5 20:52:04.708: INFO: Pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586": Phase="Pending", Reason="", readiness=false. Elapsed: 16.202012873s
Sep  5 20:52:06.741: INFO: Pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586": Phase="Pending", Reason="", readiness=false. Elapsed: 18.234419097s
Sep  5 20:52:08.827: INFO: Pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586": Phase="Pending", Reason="", readiness=false. Elapsed: 20.320462706s
Sep  5 20:52:10.837: INFO: Pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586": Phase="Pending", Reason="", readiness=false. Elapsed: 22.331056597s
Sep  5 20:52:12.846: INFO: Pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586": Phase="Pending", Reason="", readiness=false. Elapsed: 24.339718754s
Sep  5 20:52:14.956: INFO: Pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586": Phase="Pending", Reason="", readiness=false. Elapsed: 26.449942124s
Sep  5 20:52:16.966: INFO: Pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586": Phase="Running", Reason="", readiness=true. Elapsed: 28.459785544s
Sep  5 20:52:18.974: INFO: Pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586": Phase="Running", Reason="", readiness=true. Elapsed: 30.468045126s
Sep  5 20:52:20.989: INFO: Pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586": Phase="Running", Reason="", readiness=true. Elapsed: 32.482176053s
Sep  5 20:52:22.995: INFO: Pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.488727975s
STEP: Saw pod success
Sep  5 20:52:22.995: INFO: Pod "pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586" satisfied condition "Succeeded or Failed"
Sep  5 20:52:23.005: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com pod pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  5 20:52:23.097: INFO: Waiting for pod pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586 to disappear
Sep  5 20:52:23.113: INFO: Pod pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586 still exists
Sep  5 20:52:25.113: INFO: Waiting for pod pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586 to disappear
Sep  5 20:52:25.122: INFO: Pod pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586 still exists
Sep  5 20:52:27.113: INFO: Waiting for pod pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586 to disappear
Sep  5 20:52:27.121: INFO: Pod pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586 still exists
Sep  5 20:52:29.115: INFO: Waiting for pod pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586 to disappear
Sep  5 20:52:29.123: INFO: Pod pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586 still exists
Sep  5 20:52:31.114: INFO: Waiting for pod pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586 to disappear
Sep  5 20:52:31.121: INFO: Pod pod-projected-secrets-3ad5af63-f13f-4e26-9b78-c348e3abb586 no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 20:52:31.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5244" for this suite.

â€¢ [SLOW TEST:43.713 seconds]
[sig-storage] Projected secret
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":291,"completed":5,"skipped":135,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 20:52:31.329: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6732
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  5 20:52:31.738: INFO: Waiting up to 5m0s for pod "pod-ee726091-93bc-4752-99a5-f706412ddcbc" in namespace "emptydir-6732" to be "Succeeded or Failed"
Sep  5 20:52:31.759: INFO: Pod "pod-ee726091-93bc-4752-99a5-f706412ddcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 20.822096ms
Sep  5 20:52:33.767: INFO: Pod "pod-ee726091-93bc-4752-99a5-f706412ddcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028564508s
Sep  5 20:52:35.774: INFO: Pod "pod-ee726091-93bc-4752-99a5-f706412ddcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035806326s
Sep  5 20:52:37.782: INFO: Pod "pod-ee726091-93bc-4752-99a5-f706412ddcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043840101s
Sep  5 20:52:39.806: INFO: Pod "pod-ee726091-93bc-4752-99a5-f706412ddcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.067822941s
Sep  5 20:52:41.814: INFO: Pod "pod-ee726091-93bc-4752-99a5-f706412ddcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.076045757s
Sep  5 20:52:43.829: INFO: Pod "pod-ee726091-93bc-4752-99a5-f706412ddcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.090302217s
Sep  5 20:52:45.838: INFO: Pod "pod-ee726091-93bc-4752-99a5-f706412ddcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.100157637s
Sep  5 20:52:47.845: INFO: Pod "pod-ee726091-93bc-4752-99a5-f706412ddcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 16.106860217s
Sep  5 20:52:49.851: INFO: Pod "pod-ee726091-93bc-4752-99a5-f706412ddcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 18.113020375s
Sep  5 20:52:51.859: INFO: Pod "pod-ee726091-93bc-4752-99a5-f706412ddcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 20.120994925s
Sep  5 20:52:53.878: INFO: Pod "pod-ee726091-93bc-4752-99a5-f706412ddcbc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.140035295s
STEP: Saw pod success
Sep  5 20:52:53.878: INFO: Pod "pod-ee726091-93bc-4752-99a5-f706412ddcbc" satisfied condition "Succeeded or Failed"
Sep  5 20:52:53.886: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com pod pod-ee726091-93bc-4752-99a5-f706412ddcbc container test-container: <nil>
STEP: delete the pod
Sep  5 20:52:58.967: INFO: Waiting for pod pod-ee726091-93bc-4752-99a5-f706412ddcbc to disappear
Sep  5 20:52:58.981: INFO: Pod pod-ee726091-93bc-4752-99a5-f706412ddcbc still exists
Sep  5 20:53:00.981: INFO: Waiting for pod pod-ee726091-93bc-4752-99a5-f706412ddcbc to disappear
Sep  5 20:53:00.990: INFO: Pod pod-ee726091-93bc-4752-99a5-f706412ddcbc still exists
Sep  5 20:53:02.982: INFO: Waiting for pod pod-ee726091-93bc-4752-99a5-f706412ddcbc to disappear
Sep  5 20:53:02.990: INFO: Pod pod-ee726091-93bc-4752-99a5-f706412ddcbc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 20:53:02.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6732" for this suite.

â€¢ [SLOW TEST:31.994 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":6,"skipped":163,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 20:53:03.324: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8879
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  5 20:53:38.360: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 20:53:38.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8879" for this suite.

â€¢ [SLOW TEST:35.361 seconds]
[k8s.io] Container Runtime
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":291,"completed":7,"skipped":214,"failed":0}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 20:53:38.686: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2200
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-82cf0ae1-eb6a-4f2a-88a8-a2c38258105f
STEP: Creating a pod to test consume configMaps
Sep  5 20:53:39.103: INFO: Waiting up to 5m0s for pod "pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721" in namespace "configmap-2200" to be "Succeeded or Failed"
Sep  5 20:53:39.134: INFO: Pod "pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721": Phase="Pending", Reason="", readiness=false. Elapsed: 31.048125ms
Sep  5 20:53:41.142: INFO: Pod "pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039473831s
Sep  5 20:53:43.150: INFO: Pod "pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047522675s
Sep  5 20:53:45.166: INFO: Pod "pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063102743s
Sep  5 20:53:47.175: INFO: Pod "pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721": Phase="Pending", Reason="", readiness=false. Elapsed: 8.072094505s
Sep  5 20:53:49.181: INFO: Pod "pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721": Phase="Pending", Reason="", readiness=false. Elapsed: 10.078238527s
Sep  5 20:53:51.196: INFO: Pod "pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721": Phase="Pending", Reason="", readiness=false. Elapsed: 12.093339322s
Sep  5 20:53:53.205: INFO: Pod "pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721": Phase="Pending", Reason="", readiness=false. Elapsed: 14.101927283s
Sep  5 20:53:55.243: INFO: Pod "pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721": Phase="Pending", Reason="", readiness=false. Elapsed: 16.139684962s
Sep  5 20:53:57.250: INFO: Pod "pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721": Phase="Pending", Reason="", readiness=false. Elapsed: 18.1472828s
Sep  5 20:53:59.260: INFO: Pod "pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721": Phase="Pending", Reason="", readiness=false. Elapsed: 20.15682126s
Sep  5 20:54:01.271: INFO: Pod "pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721": Phase="Pending", Reason="", readiness=false. Elapsed: 22.167826605s
Sep  5 20:54:03.281: INFO: Pod "pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721": Phase="Pending", Reason="", readiness=false. Elapsed: 24.177818014s
Sep  5 20:54:05.289: INFO: Pod "pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.186345021s
STEP: Saw pod success
Sep  5 20:54:05.289: INFO: Pod "pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721" satisfied condition "Succeeded or Failed"
Sep  5 20:54:05.297: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 20:54:10.503: INFO: Waiting for pod pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721 to disappear
Sep  5 20:54:10.526: INFO: Pod pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721 still exists
Sep  5 20:54:12.526: INFO: Waiting for pod pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721 to disappear
Sep  5 20:54:12.533: INFO: Pod pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721 still exists
Sep  5 20:54:14.527: INFO: Waiting for pod pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721 to disappear
Sep  5 20:54:14.535: INFO: Pod pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721 still exists
Sep  5 20:54:16.527: INFO: Waiting for pod pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721 to disappear
Sep  5 20:54:16.537: INFO: Pod pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721 still exists
Sep  5 20:54:18.527: INFO: Waiting for pod pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721 to disappear
Sep  5 20:54:18.534: INFO: Pod pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721 still exists
Sep  5 20:54:20.526: INFO: Waiting for pod pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721 to disappear
Sep  5 20:54:20.534: INFO: Pod pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721 still exists
Sep  5 20:54:22.526: INFO: Waiting for pod pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721 to disappear
Sep  5 20:54:22.536: INFO: Pod pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721 still exists
Sep  5 20:54:24.527: INFO: Waiting for pod pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721 to disappear
Sep  5 20:54:24.535: INFO: Pod pod-configmaps-f4756592-2279-41b5-b85c-b1f4da51e721 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 20:54:24.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2200" for this suite.

â€¢ [SLOW TEST:46.593 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":291,"completed":8,"skipped":230,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 20:54:25.280: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8966
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-8966
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  5 20:54:25.709: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  5 20:54:25.910: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 20:54:27.919: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 20:54:29.921: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 20:54:31.924: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 20:54:33.920: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 20:54:35.933: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 20:54:37.921: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 20:54:39.927: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 20:54:41.924: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 20:54:43.917: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 20:54:45.921: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 20:54:47.919: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 20:54:49.938: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 20:54:51.926: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 20:54:53.919: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 20:54:55.925: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 20:54:57.937: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 20:54:59.920: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep  5 20:54:59.938: INFO: The status of Pod netserver-1 is Running (Ready = false)
Sep  5 20:55:01.952: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep  5 20:55:01.968: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep  5 20:55:20.055: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.26.1.197:8080/dial?request=hostname&protocol=http&host=172.26.1.194&port=8080&tries=1'] Namespace:pod-network-test-8966 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:55:20.055: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 20:55:20.379: INFO: Waiting for responses: map[]
Sep  5 20:55:20.392: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.26.1.197:8080/dial?request=hostname&protocol=http&host=172.26.1.195&port=8080&tries=1'] Namespace:pod-network-test-8966 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:55:20.392: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 20:55:20.567: INFO: Waiting for responses: map[]
Sep  5 20:55:20.575: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.26.1.197:8080/dial?request=hostname&protocol=http&host=172.26.1.196&port=8080&tries=1'] Namespace:pod-network-test-8966 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 20:55:20.575: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 20:55:20.715: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 20:55:20.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8966" for this suite.

â€¢ [SLOW TEST:56.031 seconds]
[sig-network] Networking
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","total":291,"completed":9,"skipped":243,"failed":0}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] 
  should support CSR API operations [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 20:55:21.311: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename certificates
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in certificates-3991
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support CSR API operations [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/certificates.k8s.io
STEP: getting /apis/certificates.k8s.io/v1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep  5 20:55:23.124: INFO: starting watch
STEP: patching
STEP: updating
Sep  5 20:55:23.198: INFO: waiting for watch events with expected annotations
Sep  5 20:55:23.198: INFO: saw patched and updated annotations
STEP: getting /approval
STEP: patching /approval
STEP: updating /approval
STEP: getting /status
STEP: patching /status
STEP: updating /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 20:55:23.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-3991" for this suite.
â€¢{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","total":291,"completed":10,"skipped":263,"failed":0}
SS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 20:55:23.693: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8399
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8399.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8399.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8399.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8399.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8399.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8399.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8399.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8399.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8399.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8399.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8399.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8399.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8399.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 164.109.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.109.164_udp@PTR;check="$$(dig +tcp +noall +answer +search 164.109.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.109.164_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8399.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8399.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8399.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8399.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8399.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8399.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8399.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8399.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8399.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8399.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8399.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8399.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8399.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 164.109.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.109.164_udp@PTR;check="$$(dig +tcp +noall +answer +search 164.109.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.109.164_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 20:56:20.460: INFO: Unable to read wheezy_udp@dns-test-service.dns-8399.svc.cluster.local from pod dns-8399/dns-test-b019961b-e632-4da0-922d-dec22a6596d5: the server could not find the requested resource (get pods dns-test-b019961b-e632-4da0-922d-dec22a6596d5)
Sep  5 20:56:20.486: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8399.svc.cluster.local from pod dns-8399/dns-test-b019961b-e632-4da0-922d-dec22a6596d5: the server could not find the requested resource (get pods dns-test-b019961b-e632-4da0-922d-dec22a6596d5)
Sep  5 20:56:20.505: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8399.svc.cluster.local from pod dns-8399/dns-test-b019961b-e632-4da0-922d-dec22a6596d5: the server could not find the requested resource (get pods dns-test-b019961b-e632-4da0-922d-dec22a6596d5)
Sep  5 20:56:20.529: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8399.svc.cluster.local from pod dns-8399/dns-test-b019961b-e632-4da0-922d-dec22a6596d5: the server could not find the requested resource (get pods dns-test-b019961b-e632-4da0-922d-dec22a6596d5)
Sep  5 20:56:20.614: INFO: Unable to read jessie_udp@dns-test-service.dns-8399.svc.cluster.local from pod dns-8399/dns-test-b019961b-e632-4da0-922d-dec22a6596d5: the server could not find the requested resource (get pods dns-test-b019961b-e632-4da0-922d-dec22a6596d5)
Sep  5 20:56:20.629: INFO: Unable to read jessie_tcp@dns-test-service.dns-8399.svc.cluster.local from pod dns-8399/dns-test-b019961b-e632-4da0-922d-dec22a6596d5: the server could not find the requested resource (get pods dns-test-b019961b-e632-4da0-922d-dec22a6596d5)
Sep  5 20:56:20.658: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8399.svc.cluster.local from pod dns-8399/dns-test-b019961b-e632-4da0-922d-dec22a6596d5: the server could not find the requested resource (get pods dns-test-b019961b-e632-4da0-922d-dec22a6596d5)
Sep  5 20:56:20.934: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8399.svc.cluster.local from pod dns-8399/dns-test-b019961b-e632-4da0-922d-dec22a6596d5: the server could not find the requested resource (get pods dns-test-b019961b-e632-4da0-922d-dec22a6596d5)
Sep  5 20:56:21.131: INFO: Lookups using dns-8399/dns-test-b019961b-e632-4da0-922d-dec22a6596d5 failed for: [wheezy_udp@dns-test-service.dns-8399.svc.cluster.local wheezy_tcp@dns-test-service.dns-8399.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8399.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8399.svc.cluster.local jessie_udp@dns-test-service.dns-8399.svc.cluster.local jessie_tcp@dns-test-service.dns-8399.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8399.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8399.svc.cluster.local]

Sep  5 20:56:26.355: INFO: DNS probes using dns-8399/dns-test-b019961b-e632-4da0-922d-dec22a6596d5 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 20:56:26.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8399" for this suite.

â€¢ [SLOW TEST:63.289 seconds]
[sig-network] DNS
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","total":291,"completed":11,"skipped":265,"failed":0}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 20:56:26.983: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8410
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  5 20:56:27.403: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2" in namespace "projected-8410" to be "Succeeded or Failed"
Sep  5 20:56:27.428: INFO: Pod "downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.804295ms
Sep  5 20:56:29.444: INFO: Pod "downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041027923s
Sep  5 20:56:31.460: INFO: Pod "downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056991166s
Sep  5 20:56:33.476: INFO: Pod "downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072577853s
Sep  5 20:56:35.485: INFO: Pod "downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.081745779s
Sep  5 20:56:37.500: INFO: Pod "downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.097107034s
Sep  5 20:56:39.508: INFO: Pod "downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.104915212s
Sep  5 20:56:41.514: INFO: Pod "downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.110839315s
Sep  5 20:56:43.521: INFO: Pod "downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.118301516s
Sep  5 20:56:45.529: INFO: Pod "downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.125774558s
Sep  5 20:56:47.541: INFO: Pod "downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.137787663s
Sep  5 20:56:49.552: INFO: Pod "downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.148778091s
Sep  5 20:56:51.562: INFO: Pod "downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.158560229s
STEP: Saw pod success
Sep  5 20:56:51.562: INFO: Pod "downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2" satisfied condition "Succeeded or Failed"
Sep  5 20:56:51.574: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2 container client-container: <nil>
STEP: delete the pod
Sep  5 20:56:56.004: INFO: Waiting for pod downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2 to disappear
Sep  5 20:56:56.027: INFO: Pod downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2 still exists
Sep  5 20:56:58.028: INFO: Waiting for pod downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2 to disappear
Sep  5 20:56:58.038: INFO: Pod downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2 still exists
Sep  5 20:57:00.029: INFO: Waiting for pod downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2 to disappear
Sep  5 20:57:00.040: INFO: Pod downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2 still exists
Sep  5 20:57:02.027: INFO: Waiting for pod downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2 to disappear
Sep  5 20:57:02.038: INFO: Pod downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2 still exists
Sep  5 20:57:04.029: INFO: Waiting for pod downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2 to disappear
Sep  5 20:57:04.037: INFO: Pod downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2 still exists
Sep  5 20:57:06.028: INFO: Waiting for pod downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2 to disappear
Sep  5 20:57:06.033: INFO: Pod downwardapi-volume-6dcf1f4a-b657-4fe9-92a3-bc9cabd5bec2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 20:57:06.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8410" for this suite.

â€¢ [SLOW TEST:39.315 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's memory request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","total":291,"completed":12,"skipped":282,"failed":0}
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 20:57:06.297: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7445
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Sep  5 20:57:31.366: INFO: Successfully updated pod "labelsupdatea4f01fbd-e248-4cfe-a733-9e9832799d1a"
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 20:57:33.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7445" for this suite.

â€¢ [SLOW TEST:27.401 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update labels on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","total":291,"completed":13,"skipped":295,"failed":0}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 20:57:33.699: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1915
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 20:58:38.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1915" for this suite.

â€¢ [SLOW TEST:64.789 seconds]
[sig-apps] Job
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","total":291,"completed":14,"skipped":317,"failed":0}
SSS
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 20:58:38.488: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-533
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 20:58:38.961: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-5d795003-2c74-48cf-892d-014f44f21052" in namespace "security-context-test-533" to be "Succeeded or Failed"
Sep  5 20:58:38.999: INFO: Pod "busybox-readonly-false-5d795003-2c74-48cf-892d-014f44f21052": Phase="Pending", Reason="", readiness=false. Elapsed: 37.656381ms
Sep  5 20:58:41.005: INFO: Pod "busybox-readonly-false-5d795003-2c74-48cf-892d-014f44f21052": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044265425s
Sep  5 20:58:43.020: INFO: Pod "busybox-readonly-false-5d795003-2c74-48cf-892d-014f44f21052": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059294801s
Sep  5 20:58:45.032: INFO: Pod "busybox-readonly-false-5d795003-2c74-48cf-892d-014f44f21052": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070860938s
Sep  5 20:58:47.044: INFO: Pod "busybox-readonly-false-5d795003-2c74-48cf-892d-014f44f21052": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082728697s
Sep  5 20:58:49.052: INFO: Pod "busybox-readonly-false-5d795003-2c74-48cf-892d-014f44f21052": Phase="Pending", Reason="", readiness=false. Elapsed: 10.091017103s
Sep  5 20:58:51.060: INFO: Pod "busybox-readonly-false-5d795003-2c74-48cf-892d-014f44f21052": Phase="Pending", Reason="", readiness=false. Elapsed: 12.099229s
Sep  5 20:58:53.068: INFO: Pod "busybox-readonly-false-5d795003-2c74-48cf-892d-014f44f21052": Phase="Pending", Reason="", readiness=false. Elapsed: 14.107500871s
Sep  5 20:58:55.080: INFO: Pod "busybox-readonly-false-5d795003-2c74-48cf-892d-014f44f21052": Phase="Pending", Reason="", readiness=false. Elapsed: 16.119136109s
Sep  5 20:58:57.108: INFO: Pod "busybox-readonly-false-5d795003-2c74-48cf-892d-014f44f21052": Phase="Pending", Reason="", readiness=false. Elapsed: 18.147484246s
Sep  5 20:58:59.122: INFO: Pod "busybox-readonly-false-5d795003-2c74-48cf-892d-014f44f21052": Phase="Pending", Reason="", readiness=false. Elapsed: 20.160628157s
Sep  5 20:59:01.136: INFO: Pod "busybox-readonly-false-5d795003-2c74-48cf-892d-014f44f21052": Phase="Pending", Reason="", readiness=false. Elapsed: 22.175432317s
Sep  5 20:59:03.159: INFO: Pod "busybox-readonly-false-5d795003-2c74-48cf-892d-014f44f21052": Phase="Pending", Reason="", readiness=false. Elapsed: 24.198088959s
Sep  5 20:59:05.170: INFO: Pod "busybox-readonly-false-5d795003-2c74-48cf-892d-014f44f21052": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.208908235s
Sep  5 20:59:05.170: INFO: Pod "busybox-readonly-false-5d795003-2c74-48cf-892d-014f44f21052" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 20:59:05.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-533" for this suite.

â€¢ [SLOW TEST:26.895 seconds]
[k8s.io] Security Context
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  When creating a pod with readOnlyRootFilesystem
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:166
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","total":291,"completed":15,"skipped":320,"failed":0}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 20:59:05.385: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8831
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating cluster-info
Sep  5 20:59:05.797: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml cluster-info'
Sep  5 20:59:06.505: INFO: stderr: ""
Sep  5 20:59:06.505: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.193.38.248:6443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.193.38.248:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
Sep  5 20:59:06.505: FAIL: Missing Kubernetes master in kubectl cluster-info

Full Stack Trace
k8s.io/kubernetes/test/e2e/kubectl.glob..func1.17.1()
	/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1092 +0x16f
k8s.io/kubernetes/test/e2e.RunE2ETests(0x48f1d7)
	_output/local/go/src/k8s.io/kubernetes/test/e2e/e2e.go:130 +0x5ff
k8s.io/kubernetes/test/e2e.TestE2E(0x0)
	_output/local/go/src/k8s.io/kubernetes/test/e2e/e2e_test.go:145 +0x19
testing.tRunner(0xc000f20d00, 0x4449488)
	/usr/lib/golang/src/testing/testing.go:1259 +0x102
created by testing.(*T).Run
	/usr/lib/golang/src/testing/testing.go:1306 +0x35a
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
STEP: Collecting events from namespace "kubectl-8831".
STEP: Found 0 events.
Sep  5 20:59:06.534: INFO: POD  NODE  PHASE  GRACE  CONDITIONS
Sep  5 20:59:06.534: INFO: 
Sep  5 20:59:06.542: INFO: 
Logging node info for node 42174da95d8c532b15b7283e9031a350
Sep  5 20:59:06.548: INFO: Node Info: &Node{ObjectMeta:{42174da95d8c532b15b7283e9031a350   /api/v1/nodes/42174da95d8c532b15b7283e9031a350 d22f1fd2-1eea-4b08-a387-8c70a78e5d1a 58900 0 2021-09-05 19:32:14 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:42174da95d8c532b15b7283e9031a350 kubernetes.io/os:linux node-role.kubernetes.io/master:] map[kubeadm.alpha.kubernetes.io/cri-socket:/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 vmware-system-workload-ip-configured: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubelet Update v1 2021-09-05 19:32:14 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}} {kubeadm Update v1 2021-09-05 19:32:19 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/master":{}}}}} {kube-controller-manager Update v1 2021-09-05 19:32:25 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:taints":{}}}} {kubectl-annotate Update v1 2021-09-05 19:40:39 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-workload-ip-configured":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332259328 0} {<nil>} 8136972Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332259328 0} {<nil>} 8136972Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-09-05 20:56:12 -0700 PDT,LastTransitionTime:2021-09-05 19:32:09 -0700 PDT,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-09-05 20:56:12 -0700 PDT,LastTransitionTime:2021-09-05 19:32:09 -0700 PDT,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-09-05 20:56:12 -0700 PDT,LastTransitionTime:2021-09-05 19:32:09 -0700 PDT,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-05 20:56:12 -0700 PDT,LastTransitionTime:2021-09-05 19:41:02 -0700 PDT,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:172.26.0.2,},NodeAddress{Type:Hostname,Address:42174da95d8c532b15b7283e9031a350,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:f229f6663c3f4882864365c2b5551463,SystemUUID:a94d1742-8c5d-2b53-15b7-283e9031a350,BootID:0ebd5374-2dbd-462c-9f09-004c258259ab,KernelVersion:4.19.198-1.ph3-esx,OSImage:VMware Photon OS/Linux,ContainerRuntimeVersion:containerd://1.4.4,KubeletVersion:v1.19.12+vmware.wcp.1,KubeProxyVersion:v1.19.12+vmware.wcp.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[localhost:5000/vmware/nsx-ncp-photon@sha256:07d0445341f274674dea40d9449da3304c62615afa544f4023e3a3e21c353c89 localhost:5000/vmware/nsx-ncp-photon:3.2.0.18464816],SizeBytes:456347510,},ContainerImage{Names:[localhost:5000/vmware/vsphere-csi@sha256:5f667055674ed889a9a5a3e52e1f074d2adedb5ccf5fdc9a2cefb56e50257346 localhost:5000/vmware/vsphere-csi:vsphere70u3-3530247],SizeBytes:218324207,},ContainerImage{Names:[localhost:5000/vmware/syncer@sha256:ebb1907ad57ee343efe2a756bd7109021701469875d29008004d2e6abe615e66 localhost:5000/vmware/syncer:vsphere70u3-3530247],SizeBytes:186035682,},ContainerImage{Names:[docker.io/vmware/wcp-authproxy:0.0.11.18508287],SizeBytes:131961954,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.21.0],SizeBytes:126851027,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.20.8],SizeBytes:123038003,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.21.0],SizeBytes:121071415,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.19.12],SizeBytes:120109363,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.20.8],SizeBytes:117573957,},ContainerImage{Names:[localhost:5000/vmware/kubectl-plugin-vsphere@sha256:50972d3c755506a39b060daf001e0fb4d7bfcc47aa059a945573d6962338a8f4 docker.io/vmware/kubectl-plugin-vsphere:0.0.11.18508287 localhost:5000/vmware/kubectl-plugin-vsphere:0.0.11.18508287],SizeBytes:114408041,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.19.12],SizeBytes:112073027,},ContainerImage{Names:[projects.registry.vmware.com/tkg/etcd:v3.4.13_vmware.1],SizeBytes:105113092,},ContainerImage{Names:[localhost:5000/vmware/cert-manager-controller@sha256:ff0caa9f76178dfc2c4398046b4b08472ab90def98d1b8649833d092a1468100 localhost:5000/vmware/cert-manager-controller:v0.15.2_vmware.3],SizeBytes:94027733,},ContainerImage{Names:[localhost:5000/vmware/cert-manager-cainjector@sha256:5137cbc469ee041ef0d1f6d35a914889d50c0a44134967ca407ba7c2f1a7628d localhost:5000/vmware/cert-manager-cainjector:v0.15.2_vmware.3],SizeBytes:88001986,},ContainerImage{Names:[localhost:5000/vmware/cert-manager-webhook@sha256:da40b26c6920331288a4809415c202e2ad3a60392ee3f43f535626024b8a9845 localhost:5000/vmware/cert-manager-webhook:v0.15.2_vmware.3],SizeBytes:87704580,},ContainerImage{Names:[docker.io/vmware/wcp-schedext:0.0.11.18508287 docker.io/vmware/wcp-schedext:v1.19.12],SizeBytes:86647081,},ContainerImage{Names:[localhost:5000/vmware/wcp-appplatform-operator-v1alpha2@sha256:39f94d105b25a4f047caa8234d74645c21ccee2e13279ee4e07fb506e27d7494 localhost:5000/vmware/wcp-appplatform-operator-v1alpha2:5e0ffb0],SizeBytes:72197580,},ContainerImage{Names:[localhost:5000/vmware/psp-operator@sha256:3a831bb95bb73b3bda416fc37e3eda8eb2ed2a54109ec2e54ad6d70932f2813e localhost:5000/vmware/psp-operator:49766149],SizeBytes:59395632,},ContainerImage{Names:[docker.io/vmware/docker-registry:2.7.0.14110537],SizeBytes:53040781,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.21.0],SizeBytes:51910456,},ContainerImage{Names:[docker.io/vmware/wcp-fip:0.1],SizeBytes:49119461,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.20.8],SizeBytes:48519474,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.19.12],SizeBytes:47737138,},ContainerImage{Names:[localhost:5000/vmware/image-controller@sha256:e8bcd8932715a95d86f857fc838d67aed6953eeab4554a3dada45c80a222b2ff localhost:5000/vmware/image-controller:0.0.11.18508287],SizeBytes:38823779,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-control-plane-controller@sha256:ae28d5c4008d5ad88db2936d9b45e0248d129245810239bbafe0c66a47f2bf7e localhost:5000/vmware/kubeadm-control-plane-controller:v0.3.17_vmware.1],SizeBytes:36065259,},ContainerImage{Names:[localhost:5002/vmware/kube-proxy:v1.19.12 localhost:5000/vmware/kube-proxy:active],SizeBytes:35229260,},ContainerImage{Names:[localhost:5000/vmware/capw-controller@sha256:56496b9339aba7d338f2f864f0bb946e1d57f50bbcffeee26c57c1a7be6e9cff localhost:5000/vmware/capw-controller:1.4.1-28-ge3e1d8b],SizeBytes:34792328,},ContainerImage{Names:[localhost:5000/vmware/cluster-api-controller@sha256:72a217e8de91b2e1adcbcb1266a5854e9683f0f37ea828808374563d2bf41861 localhost:5000/vmware/cluster-api-controller:v0.3.17_vmware.1],SizeBytes:34280507,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-bootstrap-controller@sha256:921fc3887b0cfaf4742eefaf2b6d60160fdc25d2600dc08e9a3389aed6fbdaca localhost:5000/vmware/kubeadm-bootstrap-controller:v0.3.17_vmware.1],SizeBytes:33862350,},ContainerImage{Names:[localhost:5000/vmware/ucs@sha256:0006afae8591e51b3ddfe6d1062caa779c2bee08dc20ee38eb8183614ccbdcd1 localhost:5000/vmware/ucs:1.0-34-g9eb2916],SizeBytes:32101483,},ContainerImage{Names:[localhost:5000/vmware/tmc-agent-installer@sha256:4d885d2a505ccc87baf5fb73958593f0b66ce00732397e935114f923367cd4ec localhost:5000/vmware/tmc-agent-installer:1.0],SizeBytes:30452862,},ContainerImage{Names:[localhost:5000/vmware/kube-rbac-proxy@sha256:ce03b0932d049e82747829680025a5d2e3c2a3f50ad27e761329d35e9e8949d5 localhost:5000/vmware/kube-rbac-proxy:v0.4.1_vmware.1 localhost:5000/vmware/kube-rbac-proxy:0.0.1],SizeBytes:29742750,},ContainerImage{Names:[localhost:5000/vmware/vmop@sha256:aa3d277bcb390735b0110d9bf3f3e63ab48db23437c9ac275ea487a2fa460dd5 localhost:5000/vmware/vmop:1.4-201-g3d42a58],SizeBytes:29542297,},ContainerImage{Names:[localhost:5000/vmware/tkgs-plugin@sha256:7160ccba94dbc3a68fdb31108319ca03163688bc5999c4da1ee20e32723ea612 localhost:5000/vmware/tkgs-plugin:prod],SizeBytes:27042759,},ContainerImage{Names:[localhost:5000/vmware/tkg-controller@sha256:700277052cbfe8be5e7a67e7819bde16ac0afceba352c473906dc3cc572cdaf9 localhost:5000/vmware/tkg-controller:1.4-196-g0e69a9e],SizeBytes:26884265,},ContainerImage{Names:[localhost:5000/vmware.io/fluent-bit@sha256:bc50adf05790809513d94105a169912371b4b5f656775350b1ab65b556ecdc32 localhost:5000/vmware.io/fluent-bit:v1.5.1_vmware.3],SizeBytes:26471308,},ContainerImage{Names:[localhost:5000/vmware/namespace-operator@sha256:fbb1177a60f33312e0415830fd55c5b565628a863a25cb500b2058fc475e8da4 localhost:5000/vmware/namespace-operator:c203fcd],SizeBytes:25804352,},ContainerImage{Names:[localhost:5000/vmware/license-operator@sha256:521924f17d102e95723a580ed084b6fbc15e3f317b982a81fd071718c3e84138 localhost:5000/vmware/license-operator:7f54d05],SizeBytes:25041264,},ContainerImage{Names:[localhost:5000/vmware/coredns@sha256:9df77eb5b9cd0f108d1ca4af8b7bfcd6a4074bfbd7a06a0d16e40bed46793502 localhost:5000/vmware/coredns:v1.19],SizeBytes:12884000,},ContainerImage{Names:[localhost:5000/vmware.io/csi-attacher@sha256:ef84a54cab084305c3b6638bcf3616539a1ece71054e519ce72c74a85ed6883c localhost:5000/vmware.io/csi-attacher:v3.2.1_vmware.1],SizeBytes:12847160,},ContainerImage{Names:[localhost:5000/vmware/kubernetes-csi_external-resizer/kubernetes-csi_external-resizer@sha256:eafb68e3367ac1b840b55b115351163cd24dfc1752e402c8d83210ab35d251eb localhost:5000/vmware/kubernetes-csi_external-resizer/kubernetes-csi_external-resizer:v1.2.0_vmware.1],SizeBytes:12843921,},ContainerImage{Names:[localhost:5000/vmware/csi-provisioner/csi-provisioner@sha256:f3783faf0e57904cde3702f7f0fb1f6e959c3160c90d399ea90931acd77abc18 localhost:5000/vmware/csi-provisioner/csi-provisioner:v2.1.0_vmware.4],SizeBytes:12501308,},ContainerImage{Names:[localhost:5000/vmware.io/csi-livenessprobe@sha256:7790ca0da41bfc8cd05c35c9309a36d060f847402147fddc32ca14063f9adc89 localhost:5000/vmware.io/csi-livenessprobe:v2.3.0_vmware.1],SizeBytes:5629758,},ContainerImage{Names:[docker.io/vmware/pause:1.19.12 docker.io/vmware/pause:1.20.8 docker.io/vmware/pause:1.21.0],SizeBytes:685866,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Sep  5 20:59:06.548: INFO: 
Logging kubelet events for node 42174da95d8c532b15b7283e9031a350
Sep  5 20:59:06.558: INFO: 
Logging pods the kubelet thinks is on node 42174da95d8c532b15b7283e9031a350
Sep  5 20:59:06.604: INFO: capi-webhook-69769f4c68-wkf2l started at 2021-09-05 19:36:48 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:06.604: INFO: 	Container manager ready: true, restart count 1
Sep  5 20:59:06.604: INFO: wcp-fip-42174da95d8c532b15b7283e9031a350 started at 2021-09-05 19:40:52 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container wcp-fip ready: true, restart count 0
Sep  5 20:59:06.604: INFO: etcd-42174da95d8c532b15b7283e9031a350 started at 2021-09-05 19:40:52 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container etcd ready: true, restart count 0
Sep  5 20:59:06.604: INFO: vmware-system-appplatform-operator-mgr-0 started at 2021-09-05 19:33:59 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:06.604: INFO: coredns-f5549f4bd-z75xc started at 2021-09-05 19:33:59 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container coredns ready: true, restart count 7
Sep  5 20:59:06.604: INFO: fluentbit-775kx started at 2021-09-05 19:33:59 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container fluentbit ready: true, restart count 0
Sep  5 20:59:06.604: INFO: nsx-ncp-6d7f7bf559-9mqvr started at 2021-09-05 19:33:59 -0700 PDT (1+1 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Init container nsx-ncp-upgrade ready: true, restart count 0
Sep  5 20:59:06.604: INFO: 	Container nsx-ncp ready: true, restart count 5
Sep  5 20:59:06.604: INFO: cert-manager-799b5bbfdf-p4z4n started at 2021-09-05 19:33:59 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container cert-manager ready: true, restart count 1
Sep  5 20:59:06.604: INFO: vmware-system-vmop-controller-manager-6649dd65b-hpvsz started at 2021-09-05 19:37:44 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:06.604: INFO: 	Container manager ready: true, restart count 5
Sep  5 20:59:06.604: INFO: vmware-system-nsop-controller-manager-6fcd64f5f8-sxrhx started at 2021-09-05 19:38:53 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container manager ready: true, restart count 3
Sep  5 20:59:06.604: INFO: capi-kubeadm-control-plane-webhook-5f4c87b8b9-tjwwb started at 2021-09-05 19:36:46 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:06.604: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:06.604: INFO: vmware-system-license-operator-controller-manager-8cd89d68xw886 started at 2021-09-05 19:38:29 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:06.604: INFO: capi-kubeadm-bootstrap-webhook-6766c687f9-mnx8s started at 2021-09-05 19:36:45 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:06.604: INFO: 	Container manager ready: true, restart count 1
Sep  5 20:59:06.604: INFO: vmware-system-tkg-controller-manager-575d95fb57-kpc65 started at 2021-09-05 19:37:59 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:06.604: INFO: 	Container manager ready: true, restart count 3
Sep  5 20:59:06.604: INFO: tmc-agent-installer-1630900740-9jz56 started at 2021-09-05 20:59:03 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container tmc-agent-installer ready: false, restart count 0
Sep  5 20:59:06.604: INFO: kubectl-plugin-vsphere-42174da95d8c532b15b7283e9031a350 started at 2021-09-05 19:31:40 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container kubectl-plugin-vsphere ready: true, restart count 3
Sep  5 20:59:06.604: INFO: capi-kubeadm-bootstrap-controller-manager-5f5774d559-nsw45 started at 2021-09-05 19:36:43 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:06.604: INFO: 	Container manager ready: true, restart count 2
Sep  5 20:59:06.604: INFO: masterproxy-tkgs-plugin-tbvb9 started at 2021-09-05 19:38:37 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container nginx ready: true, restart count 0
Sep  5 20:59:06.604: INFO: tkgs-plugin-server-57df5fcfbf-s6x9p started at 2021-09-05 19:38:16 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:06.604: INFO: wcp-authproxy-42174da95d8c532b15b7283e9031a350 started at 2021-09-05 19:40:52 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container wcp-authproxy ready: true, restart count 0
Sep  5 20:59:06.604: INFO: docker-registry-42174da95d8c532b15b7283e9031a350 started at 2021-09-05 19:40:52 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container docker-registry ready: true, restart count 0
Sep  5 20:59:06.604: INFO: kube-proxy-h7qwz started at 2021-09-05 19:33:59 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  5 20:59:06.604: INFO: image-controller-597bd95bc9-bnsfj started at 2021-09-05 19:33:59 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container image-controller ready: true, restart count 1
Sep  5 20:59:06.604: INFO: upgrade-compatibility-service-6849c664fd-6zgh2 started at 2021-09-05 19:33:59 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container ucs ready: true, restart count 0
Sep  5 20:59:06.604: INFO: capi-controller-manager-d586f4c8-6lfgm started at 2021-09-05 19:36:40 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:06.604: INFO: 	Container manager ready: true, restart count 2
Sep  5 20:59:06.604: INFO: capw-controller-manager-85b7cbb4bf-qkcm5 started at 2021-09-05 19:36:48 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:06.604: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:06.605: INFO: 	Container manager ready: true, restart count 4
Sep  5 20:59:06.605: INFO: kube-apiserver-42174da95d8c532b15b7283e9031a350 started at 2021-09-05 19:41:14 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.605: INFO: 	Container kube-apiserver ready: true, restart count 1
Sep  5 20:59:06.605: INFO: vsphere-csi-controller-7ff5f98858-57hcg started at 2021-09-05 19:38:24 -0700 PDT (0+6 container statuses recorded)
Sep  5 20:59:06.605: INFO: 	Container csi-attacher ready: true, restart count 2
Sep  5 20:59:06.605: INFO: 	Container csi-provisioner ready: true, restart count 2
Sep  5 20:59:06.605: INFO: 	Container csi-resizer ready: true, restart count 3
Sep  5 20:59:06.605: INFO: 	Container liveness-probe ready: true, restart count 0
Sep  5 20:59:06.605: INFO: 	Container vsphere-csi-controller ready: true, restart count 0
Sep  5 20:59:06.605: INFO: 	Container vsphere-syncer ready: true, restart count 3
Sep  5 20:59:06.605: INFO: kube-controller-manager-42174da95d8c532b15b7283e9031a350 started at 2021-09-05 19:31:40 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.605: INFO: 	Container kube-controller-manager ready: true, restart count 2
Sep  5 20:59:06.605: INFO: cert-manager-webhook-74488f47f-gbggg started at 2021-09-05 19:34:00 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.605: INFO: 	Container cert-manager ready: true, restart count 0
Sep  5 20:59:06.605: INFO: capw-webhook-58b86fb8b-x4w4j started at 2021-09-05 19:36:50 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:06.605: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:06.605: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:06.605: INFO: vmware-system-psp-operator-mgr-7888d487d7-lpn42 started at 2021-09-05 19:37:22 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.605: INFO: 	Container manager ready: true, restart count 7
Sep  5 20:59:06.605: INFO: kube-scheduler-42174da95d8c532b15b7283e9031a350 started at 2021-09-05 19:31:40 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:06.605: INFO: 	Container kube-scheduler ready: true, restart count 6
Sep  5 20:59:06.605: INFO: 	Container wcp-schedext ready: true, restart count 0
Sep  5 20:59:06.605: INFO: cert-manager-cainjector-69c886766f-6p68t started at 2021-09-05 19:34:00 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:06.605: INFO: 	Container cert-manager ready: true, restart count 3
Sep  5 20:59:06.605: INFO: capi-kubeadm-control-plane-controller-manager-7c8d4b456f-lpq8r started at 2021-09-05 19:36:46 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:06.605: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:06.605: INFO: 	Container manager ready: true, restart count 2
Sep  5 20:59:06.605: INFO: vmware-system-tkg-webhook-64fd4868cb-mfbg8 started at 2021-09-05 19:37:59 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:06.605: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:06.605: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:07.528: INFO: 
Latency metrics for node 42174da95d8c532b15b7283e9031a350
Sep  5 20:59:07.528: INFO: 
Logging node info for node 4217572e1dd2cd00e2d13546f91cde38
Sep  5 20:59:07.534: INFO: Node Info: &Node{ObjectMeta:{4217572e1dd2cd00e2d13546f91cde38   /api/v1/nodes/4217572e1dd2cd00e2d13546f91cde38 33832034-c6ff-4a6f-a2d7-d47d425fcf1f 60691 0 2021-09-05 19:41:02 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:4217572e1dd2cd00e2d13546f91cde38 kubernetes.io/os:linux node-role.kubernetes.io/master:] map[kubeadm.alpha.kubernetes.io/cri-socket:/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 vmware-system-workload-ip-configured: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubelet Update v1 2021-09-05 19:41:01 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}} {kubeadm Update v1 2021-09-05 19:41:43 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/master":{}}}}} {kubectl-annotate Update v1 2021-09-05 19:46:14 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-workload-ip-configured":{}}}}} {kube-controller-manager Update v1 2021-09-05 19:46:15 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:taints":{}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-09-05 20:58:30 -0700 PDT,LastTransitionTime:2021-09-05 19:40:55 -0700 PDT,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-09-05 20:58:30 -0700 PDT,LastTransitionTime:2021-09-05 19:40:55 -0700 PDT,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-09-05 20:58:30 -0700 PDT,LastTransitionTime:2021-09-05 19:40:55 -0700 PDT,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-05 20:58:30 -0700 PDT,LastTransitionTime:2021-09-05 19:46:35 -0700 PDT,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:172.26.0.4,},NodeAddress{Type:Hostname,Address:4217572e1dd2cd00e2d13546f91cde38,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:f1f672892c4c46e682259048153d610f,SystemUUID:2e571742-d21d-00cd-e2d1-3546f91cde38,BootID:10f4cb18-8acc-452a-8388-3bfd41df5205,KernelVersion:4.19.198-1.ph3-esx,OSImage:VMware Photon OS/Linux,ContainerRuntimeVersion:containerd://1.4.4,KubeletVersion:v1.19.12+vmware.wcp.1,KubeProxyVersion:v1.19.12+vmware.wcp.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[docker.io/vmware/wcp-authproxy:0.0.11.18508287],SizeBytes:131961954,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.21.0],SizeBytes:126851027,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.20.8],SizeBytes:123038003,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.21.0],SizeBytes:121071415,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.19.12],SizeBytes:120109363,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.20.8],SizeBytes:117573957,},ContainerImage{Names:[localhost:5000/vmware/kubectl-plugin-vsphere@sha256:50972d3c755506a39b060daf001e0fb4d7bfcc47aa059a945573d6962338a8f4 localhost:5000/vmware/kubectl-plugin-vsphere:0.0.11.18508287 docker.io/vmware/kubectl-plugin-vsphere:0.0.11.18508287],SizeBytes:114408041,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.19.12],SizeBytes:112073027,},ContainerImage{Names:[projects.registry.vmware.com/tkg/etcd:v3.4.13_vmware.1],SizeBytes:105113092,},ContainerImage{Names:[docker.io/vmware/wcp-schedext:0.0.11.18508287 docker.io/vmware/wcp-schedext:v1.19.12],SizeBytes:86647081,},ContainerImage{Names:[docker.io/vmware/docker-registry:2.7.0.14110537],SizeBytes:53040781,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.21.0],SizeBytes:51910456,},ContainerImage{Names:[docker.io/vmware/wcp-fip:0.1],SizeBytes:49119461,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.20.8],SizeBytes:48519474,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.19.12],SizeBytes:47737138,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-control-plane-controller@sha256:ae28d5c4008d5ad88db2936d9b45e0248d129245810239bbafe0c66a47f2bf7e localhost:5000/vmware/kubeadm-control-plane-controller:v0.3.17_vmware.1],SizeBytes:36065259,},ContainerImage{Names:[localhost:5002/vmware/kube-proxy:v1.19.12 localhost:5000/vmware/kube-proxy:active],SizeBytes:35229260,},ContainerImage{Names:[localhost:5000/vmware/capw-controller@sha256:56496b9339aba7d338f2f864f0bb946e1d57f50bbcffeee26c57c1a7be6e9cff localhost:5000/vmware/capw-controller:1.4.1-28-ge3e1d8b],SizeBytes:34792328,},ContainerImage{Names:[localhost:5000/vmware/cluster-api-controller@sha256:72a217e8de91b2e1adcbcb1266a5854e9683f0f37ea828808374563d2bf41861 localhost:5000/vmware/cluster-api-controller:v0.3.17_vmware.1],SizeBytes:34280507,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-bootstrap-controller@sha256:921fc3887b0cfaf4742eefaf2b6d60160fdc25d2600dc08e9a3389aed6fbdaca localhost:5000/vmware/kubeadm-bootstrap-controller:v0.3.17_vmware.1],SizeBytes:33862350,},ContainerImage{Names:[localhost:5000/vmware/ucs@sha256:0006afae8591e51b3ddfe6d1062caa779c2bee08dc20ee38eb8183614ccbdcd1 localhost:5000/vmware/ucs:1.0-34-g9eb2916],SizeBytes:32101483,},ContainerImage{Names:[localhost:5000/vmware/tmc-agent-installer@sha256:4d885d2a505ccc87baf5fb73958593f0b66ce00732397e935114f923367cd4ec localhost:5000/vmware/tmc-agent-installer:1.0],SizeBytes:30452862,},ContainerImage{Names:[localhost:5000/vmware/kube-rbac-proxy@sha256:ce03b0932d049e82747829680025a5d2e3c2a3f50ad27e761329d35e9e8949d5 localhost:5000/vmware/kube-rbac-proxy:v0.4.1_vmware.1 localhost:5000/vmware/kube-rbac-proxy:0.0.1],SizeBytes:29742750,},ContainerImage{Names:[localhost:5000/vmware/vmop@sha256:aa3d277bcb390735b0110d9bf3f3e63ab48db23437c9ac275ea487a2fa460dd5 localhost:5000/vmware/vmop:1.4-201-g3d42a58],SizeBytes:29542297,},ContainerImage{Names:[localhost:5000/vmware/tkgs-plugin@sha256:7160ccba94dbc3a68fdb31108319ca03163688bc5999c4da1ee20e32723ea612 localhost:5000/vmware/tkgs-plugin:prod],SizeBytes:27042759,},ContainerImage{Names:[localhost:5000/vmware/tkg-controller@sha256:700277052cbfe8be5e7a67e7819bde16ac0afceba352c473906dc3cc572cdaf9 localhost:5000/vmware/tkg-controller:1.4-196-g0e69a9e],SizeBytes:26884265,},ContainerImage{Names:[localhost:5000/vmware.io/fluent-bit@sha256:bc50adf05790809513d94105a169912371b4b5f656775350b1ab65b556ecdc32 localhost:5000/vmware.io/fluent-bit:v1.5.1_vmware.3],SizeBytes:26471308,},ContainerImage{Names:[localhost:5000/vmware/namespace-operator@sha256:fbb1177a60f33312e0415830fd55c5b565628a863a25cb500b2058fc475e8da4 localhost:5000/vmware/namespace-operator:c203fcd],SizeBytes:25804352,},ContainerImage{Names:[localhost:5000/vmware/license-operator@sha256:521924f17d102e95723a580ed084b6fbc15e3f317b982a81fd071718c3e84138 localhost:5000/vmware/license-operator:7f54d05],SizeBytes:25041264,},ContainerImage{Names:[localhost:5000/vmware/coredns@sha256:9df77eb5b9cd0f108d1ca4af8b7bfcd6a4074bfbd7a06a0d16e40bed46793502 localhost:5000/vmware/coredns:v1.19],SizeBytes:12884000,},ContainerImage{Names:[docker.io/vmware/pause:1.21.0 docker.io/vmware/pause:1.19.12 docker.io/vmware/pause:1.20.8],SizeBytes:685866,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Sep  5 20:59:07.534: INFO: 
Logging kubelet events for node 4217572e1dd2cd00e2d13546f91cde38
Sep  5 20:59:07.539: INFO: 
Logging pods the kubelet thinks is on node 4217572e1dd2cd00e2d13546f91cde38
Sep  5 20:59:07.582: INFO: capi-controller-manager-d586f4c8-dlqmp started at 2021-09-05 19:46:36 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:07.582: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:07.582: INFO: vmware-system-tkg-webhook-64fd4868cb-4dsnt started at 2021-09-05 19:46:36 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:07.582: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:07.582: INFO: fluentbit-ngvs5 started at 2021-09-05 19:41:50 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container fluentbit ready: true, restart count 0
Sep  5 20:59:07.582: INFO: wcp-fip-4217572e1dd2cd00e2d13546f91cde38 started at 2021-09-05 19:46:26 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container wcp-fip ready: true, restart count 0
Sep  5 20:59:07.582: INFO: capw-webhook-58b86fb8b-tws2d started at 2021-09-05 19:46:46 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:07.582: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:07.582: INFO: vmware-system-vmop-controller-manager-6649dd65b-9fvzw started at 2021-09-05 19:46:46 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:07.582: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:07.582: INFO: kubectl-plugin-vsphere-4217572e1dd2cd00e2d13546f91cde38 started at 2021-09-05 19:46:26 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container kubectl-plugin-vsphere ready: true, restart count 3
Sep  5 20:59:07.582: INFO: kube-controller-manager-4217572e1dd2cd00e2d13546f91cde38 started at 2021-09-05 19:46:26 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container kube-controller-manager ready: true, restart count 1
Sep  5 20:59:07.582: INFO: upgrade-compatibility-service-6849c664fd-4lsrz started at 2021-09-05 19:46:35 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container ucs ready: true, restart count 0
Sep  5 20:59:07.582: INFO: capi-webhook-69769f4c68-9z7gc started at 2021-09-05 19:46:36 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:07.582: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:07.582: INFO: kube-apiserver-4217572e1dd2cd00e2d13546f91cde38 started at 2021-09-05 19:46:26 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container kube-apiserver ready: true, restart count 2
Sep  5 20:59:07.582: INFO: etcd-4217572e1dd2cd00e2d13546f91cde38 started at 2021-09-05 19:46:26 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container etcd ready: true, restart count 0
Sep  5 20:59:07.582: INFO: tkgs-plugin-server-57df5fcfbf-dwlsw started at 2021-09-05 19:46:26 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:07.582: INFO: capi-kubeadm-bootstrap-controller-manager-5f5774d559-d67m9 started at 2021-09-05 19:46:36 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:07.582: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:07.582: INFO: vmware-system-tkg-controller-manager-575d95fb57-s56mg started at 2021-09-05 19:46:45 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:07.582: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:07.582: INFO: docker-registry-4217572e1dd2cd00e2d13546f91cde38 started at 2021-09-05 19:46:26 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container docker-registry ready: true, restart count 0
Sep  5 20:59:07.582: INFO: capi-kubeadm-control-plane-webhook-5f4c87b8b9-tn9d4 started at 2021-09-05 19:46:45 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:07.582: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:07.582: INFO: capi-kubeadm-bootstrap-webhook-6766c687f9-2fpss started at 2021-09-05 19:46:45 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:07.582: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:07.582: INFO: kube-scheduler-4217572e1dd2cd00e2d13546f91cde38 started at 2021-09-05 19:46:26 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container kube-scheduler ready: true, restart count 5
Sep  5 20:59:07.582: INFO: 	Container wcp-schedext ready: true, restart count 0
Sep  5 20:59:07.582: INFO: vmware-system-license-operator-controller-manager-8cd89d68bz2kx started at 2021-09-05 19:46:47 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:07.582: INFO: kube-proxy-rr54q started at 2021-09-05 19:46:36 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  5 20:59:07.582: INFO: capi-kubeadm-control-plane-controller-manager-7c8d4b456f-w66r8 started at 2021-09-05 19:46:36 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:07.582: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:07.582: INFO: capw-controller-manager-85b7cbb4bf-stxvw started at 2021-09-05 19:46:36 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:07.582: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:07.582: INFO: vmware-system-nsop-controller-manager-6fcd64f5f8-2fkjs started at 2021-09-05 19:46:36 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:07.582: INFO: masterproxy-tkgs-plugin-w42xg started at 2021-09-05 19:46:26 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container nginx ready: true, restart count 0
Sep  5 20:59:07.582: INFO: coredns-f5549f4bd-bfl7g started at 2021-09-05 19:46:45 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container coredns ready: true, restart count 0
Sep  5 20:59:07.582: INFO: wcp-authproxy-4217572e1dd2cd00e2d13546f91cde38 started at 2021-09-05 19:46:26 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:07.582: INFO: 	Container wcp-authproxy ready: true, restart count 0
Sep  5 20:59:08.145: INFO: 
Latency metrics for node 4217572e1dd2cd00e2d13546f91cde38
Sep  5 20:59:08.145: INFO: 
Logging node info for node 4217cb448371cdb65f43561dccee265e
Sep  5 20:59:08.155: INFO: Node Info: &Node{ObjectMeta:{4217cb448371cdb65f43561dccee265e   /api/v1/nodes/4217cb448371cdb65f43561dccee265e 29d721d9-d86b-4e91-9af8-29d5f88abb63 58107 0 2021-09-05 19:41:02 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:linux kubernetes.io/arch:amd64 kubernetes.io/hostname:4217cb448371cdb65f43561dccee265e kubernetes.io/os:linux node-role.kubernetes.io/master:] map[kubeadm.alpha.kubernetes.io/cri-socket:/run/containerd/containerd.sock node.alpha.kubernetes.io/ttl:0 vmware-system-workload-ip-configured: volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kubeadm Update v1 2021-09-05 19:41:37 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:kubeadm.alpha.kubernetes.io/cri-socket":{}},"f:labels":{"f:node-role.kubernetes.io/master":{}}}}} {kubectl-annotate Update v1 2021-09-05 19:45:29 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-workload-ip-configured":{}}}}} {kube-controller-manager Update v1 2021-09-05 19:50:05 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}}},"f:spec":{"f:taints":{}}}} {kubelet Update v1 2021-09-05 19:50:13 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{".":{},"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:ephemeral-storage":{},"f:hugepages-1Gi":{},"f:hugepages-2Mi":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"DiskPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"MemoryPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"PIDPressure\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:images":{},"f:nodeInfo":{"f:architecture":{},"f:bootID":{},"f:containerRuntimeVersion":{},"f:kernelVersion":{},"f:kubeProxyVersion":{},"f:kubeletVersion":{},"f:machineID":{},"f:operatingSystem":{},"f:osImage":{},"f:systemUUID":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{Taint{Key:node-role.kubernetes.io/master,Value:,Effect:NoSchedule,TimeAdded:<nil>,},},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Allocatable:ResourceList{cpu: {{2 0} {<nil>} 2 DecimalSI},ephemeral-storage: {{33670557696 0} {<nil>} 32881404Ki BinarySI},hugepages-1Gi: {{0 0} {<nil>} 0 DecimalSI},hugepages-2Mi: {{0 0} {<nil>} 0 DecimalSI},memory: {{8332275712 0} {<nil>} 8136988Ki BinarySI},pods: {{110 0} {<nil>} 110 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2021-09-05 20:55:17 -0700 PDT,LastTransitionTime:2021-09-05 19:50:13 -0700 PDT,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,},NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2021-09-05 20:55:17 -0700 PDT,LastTransitionTime:2021-09-05 19:50:13 -0700 PDT,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,},NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2021-09-05 20:55:17 -0700 PDT,LastTransitionTime:2021-09-05 19:50:13 -0700 PDT,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,},NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-05 20:55:17 -0700 PDT,LastTransitionTime:2021-09-05 19:50:13 -0700 PDT,Reason:KubeletReady,Message:kubelet is posting ready status,},},Addresses:[]NodeAddress{NodeAddress{Type:InternalIP,Address:172.26.0.3,},NodeAddress{Type:Hostname,Address:4217cb448371cdb65f43561dccee265e,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:3ab8336ae365472e8d043c749fe0e778,SystemUUID:44cb1742-7183-b6cd-5f43-561dccee265e,BootID:8a5b99c0-7073-41bf-96ca-380063ead145,KernelVersion:4.19.198-1.ph3-esx,OSImage:VMware Photon OS/Linux,ContainerRuntimeVersion:containerd://1.4.4,KubeletVersion:v1.19.12+vmware.wcp.1,KubeProxyVersion:v1.19.12+vmware.wcp.1,OperatingSystem:linux,Architecture:amd64,},Images:[]ContainerImage{ContainerImage{Names:[docker.io/vmware/wcp-authproxy:0.0.11.18508287],SizeBytes:131961954,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.21.0],SizeBytes:126851027,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.20.8],SizeBytes:123038003,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.21.0],SizeBytes:121071415,},ContainerImage{Names:[docker.io/vmware/kube-apiserver:v1.19.12],SizeBytes:120109363,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.20.8],SizeBytes:117573957,},ContainerImage{Names:[localhost:5000/vmware/kubectl-plugin-vsphere@sha256:50972d3c755506a39b060daf001e0fb4d7bfcc47aa059a945573d6962338a8f4 localhost:5000/vmware/kubectl-plugin-vsphere:0.0.11.18508287 docker.io/vmware/kubectl-plugin-vsphere:0.0.11.18508287],SizeBytes:114408041,},ContainerImage{Names:[docker.io/vmware/kube-controller-manager:v1.19.12],SizeBytes:112073027,},ContainerImage{Names:[projects.registry.vmware.com/tkg/etcd:v3.4.13_vmware.1],SizeBytes:105113092,},ContainerImage{Names:[docker.io/vmware/wcp-schedext:0.0.11.18508287 docker.io/vmware/wcp-schedext:v1.19.12],SizeBytes:86647081,},ContainerImage{Names:[localhost:5000/vmware/registry-agent@sha256:a02a9f68366f57abdd0833d34f1394e402a519d3fe82ff8c1106c67fc7c80392 localhost:5000/vmware/registry-agent:0.0.11.18508287],SizeBytes:76983788,},ContainerImage{Names:[docker.io/vmware/docker-registry:2.7.0.14110537],SizeBytes:53040781,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.21.0],SizeBytes:51910456,},ContainerImage{Names:[docker.io/vmware/wcp-fip:0.1],SizeBytes:49119461,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.20.8],SizeBytes:48519474,},ContainerImage{Names:[docker.io/vmware/kube-scheduler:v1.19.12],SizeBytes:47737138,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-control-plane-controller@sha256:ae28d5c4008d5ad88db2936d9b45e0248d129245810239bbafe0c66a47f2bf7e localhost:5000/vmware/kubeadm-control-plane-controller:v0.3.17_vmware.1],SizeBytes:36065259,},ContainerImage{Names:[localhost:5002/vmware/kube-proxy:v1.19.12 localhost:5000/vmware/kube-proxy:active],SizeBytes:35229260,},ContainerImage{Names:[localhost:5000/vmware/capw-controller@sha256:56496b9339aba7d338f2f864f0bb946e1d57f50bbcffeee26c57c1a7be6e9cff localhost:5000/vmware/capw-controller:1.4.1-28-ge3e1d8b],SizeBytes:34792328,},ContainerImage{Names:[localhost:5000/vmware/cluster-api-controller@sha256:72a217e8de91b2e1adcbcb1266a5854e9683f0f37ea828808374563d2bf41861 localhost:5000/vmware/cluster-api-controller:v0.3.17_vmware.1],SizeBytes:34280507,},ContainerImage{Names:[localhost:5000/vmware/kubeadm-bootstrap-controller@sha256:921fc3887b0cfaf4742eefaf2b6d60160fdc25d2600dc08e9a3389aed6fbdaca localhost:5000/vmware/kubeadm-bootstrap-controller:v0.3.17_vmware.1],SizeBytes:33862350,},ContainerImage{Names:[localhost:5000/vmware/ucs@sha256:0006afae8591e51b3ddfe6d1062caa779c2bee08dc20ee38eb8183614ccbdcd1 localhost:5000/vmware/ucs:1.0-34-g9eb2916],SizeBytes:32101483,},ContainerImage{Names:[localhost:5000/vmware/tmc-agent-installer@sha256:4d885d2a505ccc87baf5fb73958593f0b66ce00732397e935114f923367cd4ec localhost:5000/vmware/tmc-agent-installer:1.0],SizeBytes:30452862,},ContainerImage{Names:[localhost:5000/vmware/kube-rbac-proxy@sha256:ce03b0932d049e82747829680025a5d2e3c2a3f50ad27e761329d35e9e8949d5 localhost:5000/vmware/kube-rbac-proxy:0.0.1 localhost:5000/vmware/kube-rbac-proxy:v0.4.1_vmware.1],SizeBytes:29742750,},ContainerImage{Names:[localhost:5000/vmware/vmop@sha256:aa3d277bcb390735b0110d9bf3f3e63ab48db23437c9ac275ea487a2fa460dd5 localhost:5000/vmware/vmop:1.4-201-g3d42a58],SizeBytes:29542297,},ContainerImage{Names:[localhost:5000/vmware/tkgs-plugin@sha256:7160ccba94dbc3a68fdb31108319ca03163688bc5999c4da1ee20e32723ea612 localhost:5000/vmware/tkgs-plugin:prod],SizeBytes:27042759,},ContainerImage{Names:[localhost:5000/vmware/tkg-controller@sha256:700277052cbfe8be5e7a67e7819bde16ac0afceba352c473906dc3cc572cdaf9 localhost:5000/vmware/tkg-controller:1.4-196-g0e69a9e],SizeBytes:26884265,},ContainerImage{Names:[localhost:5000/vmware.io/fluent-bit@sha256:bc50adf05790809513d94105a169912371b4b5f656775350b1ab65b556ecdc32 localhost:5000/vmware.io/fluent-bit:v1.5.1_vmware.3],SizeBytes:26471308,},ContainerImage{Names:[localhost:5000/vmware/namespace-operator@sha256:fbb1177a60f33312e0415830fd55c5b565628a863a25cb500b2058fc475e8da4 localhost:5000/vmware/namespace-operator:c203fcd],SizeBytes:25804352,},ContainerImage{Names:[localhost:5000/vmware/license-operator@sha256:521924f17d102e95723a580ed084b6fbc15e3f317b982a81fd071718c3e84138 localhost:5000/vmware/license-operator:7f54d05],SizeBytes:25041264,},ContainerImage{Names:[localhost:5000/vmware/coredns@sha256:9df77eb5b9cd0f108d1ca4af8b7bfcd6a4074bfbd7a06a0d16e40bed46793502 localhost:5000/vmware/coredns:v1.19],SizeBytes:12884000,},ContainerImage{Names:[docker.io/vmware/pause:1.19.12 docker.io/vmware/pause:1.20.8 docker.io/vmware/pause:1.21.0],SizeBytes:685866,},},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Sep  5 20:59:08.155: INFO: 
Logging kubelet events for node 4217cb448371cdb65f43561dccee265e
Sep  5 20:59:08.165: INFO: 
Logging pods the kubelet thinks is on node 4217cb448371cdb65f43561dccee265e
Sep  5 20:59:08.229: INFO: fluentbit-n8vtg started at 2021-09-05 19:41:51 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container fluentbit ready: true, restart count 0
Sep  5 20:59:08.229: INFO: tmc-agent-installer-1630900680-mnbzz started at 2021-09-05 20:58:03 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container tmc-agent-installer ready: false, restart count 0
Sep  5 20:59:08.229: INFO: tkgs-plugin-server-57df5fcfbf-hbxzq started at 2021-09-05 19:45:41 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:08.229: INFO: capw-webhook-58b86fb8b-wfkkq started at 2021-09-05 19:45:51 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:08.229: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:08.229: INFO: kube-controller-manager-4217cb448371cdb65f43561dccee265e started at 2021-09-05 19:41:24 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container kube-controller-manager ready: true, restart count 0
Sep  5 20:59:08.229: INFO: capi-kubeadm-control-plane-webhook-5f4c87b8b9-tzv6f started at 2021-09-05 19:45:51 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:08.229: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:08.229: INFO: capi-kubeadm-bootstrap-controller-manager-5f5774d559-892lm started at 2021-09-05 19:46:00 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:08.229: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:08.229: INFO: vmware-system-license-operator-controller-manager-8cd89d68srs4j started at 2021-09-05 19:45:51 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:08.229: INFO: capi-kubeadm-control-plane-controller-manager-7c8d4b456f-cwgsq started at 2021-09-05 19:46:01 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:08.229: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:08.229: INFO: coredns-f5549f4bd-hcc4d started at 2021-09-05 19:45:51 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container coredns ready: true, restart count 0
Sep  5 20:59:08.229: INFO: capi-controller-manager-d586f4c8-26p6g started at 2021-09-05 19:45:51 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:08.229: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:08.229: INFO: vmware-system-vmop-controller-manager-6649dd65b-qhl2f started at 2021-09-05 19:45:51 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:08.229: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:08.229: INFO: capw-controller-manager-85b7cbb4bf-mq4c7 started at 2021-09-05 19:45:51 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:08.229: INFO: 	Container manager ready: true, restart count 1
Sep  5 20:59:08.229: INFO: kube-scheduler-4217cb448371cdb65f43561dccee265e started at 2021-09-05 19:45:41 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container kube-scheduler ready: true, restart count 1
Sep  5 20:59:08.229: INFO: 	Container wcp-schedext ready: true, restart count 0
Sep  5 20:59:08.229: INFO: wcp-authproxy-4217cb448371cdb65f43561dccee265e started at 2021-09-05 19:45:41 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container wcp-authproxy ready: true, restart count 0
Sep  5 20:59:08.229: INFO: wcp-fip-4217cb448371cdb65f43561dccee265e started at 2021-09-05 19:45:41 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container wcp-fip ready: true, restart count 0
Sep  5 20:59:08.229: INFO: docker-registry-4217cb448371cdb65f43561dccee265e started at 2021-09-05 19:45:41 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container docker-registry ready: true, restart count 0
Sep  5 20:59:08.229: INFO: capi-kubeadm-bootstrap-webhook-6766c687f9-md7gc started at 2021-09-05 19:45:52 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:08.229: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:08.229: INFO: vmware-registry-controller-manager-75fff77685-jrq2f started at 2021-09-05 19:46:02 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container admin-agent ready: true, restart count 0
Sep  5 20:59:08.229: INFO: 	Container service-agent ready: true, restart count 4
Sep  5 20:59:08.229: INFO: upgrade-compatibility-service-6849c664fd-cqhl8 started at 2021-09-05 19:45:51 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container ucs ready: true, restart count 0
Sep  5 20:59:08.229: INFO: etcd-4217cb448371cdb65f43561dccee265e started at 2021-09-05 19:45:41 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container etcd ready: true, restart count 0
Sep  5 20:59:08.229: INFO: masterproxy-tkgs-plugin-bpmwc started at 2021-09-05 19:45:41 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container nginx ready: true, restart count 0
Sep  5 20:59:08.229: INFO: kube-proxy-wnlg6 started at 2021-09-05 19:45:51 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container kube-proxy ready: true, restart count 0
Sep  5 20:59:08.229: INFO: vmware-system-nsop-controller-manager-6fcd64f5f8-rs2gz started at 2021-09-05 19:45:51 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:08.229: INFO: vmware-system-tkg-controller-manager-575d95fb57-qlncw started at 2021-09-05 19:46:00 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:08.229: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:08.229: INFO: kubectl-plugin-vsphere-4217cb448371cdb65f43561dccee265e started at 2021-09-05 19:45:41 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container kubectl-plugin-vsphere ready: true, restart count 2
Sep  5 20:59:08.229: INFO: kube-apiserver-4217cb448371cdb65f43561dccee265e started at 2021-09-05 19:45:41 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container kube-apiserver ready: true, restart count 3
Sep  5 20:59:08.229: INFO: capi-webhook-69769f4c68-g7hrx started at 2021-09-05 19:45:51 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:08.229: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:08.229: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:08.229: INFO: vmware-system-tkg-webhook-64fd4868cb-pws4c started at 2021-09-05 19:46:00 -0700 PDT (0+2 container statuses recorded)
Sep  5 20:59:08.230: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Sep  5 20:59:08.230: INFO: 	Container manager ready: true, restart count 0
Sep  5 20:59:08.881: INFO: 
Latency metrics for node 4217cb448371cdb65f43561dccee265e
Sep  5 20:59:08.881: INFO: 
Logging node info for node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
Sep  5 20:59:08.889: INFO: Node Info: &Node{ObjectMeta:{sc2-rdops-vm09-dhcp-34-149.eng.vmware.com   /api/v1/nodes/sc2-rdops-vm09-dhcp-34-149.eng.vmware.com e911557b-8705-4f96-b43a-3522d859383b 61178 0 2021-09-05 19:43:11 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:CRX kubernetes.io/arch:amd64 kubernetes.io/hostname:sc2-rdops-vm09-dhcp-34-149.eng.vmware.com kubernetes.io/os:CRX node-role.kubernetes.io/agent:agent node.kubernetes.io/role:agent type:virtual-kubelet] map[ncp/transport_node:3f2eecab-c186-4db0-a4d7-357e9b3bb692 node.alpha.kubernetes.io/ttl:0 vmware-system-esxi-node-moid:host-16 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{wcpsvc Update v1 2021-09-05 19:43:11 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-esxi-node-moid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{"f:node-role.kubernetes.io/agent":{}}}}} {spherelet Update v1 2021-09-05 19:43:30 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:ncp/transport_node":{}},"f:labels":{".":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node.kubernetes.io/role":{},"f:type":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:nodeInfo":{"f:architecture":{},"f:kubeletVersion":{},"f:operatingSystem":{}}}}} {kube-controller-manager Update v1 2021-09-05 20:05:54 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}},"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:status":{"f:volumesAttached":{}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{12 0} {<nil>} 12 DecimalSI},memory: {{34359136256 0} {<nil>} 33553844Ki BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Allocatable:ResourceList{cpu: {{10 0} {<nil>} 10 DecimalSI},memory: {{5357174784 0} {<nil>} 5109Mi BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-05 20:59:04 -0700 PDT,LastTransitionTime:2021-09-05 20:59:04 -0700 PDT,Reason:KubeletReady,Message:Spherelet is ready.,},},Addresses:[]NodeAddress{NodeAddress{Type:Hostname,Address:sc2-rdops-vm09-dhcp-34-149.eng.vmware.com,},NodeAddress{Type:InternalIP,Address:10.193.34.149,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:,SystemUUID:,BootID:,KernelVersion:,OSImage:,ContainerRuntimeVersion:,KubeletVersion:v1.19.1-sph-b0161d9,KubeProxyVersion:,OperatingSystem:ESXi,Architecture:amd64,},Images:[]ContainerImage{},VolumesInUse:[],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/csi/csi.vsphere.vmware.com^77be9789-d0cc-4d7e-bb36-7cb6c1263606,DevicePath:,},},Config:nil,},}
Sep  5 20:59:08.889: INFO: 
Logging kubelet events for node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
Sep  5 20:59:08.897: INFO: 
Logging pods the kubelet thinks is on node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
Sep  5 20:59:08.934: INFO: helloworld started at 2021-09-05 20:09:22 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.934: INFO: 	Container hello ready: true, restart count 0
Sep  5 20:59:08.934: INFO: curl-pod started at 2021-09-05 20:14:04 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.934: INFO: 	Container curl-container ready: true, restart count 0
Sep  5 20:59:08.934: INFO: busybox-readonly-false-5d795003-2c74-48cf-892d-014f44f21052 started at 2021-09-05 20:59:00 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.934: INFO: 	Container busybox-readonly-false-5d795003-2c74-48cf-892d-014f44f21052 ready: true, restart count 0
Sep  5 20:59:08.934: INFO: nginx-private started at 2021-09-05 19:59:11 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.934: INFO: 	Container nginx-private-container ready: true, restart count 0
Sep  5 20:59:08.934: INFO: schedext-test-node-selector-1 started at 2021-09-05 20:00:20 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.934: INFO: 	Container hello ready: true, restart count 0
Sep  5 20:59:08.934: INFO: podwithpersistentvolume started at 2021-09-05 20:05:57 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.934: INFO: 	Container hello ready: true, restart count 0
Sep  5 20:59:08.934: INFO: wcp-sanity-busybox-6f999d6849-46njm started at 2021-09-05 20:07:43 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.934: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 20:59:08.934: INFO: hello-web-6b97664bd5-f5452 started at 2021-09-05 20:10:51 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.934: INFO: 	Container hello-app ready: true, restart count 0
Sep  5 20:59:08.934: INFO: busybox started at 2021-09-05 20:15:25 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.934: INFO: 	Container busybox ready: true, restart count 0
Sep  5 20:59:08.934: INFO: wcp-sanity-busybox-6f999d6849-45jct started at 2021-09-05 19:57:27 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.934: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 20:59:08.934: INFO: busybox-annotation started at 2021-09-05 20:03:14 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:08.934: INFO: 	Container busybox-annotation ready: true, restart count 0
Sep  5 20:59:08.970: INFO: 
Latency metrics for node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
Sep  5 20:59:08.970: INFO: 
Logging node info for node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com
Sep  5 20:59:08.977: INFO: Node Info: &Node{ObjectMeta:{sc2-rdops-vm09-dhcp-39-55.eng.vmware.com   /api/v1/nodes/sc2-rdops-vm09-dhcp-39-55.eng.vmware.com f3619b8a-4580-44b4-a376-dbfef07612de 61156 0 2021-09-05 19:43:30 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:CRX kubernetes.io/arch:amd64 kubernetes.io/hostname:sc2-rdops-vm09-dhcp-39-55.eng.vmware.com kubernetes.io/os:CRX node-role.kubernetes.io/agent:agent node.kubernetes.io/role:agent type:virtual-kubelet] map[ncp/transport_node:ae4bd85c-af6b-4b32-aff9-91354cedf7d8 node.alpha.kubernetes.io/ttl:0 vmware-system-esxi-node-moid:host-13 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{kube-controller-manager Update v1 2021-09-05 19:43:30 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}},"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}}}} {spherelet Update v1 2021-09-05 19:43:30 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:ncp/transport_node":{}},"f:labels":{".":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node.kubernetes.io/role":{},"f:type":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:nodeInfo":{"f:architecture":{},"f:kubeletVersion":{},"f:operatingSystem":{}}}}} {wcpsvc Update v1 2021-09-05 19:43:30 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-esxi-node-moid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{"f:node-role.kubernetes.io/agent":{}}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{12 0} {<nil>} 12 DecimalSI},memory: {{34359136256 0} {<nil>} 33553844Ki BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Allocatable:ResourceList{cpu: {{9 0} {<nil>} 9 DecimalSI},memory: {{5298454528 0} {<nil>} 5053Mi BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-05 20:59:02 -0700 PDT,LastTransitionTime:2021-09-05 20:59:02 -0700 PDT,Reason:KubeletReady,Message:Spherelet is ready.,},},Addresses:[]NodeAddress{NodeAddress{Type:Hostname,Address:sc2-rdops-vm09-dhcp-39-55.eng.vmware.com,},NodeAddress{Type:InternalIP,Address:10.193.39.55,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:,SystemUUID:,BootID:,KernelVersion:,OSImage:,ContainerRuntimeVersion:,KubeletVersion:v1.19.1-sph-b0161d9,KubeProxyVersion:,OperatingSystem:ESXi,Architecture:amd64,},Images:[]ContainerImage{},VolumesInUse:[],VolumesAttached:[]AttachedVolume{},Config:nil,},}
Sep  5 20:59:08.978: INFO: 
Logging kubelet events for node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com
Sep  5 20:59:08.986: INFO: 
Logging pods the kubelet thinks is on node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com
Sep  5 20:59:09.012: INFO: test-docker-registry started at 2021-09-05 20:16:11 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:09.012: INFO: 	Container test-docker-registry ready: true, restart count 0
Sep  5 20:59:09.012: INFO: helloworld started at 2021-09-05 19:58:27 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:09.012: INFO: 	Container hello ready: true, restart count 0
Sep  5 20:59:09.012: INFO: schedext-test-node-selector-2 started at 2021-09-05 20:00:22 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:09.012: INFO: 	Container hello ready: true, restart count 0
Sep  5 20:59:09.012: INFO: schedext-test-affinity-1 started at 2021-09-05 20:00:51 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:09.012: INFO: 	Container hello ready: true, restart count 0
Sep  5 20:59:09.012: INFO: schedext-test-affinity-2 started at 2021-09-05 20:01:16 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:09.012: INFO: 	Container hello ready: true, restart count 0
Sep  5 20:59:09.012: INFO: helloworld started at 2021-09-05 20:08:34 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:09.012: INFO: 	Container hello ready: true, restart count 0
Sep  5 20:59:09.012: INFO: curl-pod started at 2021-09-05 20:10:11 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:09.012: INFO: 	Container curl-container ready: true, restart count 0
Sep  5 20:59:09.071: INFO: 
Latency metrics for node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com
Sep  5 20:59:09.071: INFO: 
Logging node info for node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com
Sep  5 20:59:09.085: INFO: Node Info: &Node{ObjectMeta:{sc2-rdops-vm09-dhcp-43-208.eng.vmware.com   /api/v1/nodes/sc2-rdops-vm09-dhcp-43-208.eng.vmware.com 0e182747-dbb0-4ad1-9463-927c8d306bc0 61236 0 2021-09-05 19:43:08 -0700 PDT <nil> <nil> map[beta.kubernetes.io/arch:amd64 beta.kubernetes.io/os:CRX kubernetes.io/arch:amd64 kubernetes.io/hostname:sc2-rdops-vm09-dhcp-43-208.eng.vmware.com kubernetes.io/os:CRX node-role.kubernetes.io/agent:agent node.kubernetes.io/role:agent type:virtual-kubelet] map[ncp/transport_node:c20af614-7327-4d68-845d-50ed51a247c3 node.alpha.kubernetes.io/ttl:0 vmware-system-esxi-node-moid:host-18 volumes.kubernetes.io/controller-managed-attach-detach:true] [] []  [{wcpsvc Update v1 2021-09-05 19:43:08 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-esxi-node-moid":{},"f:volumes.kubernetes.io/controller-managed-attach-detach":{}},"f:labels":{"f:node-role.kubernetes.io/agent":{}}}}} {spherelet Update v1 2021-09-05 19:43:27 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:ncp/transport_node":{}},"f:labels":{".":{},"f:kubernetes.io/arch":{},"f:kubernetes.io/hostname":{},"f:kubernetes.io/os":{},"f:node.kubernetes.io/role":{},"f:type":{}}},"f:status":{"f:addresses":{".":{},"k:{\"type\":\"Hostname\"}":{".":{},"f:address":{},"f:type":{}},"k:{\"type\":\"InternalIP\"}":{".":{},"f:address":{},"f:type":{}}},"f:allocatable":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:capacity":{".":{},"f:cpu":{},"f:memory":{},"f:pods":{}},"f:conditions":{".":{},"k:{\"type\":\"Ready\"}":{".":{},"f:lastHeartbeatTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:daemonEndpoints":{"f:kubeletEndpoint":{"f:Port":{}}},"f:nodeInfo":{"f:architecture":{},"f:kubeletVersion":{},"f:operatingSystem":{}}}}} {kube-controller-manager Update v1 2021-09-05 20:06:40 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:node.alpha.kubernetes.io/ttl":{}},"f:labels":{"f:beta.kubernetes.io/arch":{},"f:beta.kubernetes.io/os":{}}},"f:status":{"f:volumesAttached":{}}}}]},Spec:NodeSpec{PodCIDR:,DoNotUseExternalID:,ProviderID:,Unschedulable:false,Taints:[]Taint{},ConfigSource:nil,PodCIDRs:[],},Status:NodeStatus{Capacity:ResourceList{cpu: {{12 0} {<nil>} 12 DecimalSI},memory: {{34359136256 0} {<nil>} 33553844Ki BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Allocatable:ResourceList{cpu: {{10 0} {<nil>} 10 DecimalSI},memory: {{4711251968 0} {<nil>} 4493Mi BinarySI},pods: {{168 0} {<nil>} 168 DecimalSI},},Phase:,Conditions:[]NodeCondition{NodeCondition{Type:Ready,Status:True,LastHeartbeatTime:2021-09-05 20:59:07 -0700 PDT,LastTransitionTime:2021-09-05 20:59:07 -0700 PDT,Reason:KubeletReady,Message:Spherelet is ready.,},},Addresses:[]NodeAddress{NodeAddress{Type:Hostname,Address:sc2-rdops-vm09-dhcp-43-208.eng.vmware.com,},NodeAddress{Type:InternalIP,Address:10.193.43.208,},},DaemonEndpoints:NodeDaemonEndpoints{KubeletEndpoint:DaemonEndpoint{Port:10250,},},NodeInfo:NodeSystemInfo{MachineID:,SystemUUID:,BootID:,KernelVersion:,OSImage:,ContainerRuntimeVersion:,KubeletVersion:v1.19.1-sph-b0161d9,KubeProxyVersion:,OperatingSystem:ESXi,Architecture:amd64,},Images:[]ContainerImage{},VolumesInUse:[],VolumesAttached:[]AttachedVolume{AttachedVolume{Name:kubernetes.io/csi/csi.vsphere.vmware.com^38a5f915-941e-4d30-8ccd-447c4b63aa1a,DevicePath:,},},Config:nil,},}
Sep  5 20:59:09.086: INFO: 
Logging kubelet events for node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com
Sep  5 20:59:09.095: INFO: 
Logging pods the kubelet thinks is on node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com
Sep  5 20:59:09.125: INFO: podwithpersistentvolume started at 2021-09-05 20:06:44 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:09.125: INFO: 	Container hello ready: true, restart count 0
Sep  5 20:59:09.125: INFO: wcp-sanity-busybox-6f999d6849-p2kf7 started at 2021-09-05 20:07:40 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:09.125: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 20:59:09.125: INFO: hello-web-2-f779cbdff-sdrnx started at 2021-09-05 20:13:39 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:09.126: INFO: 	Container hello-app ready: true, restart count 0
Sep  5 20:59:09.126: INFO: wcp-sanity-busybox-6f999d6849-75q27 started at 2021-09-05 19:57:25 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:09.126: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 20:59:09.126: INFO: helloworld started at 2021-09-05 19:59:45 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:09.126: INFO: 	Container hello ready: true, restart count 0
Sep  5 20:59:09.126: INFO: schedext-test-affinity-3 started at 2021-09-05 20:01:16 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:09.126: INFO: 	Container hello ready: true, restart count 0
Sep  5 20:59:09.126: INFO: wcp-sanity-busybox-6f999d6849-nn6pn started at 2021-09-05 20:07:38 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:09.126: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 20:59:09.126: INFO: hello-web-1-6b97664bd5-g8z8h started at 2021-09-05 20:13:39 -0700 PDT (0+1 container statuses recorded)
Sep  5 20:59:09.126: INFO: 	Container hello-app ready: true, restart count 0
Sep  5 20:59:09.177: INFO: 
Latency metrics for node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com
Sep  5 20:59:09.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8831" for this suite.

â€¢ Failure [3.973 seconds]
[sig-cli] Kubectl client
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1079
    should check if Kubernetes master services is included in cluster-info  [Conformance] [It]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597

    Sep  5 20:59:06.505: Missing Kubernetes master in kubectl cluster-info

    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1092
------------------------------
{"msg":"FAILED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]","total":291,"completed":15,"skipped":361,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 20:59:09.358: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4123
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Sep  5 20:59:09.806: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:01:27.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4123" for this suite.

â€¢ [SLOW TEST:138.542 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","total":291,"completed":16,"skipped":368,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:01:27.901: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7232
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  5 21:01:28.319: INFO: Waiting up to 5m0s for pod "downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b" in namespace "downward-api-7232" to be "Succeeded or Failed"
Sep  5 21:01:28.327: INFO: Pod "downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.041566ms
Sep  5 21:01:30.344: INFO: Pod "downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024569778s
Sep  5 21:01:32.355: INFO: Pod "downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036218157s
Sep  5 21:01:34.364: INFO: Pod "downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044768294s
Sep  5 21:01:36.378: INFO: Pod "downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.058560834s
Sep  5 21:01:38.387: INFO: Pod "downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.067898579s
Sep  5 21:01:40.401: INFO: Pod "downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.081951312s
Sep  5 21:01:42.409: INFO: Pod "downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.090296816s
Sep  5 21:01:44.417: INFO: Pod "downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.097688734s
Sep  5 21:01:46.423: INFO: Pod "downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.104394595s
Sep  5 21:01:48.431: INFO: Pod "downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.112425538s
Sep  5 21:01:50.442: INFO: Pod "downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.123018949s
Sep  5 21:01:52.459: INFO: Pod "downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.139798133s
STEP: Saw pod success
Sep  5 21:01:52.459: INFO: Pod "downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b" satisfied condition "Succeeded or Failed"
Sep  5 21:01:52.465: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com pod downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b container client-container: <nil>
STEP: delete the pod
Sep  5 21:01:56.887: INFO: Waiting for pod downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b to disappear
Sep  5 21:01:56.897: INFO: Pod downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b still exists
Sep  5 21:01:58.898: INFO: Waiting for pod downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b to disappear
Sep  5 21:01:58.906: INFO: Pod downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b still exists
Sep  5 21:02:00.897: INFO: Waiting for pod downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b to disappear
Sep  5 21:02:00.910: INFO: Pod downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b still exists
Sep  5 21:02:02.897: INFO: Waiting for pod downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b to disappear
Sep  5 21:02:02.907: INFO: Pod downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b still exists
Sep  5 21:02:04.899: INFO: Waiting for pod downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b to disappear
Sep  5 21:02:04.969: INFO: Pod downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b still exists
Sep  5 21:02:06.898: INFO: Waiting for pod downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b to disappear
Sep  5 21:02:06.905: INFO: Pod downwardapi-volume-627ac0ee-1b51-4a85-add3-95f53ca9721b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:02:06.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7232" for this suite.

â€¢ [SLOW TEST:39.263 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's memory limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","total":291,"completed":17,"skipped":376,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:02:07.165: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8440
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-8440
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Sep  5 21:02:07.745: INFO: Found 0 stateful pods, waiting for 3
Sep  5 21:02:17.755: INFO: Found 1 stateful pods, waiting for 3
Sep  5 21:02:27.753: INFO: Found 1 stateful pods, waiting for 3
Sep  5 21:02:37.758: INFO: Found 1 stateful pods, waiting for 3
Sep  5 21:02:47.761: INFO: Found 2 stateful pods, waiting for 3
Sep  5 21:02:57.751: INFO: Found 2 stateful pods, waiting for 3
Sep  5 21:03:07.755: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:03:07.755: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:03:07.755: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 21:03:17.754: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:03:17.754: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:03:17.754: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 21:03:27.754: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:03:27.755: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:03:27.755: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from mirror.gcr.io/library/httpd:2.4.38-alpine to mirror.gcr.io/library/httpd:2.4.39-alpine
Sep  5 21:03:27.805: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep  5 21:03:37.888: INFO: Updating stateful set ss2
Sep  5 21:03:37.923: INFO: Waiting for Pod statefulset-8440/ss2-2 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 21:03:47.940: INFO: Waiting for Pod statefulset-8440/ss2-2 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
STEP: Restoring Pods to the correct revision when they are deleted
Sep  5 21:03:58.089: INFO: Found 2 stateful pods, waiting for 3
Sep  5 21:04:08.101: INFO: Found 2 stateful pods, waiting for 3
Sep  5 21:04:18.101: INFO: Found 2 stateful pods, waiting for 3
Sep  5 21:04:28.107: INFO: Found 2 stateful pods, waiting for 3
Sep  5 21:04:38.109: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:04:38.109: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:04:38.109: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 21:04:48.105: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:04:48.105: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:04:48.105: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 21:04:58.098: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:04:58.098: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:04:58.098: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep  5 21:04:58.159: INFO: Updating stateful set ss2
Sep  5 21:04:58.238: INFO: Waiting for Pod statefulset-8440/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 21:05:08.262: INFO: Waiting for Pod statefulset-8440/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 21:05:18.293: INFO: Updating stateful set ss2
Sep  5 21:05:18.314: INFO: Waiting for StatefulSet statefulset-8440/ss2 to complete update
Sep  5 21:05:18.314: INFO: Waiting for Pod statefulset-8440/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 21:05:28.335: INFO: Waiting for StatefulSet statefulset-8440/ss2 to complete update
Sep  5 21:05:28.335: INFO: Waiting for Pod statefulset-8440/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 21:05:38.329: INFO: Waiting for StatefulSet statefulset-8440/ss2 to complete update
Sep  5 21:05:38.329: INFO: Waiting for Pod statefulset-8440/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 21:05:48.335: INFO: Waiting for StatefulSet statefulset-8440/ss2 to complete update
Sep  5 21:05:58.335: INFO: Waiting for StatefulSet statefulset-8440/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep  5 21:06:08.332: INFO: Deleting all statefulset in ns statefulset-8440
Sep  5 21:06:08.340: INFO: Scaling statefulset ss2 to 0
Sep  5 21:06:48.391: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 21:06:48.400: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:06:48.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8440" for this suite.

â€¢ [SLOW TEST:281.679 seconds]
[sig-apps] StatefulSet
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","total":291,"completed":18,"skipped":424,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:06:48.845: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-3044
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-3044
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-3044
STEP: creating replication controller externalsvc in namespace services-3044
STEP: changing the ClusterIP service to type=ExternalName
Sep  5 21:07:19.699: INFO: Creating new exec pod
Sep  5 21:07:35.795: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=services-3044 execpod5wb4n -- /bin/sh -x -c nslookup clusterip-service.services-3044.svc.cluster.local'
Sep  5 21:07:36.416: INFO: stderr: "+ nslookup clusterip-service.services-3044.svc.cluster.local\n"
Sep  5 21:07:36.416: INFO: stdout: "Server:\t\t172.24.0.10\nAddress:\t172.24.0.10#53\n\nclusterip-service.services-3044.svc.cluster.local\tcanonical name = externalsvc.services-3044.svc.cluster.local.\nName:\texternalsvc.services-3044.svc.cluster.local\nAddress: 172.24.126.53\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-3044, will wait for the garbage collector to delete the pods
Sep  5 21:07:36.501: INFO: Deleting ReplicationController externalsvc took: 27.870697ms
Sep  5 21:07:38.602: INFO: Terminating ReplicationController externalsvc pods took: 2.100755293s
Sep  5 21:07:52.038: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:07:52.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3044" for this suite.
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:63.593 seconds]
[sig-network] Services
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","total":291,"completed":19,"skipped":540,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Disruptive] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:07:52.440: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename taint-single-pod
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-single-pod-3818
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:164
Sep  5 21:07:52.978: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  5 21:08:53.119: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 21:08:53.131: INFO: Starting informer...
STEP: Starting pod...
Sep  5 21:08:53.382: INFO: Pod is running on sc2-rdops-vm09-dhcp-43-208.eng.vmware.com. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Sep  5 21:08:53.459: INFO: Pod wasn't evicted. Proceeding
Sep  5 21:08:53.459: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Sep  5 21:10:08.576: INFO: Pod wasn't evicted. Test successful
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:10:08.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-3818" for this suite.

â€¢ [SLOW TEST:136.457 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  removing taint cancels eviction [Disruptive] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","total":291,"completed":20,"skipped":561,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:10:08.897: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6050
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  5 21:10:09.349: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841" in namespace "projected-6050" to be "Succeeded or Failed"
Sep  5 21:10:09.372: INFO: Pod "downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841": Phase="Pending", Reason="", readiness=false. Elapsed: 23.587726ms
Sep  5 21:10:11.383: INFO: Pod "downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034257422s
Sep  5 21:10:13.401: INFO: Pod "downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052397844s
Sep  5 21:10:15.412: INFO: Pod "downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841": Phase="Pending", Reason="", readiness=false. Elapsed: 6.062947984s
Sep  5 21:10:17.418: INFO: Pod "downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841": Phase="Pending", Reason="", readiness=false. Elapsed: 8.069744792s
Sep  5 21:10:19.425: INFO: Pod "downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841": Phase="Pending", Reason="", readiness=false. Elapsed: 10.076226956s
Sep  5 21:10:21.433: INFO: Pod "downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841": Phase="Pending", Reason="", readiness=false. Elapsed: 12.083991355s
Sep  5 21:10:23.442: INFO: Pod "downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841": Phase="Pending", Reason="", readiness=false. Elapsed: 14.09379731s
Sep  5 21:10:25.452: INFO: Pod "downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841": Phase="Pending", Reason="", readiness=false. Elapsed: 16.102931372s
Sep  5 21:10:27.459: INFO: Pod "downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841": Phase="Pending", Reason="", readiness=false. Elapsed: 18.11074942s
Sep  5 21:10:29.482: INFO: Pod "downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841": Phase="Pending", Reason="", readiness=false. Elapsed: 20.133167441s
Sep  5 21:10:31.491: INFO: Pod "downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.14194909s
STEP: Saw pod success
Sep  5 21:10:31.491: INFO: Pod "downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841" satisfied condition "Succeeded or Failed"
Sep  5 21:10:31.496: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com pod downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841 container client-container: <nil>
STEP: delete the pod
Sep  5 21:10:35.830: INFO: Waiting for pod downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841 to disappear
Sep  5 21:10:35.842: INFO: Pod downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841 still exists
Sep  5 21:10:37.843: INFO: Waiting for pod downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841 to disappear
Sep  5 21:10:37.852: INFO: Pod downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841 still exists
Sep  5 21:10:39.843: INFO: Waiting for pod downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841 to disappear
Sep  5 21:10:39.856: INFO: Pod downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841 still exists
Sep  5 21:10:41.843: INFO: Waiting for pod downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841 to disappear
Sep  5 21:10:41.857: INFO: Pod downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841 still exists
Sep  5 21:10:43.843: INFO: Waiting for pod downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841 to disappear
Sep  5 21:10:43.851: INFO: Pod downwardapi-volume-b83152a1-0e4c-4d13-a5ff-7293d7883841 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:10:43.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6050" for this suite.

â€¢ [SLOW TEST:35.162 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":21,"skipped":579,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:10:44.059: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-4040
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Sep  5 21:10:44.482: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Sep  5 21:11:10.156: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 21:11:18.164: INFO: >>> kubeConfig: ./kconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:11:45.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4040" for this suite.

â€¢ [SLOW TEST:61.945 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","total":291,"completed":22,"skipped":593,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:11:46.005: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9157
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be submitted and removed [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep  5 21:11:46.392: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:12:34.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9157" for this suite.

â€¢ [SLOW TEST:49.624 seconds]
[k8s.io] Pods
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be submitted and removed [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]","total":291,"completed":23,"skipped":613,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should run through the lifecycle of a ServiceAccount [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:12:35.629: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2383
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ServiceAccount
STEP: watching for the ServiceAccount to be added
STEP: patching the ServiceAccount
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector)
STEP: deleting the ServiceAccount
[AfterEach] [sig-auth] ServiceAccounts
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:12:36.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2383" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","total":291,"completed":24,"skipped":627,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:12:36.604: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4188
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  5 21:12:37.131: INFO: Waiting up to 5m0s for pod "downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc" in namespace "projected-4188" to be "Succeeded or Failed"
Sep  5 21:12:37.150: INFO: Pod "downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc": Phase="Pending", Reason="", readiness=false. Elapsed: 18.673253ms
Sep  5 21:12:39.163: INFO: Pod "downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031925546s
Sep  5 21:12:41.176: INFO: Pod "downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04411553s
Sep  5 21:12:43.181: INFO: Pod "downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049076516s
Sep  5 21:12:45.189: INFO: Pod "downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.057644417s
Sep  5 21:12:47.196: INFO: Pod "downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.064786909s
Sep  5 21:12:49.212: INFO: Pod "downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.080046638s
Sep  5 21:12:51.218: INFO: Pod "downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.086354964s
Sep  5 21:12:53.226: INFO: Pod "downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc": Phase="Pending", Reason="", readiness=false. Elapsed: 16.09424759s
Sep  5 21:12:55.234: INFO: Pod "downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc": Phase="Pending", Reason="", readiness=false. Elapsed: 18.102319645s
Sep  5 21:12:57.244: INFO: Pod "downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc": Phase="Pending", Reason="", readiness=false. Elapsed: 20.11297334s
Sep  5 21:12:59.257: INFO: Pod "downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc": Phase="Pending", Reason="", readiness=false. Elapsed: 22.125539731s
Sep  5 21:13:01.273: INFO: Pod "downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc": Phase="Running", Reason="", readiness=true. Elapsed: 24.141214744s
Sep  5 21:13:03.284: INFO: Pod "downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc": Phase="Running", Reason="", readiness=true. Elapsed: 26.15227206s
Sep  5 21:13:05.297: INFO: Pod "downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc": Phase="Running", Reason="", readiness=true. Elapsed: 28.165663716s
Sep  5 21:13:07.312: INFO: Pod "downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.180955123s
STEP: Saw pod success
Sep  5 21:13:07.313: INFO: Pod "downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc" satisfied condition "Succeeded or Failed"
Sep  5 21:13:07.320: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc container client-container: <nil>
STEP: delete the pod
Sep  5 21:13:11.571: INFO: Waiting for pod downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc to disappear
Sep  5 21:13:11.595: INFO: Pod downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc still exists
Sep  5 21:13:13.596: INFO: Waiting for pod downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc to disappear
Sep  5 21:13:13.625: INFO: Pod downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc still exists
Sep  5 21:13:15.596: INFO: Waiting for pod downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc to disappear
Sep  5 21:13:15.605: INFO: Pod downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc still exists
Sep  5 21:13:17.596: INFO: Waiting for pod downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc to disappear
Sep  5 21:13:17.602: INFO: Pod downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc still exists
Sep  5 21:13:19.596: INFO: Waiting for pod downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc to disappear
Sep  5 21:13:19.619: INFO: Pod downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc still exists
Sep  5 21:13:21.596: INFO: Waiting for pod downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc to disappear
Sep  5 21:13:21.603: INFO: Pod downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc still exists
Sep  5 21:13:23.596: INFO: Waiting for pod downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc to disappear
Sep  5 21:13:23.633: INFO: Pod downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc still exists
Sep  5 21:13:25.596: INFO: Waiting for pod downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc to disappear
Sep  5 21:13:25.601: INFO: Pod downwardapi-volume-43d30981-abef-4a0c-b396-96b2e70c73dc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:13:25.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4188" for this suite.

â€¢ [SLOW TEST:49.180 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's cpu limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","total":291,"completed":25,"skipped":706,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:13:25.784: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8396
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 21:13:26.273: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep  5 21:13:33.544: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-8396 create -f -'
Sep  5 21:13:34.655: INFO: stderr: ""
Sep  5 21:13:34.655: INFO: stdout: "e2e-test-crd-publish-openapi-8422-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  5 21:13:34.655: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-8396 delete e2e-test-crd-publish-openapi-8422-crds test-cr'
Sep  5 21:13:34.787: INFO: stderr: ""
Sep  5 21:13:34.787: INFO: stdout: "e2e-test-crd-publish-openapi-8422-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Sep  5 21:13:34.787: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-8396 apply -f -'
Sep  5 21:13:35.267: INFO: stderr: ""
Sep  5 21:13:35.267: INFO: stdout: "e2e-test-crd-publish-openapi-8422-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Sep  5 21:13:35.267: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-8396 delete e2e-test-crd-publish-openapi-8422-crds test-cr'
Sep  5 21:13:35.401: INFO: stderr: ""
Sep  5 21:13:35.401: INFO: stdout: "e2e-test-crd-publish-openapi-8422-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep  5 21:13:35.401: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml explain e2e-test-crd-publish-openapi-8422-crds'
Sep  5 21:13:35.998: INFO: stderr: ""
Sep  5 21:13:35.998: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-8422-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:13:44.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8396" for this suite.

â€¢ [SLOW TEST:18.607 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","total":291,"completed":26,"skipped":711,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:13:44.391: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6056
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should have an terminated reason [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:14:13.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6056" for this suite.

â€¢ [SLOW TEST:28.911 seconds]
[k8s.io] Kubelet
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a busybox command that always fails in a pod
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:79
    should have an terminated reason [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","total":291,"completed":27,"skipped":720,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:14:13.302: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1253
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Sep  5 21:14:35.936: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1253 PodName:pod-sharedvolume-9985b3fc-0cd5-4919-af1c-25be1bc06cfd ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 21:14:35.936: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 21:14:36.072: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:14:36.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1253" for this suite.

â€¢ [SLOW TEST:22.988 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  pod should support shared volumes between containers [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","total":291,"completed":28,"skipped":734,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:14:36.291: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-9271
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep  5 21:14:37.517: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep  5 21:14:39.540: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:14:41.562: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:14:43.571: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:14:45.552: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:14:47.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:14:49.548: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:14:51.548: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:14:53.548: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:14:55.551: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:14:57.549: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:14:59.548: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 14, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 21:15:02.612: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 21:15:02.621: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:15:04.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9271" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:29.674 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","total":291,"completed":29,"skipped":742,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
[k8s.io] Pods 
  should delete a collection of pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:15:05.965: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-211
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should delete a collection of pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pods
Sep  5 21:15:06.566: INFO: created test-pod-1
Sep  5 21:15:06.603: INFO: created test-pod-2
Sep  5 21:15:06.673: INFO: created test-pod-3
STEP: waiting for all 3 pods to be located
STEP: waiting for all pods to be deleted
[AfterEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:15:07.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-211" for this suite.
â€¢{"msg":"PASSED [k8s.io] Pods should delete a collection of pods [Conformance]","total":291,"completed":30,"skipped":742,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}

------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:15:08.195: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5956
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-9b5810b0-eadd-4faf-8ab4-04faefdc2674
STEP: Creating a pod to test consume configMaps
Sep  5 21:15:08.845: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a" in namespace "projected-5956" to be "Succeeded or Failed"
Sep  5 21:15:08.899: INFO: Pod "pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a": Phase="Pending", Reason="", readiness=false. Elapsed: 54.252761ms
Sep  5 21:15:10.943: INFO: Pod "pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.098047539s
Sep  5 21:15:12.952: INFO: Pod "pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.107516574s
Sep  5 21:15:14.982: INFO: Pod "pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.13681348s
Sep  5 21:15:16.989: INFO: Pod "pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.144208632s
Sep  5 21:15:18.998: INFO: Pod "pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.152906671s
Sep  5 21:15:21.020: INFO: Pod "pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.175279602s
Sep  5 21:15:23.028: INFO: Pod "pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.183126326s
Sep  5 21:15:25.046: INFO: Pod "pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.201482273s
Sep  5 21:15:27.055: INFO: Pod "pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.210258246s
Sep  5 21:15:29.069: INFO: Pod "pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.224396875s
Sep  5 21:15:31.079: INFO: Pod "pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.23385866s
Sep  5 21:15:33.087: INFO: Pod "pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a": Phase="Pending", Reason="", readiness=false. Elapsed: 24.241943648s
Sep  5 21:15:35.093: INFO: Pod "pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a": Phase="Pending", Reason="", readiness=false. Elapsed: 26.248058495s
Sep  5 21:15:37.098: INFO: Pod "pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.253663521s
STEP: Saw pod success
Sep  5 21:15:37.098: INFO: Pod "pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a" satisfied condition "Succeeded or Failed"
Sep  5 21:15:37.117: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 21:15:41.707: INFO: Waiting for pod pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a to disappear
Sep  5 21:15:41.737: INFO: Pod pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a still exists
Sep  5 21:15:43.738: INFO: Waiting for pod pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a to disappear
Sep  5 21:15:43.745: INFO: Pod pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a still exists
Sep  5 21:15:45.738: INFO: Waiting for pod pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a to disappear
Sep  5 21:15:45.747: INFO: Pod pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a still exists
Sep  5 21:15:47.738: INFO: Waiting for pod pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a to disappear
Sep  5 21:15:47.745: INFO: Pod pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a still exists
Sep  5 21:15:49.738: INFO: Waiting for pod pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a to disappear
Sep  5 21:15:49.744: INFO: Pod pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a still exists
Sep  5 21:15:51.738: INFO: Waiting for pod pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a to disappear
Sep  5 21:15:51.744: INFO: Pod pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a still exists
Sep  5 21:15:53.738: INFO: Waiting for pod pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a to disappear
Sep  5 21:15:53.744: INFO: Pod pod-projected-configmaps-fb82db58-1b45-4278-b830-e5a95fd4503a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:15:53.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5956" for this suite.

â€¢ [SLOW TEST:45.749 seconds]
[sig-storage] Projected configMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":31,"skipped":742,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:15:53.945: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3888
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-5706797c-22ff-4c93-b5e8-d60ebbeb4928
STEP: Creating configMap with name cm-test-opt-upd-437dd495-dd65-422d-80f4-9f0f160994c5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-5706797c-22ff-4c93-b5e8-d60ebbeb4928
STEP: Updating configmap cm-test-opt-upd-437dd495-dd65-422d-80f4-9f0f160994c5
STEP: Creating configMap with name cm-test-opt-create-b698f6bd-3462-4ad7-bdd9-0e94baeb0fd1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:17:07.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3888" for this suite.

â€¢ [SLOW TEST:74.237 seconds]
[sig-storage] Projected configMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":291,"completed":32,"skipped":760,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:17:08.182: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9754
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep  5 21:17:08.620: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  5 21:17:08.641: INFO: Waiting for terminating namespaces to be deleted...
Sep  5 21:17:08.655: INFO: 
Logging pods the apiserver thinks is on node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com before test
Sep  5 21:17:08.677: INFO: podwithpersistentvolume from storage-class-test-2 started at 2021-09-05 20:05:57 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.677: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:17:08.677: INFO: hello-web-6b97664bd5-f5452 from test-cluster-ip-service started at 2021-09-05 20:10:51 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.677: INFO: 	Container hello-app ready: true, restart count 0
Sep  5 21:17:08.677: INFO: wcp-sanity-busybox-6f999d6849-45jct from test-dataprovider-podvms-ns started at 2021-09-05 19:57:27 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.677: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 21:17:08.677: INFO: wcp-sanity-busybox-6f999d6849-c27n2 from test-dataprovider-podvms-ns started at 2021-09-05 21:11:14 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.677: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 21:17:08.677: INFO: nginx-private from test-image-pull-secrets-ns started at 2021-09-05 19:59:11 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.677: INFO: 	Container nginx-private-container ready: true, restart count 0
Sep  5 21:17:08.677: INFO: curl-pod from test-network-policy started at 2021-09-05 20:14:04 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.677: INFO: 	Container curl-container ready: true, restart count 0
Sep  5 21:17:08.677: INFO: hello-web-1-6b97664bd5-cl9tj from test-network-policy started at 2021-09-05 21:09:17 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.677: INFO: 	Container hello-app ready: true, restart count 0
Sep  5 21:17:08.677: INFO: schedext-test-node-selector-1 from test-node-selector started at 2021-09-05 20:00:20 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.677: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:17:08.677: INFO: busybox from test-pod-external-nw-access started at 2021-09-05 20:15:25 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.677: INFO: 	Container busybox ready: true, restart count 0
Sep  5 21:17:08.677: INFO: busybox-annotation from test-podvm-annotations started at 2021-09-05 20:03:14 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.677: INFO: 	Container busybox-annotation ready: true, restart count 0
Sep  5 21:17:08.677: INFO: helloworld from test-telemetry started at 2021-09-05 20:09:22 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.677: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:17:08.677: INFO: wcp-sanity-busybox-6f999d6849-46njm from test-update-workload-ns started at 2021-09-05 20:07:43 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.677: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 21:17:08.677: INFO: 
Logging pods the apiserver thinks is on node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com before test
Sep  5 21:17:08.739: INFO: curl-pod from test-cluster-ip-service started at 2021-09-05 20:10:11 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.739: INFO: 	Container curl-container ready: true, restart count 0
Sep  5 21:17:08.739: INFO: helloworld from test-exec-ns started at 2021-09-05 19:58:27 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.739: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:17:08.739: INFO: schedext-test-node-selector-2 from test-node-selector started at 2021-09-05 20:00:22 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.739: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:17:08.739: INFO: schedext-test-affinity-1 from test-pod-affinity started at 2021-09-05 20:00:51 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.739: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:17:08.739: INFO: schedext-test-affinity-2 from test-pod-affinity started at 2021-09-05 20:01:16 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.739: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:17:08.739: INFO: test-docker-registry from test-private-image-registry-ns started at 2021-09-05 20:16:11 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.739: INFO: 	Container test-docker-registry ready: true, restart count 0
Sep  5 21:17:08.739: INFO: helloworld from test-update-workload-ns started at 2021-09-05 20:08:34 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.739: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:17:08.739: INFO: 
Logging pods the apiserver thinks is on node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com before test
Sep  5 21:17:08.783: INFO: pod-projected-configmaps-021c4151-8b60-4191-811b-7001487699fe from projected-3888 started at 2021-09-05 21:16:15 -0700 PDT (3 container statuses recorded)
Sep  5 21:17:08.783: INFO: 	Container delcm-volume-test ready: true, restart count 0
Sep  5 21:17:08.783: INFO: 	Container updcm-volume-test ready: true, restart count 0
Sep  5 21:17:08.783: INFO: 	Container createcm-volume-test ready: true, restart count 0
Sep  5 21:17:08.783: INFO: hello-web-2-f779cbdff-hffpj from test-network-policy started at 2021-09-05 21:09:20 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.783: INFO: 	Container hello-app ready: true, restart count 0
Sep  5 21:17:08.783: INFO: wcp-sanity-busybox-6f999d6849-856nv from test-update-workload-ns started at 2021-09-05 21:10:00 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.783: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 21:17:08.783: INFO: wcp-sanity-busybox-6f999d6849-mts92 from test-update-workload-ns started at 2021-09-05 21:10:00 -0700 PDT (1 container statuses recorded)
Sep  5 21:17:08.783: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-da5f8643-26d7-4b07-89f0-6786ab73fee1 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-da5f8643-26d7-4b07-89f0-6786ab73fee1 off the node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
STEP: verifying the node doesn't have the label kubernetes.io/e2e-da5f8643-26d7-4b07-89f0-6786ab73fee1
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:22:43.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9754" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

â€¢ [SLOW TEST:335.275 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","total":291,"completed":33,"skipped":763,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should run through a ConfigMap lifecycle [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:22:43.457: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1529
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run through a ConfigMap lifecycle [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a ConfigMap
STEP: fetching the ConfigMap
STEP: patching the ConfigMap
STEP: listing all ConfigMaps in all namespaces with a label selector
STEP: deleting the ConfigMap by collection with a label selector
STEP: listing all ConfigMaps in test namespace
[AfterEach] [sig-node] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:22:43.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1529" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","total":291,"completed":34,"skipped":775,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:22:44.158: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7702
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create services for rc  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Sep  5 21:22:44.653: INFO: namespace kubectl-7702
Sep  5 21:22:44.653: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml create -f - --namespace=kubectl-7702'
Sep  5 21:22:45.231: INFO: stderr: ""
Sep  5 21:22:45.231: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep  5 21:22:46.241: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:22:46.241: INFO: Found 0 / 1
Sep  5 21:22:47.239: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:22:47.239: INFO: Found 0 / 1
Sep  5 21:22:48.238: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:22:48.238: INFO: Found 0 / 1
Sep  5 21:22:49.242: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:22:49.242: INFO: Found 0 / 1
Sep  5 21:22:50.247: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:22:50.247: INFO: Found 0 / 1
Sep  5 21:22:51.244: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:22:51.244: INFO: Found 0 / 1
Sep  5 21:22:52.245: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:22:52.245: INFO: Found 0 / 1
Sep  5 21:22:53.240: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:22:53.240: INFO: Found 0 / 1
Sep  5 21:22:54.242: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:22:54.242: INFO: Found 0 / 1
Sep  5 21:22:55.237: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:22:55.237: INFO: Found 0 / 1
Sep  5 21:22:56.243: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:22:56.244: INFO: Found 0 / 1
Sep  5 21:22:57.255: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:22:57.255: INFO: Found 0 / 1
Sep  5 21:22:58.238: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:22:58.239: INFO: Found 0 / 1
Sep  5 21:22:59.249: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:22:59.249: INFO: Found 0 / 1
Sep  5 21:23:00.258: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:23:00.258: INFO: Found 0 / 1
Sep  5 21:23:01.240: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:23:01.240: INFO: Found 0 / 1
Sep  5 21:23:02.240: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:23:02.240: INFO: Found 0 / 1
Sep  5 21:23:03.242: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:23:03.242: INFO: Found 0 / 1
Sep  5 21:23:04.246: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:23:04.246: INFO: Found 0 / 1
Sep  5 21:23:05.238: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:23:05.238: INFO: Found 0 / 1
Sep  5 21:23:06.239: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:23:06.239: INFO: Found 0 / 1
Sep  5 21:23:07.244: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:23:07.244: INFO: Found 0 / 1
Sep  5 21:23:08.237: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:23:08.237: INFO: Found 0 / 1
Sep  5 21:23:09.240: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:23:09.240: INFO: Found 0 / 1
Sep  5 21:23:10.240: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:23:10.240: INFO: Found 0 / 1
Sep  5 21:23:11.238: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:23:11.238: INFO: Found 0 / 1
Sep  5 21:23:12.250: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:23:12.250: INFO: Found 0 / 1
Sep  5 21:23:13.238: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:23:13.238: INFO: Found 1 / 1
Sep  5 21:23:13.238: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  5 21:23:13.244: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 21:23:13.244: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  5 21:23:13.244: INFO: wait on agnhost-primary startup in kubectl-7702 
Sep  5 21:23:13.244: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml logs agnhost-primary-w45pn agnhost-primary --namespace=kubectl-7702'
Sep  5 21:23:13.417: INFO: stderr: ""
Sep  5 21:23:13.417: INFO: stdout: "Paused\n"
STEP: exposing RC
Sep  5 21:23:13.417: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-7702'
Sep  5 21:23:13.565: INFO: stderr: ""
Sep  5 21:23:13.565: INFO: stdout: "service/rm2 exposed\n"
Sep  5 21:23:13.597: INFO: Service rm2 in namespace kubectl-7702 found.
STEP: exposing service
Sep  5 21:23:15.615: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-7702'
Sep  5 21:23:15.759: INFO: stderr: ""
Sep  5 21:23:15.759: INFO: stdout: "service/rm3 exposed\n"
Sep  5 21:23:15.823: INFO: Service rm3 in namespace kubectl-7702 found.
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:23:17.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7702" for this suite.

â€¢ [SLOW TEST:33.923 seconds]
[sig-cli] Kubectl client
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1246
    should create services for rc  [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","total":291,"completed":35,"skipped":779,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:23:18.081: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9562
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  5 21:23:18.617: INFO: Waiting up to 5m0s for pod "pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3" in namespace "emptydir-9562" to be "Succeeded or Failed"
Sep  5 21:23:18.642: INFO: Pod "pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3": Phase="Pending", Reason="", readiness=false. Elapsed: 24.601468ms
Sep  5 21:23:20.655: INFO: Pod "pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03812257s
Sep  5 21:23:22.662: INFO: Pod "pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045246753s
Sep  5 21:23:24.681: INFO: Pod "pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063687055s
Sep  5 21:23:26.689: INFO: Pod "pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.072203281s
Sep  5 21:23:28.698: INFO: Pod "pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.081076027s
Sep  5 21:23:30.707: INFO: Pod "pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.090025901s
Sep  5 21:23:32.717: INFO: Pod "pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.099857339s
Sep  5 21:23:34.730: INFO: Pod "pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.113109907s
Sep  5 21:23:36.739: INFO: Pod "pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.121693101s
Sep  5 21:23:38.747: INFO: Pod "pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3": Phase="Running", Reason="", readiness=true. Elapsed: 20.13007203s
Sep  5 21:23:40.755: INFO: Pod "pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3": Phase="Running", Reason="", readiness=true. Elapsed: 22.13804357s
Sep  5 21:23:42.764: INFO: Pod "pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.147459402s
STEP: Saw pod success
Sep  5 21:23:42.764: INFO: Pod "pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3" satisfied condition "Succeeded or Failed"
Sep  5 21:23:42.773: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3 container test-container: <nil>
STEP: delete the pod
Sep  5 21:23:42.880: INFO: Waiting for pod pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3 to disappear
Sep  5 21:23:42.909: INFO: Pod pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3 still exists
Sep  5 21:23:44.910: INFO: Waiting for pod pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3 to disappear
Sep  5 21:23:44.922: INFO: Pod pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3 still exists
Sep  5 21:23:46.910: INFO: Waiting for pod pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3 to disappear
Sep  5 21:23:46.923: INFO: Pod pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3 still exists
Sep  5 21:23:48.910: INFO: Waiting for pod pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3 to disappear
Sep  5 21:23:48.927: INFO: Pod pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3 still exists
Sep  5 21:23:50.910: INFO: Waiting for pod pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3 to disappear
Sep  5 21:23:50.916: INFO: Pod pod-ddab60bf-398f-4bc3-a9c0-8ba28535b6b3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:23:50.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9562" for this suite.

â€¢ [SLOW TEST:33.247 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":36,"skipped":789,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:23:51.329: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5420
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-40121161-a87c-4723-a1d6-495b6a8573ee in namespace container-probe-5420
Sep  5 21:24:13.810: INFO: Started pod busybox-40121161-a87c-4723-a1d6-495b6a8573ee in namespace container-probe-5420
STEP: checking the pod's current state and verifying that restartCount is present
Sep  5 21:24:13.817: INFO: Initial restart count of pod busybox-40121161-a87c-4723-a1d6-495b6a8573ee is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:28:15.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5420" for this suite.

â€¢ [SLOW TEST:264.290 seconds]
[k8s.io] Probing container
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":291,"completed":37,"skipped":809,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:28:15.619: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8159
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override all
Sep  5 21:28:16.054: INFO: Waiting up to 5m0s for pod "client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd" in namespace "containers-8159" to be "Succeeded or Failed"
Sep  5 21:28:16.080: INFO: Pod "client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd": Phase="Pending", Reason="", readiness=false. Elapsed: 26.639309ms
Sep  5 21:28:18.089: INFO: Pod "client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034807109s
Sep  5 21:28:20.098: INFO: Pod "client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043776886s
Sep  5 21:28:22.114: INFO: Pod "client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.060425356s
Sep  5 21:28:24.127: INFO: Pod "client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.072991657s
Sep  5 21:28:26.138: INFO: Pod "client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.084445858s
Sep  5 21:28:28.148: INFO: Pod "client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.094051238s
Sep  5 21:28:30.163: INFO: Pod "client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.109470707s
Sep  5 21:28:32.174: INFO: Pod "client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.119734472s
Sep  5 21:28:34.183: INFO: Pod "client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.128798012s
Sep  5 21:28:36.191: INFO: Pod "client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd": Phase="Pending", Reason="", readiness=false. Elapsed: 20.13676055s
Sep  5 21:28:38.196: INFO: Pod "client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd": Phase="Pending", Reason="", readiness=false. Elapsed: 22.141941175s
Sep  5 21:28:40.205: INFO: Pod "client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.151279038s
STEP: Saw pod success
Sep  5 21:28:40.205: INFO: Pod "client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd" satisfied condition "Succeeded or Failed"
Sep  5 21:28:40.212: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd container test-container: <nil>
STEP: delete the pod
Sep  5 21:28:44.541: INFO: Waiting for pod client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd to disappear
Sep  5 21:28:44.552: INFO: Pod client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd still exists
Sep  5 21:28:46.552: INFO: Waiting for pod client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd to disappear
Sep  5 21:28:46.584: INFO: Pod client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd still exists
Sep  5 21:28:48.553: INFO: Waiting for pod client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd to disappear
Sep  5 21:28:48.560: INFO: Pod client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd still exists
Sep  5 21:28:50.552: INFO: Waiting for pod client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd to disappear
Sep  5 21:28:50.559: INFO: Pod client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd still exists
Sep  5 21:28:52.553: INFO: Waiting for pod client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd to disappear
Sep  5 21:28:52.562: INFO: Pod client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd still exists
Sep  5 21:28:54.553: INFO: Waiting for pod client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd to disappear
Sep  5 21:28:54.563: INFO: Pod client-containers-5dbe5c12-9d49-492e-a3b7-faec96d587fd no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:28:54.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8159" for this suite.

â€¢ [SLOW TEST:39.209 seconds]
[k8s.io] Docker Containers
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","total":291,"completed":38,"skipped":820,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:28:54.828: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1766
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod busybox-817df3c7-f998-441a-b7b7-59c212bce55e in namespace container-probe-1766
Sep  5 21:29:19.510: INFO: Started pod busybox-817df3c7-f998-441a-b7b7-59c212bce55e in namespace container-probe-1766
STEP: checking the pod's current state and verifying that restartCount is present
Sep  5 21:29:19.518: INFO: Initial restart count of pod busybox-817df3c7-f998-441a-b7b7-59c212bce55e is 0
Sep  5 21:29:33.682: INFO: Restart count of pod container-probe-1766/busybox-817df3c7-f998-441a-b7b7-59c212bce55e is now 1 (14.164539929s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:29:33.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1766" for this suite.

â€¢ [SLOW TEST:39.102 seconds]
[k8s.io] Probing container
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","total":291,"completed":39,"skipped":822,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:29:33.931: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5071
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep  5 21:29:34.427: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5071 /api/v1/namespaces/watch-5071/configmaps/e2e-watch-test-label-changed 5377f5ab-4847-4909-8193-5eddb84b4177 83210 0 2021-09-05 21:29:33 -0700 PDT <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-05 21:29:33 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  5 21:29:34.428: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5071 /api/v1/namespaces/watch-5071/configmaps/e2e-watch-test-label-changed 5377f5ab-4847-4909-8193-5eddb84b4177 83212 0 2021-09-05 21:29:33 -0700 PDT <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-05 21:29:33 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  5 21:29:34.428: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5071 /api/v1/namespaces/watch-5071/configmaps/e2e-watch-test-label-changed 5377f5ab-4847-4909-8193-5eddb84b4177 83214 0 2021-09-05 21:29:33 -0700 PDT <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-05 21:29:33 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep  5 21:29:44.563: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5071 /api/v1/namespaces/watch-5071/configmaps/e2e-watch-test-label-changed 5377f5ab-4847-4909-8193-5eddb84b4177 83347 0 2021-09-05 21:29:33 -0700 PDT <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-05 21:29:33 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  5 21:29:44.564: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5071 /api/v1/namespaces/watch-5071/configmaps/e2e-watch-test-label-changed 5377f5ab-4847-4909-8193-5eddb84b4177 83349 0 2021-09-05 21:29:33 -0700 PDT <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-05 21:29:33 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  5 21:29:44.564: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5071 /api/v1/namespaces/watch-5071/configmaps/e2e-watch-test-label-changed 5377f5ab-4847-4909-8193-5eddb84b4177 83350 0 2021-09-05 21:29:33 -0700 PDT <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  [{e2e.test Update v1 2021-09-05 21:29:33 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:29:44.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5071" for this suite.

â€¢ [SLOW TEST:10.823 seconds]
[sig-api-machinery] Watchers
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","total":291,"completed":40,"skipped":865,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:29:44.755: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4568
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep  5 21:29:45.250: INFO: Waiting up to 5m0s for pod "pod-9069e8ed-4742-45da-b851-1a30acd08370" in namespace "emptydir-4568" to be "Succeeded or Failed"
Sep  5 21:29:45.255: INFO: Pod "pod-9069e8ed-4742-45da-b851-1a30acd08370": Phase="Pending", Reason="", readiness=false. Elapsed: 5.038242ms
Sep  5 21:29:47.275: INFO: Pod "pod-9069e8ed-4742-45da-b851-1a30acd08370": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025278135s
Sep  5 21:29:49.283: INFO: Pod "pod-9069e8ed-4742-45da-b851-1a30acd08370": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032575438s
Sep  5 21:29:51.290: INFO: Pod "pod-9069e8ed-4742-45da-b851-1a30acd08370": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040019189s
Sep  5 21:29:53.299: INFO: Pod "pod-9069e8ed-4742-45da-b851-1a30acd08370": Phase="Pending", Reason="", readiness=false. Elapsed: 8.048686603s
Sep  5 21:29:55.305: INFO: Pod "pod-9069e8ed-4742-45da-b851-1a30acd08370": Phase="Pending", Reason="", readiness=false. Elapsed: 10.055297725s
Sep  5 21:29:57.314: INFO: Pod "pod-9069e8ed-4742-45da-b851-1a30acd08370": Phase="Pending", Reason="", readiness=false. Elapsed: 12.064162186s
Sep  5 21:29:59.325: INFO: Pod "pod-9069e8ed-4742-45da-b851-1a30acd08370": Phase="Pending", Reason="", readiness=false. Elapsed: 14.074790844s
Sep  5 21:30:01.333: INFO: Pod "pod-9069e8ed-4742-45da-b851-1a30acd08370": Phase="Pending", Reason="", readiness=false. Elapsed: 16.082968895s
Sep  5 21:30:03.344: INFO: Pod "pod-9069e8ed-4742-45da-b851-1a30acd08370": Phase="Pending", Reason="", readiness=false. Elapsed: 18.093890486s
Sep  5 21:30:05.354: INFO: Pod "pod-9069e8ed-4742-45da-b851-1a30acd08370": Phase="Pending", Reason="", readiness=false. Elapsed: 20.103897788s
Sep  5 21:30:07.372: INFO: Pod "pod-9069e8ed-4742-45da-b851-1a30acd08370": Phase="Pending", Reason="", readiness=false. Elapsed: 22.122319789s
Sep  5 21:30:09.388: INFO: Pod "pod-9069e8ed-4742-45da-b851-1a30acd08370": Phase="Pending", Reason="", readiness=false. Elapsed: 24.137662047s
Sep  5 21:30:11.397: INFO: Pod "pod-9069e8ed-4742-45da-b851-1a30acd08370": Phase="Pending", Reason="", readiness=false. Elapsed: 26.147122963s
Sep  5 21:30:13.409: INFO: Pod "pod-9069e8ed-4742-45da-b851-1a30acd08370": Phase="Pending", Reason="", readiness=false. Elapsed: 28.159499462s
Sep  5 21:30:15.417: INFO: Pod "pod-9069e8ed-4742-45da-b851-1a30acd08370": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.167241767s
STEP: Saw pod success
Sep  5 21:30:15.417: INFO: Pod "pod-9069e8ed-4742-45da-b851-1a30acd08370" satisfied condition "Succeeded or Failed"
Sep  5 21:30:15.423: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-9069e8ed-4742-45da-b851-1a30acd08370 container test-container: <nil>
STEP: delete the pod
Sep  5 21:30:19.934: INFO: Waiting for pod pod-9069e8ed-4742-45da-b851-1a30acd08370 to disappear
Sep  5 21:30:19.991: INFO: Pod pod-9069e8ed-4742-45da-b851-1a30acd08370 still exists
Sep  5 21:30:21.991: INFO: Waiting for pod pod-9069e8ed-4742-45da-b851-1a30acd08370 to disappear
Sep  5 21:30:22.004: INFO: Pod pod-9069e8ed-4742-45da-b851-1a30acd08370 still exists
Sep  5 21:30:23.991: INFO: Waiting for pod pod-9069e8ed-4742-45da-b851-1a30acd08370 to disappear
Sep  5 21:30:24.001: INFO: Pod pod-9069e8ed-4742-45da-b851-1a30acd08370 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:30:24.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4568" for this suite.

â€¢ [SLOW TEST:39.512 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":41,"skipped":881,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:30:24.267: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8344
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should retry creating failed daemon pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  5 21:30:24.957: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:24.957: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:24.957: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:24.986: INFO: Number of nodes with available pods: 0
Sep  5 21:30:24.986: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:26.013: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:26.013: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:26.013: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:26.084: INFO: Number of nodes with available pods: 0
Sep  5 21:30:26.084: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:27.007: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:27.007: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:27.007: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:27.018: INFO: Number of nodes with available pods: 0
Sep  5 21:30:27.018: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:27.998: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:27.998: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:27.998: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:28.003: INFO: Number of nodes with available pods: 0
Sep  5 21:30:28.003: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:28.997: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:28.997: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:28.997: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:29.005: INFO: Number of nodes with available pods: 0
Sep  5 21:30:29.005: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:29.998: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:29.998: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:29.998: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:30.005: INFO: Number of nodes with available pods: 0
Sep  5 21:30:30.005: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:31.000: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:31.000: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:31.000: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:31.013: INFO: Number of nodes with available pods: 0
Sep  5 21:30:31.013: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:32.039: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:32.039: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:32.039: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:32.068: INFO: Number of nodes with available pods: 0
Sep  5 21:30:32.068: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:32.997: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:32.997: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:32.997: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:33.004: INFO: Number of nodes with available pods: 0
Sep  5 21:30:33.004: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:34.010: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:34.010: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:34.010: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:34.019: INFO: Number of nodes with available pods: 0
Sep  5 21:30:34.019: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:35.005: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:35.005: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:35.005: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:35.024: INFO: Number of nodes with available pods: 0
Sep  5 21:30:35.024: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:36.044: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:36.044: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:36.044: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:36.077: INFO: Number of nodes with available pods: 0
Sep  5 21:30:36.077: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:37.003: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:37.003: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:37.003: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:37.013: INFO: Number of nodes with available pods: 0
Sep  5 21:30:37.013: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:38.003: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:38.003: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:38.004: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:38.029: INFO: Number of nodes with available pods: 0
Sep  5 21:30:38.029: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:38.997: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:38.997: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:38.997: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:39.016: INFO: Number of nodes with available pods: 0
Sep  5 21:30:39.016: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:40.055: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:40.055: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:40.055: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:40.082: INFO: Number of nodes with available pods: 0
Sep  5 21:30:40.082: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:40.999: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:40.999: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:40.999: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:41.014: INFO: Number of nodes with available pods: 0
Sep  5 21:30:41.014: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:42.001: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:42.001: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:42.001: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:42.009: INFO: Number of nodes with available pods: 0
Sep  5 21:30:42.009: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:42.997: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:42.998: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:42.998: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:43.007: INFO: Number of nodes with available pods: 0
Sep  5 21:30:43.007: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:44.002: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:44.002: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:44.002: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:44.009: INFO: Number of nodes with available pods: 0
Sep  5 21:30:44.009: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:44.996: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:44.996: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:44.997: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:45.003: INFO: Number of nodes with available pods: 0
Sep  5 21:30:45.003: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:45.999: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:45.999: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:45.999: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:46.009: INFO: Number of nodes with available pods: 0
Sep  5 21:30:46.009: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:46.999: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:46.999: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:46.999: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:47.010: INFO: Number of nodes with available pods: 0
Sep  5 21:30:47.010: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:47.997: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:47.998: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:47.998: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:48.008: INFO: Number of nodes with available pods: 0
Sep  5 21:30:48.008: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:49.004: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:49.004: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:49.004: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:49.020: INFO: Number of nodes with available pods: 1
Sep  5 21:30:49.020: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:49.998: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:49.998: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:49.998: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:50.008: INFO: Number of nodes with available pods: 1
Sep  5 21:30:50.008: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:51.004: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:51.004: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:51.004: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:51.030: INFO: Number of nodes with available pods: 1
Sep  5 21:30:51.030: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:52.009: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:52.009: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:52.009: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:52.019: INFO: Number of nodes with available pods: 1
Sep  5 21:30:52.020: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:52.995: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:52.996: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:52.996: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:53.005: INFO: Number of nodes with available pods: 1
Sep  5 21:30:53.005: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:54.003: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:54.003: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:54.003: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:54.029: INFO: Number of nodes with available pods: 2
Sep  5 21:30:54.029: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 21:30:55.000: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:55.000: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:55.000: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:55.012: INFO: Number of nodes with available pods: 3
Sep  5 21:30:55.012: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep  5 21:30:55.081: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:55.081: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:55.081: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 21:30:55.104: INFO: Number of nodes with available pods: 3
Sep  5 21:30:55.104: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8344, will wait for the garbage collector to delete the pods
Sep  5 21:31:03.313: INFO: Deleting DaemonSet.extensions daemon-set took: 50.270232ms
Sep  5 21:31:05.414: INFO: Terminating DaemonSet.extensions daemon-set pods took: 2.100926087s
Sep  5 21:31:17.623: INFO: Number of nodes with available pods: 0
Sep  5 21:31:17.623: INFO: Number of running nodes: 0, number of available pods: 0
Sep  5 21:31:17.633: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8344/daemonsets","resourceVersion":"84568"},"items":null}

Sep  5 21:31:17.640: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8344/pods","resourceVersion":"84568"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:31:17.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8344" for this suite.

â€¢ [SLOW TEST:53.672 seconds]
[sig-apps] Daemon set [Serial]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","total":291,"completed":42,"skipped":901,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:31:17.939: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3981
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep  5 21:31:18.565: INFO: Waiting up to 5m0s for pod "downward-api-42686412-ed65-475e-b23d-1b2495de325f" in namespace "downward-api-3981" to be "Succeeded or Failed"
Sep  5 21:31:18.597: INFO: Pod "downward-api-42686412-ed65-475e-b23d-1b2495de325f": Phase="Pending", Reason="", readiness=false. Elapsed: 31.703433ms
Sep  5 21:31:20.605: INFO: Pod "downward-api-42686412-ed65-475e-b23d-1b2495de325f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039033261s
Sep  5 21:31:22.613: INFO: Pod "downward-api-42686412-ed65-475e-b23d-1b2495de325f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047381441s
Sep  5 21:31:24.627: INFO: Pod "downward-api-42686412-ed65-475e-b23d-1b2495de325f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.060998675s
Sep  5 21:31:26.637: INFO: Pod "downward-api-42686412-ed65-475e-b23d-1b2495de325f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.071403398s
Sep  5 21:31:28.646: INFO: Pod "downward-api-42686412-ed65-475e-b23d-1b2495de325f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.080199851s
Sep  5 21:31:30.659: INFO: Pod "downward-api-42686412-ed65-475e-b23d-1b2495de325f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.093785921s
Sep  5 21:31:32.724: INFO: Pod "downward-api-42686412-ed65-475e-b23d-1b2495de325f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.158454154s
Sep  5 21:31:34.837: INFO: Pod "downward-api-42686412-ed65-475e-b23d-1b2495de325f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.271559879s
Sep  5 21:31:36.850: INFO: Pod "downward-api-42686412-ed65-475e-b23d-1b2495de325f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.284651854s
Sep  5 21:31:38.863: INFO: Pod "downward-api-42686412-ed65-475e-b23d-1b2495de325f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.297115747s
Sep  5 21:31:40.882: INFO: Pod "downward-api-42686412-ed65-475e-b23d-1b2495de325f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.316520955s
Sep  5 21:31:42.892: INFO: Pod "downward-api-42686412-ed65-475e-b23d-1b2495de325f": Phase="Pending", Reason="", readiness=false. Elapsed: 24.326468162s
Sep  5 21:31:44.903: INFO: Pod "downward-api-42686412-ed65-475e-b23d-1b2495de325f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.337084123s
STEP: Saw pod success
Sep  5 21:31:44.903: INFO: Pod "downward-api-42686412-ed65-475e-b23d-1b2495de325f" satisfied condition "Succeeded or Failed"
Sep  5 21:31:44.916: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod downward-api-42686412-ed65-475e-b23d-1b2495de325f container dapi-container: <nil>
STEP: delete the pod
Sep  5 21:31:44.959: INFO: Waiting for pod downward-api-42686412-ed65-475e-b23d-1b2495de325f to disappear
Sep  5 21:31:44.973: INFO: Pod downward-api-42686412-ed65-475e-b23d-1b2495de325f still exists
Sep  5 21:31:46.975: INFO: Waiting for pod downward-api-42686412-ed65-475e-b23d-1b2495de325f to disappear
Sep  5 21:31:46.984: INFO: Pod downward-api-42686412-ed65-475e-b23d-1b2495de325f still exists
Sep  5 21:31:48.974: INFO: Waiting for pod downward-api-42686412-ed65-475e-b23d-1b2495de325f to disappear
Sep  5 21:31:48.985: INFO: Pod downward-api-42686412-ed65-475e-b23d-1b2495de325f still exists
Sep  5 21:31:50.974: INFO: Waiting for pod downward-api-42686412-ed65-475e-b23d-1b2495de325f to disappear
Sep  5 21:31:50.985: INFO: Pod downward-api-42686412-ed65-475e-b23d-1b2495de325f still exists
Sep  5 21:31:52.974: INFO: Waiting for pod downward-api-42686412-ed65-475e-b23d-1b2495de325f to disappear
Sep  5 21:31:52.980: INFO: Pod downward-api-42686412-ed65-475e-b23d-1b2495de325f no longer exists
[AfterEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:31:52.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3981" for this suite.

â€¢ [SLOW TEST:35.279 seconds]
[sig-node] Downward API
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","total":291,"completed":43,"skipped":920,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:31:53.218: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-53
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-9f5dd3bb-8d0e-4689-854e-80f11e98d12d
STEP: Creating a pod to test consume secrets
Sep  5 21:31:53.713: INFO: Waiting up to 5m0s for pod "pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041" in namespace "secrets-53" to be "Succeeded or Failed"
Sep  5 21:31:53.720: INFO: Pod "pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041": Phase="Pending", Reason="", readiness=false. Elapsed: 7.309232ms
Sep  5 21:31:55.728: INFO: Pod "pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014833909s
Sep  5 21:31:57.736: INFO: Pod "pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023428236s
Sep  5 21:31:59.787: INFO: Pod "pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074354s
Sep  5 21:32:01.802: INFO: Pod "pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041": Phase="Pending", Reason="", readiness=false. Elapsed: 8.088725517s
Sep  5 21:32:03.830: INFO: Pod "pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041": Phase="Pending", Reason="", readiness=false. Elapsed: 10.117281956s
Sep  5 21:32:05.837: INFO: Pod "pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041": Phase="Pending", Reason="", readiness=false. Elapsed: 12.123983771s
Sep  5 21:32:07.844: INFO: Pod "pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041": Phase="Pending", Reason="", readiness=false. Elapsed: 14.130689119s
Sep  5 21:32:09.854: INFO: Pod "pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041": Phase="Pending", Reason="", readiness=false. Elapsed: 16.141032281s
Sep  5 21:32:11.860: INFO: Pod "pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041": Phase="Pending", Reason="", readiness=false. Elapsed: 18.14727441s
Sep  5 21:32:13.928: INFO: Pod "pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041": Phase="Pending", Reason="", readiness=false. Elapsed: 20.214814876s
Sep  5 21:32:15.936: INFO: Pod "pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041": Phase="Running", Reason="", readiness=true. Elapsed: 22.222791232s
Sep  5 21:32:17.945: INFO: Pod "pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041": Phase="Running", Reason="", readiness=true. Elapsed: 24.232459269s
Sep  5 21:32:19.954: INFO: Pod "pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.241490664s
STEP: Saw pod success
Sep  5 21:32:19.955: INFO: Pod "pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041" satisfied condition "Succeeded or Failed"
Sep  5 21:32:19.963: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 21:32:20.040: INFO: Waiting for pod pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041 to disappear
Sep  5 21:32:20.057: INFO: Pod pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041 still exists
Sep  5 21:32:22.059: INFO: Waiting for pod pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041 to disappear
Sep  5 21:32:22.066: INFO: Pod pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041 still exists
Sep  5 21:32:24.059: INFO: Waiting for pod pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041 to disappear
Sep  5 21:32:24.066: INFO: Pod pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041 still exists
Sep  5 21:32:26.057: INFO: Waiting for pod pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041 to disappear
Sep  5 21:32:26.064: INFO: Pod pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041 still exists
Sep  5 21:32:28.058: INFO: Waiting for pod pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041 to disappear
Sep  5 21:32:28.064: INFO: Pod pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041 still exists
Sep  5 21:32:30.058: INFO: Waiting for pod pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041 to disappear
Sep  5 21:32:30.065: INFO: Pod pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041 still exists
Sep  5 21:32:32.059: INFO: Waiting for pod pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041 to disappear
Sep  5 21:32:32.071: INFO: Pod pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041 still exists
Sep  5 21:32:34.059: INFO: Waiting for pod pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041 to disappear
Sep  5 21:32:34.065: INFO: Pod pod-secrets-a00050f2-9d58-47e3-9f7e-99d30b12a041 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:32:34.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-53" for this suite.

â€¢ [SLOW TEST:41.076 seconds]
[sig-storage] Secrets
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":44,"skipped":926,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should find a service from listing all namespaces [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:32:34.295: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-855
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should find a service from listing all namespaces [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching services
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:32:34.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-855" for this suite.
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
â€¢{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","total":291,"completed":45,"skipped":948,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:32:34.943: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4646
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating secret secrets-4646/secret-test-365a6e31-409e-4413-b5ed-f5c011a7948b
STEP: Creating a pod to test consume secrets
Sep  5 21:32:35.528: INFO: Waiting up to 5m0s for pod "pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa" in namespace "secrets-4646" to be "Succeeded or Failed"
Sep  5 21:32:35.542: INFO: Pod "pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa": Phase="Pending", Reason="", readiness=false. Elapsed: 14.16544ms
Sep  5 21:32:37.561: INFO: Pod "pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033386929s
Sep  5 21:32:39.576: INFO: Pod "pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047782628s
Sep  5 21:32:41.594: INFO: Pod "pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.066020861s
Sep  5 21:32:43.603: INFO: Pod "pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa": Phase="Pending", Reason="", readiness=false. Elapsed: 8.075100221s
Sep  5 21:32:45.610: INFO: Pod "pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa": Phase="Pending", Reason="", readiness=false. Elapsed: 10.082743318s
Sep  5 21:32:47.619: INFO: Pod "pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa": Phase="Pending", Reason="", readiness=false. Elapsed: 12.09148365s
Sep  5 21:32:49.628: INFO: Pod "pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa": Phase="Pending", Reason="", readiness=false. Elapsed: 14.100235348s
Sep  5 21:32:51.637: INFO: Pod "pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa": Phase="Pending", Reason="", readiness=false. Elapsed: 16.109671407s
Sep  5 21:32:53.645: INFO: Pod "pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa": Phase="Pending", Reason="", readiness=false. Elapsed: 18.117653864s
Sep  5 21:32:55.652: INFO: Pod "pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa": Phase="Pending", Reason="", readiness=false. Elapsed: 20.124546484s
Sep  5 21:32:57.658: INFO: Pod "pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa": Phase="Pending", Reason="", readiness=false. Elapsed: 22.130212161s
Sep  5 21:32:59.666: INFO: Pod "pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.137916534s
STEP: Saw pod success
Sep  5 21:32:59.666: INFO: Pod "pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa" satisfied condition "Succeeded or Failed"
Sep  5 21:32:59.672: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa container env-test: <nil>
STEP: delete the pod
Sep  5 21:32:59.733: INFO: Waiting for pod pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa to disappear
Sep  5 21:32:59.753: INFO: Pod pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa still exists
Sep  5 21:33:01.753: INFO: Waiting for pod pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa to disappear
Sep  5 21:33:01.764: INFO: Pod pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa still exists
Sep  5 21:33:03.754: INFO: Waiting for pod pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa to disappear
Sep  5 21:33:03.806: INFO: Pod pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa still exists
Sep  5 21:33:05.754: INFO: Waiting for pod pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa to disappear
Sep  5 21:33:05.771: INFO: Pod pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa still exists
Sep  5 21:33:07.753: INFO: Waiting for pod pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa to disappear
Sep  5 21:33:07.763: INFO: Pod pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa still exists
Sep  5 21:33:09.753: INFO: Waiting for pod pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa to disappear
Sep  5 21:33:09.763: INFO: Pod pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa still exists
Sep  5 21:33:11.753: INFO: Waiting for pod pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa to disappear
Sep  5 21:33:11.763: INFO: Pod pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa still exists
Sep  5 21:33:13.753: INFO: Waiting for pod pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa to disappear
Sep  5 21:33:13.760: INFO: Pod pod-configmaps-cf7e0dfa-c51f-4c36-82fc-da559d01a5fa no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:33:13.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4646" for this suite.

â€¢ [SLOW TEST:39.058 seconds]
[sig-api-machinery] Secrets
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should be consumable via the environment [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]","total":291,"completed":46,"skipped":977,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:33:14.001: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3872
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test env composition
Sep  5 21:33:14.481: INFO: Waiting up to 5m0s for pod "var-expansion-35e53209-de1d-40b3-af20-434164dc94a8" in namespace "var-expansion-3872" to be "Succeeded or Failed"
Sep  5 21:33:14.491: INFO: Pod "var-expansion-35e53209-de1d-40b3-af20-434164dc94a8": Phase="Pending", Reason="", readiness=false. Elapsed: 9.308783ms
Sep  5 21:33:16.504: INFO: Pod "var-expansion-35e53209-de1d-40b3-af20-434164dc94a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023039939s
Sep  5 21:33:18.514: INFO: Pod "var-expansion-35e53209-de1d-40b3-af20-434164dc94a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032495523s
Sep  5 21:33:20.530: INFO: Pod "var-expansion-35e53209-de1d-40b3-af20-434164dc94a8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048324341s
Sep  5 21:33:22.537: INFO: Pod "var-expansion-35e53209-de1d-40b3-af20-434164dc94a8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.05549046s
Sep  5 21:33:24.558: INFO: Pod "var-expansion-35e53209-de1d-40b3-af20-434164dc94a8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.076468301s
Sep  5 21:33:26.569: INFO: Pod "var-expansion-35e53209-de1d-40b3-af20-434164dc94a8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.087712008s
Sep  5 21:33:28.679: INFO: Pod "var-expansion-35e53209-de1d-40b3-af20-434164dc94a8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.197977456s
Sep  5 21:33:30.690: INFO: Pod "var-expansion-35e53209-de1d-40b3-af20-434164dc94a8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.208587727s
Sep  5 21:33:32.698: INFO: Pod "var-expansion-35e53209-de1d-40b3-af20-434164dc94a8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.216296871s
Sep  5 21:33:34.705: INFO: Pod "var-expansion-35e53209-de1d-40b3-af20-434164dc94a8": Phase="Pending", Reason="", readiness=false. Elapsed: 20.223439252s
Sep  5 21:33:36.713: INFO: Pod "var-expansion-35e53209-de1d-40b3-af20-434164dc94a8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.231740303s
Sep  5 21:33:38.723: INFO: Pod "var-expansion-35e53209-de1d-40b3-af20-434164dc94a8": Phase="Pending", Reason="", readiness=false. Elapsed: 24.241625835s
Sep  5 21:33:40.731: INFO: Pod "var-expansion-35e53209-de1d-40b3-af20-434164dc94a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.249241138s
STEP: Saw pod success
Sep  5 21:33:40.731: INFO: Pod "var-expansion-35e53209-de1d-40b3-af20-434164dc94a8" satisfied condition "Succeeded or Failed"
Sep  5 21:33:40.740: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod var-expansion-35e53209-de1d-40b3-af20-434164dc94a8 container dapi-container: <nil>
STEP: delete the pod
Sep  5 21:33:40.811: INFO: Waiting for pod var-expansion-35e53209-de1d-40b3-af20-434164dc94a8 to disappear
Sep  5 21:33:40.856: INFO: Pod var-expansion-35e53209-de1d-40b3-af20-434164dc94a8 still exists
Sep  5 21:33:42.856: INFO: Waiting for pod var-expansion-35e53209-de1d-40b3-af20-434164dc94a8 to disappear
Sep  5 21:33:42.884: INFO: Pod var-expansion-35e53209-de1d-40b3-af20-434164dc94a8 still exists
Sep  5 21:33:44.856: INFO: Waiting for pod var-expansion-35e53209-de1d-40b3-af20-434164dc94a8 to disappear
Sep  5 21:33:44.865: INFO: Pod var-expansion-35e53209-de1d-40b3-af20-434164dc94a8 still exists
Sep  5 21:33:46.857: INFO: Waiting for pod var-expansion-35e53209-de1d-40b3-af20-434164dc94a8 to disappear
Sep  5 21:33:46.866: INFO: Pod var-expansion-35e53209-de1d-40b3-af20-434164dc94a8 still exists
Sep  5 21:33:48.856: INFO: Waiting for pod var-expansion-35e53209-de1d-40b3-af20-434164dc94a8 to disappear
Sep  5 21:33:48.864: INFO: Pod var-expansion-35e53209-de1d-40b3-af20-434164dc94a8 still exists
Sep  5 21:33:50.856: INFO: Waiting for pod var-expansion-35e53209-de1d-40b3-af20-434164dc94a8 to disappear
Sep  5 21:33:50.864: INFO: Pod var-expansion-35e53209-de1d-40b3-af20-434164dc94a8 still exists
Sep  5 21:33:52.857: INFO: Waiting for pod var-expansion-35e53209-de1d-40b3-af20-434164dc94a8 to disappear
Sep  5 21:33:52.865: INFO: Pod var-expansion-35e53209-de1d-40b3-af20-434164dc94a8 still exists
Sep  5 21:33:54.856: INFO: Waiting for pod var-expansion-35e53209-de1d-40b3-af20-434164dc94a8 to disappear
Sep  5 21:33:54.867: INFO: Pod var-expansion-35e53209-de1d-40b3-af20-434164dc94a8 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:33:54.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3872" for this suite.

â€¢ [SLOW TEST:41.108 seconds]
[k8s.io] Variable Expansion
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","total":291,"completed":47,"skipped":1003,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should patch a secret [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:33:55.109: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6504
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a secret [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a secret
STEP: listing secrets in all namespaces to ensure that there are more than zero
STEP: patching the secret
STEP: deleting the secret using a LabelSelector
STEP: listing secrets in all namespaces, searching for label name and value in patch
[AfterEach] [sig-api-machinery] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:33:55.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6504" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should patch a secret [Conformance]","total":291,"completed":48,"skipped":1015,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:33:56.032: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4347
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-70f35dbc-b8da-4923-af54-d9a8f75cfbe5 in namespace container-probe-4347
Sep  5 21:34:20.606: INFO: Started pod liveness-70f35dbc-b8da-4923-af54-d9a8f75cfbe5 in namespace container-probe-4347
STEP: checking the pod's current state and verifying that restartCount is present
Sep  5 21:34:20.612: INFO: Initial restart count of pod liveness-70f35dbc-b8da-4923-af54-d9a8f75cfbe5 is 0
Sep  5 21:34:34.712: INFO: Restart count of pod container-probe-4347/liveness-70f35dbc-b8da-4923-af54-d9a8f75cfbe5 is now 1 (14.100228247s elapsed)
Sep  5 21:34:50.794: INFO: Restart count of pod container-probe-4347/liveness-70f35dbc-b8da-4923-af54-d9a8f75cfbe5 is now 2 (30.182078s elapsed)
Sep  5 21:35:04.987: INFO: Restart count of pod container-probe-4347/liveness-70f35dbc-b8da-4923-af54-d9a8f75cfbe5 is now 3 (44.37445774s elapsed)
Sep  5 21:35:21.168: INFO: Restart count of pod container-probe-4347/liveness-70f35dbc-b8da-4923-af54-d9a8f75cfbe5 is now 4 (1m0.555661931s elapsed)
Sep  5 21:35:35.287: INFO: Restart count of pod container-probe-4347/liveness-70f35dbc-b8da-4923-af54-d9a8f75cfbe5 is now 5 (1m14.675217432s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:35:35.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4347" for this suite.

â€¢ [SLOW TEST:99.624 seconds]
[k8s.io] Probing container
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","total":291,"completed":49,"skipped":1021,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:35:35.656: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4955
STEP: Waiting for a default service account to be provisioned in namespace
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4955 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4955;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4955 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4955;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4955.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4955.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4955.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4955.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4955.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4955.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4955.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4955.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4955.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4955.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4955.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4955.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4955.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 36.80.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.80.36_udp@PTR;check="$$(dig +tcp +noall +answer +search 36.80.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.80.36_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4955 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4955;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4955 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4955;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4955.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4955.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4955.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4955.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4955.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4955.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4955.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4955.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4955.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4955.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4955.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4955.svc;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4955.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 36.80.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.80.36_udp@PTR;check="$$(dig +tcp +noall +answer +search 36.80.24.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.24.80.36_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 21:36:06.305: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4955/dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c: the server could not find the requested resource (get pods dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c)
Sep  5 21:36:06.320: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4955/dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c: the server could not find the requested resource (get pods dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c)
Sep  5 21:36:06.329: INFO: Unable to read wheezy_udp@dns-test-service.dns-4955 from pod dns-4955/dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c: the server could not find the requested resource (get pods dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c)
Sep  5 21:36:06.342: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4955 from pod dns-4955/dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c: the server could not find the requested resource (get pods dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c)
Sep  5 21:36:06.355: INFO: Unable to read wheezy_udp@dns-test-service.dns-4955.svc from pod dns-4955/dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c: the server could not find the requested resource (get pods dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c)
Sep  5 21:36:06.365: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4955.svc from pod dns-4955/dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c: the server could not find the requested resource (get pods dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c)
Sep  5 21:36:06.378: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4955.svc from pod dns-4955/dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c: the server could not find the requested resource (get pods dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c)
Sep  5 21:36:06.493: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4955/dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c: the server could not find the requested resource (get pods dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c)
Sep  5 21:36:06.514: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4955/dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c: the server could not find the requested resource (get pods dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c)
Sep  5 21:36:06.529: INFO: Unable to read jessie_udp@dns-test-service.dns-4955 from pod dns-4955/dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c: the server could not find the requested resource (get pods dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c)
Sep  5 21:36:06.543: INFO: Unable to read jessie_tcp@dns-test-service.dns-4955 from pod dns-4955/dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c: the server could not find the requested resource (get pods dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c)
Sep  5 21:36:06.553: INFO: Unable to read jessie_udp@dns-test-service.dns-4955.svc from pod dns-4955/dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c: the server could not find the requested resource (get pods dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c)
Sep  5 21:36:06.565: INFO: Unable to read jessie_tcp@dns-test-service.dns-4955.svc from pod dns-4955/dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c: the server could not find the requested resource (get pods dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c)
Sep  5 21:36:06.657: INFO: Lookups using dns-4955/dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4955 wheezy_tcp@dns-test-service.dns-4955 wheezy_udp@dns-test-service.dns-4955.svc wheezy_tcp@dns-test-service.dns-4955.svc wheezy_udp@_http._tcp.dns-test-service.dns-4955.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4955 jessie_tcp@dns-test-service.dns-4955 jessie_udp@dns-test-service.dns-4955.svc jessie_tcp@dns-test-service.dns-4955.svc]

Sep  5 21:36:12.075: INFO: DNS probes using dns-4955/dns-test-dbd85d69-f84f-4c2b-85e3-224f0951962c succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:36:12.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4955" for this suite.

â€¢ [SLOW TEST:37.155 seconds]
[sig-network] DNS
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","total":291,"completed":50,"skipped":1030,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:36:12.811: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-8523
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:36:37.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8523" for this suite.

â€¢ [SLOW TEST:24.995 seconds]
[sig-storage] EmptyDir wrapper volumes
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","total":291,"completed":51,"skipped":1033,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:36:37.806: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2897
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 21:37:02.303: INFO: Deleting pod "var-expansion-e92d32de-a58b-4b59-bfe8-74d0432ea50f" in namespace "var-expansion-2897"
Sep  5 21:37:02.356: INFO: Wait up to 5m0s for pod "var-expansion-e92d32de-a58b-4b59-bfe8-74d0432ea50f" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:37:16.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2897" for this suite.

â€¢ [SLOW TEST:38.809 seconds]
[k8s.io] Variable Expansion
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with backticks [sig-storage][Slow] [Conformance]","total":291,"completed":52,"skipped":1038,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Events
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:37:16.616: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7406
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep  5 21:37:41.117: INFO: &Pod{ObjectMeta:{send-events-0c631382-c61b-4dea-9685-27b46afedefd  events-7406 /api/v1/namespaces/events-7406/pods/send-events-0c631382-c61b-4dea-9685-27b46afedefd 3c42c512-9dad-429c-b74d-384d08e1e6e7 89538 0 2021-09-05 21:37:16 -0700 PDT <nil> <nil> map[name:foo time:992425912] map[attachment_id:b1f40f76-6f28-4e8d-b4f8-eb808324483a kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:60:1c vlan:None vmware-system-ephemeral-disk-uuid:6000C291-efc4-82a1-850f-a9470671a21c vmware-system-image-references:{"p":"agnhost-e4bbf9f419e02f0332193ee525b99e9c3c927d53-v39971"} vmware-system-vm-moid:vm-500:7badc608-dd2f-42a9-952d-0d4c30d5f283 vmware-system-vm-uuid:501792ae-7c8a-d046-7397-256b3f5c7413] [] [lifecycle-controller/system.vmware.com]  [{e2e.test Update v1 2021-09-05 21:37:16 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:time":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"p\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":80,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-05 21:37:16 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {nsx-ncp-6d7f7bf559-9mqvr Update v1 2021-09-05 21:37:25 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-05 21:37:32 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-05 21:37:40 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-6q89l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-6q89l,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:p,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-6q89l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-43-208.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 21:37:16 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 21:37:41 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 21:37:41 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 21:37:41 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.43.208,PodIP:172.26.1.194,StartTime:2021-09-05 21:37:36 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-05 21:37:36 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:agnhost-e4bbf9f419e02f0332193ee525b99e9c3c927d53-v39971,ContainerID:9c26473f-3055-438c-9f76-aab42aaa81df,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Sep  5 21:37:43.126: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep  5 21:37:45.133: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:37:45.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7406" for this suite.

â€¢ [SLOW TEST:28.744 seconds]
[k8s.io] [sig-node] Events
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]","total":291,"completed":53,"skipped":1044,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:37:45.362: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-192
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-c2587548-7357-4ab4-9c46-c222009ef6c5 in namespace container-probe-192
Sep  5 21:38:05.843: INFO: Started pod liveness-c2587548-7357-4ab4-9c46-c222009ef6c5 in namespace container-probe-192
STEP: checking the pod's current state and verifying that restartCount is present
Sep  5 21:38:05.848: INFO: Initial restart count of pod liveness-c2587548-7357-4ab4-9c46-c222009ef6c5 is 0
Sep  5 21:38:21.937: INFO: Restart count of pod container-probe-192/liveness-c2587548-7357-4ab4-9c46-c222009ef6c5 is now 1 (16.088562713s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:38:21.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-192" for this suite.

â€¢ [SLOW TEST:36.861 seconds]
[k8s.io] Probing container
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":291,"completed":54,"skipped":1073,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:38:22.224: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7773
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-39678d6d-03e4-4294-bc1a-33755b54c5db
STEP: Creating a pod to test consume secrets
Sep  5 21:38:22.640: INFO: Waiting up to 5m0s for pod "pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1" in namespace "secrets-7773" to be "Succeeded or Failed"
Sep  5 21:38:22.654: INFO: Pod "pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.963205ms
Sep  5 21:38:24.660: INFO: Pod "pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019966617s
Sep  5 21:38:26.674: INFO: Pod "pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034534838s
Sep  5 21:38:28.688: INFO: Pod "pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04853792s
Sep  5 21:38:30.697: INFO: Pod "pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.057365792s
Sep  5 21:38:32.715: INFO: Pod "pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.075522464s
Sep  5 21:38:34.745: INFO: Pod "pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.104802633s
Sep  5 21:38:36.759: INFO: Pod "pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.119458573s
Sep  5 21:38:38.790: INFO: Pod "pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.150135085s
Sep  5 21:38:40.798: INFO: Pod "pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.15848589s
Sep  5 21:38:42.806: INFO: Pod "pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1": Phase="Running", Reason="", readiness=true. Elapsed: 20.166663874s
Sep  5 21:38:44.816: INFO: Pod "pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1": Phase="Running", Reason="", readiness=true. Elapsed: 22.175715587s
Sep  5 21:38:46.825: INFO: Pod "pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1": Phase="Running", Reason="", readiness=true. Elapsed: 24.185461216s
Sep  5 21:38:48.967: INFO: Pod "pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.327352701s
STEP: Saw pod success
Sep  5 21:38:48.967: INFO: Pod "pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1" satisfied condition "Succeeded or Failed"
Sep  5 21:38:48.987: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com pod pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 21:38:54.069: INFO: Waiting for pod pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1 to disappear
Sep  5 21:38:54.079: INFO: Pod pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1 still exists
Sep  5 21:38:56.080: INFO: Waiting for pod pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1 to disappear
Sep  5 21:38:56.091: INFO: Pod pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1 still exists
Sep  5 21:38:58.080: INFO: Waiting for pod pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1 to disappear
Sep  5 21:38:58.101: INFO: Pod pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1 still exists
Sep  5 21:39:00.079: INFO: Waiting for pod pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1 to disappear
Sep  5 21:39:00.088: INFO: Pod pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1 still exists
Sep  5 21:39:02.081: INFO: Waiting for pod pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1 to disappear
Sep  5 21:39:02.095: INFO: Pod pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1 still exists
Sep  5 21:39:04.080: INFO: Waiting for pod pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1 to disappear
Sep  5 21:39:04.091: INFO: Pod pod-secrets-85a2b6c0-6e26-4993-ac6a-3483801558c1 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:39:04.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7773" for this suite.

â€¢ [SLOW TEST:42.097 seconds]
[sig-storage] Secrets
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":55,"skipped":1085,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:39:04.321: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-5444
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 21:39:04.785: INFO: Waiting up to 5m0s for pod "busybox-user-65534-ba8e2bef-8994-45d1-94c2-84f7a0045c7a" in namespace "security-context-test-5444" to be "Succeeded or Failed"
Sep  5 21:39:04.794: INFO: Pod "busybox-user-65534-ba8e2bef-8994-45d1-94c2-84f7a0045c7a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.645779ms
Sep  5 21:39:06.810: INFO: Pod "busybox-user-65534-ba8e2bef-8994-45d1-94c2-84f7a0045c7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025549249s
Sep  5 21:39:08.863: INFO: Pod "busybox-user-65534-ba8e2bef-8994-45d1-94c2-84f7a0045c7a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.078451452s
Sep  5 21:39:10.888: INFO: Pod "busybox-user-65534-ba8e2bef-8994-45d1-94c2-84f7a0045c7a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.10293199s
Sep  5 21:39:12.896: INFO: Pod "busybox-user-65534-ba8e2bef-8994-45d1-94c2-84f7a0045c7a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.110763768s
Sep  5 21:39:14.910: INFO: Pod "busybox-user-65534-ba8e2bef-8994-45d1-94c2-84f7a0045c7a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.12503467s
Sep  5 21:39:16.919: INFO: Pod "busybox-user-65534-ba8e2bef-8994-45d1-94c2-84f7a0045c7a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.134550592s
Sep  5 21:39:18.934: INFO: Pod "busybox-user-65534-ba8e2bef-8994-45d1-94c2-84f7a0045c7a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.148833331s
Sep  5 21:39:20.944: INFO: Pod "busybox-user-65534-ba8e2bef-8994-45d1-94c2-84f7a0045c7a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.159257818s
Sep  5 21:39:22.951: INFO: Pod "busybox-user-65534-ba8e2bef-8994-45d1-94c2-84f7a0045c7a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.166196047s
Sep  5 21:39:24.957: INFO: Pod "busybox-user-65534-ba8e2bef-8994-45d1-94c2-84f7a0045c7a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.172520209s
Sep  5 21:39:26.969: INFO: Pod "busybox-user-65534-ba8e2bef-8994-45d1-94c2-84f7a0045c7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.18454492s
Sep  5 21:39:26.969: INFO: Pod "busybox-user-65534-ba8e2bef-8994-45d1-94c2-84f7a0045c7a" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:39:26.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5444" for this suite.

â€¢ [SLOW TEST:22.837 seconds]
[k8s.io] Security Context
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  When creating a container with runAsUser
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:45
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":56,"skipped":1093,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:39:27.158: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4545
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:40:27.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4545" for this suite.

â€¢ [SLOW TEST:60.644 seconds]
[k8s.io] Probing container
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","total":291,"completed":57,"skipped":1095,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:40:27.802: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6264
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-6264
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6264
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6264
Sep  5 21:40:28.299: INFO: Found 0 stateful pods, waiting for 1
Sep  5 21:40:38.309: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 21:40:48.305: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 21:40:58.309: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep  5 21:40:58.316: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-6264 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 21:40:59.051: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 21:40:59.051: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 21:40:59.051: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  5 21:40:59.058: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  5 21:41:09.102: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 21:41:09.103: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 21:41:09.170: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999589s
Sep  5 21:41:10.268: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.980119561s
Sep  5 21:41:11.275: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.881667428s
Sep  5 21:41:12.285: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.873817409s
Sep  5 21:41:13.291: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.86459015s
Sep  5 21:41:14.298: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.858839474s
Sep  5 21:41:15.308: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.850696291s
Sep  5 21:41:16.316: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.84177664s
Sep  5 21:41:17.324: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.833130697s
Sep  5 21:41:18.331: INFO: Verifying statefulset ss doesn't scale past 1 for another 825.202918ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6264
Sep  5 21:41:19.345: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-6264 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 21:41:19.582: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  5 21:41:19.582: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  5 21:41:19.582: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  5 21:41:19.590: INFO: Found 1 stateful pods, waiting for 3
Sep  5 21:41:29.603: INFO: Found 2 stateful pods, waiting for 3
Sep  5 21:41:39.598: INFO: Found 2 stateful pods, waiting for 3
Sep  5 21:41:49.599: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:41:49.599: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:41:49.599: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 21:41:59.601: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:41:59.601: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:41:59.601: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 21:42:09.606: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:42:09.606: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 21:42:09.606: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep  5 21:42:09.623: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-6264 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 21:42:10.011: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 21:42:10.011: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 21:42:10.011: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  5 21:42:10.011: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-6264 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 21:42:10.271: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 21:42:10.271: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 21:42:10.271: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  5 21:42:10.271: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-6264 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 21:42:10.526: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 21:42:10.526: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 21:42:10.526: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  5 21:42:10.526: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 21:42:10.542: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep  5 21:42:20.562: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 21:42:20.562: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 21:42:20.562: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 21:42:20.604: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999164s
Sep  5 21:42:21.624: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989951462s
Sep  5 21:42:22.636: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.969367244s
Sep  5 21:42:23.648: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.956467691s
Sep  5 21:42:24.656: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.9451122s
Sep  5 21:42:25.665: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.937797994s
Sep  5 21:42:26.675: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.928589402s
Sep  5 21:42:27.684: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.919091786s
Sep  5 21:42:28.694: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.908553838s
Sep  5 21:42:29.716: INFO: Verifying statefulset ss doesn't scale past 3 for another 899.394463ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6264
Sep  5 21:42:30.732: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-6264 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 21:42:30.969: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  5 21:42:30.969: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  5 21:42:30.969: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  5 21:42:30.969: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-6264 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 21:42:31.182: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  5 21:42:31.182: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  5 21:42:31.182: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  5 21:42:31.182: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-6264 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 21:42:31.408: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  5 21:42:31.408: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  5 21:42:31.408: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  5 21:42:31.408: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep  5 21:43:11.443: INFO: Deleting all statefulset in ns statefulset-6264
Sep  5 21:43:11.451: INFO: Scaling statefulset ss to 0
Sep  5 21:43:11.485: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 21:43:11.493: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:43:11.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6264" for this suite.

â€¢ [SLOW TEST:164.105 seconds]
[sig-apps] StatefulSet
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","total":291,"completed":58,"skipped":1098,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:43:11.908: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4679
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Sep  5 21:45:56.684: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Sep  5 21:45:56.684: INFO: Deleting pod "simpletest.rc-2tdnm" in namespace "gc-4679"
Sep  5 21:45:56.705: INFO: Deleting pod "simpletest.rc-44nh9" in namespace "gc-4679"
Sep  5 21:45:56.737: INFO: Deleting pod "simpletest.rc-6lgr9" in namespace "gc-4679"
Sep  5 21:45:56.762: INFO: Deleting pod "simpletest.rc-dv7jc" in namespace "gc-4679"
Sep  5 21:45:56.789: INFO: Deleting pod "simpletest.rc-gftfl" in namespace "gc-4679"
Sep  5 21:45:56.822: INFO: Deleting pod "simpletest.rc-klvgx" in namespace "gc-4679"
Sep  5 21:45:56.868: INFO: Deleting pod "simpletest.rc-nl8z2" in namespace "gc-4679"
Sep  5 21:45:56.972: INFO: Deleting pod "simpletest.rc-sslbq" in namespace "gc-4679"
Sep  5 21:45:57.043: INFO: Deleting pod "simpletest.rc-w82jl" in namespace "gc-4679"
Sep  5 21:45:57.114: INFO: Deleting pod "simpletest.rc-xcsmj" in namespace "gc-4679"
[AfterEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:45:57.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4679" for this suite.

â€¢ [SLOW TEST:165.786 seconds]
[sig-api-machinery] Garbage collector
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","total":291,"completed":59,"skipped":1116,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:45:57.695: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6180
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep  5 21:45:58.230: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  5 21:45:58.286: INFO: Waiting for terminating namespaces to be deleted...
Sep  5 21:45:58.305: INFO: 
Logging pods the apiserver thinks is on node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com before test
Sep  5 21:45:58.335: INFO: simpletest.rc-nl8z2 from gc-4679 started at 2021-09-05 21:43:37 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.335: INFO: 	Container nginx ready: true, restart count 0
Sep  5 21:45:58.335: INFO: simpletest.rc-w82jl from gc-4679 started at 2021-09-05 21:43:38 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.335: INFO: 	Container nginx ready: true, restart count 0
Sep  5 21:45:58.335: INFO: podwithpersistentvolume from storage-class-test-2 started at 2021-09-05 20:05:57 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.335: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:45:58.335: INFO: hello-web-6b97664bd5-f5452 from test-cluster-ip-service started at 2021-09-05 20:10:51 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.336: INFO: 	Container hello-app ready: true, restart count 0
Sep  5 21:45:58.336: INFO: wcp-sanity-busybox-6f999d6849-45jct from test-dataprovider-podvms-ns started at 2021-09-05 19:57:27 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.336: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 21:45:58.336: INFO: wcp-sanity-busybox-6f999d6849-c27n2 from test-dataprovider-podvms-ns started at 2021-09-05 21:11:14 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.336: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 21:45:58.336: INFO: nginx-private from test-image-pull-secrets-ns started at 2021-09-05 19:59:11 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.336: INFO: 	Container nginx-private-container ready: true, restart count 0
Sep  5 21:45:58.336: INFO: curl-pod from test-network-policy started at 2021-09-05 20:14:04 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.336: INFO: 	Container curl-container ready: true, restart count 0
Sep  5 21:45:58.336: INFO: hello-web-1-6b97664bd5-cl9tj from test-network-policy started at 2021-09-05 21:09:17 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.336: INFO: 	Container hello-app ready: true, restart count 0
Sep  5 21:45:58.336: INFO: schedext-test-node-selector-1 from test-node-selector started at 2021-09-05 20:00:20 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.336: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:45:58.336: INFO: busybox from test-pod-external-nw-access started at 2021-09-05 20:15:25 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.336: INFO: 	Container busybox ready: true, restart count 0
Sep  5 21:45:58.336: INFO: busybox-annotation from test-podvm-annotations started at 2021-09-05 20:03:14 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.336: INFO: 	Container busybox-annotation ready: true, restart count 0
Sep  5 21:45:58.336: INFO: helloworld from test-telemetry started at 2021-09-05 20:09:22 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.336: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:45:58.336: INFO: wcp-sanity-busybox-6f999d6849-46njm from test-update-workload-ns started at 2021-09-05 20:07:43 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.336: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 21:45:58.336: INFO: 
Logging pods the apiserver thinks is on node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com before test
Sep  5 21:45:58.384: INFO: simpletest.rc-2tdnm from gc-4679 started at 2021-09-05 21:43:37 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.384: INFO: 	Container nginx ready: true, restart count 0
Sep  5 21:45:58.384: INFO: simpletest.rc-dv7jc from gc-4679 started at 2021-09-05 21:43:36 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.384: INFO: 	Container nginx ready: true, restart count 0
Sep  5 21:45:58.384: INFO: curl-pod from test-cluster-ip-service started at 2021-09-05 20:10:11 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.384: INFO: 	Container curl-container ready: true, restart count 0
Sep  5 21:45:58.384: INFO: helloworld from test-exec-ns started at 2021-09-05 19:58:27 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.384: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:45:58.384: INFO: schedext-test-node-selector-2 from test-node-selector started at 2021-09-05 20:00:22 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.384: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:45:58.384: INFO: schedext-test-affinity-1 from test-pod-affinity started at 2021-09-05 20:00:51 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.384: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:45:58.384: INFO: schedext-test-affinity-2 from test-pod-affinity started at 2021-09-05 20:01:16 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.384: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:45:58.384: INFO: test-docker-registry from test-private-image-registry-ns started at 2021-09-05 20:16:11 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.384: INFO: 	Container test-docker-registry ready: true, restart count 0
Sep  5 21:45:58.384: INFO: helloworld from test-update-workload-ns started at 2021-09-05 20:08:34 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.384: INFO: 	Container hello ready: true, restart count 0
Sep  5 21:45:58.384: INFO: 
Logging pods the apiserver thinks is on node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com before test
Sep  5 21:45:58.413: INFO: simpletest.rc-44nh9 from gc-4679 started at 2021-09-05 21:43:43 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.413: INFO: 	Container nginx ready: true, restart count 0
Sep  5 21:45:58.413: INFO: simpletest.rc-6lgr9 from gc-4679 started at 2021-09-05 21:43:44 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.413: INFO: 	Container nginx ready: true, restart count 0
Sep  5 21:45:58.413: INFO: simpletest.rc-gftfl from gc-4679 started at 2021-09-05 21:43:42 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.413: INFO: 	Container nginx ready: true, restart count 0
Sep  5 21:45:58.413: INFO: simpletest.rc-klvgx from gc-4679 started at 2021-09-05 21:43:44 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.413: INFO: 	Container nginx ready: true, restart count 0
Sep  5 21:45:58.413: INFO: simpletest.rc-sslbq from gc-4679 started at 2021-09-05 21:43:42 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.413: INFO: 	Container nginx ready: true, restart count 0
Sep  5 21:45:58.413: INFO: simpletest.rc-xcsmj from gc-4679 started at 2021-09-05 21:43:42 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.413: INFO: 	Container nginx ready: true, restart count 0
Sep  5 21:45:58.413: INFO: hello-web-2-f779cbdff-hffpj from test-network-policy started at 2021-09-05 21:09:20 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.413: INFO: 	Container hello-app ready: true, restart count 0
Sep  5 21:45:58.413: INFO: wcp-sanity-busybox-6f999d6849-856nv from test-update-workload-ns started at 2021-09-05 21:10:00 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.413: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 21:45:58.413: INFO: wcp-sanity-busybox-6f999d6849-mts92 from test-update-workload-ns started at 2021-09-05 21:10:00 -0700 PDT (1 container statuses recorded)
Sep  5 21:45:58.413: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-7b75c239-19d3-4672-95e1-222a278dc89d 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-7b75c239-19d3-4672-95e1-222a278dc89d off the node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
STEP: verifying the node doesn't have the label kubernetes.io/e2e-7b75c239-19d3-4672-95e1-222a278dc89d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:47:16.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6180" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

â€¢ [SLOW TEST:79.195 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]","total":291,"completed":60,"skipped":1160,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:47:16.890: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7796
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-d6b2cb1f-cccf-4036-8e8c-c7f05ca1f662
STEP: Creating a pod to test consume secrets
Sep  5 21:47:17.318: INFO: Waiting up to 5m0s for pod "pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a" in namespace "secrets-7796" to be "Succeeded or Failed"
Sep  5 21:47:17.330: INFO: Pod "pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.150187ms
Sep  5 21:47:19.339: INFO: Pod "pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020880065s
Sep  5 21:47:21.346: INFO: Pod "pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028025818s
Sep  5 21:47:23.358: INFO: Pod "pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040449602s
Sep  5 21:47:25.365: INFO: Pod "pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.047573698s
Sep  5 21:47:27.388: INFO: Pod "pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.069927044s
Sep  5 21:47:29.399: INFO: Pod "pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.081622838s
Sep  5 21:47:31.410: INFO: Pod "pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.092238353s
Sep  5 21:47:33.418: INFO: Pod "pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.099858445s
Sep  5 21:47:35.437: INFO: Pod "pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.118928862s
Sep  5 21:47:37.445: INFO: Pod "pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.126878977s
Sep  5 21:47:39.454: INFO: Pod "pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.136261266s
STEP: Saw pod success
Sep  5 21:47:39.454: INFO: Pod "pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a" satisfied condition "Succeeded or Failed"
Sep  5 21:47:39.460: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a container secret-env-test: <nil>
STEP: delete the pod
Sep  5 21:47:44.051: INFO: Waiting for pod pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a to disappear
Sep  5 21:47:44.087: INFO: Pod pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a still exists
Sep  5 21:47:46.088: INFO: Waiting for pod pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a to disappear
Sep  5 21:47:46.110: INFO: Pod pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a still exists
Sep  5 21:47:48.088: INFO: Waiting for pod pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a to disappear
Sep  5 21:47:48.094: INFO: Pod pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a still exists
Sep  5 21:47:50.088: INFO: Waiting for pod pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a to disappear
Sep  5 21:47:50.119: INFO: Pod pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a still exists
Sep  5 21:47:52.092: INFO: Waiting for pod pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a to disappear
Sep  5 21:47:52.103: INFO: Pod pod-secrets-b872fdfd-ed28-41bf-b232-dbb3730a457a no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:47:52.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7796" for this suite.

â€¢ [SLOW TEST:35.451 seconds]
[sig-api-machinery] Secrets
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:36
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","total":291,"completed":61,"skipped":1165,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:47:52.342: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-573
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-ab117fd0-cedf-495d-bdb4-fdddf635c439
STEP: Creating a pod to test consume configMaps
Sep  5 21:47:52.890: INFO: Waiting up to 5m0s for pod "pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765" in namespace "configmap-573" to be "Succeeded or Failed"
Sep  5 21:47:52.919: INFO: Pod "pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765": Phase="Pending", Reason="", readiness=false. Elapsed: 28.928047ms
Sep  5 21:47:54.928: INFO: Pod "pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037227573s
Sep  5 21:47:56.936: INFO: Pod "pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045501376s
Sep  5 21:47:58.950: INFO: Pod "pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765": Phase="Pending", Reason="", readiness=false. Elapsed: 6.059995593s
Sep  5 21:48:00.963: INFO: Pod "pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765": Phase="Pending", Reason="", readiness=false. Elapsed: 8.072565178s
Sep  5 21:48:02.983: INFO: Pod "pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765": Phase="Pending", Reason="", readiness=false. Elapsed: 10.092556627s
Sep  5 21:48:04.999: INFO: Pod "pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765": Phase="Pending", Reason="", readiness=false. Elapsed: 12.108656251s
Sep  5 21:48:07.008: INFO: Pod "pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765": Phase="Pending", Reason="", readiness=false. Elapsed: 14.117062576s
Sep  5 21:48:09.029: INFO: Pod "pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765": Phase="Pending", Reason="", readiness=false. Elapsed: 16.13873153s
Sep  5 21:48:11.035: INFO: Pod "pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765": Phase="Pending", Reason="", readiness=false. Elapsed: 18.144627396s
Sep  5 21:48:13.043: INFO: Pod "pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765": Phase="Pending", Reason="", readiness=false. Elapsed: 20.152289905s
Sep  5 21:48:15.049: INFO: Pod "pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765": Phase="Pending", Reason="", readiness=false. Elapsed: 22.15890488s
Sep  5 21:48:17.058: INFO: Pod "pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765": Phase="Pending", Reason="", readiness=false. Elapsed: 24.167889653s
Sep  5 21:48:19.068: INFO: Pod "pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.177663049s
STEP: Saw pod success
Sep  5 21:48:19.068: INFO: Pod "pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765" satisfied condition "Succeeded or Failed"
Sep  5 21:48:19.075: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 21:48:23.592: INFO: Waiting for pod pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765 to disappear
Sep  5 21:48:23.608: INFO: Pod pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765 still exists
Sep  5 21:48:25.608: INFO: Waiting for pod pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765 to disappear
Sep  5 21:48:25.617: INFO: Pod pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765 still exists
Sep  5 21:48:27.609: INFO: Waiting for pod pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765 to disappear
Sep  5 21:48:27.618: INFO: Pod pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765 still exists
Sep  5 21:48:29.609: INFO: Waiting for pod pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765 to disappear
Sep  5 21:48:29.618: INFO: Pod pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765 still exists
Sep  5 21:48:31.609: INFO: Waiting for pod pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765 to disappear
Sep  5 21:48:31.616: INFO: Pod pod-configmaps-0a25a734-b843-41d2-98cc-ab95d21ec765 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:48:31.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-573" for this suite.

â€¢ [SLOW TEST:39.479 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","total":291,"completed":62,"skipped":1174,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:48:31.821: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-381
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 21:48:32.925: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  5 21:48:34.951: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:48:36.970: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:48:38.959: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:48:40.958: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:48:42.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:48:44.960: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:48:46.960: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:48:48.968: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:48:50.960: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:48:52.960: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:48:54.958: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:48:56.957: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 48, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 21:48:59.993: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 21:49:00.000: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:49:01.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-381" for this suite.
STEP: Destroying namespace "webhook-381-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:31.626 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","total":291,"completed":63,"skipped":1206,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff 
  should check if kubectl diff finds a difference for Deployments [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:49:03.447: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2621
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create deployment with httpd image
Sep  5 21:49:03.893: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml create -f -'
Sep  5 21:49:04.377: INFO: stderr: ""
Sep  5 21:49:04.377: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image
Sep  5 21:49:04.377: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml diff -f -'
Sep  5 21:49:05.017: INFO: rc: 1
Sep  5 21:49:05.017: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml delete -f -'
Sep  5 21:49:05.297: INFO: stderr: ""
Sep  5 21:49:05.297: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:49:05.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2621" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","total":291,"completed":64,"skipped":1214,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:49:05.559: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5495
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  5 21:49:06.075: INFO: Waiting up to 5m0s for pod "pod-be73e4c2-4c34-418d-b41f-fda066618cb2" in namespace "emptydir-5495" to be "Succeeded or Failed"
Sep  5 21:49:06.106: INFO: Pod "pod-be73e4c2-4c34-418d-b41f-fda066618cb2": Phase="Pending", Reason="", readiness=false. Elapsed: 30.604627ms
Sep  5 21:49:08.128: INFO: Pod "pod-be73e4c2-4c34-418d-b41f-fda066618cb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052815571s
Sep  5 21:49:10.198: INFO: Pod "pod-be73e4c2-4c34-418d-b41f-fda066618cb2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.122575092s
Sep  5 21:49:12.223: INFO: Pod "pod-be73e4c2-4c34-418d-b41f-fda066618cb2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.147689819s
Sep  5 21:49:14.230: INFO: Pod "pod-be73e4c2-4c34-418d-b41f-fda066618cb2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.154382539s
Sep  5 21:49:16.240: INFO: Pod "pod-be73e4c2-4c34-418d-b41f-fda066618cb2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.164707912s
Sep  5 21:49:18.250: INFO: Pod "pod-be73e4c2-4c34-418d-b41f-fda066618cb2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.174177113s
Sep  5 21:49:20.258: INFO: Pod "pod-be73e4c2-4c34-418d-b41f-fda066618cb2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.182730069s
Sep  5 21:49:22.265: INFO: Pod "pod-be73e4c2-4c34-418d-b41f-fda066618cb2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.189230288s
Sep  5 21:49:24.271: INFO: Pod "pod-be73e4c2-4c34-418d-b41f-fda066618cb2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.195355507s
Sep  5 21:49:26.279: INFO: Pod "pod-be73e4c2-4c34-418d-b41f-fda066618cb2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.203614202s
Sep  5 21:49:28.289: INFO: Pod "pod-be73e4c2-4c34-418d-b41f-fda066618cb2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.213910662s
Sep  5 21:49:30.296: INFO: Pod "pod-be73e4c2-4c34-418d-b41f-fda066618cb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.220024229s
STEP: Saw pod success
Sep  5 21:49:30.296: INFO: Pod "pod-be73e4c2-4c34-418d-b41f-fda066618cb2" satisfied condition "Succeeded or Failed"
Sep  5 21:49:30.300: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-be73e4c2-4c34-418d-b41f-fda066618cb2 container test-container: <nil>
STEP: delete the pod
Sep  5 21:49:35.832: INFO: Waiting for pod pod-be73e4c2-4c34-418d-b41f-fda066618cb2 to disappear
Sep  5 21:49:35.846: INFO: Pod pod-be73e4c2-4c34-418d-b41f-fda066618cb2 still exists
Sep  5 21:49:37.846: INFO: Waiting for pod pod-be73e4c2-4c34-418d-b41f-fda066618cb2 to disappear
Sep  5 21:49:37.855: INFO: Pod pod-be73e4c2-4c34-418d-b41f-fda066618cb2 still exists
Sep  5 21:49:39.847: INFO: Waiting for pod pod-be73e4c2-4c34-418d-b41f-fda066618cb2 to disappear
Sep  5 21:49:39.856: INFO: Pod pod-be73e4c2-4c34-418d-b41f-fda066618cb2 still exists
Sep  5 21:49:41.846: INFO: Waiting for pod pod-be73e4c2-4c34-418d-b41f-fda066618cb2 to disappear
Sep  5 21:49:41.854: INFO: Pod pod-be73e4c2-4c34-418d-b41f-fda066618cb2 still exists
Sep  5 21:49:43.846: INFO: Waiting for pod pod-be73e4c2-4c34-418d-b41f-fda066618cb2 to disappear
Sep  5 21:49:43.856: INFO: Pod pod-be73e4c2-4c34-418d-b41f-fda066618cb2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:49:43.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5495" for this suite.

â€¢ [SLOW TEST:38.513 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":65,"skipped":1220,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:49:44.073: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2815
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl replace
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1581
[It] should update a single-container pod's image  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image mirror.gcr.io/library/httpd:2.4.38-alpine
Sep  5 21:49:44.553: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml run e2e-test-httpd-pod --image=mirror.gcr.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-2815'
Sep  5 21:49:44.686: INFO: stderr: ""
Sep  5 21:49:44.686: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Sep  5 21:50:09.740: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pod e2e-test-httpd-pod --namespace=kubectl-2815 -o json'
Sep  5 21:50:09.838: INFO: stderr: ""
Sep  5 21:50:09.839: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"attachment_id\": \"bc42cf04-a974-47b8-b8c0-121081fc2e66\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\",\n            \"mac\": \"04:50:56:00:60:10\",\n            \"vlan\": \"None\",\n            \"vmware-system-ephemeral-disk-uuid\": \"6000C291-6817-f190-28de-904b3d6c70e9\",\n            \"vmware-system-image-references\": \"{\\\"e2e-test-httpd-pod\\\":\\\"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v4306\\\"}\",\n            \"vmware-system-vm-moid\": \"vm-568:7badc608-dd2f-42a9-952d-0d4c30d5f283\",\n            \"vmware-system-vm-uuid\": \"5017eeab-c1d4-a7b6-53b1-e5687dfa3074\"\n        },\n        \"creationTimestamp\": \"2021-09-06T04:49:44Z\",\n        \"finalizers\": [\n            \"lifecycle-controller/system.vmware.com\"\n        ],\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2815\",\n        \"resourceVersion\": \"99018\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2815/pods/e2e-test-httpd-pod\",\n        \"uid\": \"ef0b208a-42bc-480f-839c-9c718bc1b34d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"mirror.gcr.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-gfbkz\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"sc2-rdops-vm09-dhcp-43-208.eng.vmware.com\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-gfbkz\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-gfbkz\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-06T04:49:44Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-06T04:50:09Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-06T04:50:09Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2021-09-06T04:50:09Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"795552f9-c35a-4b01-a819-1630359965b3\",\n                \"image\": \"mirror.gcr.io/library/httpd:2.4.38-alpine\",\n                \"imageID\": \"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v4306\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2021-09-06T04:50:07Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.193.43.208\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.26.1.194\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.26.1.194\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2021-09-06T04:50:06Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep  5 21:50:09.839: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml replace -f - --namespace=kubectl-2815'
Sep  5 21:50:10.557: INFO: stderr: ""
Sep  5 21:50:10.557: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image mirror.gcr.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1586
Sep  5 21:50:10.579: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml delete pods e2e-test-httpd-pod --namespace=kubectl-2815'
Sep  5 21:50:21.097: INFO: stderr: ""
Sep  5 21:50:21.097: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:50:21.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2815" for this suite.

â€¢ [SLOW TEST:37.251 seconds]
[sig-cli] Kubectl client
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1577
    should update a single-container pod's image  [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","total":291,"completed":66,"skipped":1241,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:50:21.325: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5362
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 21:50:22.785: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  5 21:50:24.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:50:26.822: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:50:28.821: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:50:30.817: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:50:32.817: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:50:34.822: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:50:36.838: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:50:38.828: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:50:40.824: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:50:42.820: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 50, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 21:50:45.862: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Sep  5 21:51:10.008: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml attach --namespace=webhook-5362 to-be-attached-pod -i -c=container1'
Sep  5 21:51:10.729: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:51:10.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5362" for this suite.
STEP: Destroying namespace "webhook-5362-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:50.281 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","total":291,"completed":67,"skipped":1258,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:51:11.606: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-2311
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-2311
[It] should have a working scale subresource [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating statefulset ss in namespace statefulset-2311
Sep  5 21:51:12.148: INFO: Found 0 stateful pods, waiting for 1
Sep  5 21:51:22.171: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 21:51:32.164: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 21:51:42.157: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep  5 21:51:42.199: INFO: Deleting all statefulset in ns statefulset-2311
Sep  5 21:51:42.221: INFO: Scaling statefulset ss to 0
Sep  5 21:52:02.281: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 21:52:02.290: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:52:02.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2311" for this suite.

â€¢ [SLOW TEST:51.109 seconds]
[sig-apps] StatefulSet
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should have a working scale subresource [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","total":291,"completed":68,"skipped":1268,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:52:02.715: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7827
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 21:52:04.425: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  5 21:52:06.447: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:08.469: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:10.456: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:12.456: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:14.457: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:16.457: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:18.463: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:20.472: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:22.456: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:24.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:26.453: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:52:28.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 52, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 21:52:31.534: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:52:31.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7827" for this suite.
STEP: Destroying namespace "webhook-7827-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:29.843 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","total":291,"completed":69,"skipped":1276,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:52:32.559: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2820
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should print the output to logs [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:52:55.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2820" for this suite.

â€¢ [SLOW TEST:22.806 seconds]
[k8s.io] Kubelet
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a busybox command in a pod
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:41
    should print the output to logs [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","total":291,"completed":70,"skipped":1322,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:52:55.365: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3821
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should serve a basic image on each replica with a public image  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating replication controller my-hostname-basic-82b773ea-ae6c-4b55-84f2-a75637f27982
Sep  5 21:52:56.257: INFO: Pod name my-hostname-basic-82b773ea-ae6c-4b55-84f2-a75637f27982: Found 0 pods out of 1
Sep  5 21:53:01.275: INFO: Pod name my-hostname-basic-82b773ea-ae6c-4b55-84f2-a75637f27982: Found 1 pods out of 1
Sep  5 21:53:01.275: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-82b773ea-ae6c-4b55-84f2-a75637f27982" are running
Sep  5 21:53:21.294: INFO: Pod "my-hostname-basic-82b773ea-ae6c-4b55-84f2-a75637f27982-hsgcl" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-05 21:52:56 -0700 PDT Reason: Message:}])
Sep  5 21:53:21.296: INFO: Trying to dial the pod
Sep  5 21:53:26.349: INFO: Controller my-hostname-basic-82b773ea-ae6c-4b55-84f2-a75637f27982: Got expected result from replica 1 [my-hostname-basic-82b773ea-ae6c-4b55-84f2-a75637f27982-hsgcl]: "my-hostname-basic-82b773ea-ae6c-4b55-84f2-a75637f27982-hsgcl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:53:26.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3821" for this suite.

â€¢ [SLOW TEST:31.212 seconds]
[sig-apps] ReplicationController
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","total":291,"completed":71,"skipped":1324,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
S
------------------------------
[k8s.io] Variable Expansion 
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:53:26.577: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1720
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 21:53:51.071: INFO: Deleting pod "var-expansion-d8bbe245-26c2-4af9-8ff2-af0d3e1a17d9" in namespace "var-expansion-1720"
Sep  5 21:53:51.092: INFO: Wait up to 5m0s for pod "var-expansion-d8bbe245-26c2-4af9-8ff2-af0d3e1a17d9" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:54:05.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1720" for this suite.

â€¢ [SLOW TEST:38.891 seconds]
[k8s.io] Variable Expansion
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should fail substituting values in a volume subpath with absolute path [sig-storage][Slow] [Conformance]","total":291,"completed":72,"skipped":1325,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:54:05.469: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8015
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
Sep  5 21:54:06.632: INFO: created pod pod-service-account-defaultsa
Sep  5 21:54:06.632: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep  5 21:54:06.655: INFO: created pod pod-service-account-mountsa
Sep  5 21:54:06.655: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep  5 21:54:06.677: INFO: created pod pod-service-account-nomountsa
Sep  5 21:54:06.677: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep  5 21:54:06.717: INFO: created pod pod-service-account-defaultsa-mountspec
Sep  5 21:54:06.717: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep  5 21:54:06.775: INFO: created pod pod-service-account-mountsa-mountspec
Sep  5 21:54:06.775: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep  5 21:54:06.811: INFO: created pod pod-service-account-nomountsa-mountspec
Sep  5 21:54:06.811: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep  5 21:54:06.854: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep  5 21:54:06.854: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep  5 21:54:06.906: INFO: created pod pod-service-account-mountsa-nomountspec
Sep  5 21:54:06.906: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep  5 21:54:06.944: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep  5 21:54:06.944: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:54:06.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8015" for this suite.
â€¢{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","total":291,"completed":73,"skipped":1333,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Service endpoints latency
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:54:07.243: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-858
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 21:54:07.923: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: creating replication controller svc-latency-rc in namespace svc-latency-858
Sep  5 21:54:36.173: INFO: Created: latency-svc-flzn8
Sep  5 21:54:36.254: INFO: Got endpoints: latency-svc-flzn8 [115.942851ms]
Sep  5 21:54:36.344: INFO: Created: latency-svc-rt449
Sep  5 21:54:36.432: INFO: Got endpoints: latency-svc-rt449 [176.903795ms]
Sep  5 21:54:36.449: INFO: Created: latency-svc-fsvrz
Sep  5 21:54:36.464: INFO: Created: latency-svc-m82t9
Sep  5 21:54:36.514: INFO: Got endpoints: latency-svc-fsvrz [259.63512ms]
Sep  5 21:54:36.565: INFO: Got endpoints: latency-svc-m82t9 [310.615501ms]
Sep  5 21:54:36.613: INFO: Created: latency-svc-58gmf
Sep  5 21:54:36.644: INFO: Got endpoints: latency-svc-58gmf [388.69176ms]
Sep  5 21:54:36.661: INFO: Created: latency-svc-9hg6r
Sep  5 21:54:36.680: INFO: Got endpoints: latency-svc-9hg6r [425.130142ms]
Sep  5 21:54:36.722: INFO: Created: latency-svc-8t74x
Sep  5 21:54:36.749: INFO: Got endpoints: latency-svc-8t74x [494.126105ms]
Sep  5 21:54:37.009: INFO: Created: latency-svc-6m5qt
Sep  5 21:54:37.063: INFO: Got endpoints: latency-svc-6m5qt [807.936064ms]
Sep  5 21:54:37.236: INFO: Created: latency-svc-jdxgk
Sep  5 21:54:37.277: INFO: Got endpoints: latency-svc-jdxgk [1.022440843s]
Sep  5 21:54:37.288: INFO: Created: latency-svc-vkjkp
Sep  5 21:54:37.327: INFO: Got endpoints: latency-svc-vkjkp [1.072031622s]
Sep  5 21:54:37.356: INFO: Created: latency-svc-sdqfh
Sep  5 21:54:37.395: INFO: Got endpoints: latency-svc-sdqfh [1.139834893s]
Sep  5 21:54:37.410: INFO: Created: latency-svc-bsmrw
Sep  5 21:54:37.441: INFO: Created: latency-svc-8fpjv
Sep  5 21:54:37.452: INFO: Got endpoints: latency-svc-bsmrw [1.197070122s]
Sep  5 21:54:37.474: INFO: Got endpoints: latency-svc-8fpjv [1.218313654s]
Sep  5 21:54:37.486: INFO: Created: latency-svc-vdcns
Sep  5 21:54:37.519: INFO: Got endpoints: latency-svc-vdcns [1.264221813s]
Sep  5 21:54:37.547: INFO: Created: latency-svc-jmk9b
Sep  5 21:54:37.567: INFO: Got endpoints: latency-svc-jmk9b [1.311307423s]
Sep  5 21:54:37.588: INFO: Created: latency-svc-4psjp
Sep  5 21:54:37.609: INFO: Got endpoints: latency-svc-4psjp [1.353931921s]
Sep  5 21:54:37.631: INFO: Created: latency-svc-pns4w
Sep  5 21:54:37.655: INFO: Got endpoints: latency-svc-pns4w [1.223726387s]
Sep  5 21:54:37.673: INFO: Created: latency-svc-lkj7m
Sep  5 21:54:37.704: INFO: Got endpoints: latency-svc-lkj7m [1.190308148s]
Sep  5 21:54:37.720: INFO: Created: latency-svc-prlr5
Sep  5 21:54:37.729: INFO: Got endpoints: latency-svc-prlr5 [1.164223606s]
Sep  5 21:54:37.767: INFO: Created: latency-svc-rddcd
Sep  5 21:54:37.794: INFO: Got endpoints: latency-svc-rddcd [1.149707304s]
Sep  5 21:54:37.815: INFO: Created: latency-svc-b64mr
Sep  5 21:54:37.844: INFO: Got endpoints: latency-svc-b64mr [1.164110332s]
Sep  5 21:54:37.889: INFO: Created: latency-svc-4llvp
Sep  5 21:54:37.914: INFO: Got endpoints: latency-svc-4llvp [1.164803902s]
Sep  5 21:54:37.939: INFO: Created: latency-svc-nq6mb
Sep  5 21:54:37.946: INFO: Got endpoints: latency-svc-nq6mb [882.952875ms]
Sep  5 21:54:38.009: INFO: Created: latency-svc-8zmzk
Sep  5 21:54:38.023: INFO: Got endpoints: latency-svc-8zmzk [745.612171ms]
Sep  5 21:54:38.054: INFO: Created: latency-svc-5dwkj
Sep  5 21:54:38.091: INFO: Got endpoints: latency-svc-5dwkj [763.70322ms]
Sep  5 21:54:38.137: INFO: Created: latency-svc-xz9h7
Sep  5 21:54:38.185: INFO: Got endpoints: latency-svc-xz9h7 [789.763332ms]
Sep  5 21:54:38.259: INFO: Created: latency-svc-w7p25
Sep  5 21:54:38.324: INFO: Got endpoints: latency-svc-w7p25 [871.475792ms]
Sep  5 21:54:38.337: INFO: Created: latency-svc-p65ns
Sep  5 21:54:38.384: INFO: Got endpoints: latency-svc-p65ns [910.100614ms]
Sep  5 21:54:38.428: INFO: Created: latency-svc-fmkx2
Sep  5 21:54:38.463: INFO: Got endpoints: latency-svc-fmkx2 [943.000485ms]
Sep  5 21:54:38.515: INFO: Created: latency-svc-lncq2
Sep  5 21:54:38.567: INFO: Got endpoints: latency-svc-lncq2 [1.000340631s]
Sep  5 21:54:38.598: INFO: Created: latency-svc-q2k8d
Sep  5 21:54:38.613: INFO: Got endpoints: latency-svc-q2k8d [1.003501579s]
Sep  5 21:54:38.664: INFO: Created: latency-svc-h767p
Sep  5 21:54:38.694: INFO: Got endpoints: latency-svc-h767p [1.038427708s]
Sep  5 21:54:38.725: INFO: Created: latency-svc-84f56
Sep  5 21:54:38.753: INFO: Got endpoints: latency-svc-84f56 [1.048268288s]
Sep  5 21:54:38.771: INFO: Created: latency-svc-ts86w
Sep  5 21:54:38.814: INFO: Got endpoints: latency-svc-ts86w [1.08484796s]
Sep  5 21:54:38.839: INFO: Created: latency-svc-5xgmb
Sep  5 21:54:38.883: INFO: Got endpoints: latency-svc-5xgmb [1.0889334s]
Sep  5 21:54:38.933: INFO: Created: latency-svc-rsbpp
Sep  5 21:54:38.956: INFO: Got endpoints: latency-svc-rsbpp [1.111850022s]
Sep  5 21:54:39.003: INFO: Created: latency-svc-xk466
Sep  5 21:54:39.025: INFO: Got endpoints: latency-svc-xk466 [1.111221008s]
Sep  5 21:54:39.041: INFO: Created: latency-svc-nkc2w
Sep  5 21:54:39.061: INFO: Got endpoints: latency-svc-nkc2w [1.114745442s]
Sep  5 21:54:39.096: INFO: Created: latency-svc-vxtkc
Sep  5 21:54:39.135: INFO: Got endpoints: latency-svc-vxtkc [1.111419388s]
Sep  5 21:54:39.199: INFO: Created: latency-svc-mrklx
Sep  5 21:54:39.221: INFO: Got endpoints: latency-svc-mrklx [1.130799506s]
Sep  5 21:54:39.291: INFO: Created: latency-svc-pm9lq
Sep  5 21:54:39.336: INFO: Got endpoints: latency-svc-pm9lq [1.151388692s]
Sep  5 21:54:39.398: INFO: Created: latency-svc-j22kq
Sep  5 21:54:39.452: INFO: Got endpoints: latency-svc-j22kq [1.128670244s]
Sep  5 21:54:39.497: INFO: Created: latency-svc-rxvwb
Sep  5 21:54:39.530: INFO: Got endpoints: latency-svc-rxvwb [1.146013199s]
Sep  5 21:54:39.678: INFO: Created: latency-svc-qhjbm
Sep  5 21:54:39.699: INFO: Got endpoints: latency-svc-qhjbm [1.236422301s]
Sep  5 21:54:39.802: INFO: Created: latency-svc-blc8t
Sep  5 21:54:39.810: INFO: Got endpoints: latency-svc-blc8t [1.243476312s]
Sep  5 21:54:39.855: INFO: Created: latency-svc-tqrqx
Sep  5 21:54:39.878: INFO: Got endpoints: latency-svc-tqrqx [1.265391063s]
Sep  5 21:54:39.912: INFO: Created: latency-svc-2cptp
Sep  5 21:54:39.927: INFO: Got endpoints: latency-svc-2cptp [1.233185217s]
Sep  5 21:54:39.978: INFO: Created: latency-svc-m7j7z
Sep  5 21:54:40.008: INFO: Got endpoints: latency-svc-m7j7z [1.255034861s]
Sep  5 21:54:40.054: INFO: Created: latency-svc-sxpcc
Sep  5 21:54:40.090: INFO: Got endpoints: latency-svc-sxpcc [1.275127478s]
Sep  5 21:54:40.158: INFO: Created: latency-svc-zd9k8
Sep  5 21:54:40.221: INFO: Got endpoints: latency-svc-zd9k8 [1.338513694s]
Sep  5 21:54:40.259: INFO: Created: latency-svc-wd2ls
Sep  5 21:54:40.275: INFO: Got endpoints: latency-svc-wd2ls [1.318396547s]
Sep  5 21:54:40.299: INFO: Created: latency-svc-8b78g
Sep  5 21:54:40.321: INFO: Got endpoints: latency-svc-8b78g [1.295427177s]
Sep  5 21:54:40.380: INFO: Created: latency-svc-4bggg
Sep  5 21:54:40.409: INFO: Got endpoints: latency-svc-4bggg [1.348484856s]
Sep  5 21:54:40.441: INFO: Created: latency-svc-8cjfm
Sep  5 21:54:40.481: INFO: Got endpoints: latency-svc-8cjfm [1.346537691s]
Sep  5 21:54:40.533: INFO: Created: latency-svc-z8rjm
Sep  5 21:54:40.558: INFO: Got endpoints: latency-svc-z8rjm [1.336785562s]
Sep  5 21:54:40.566: INFO: Created: latency-svc-cpgdc
Sep  5 21:54:40.601: INFO: Got endpoints: latency-svc-cpgdc [1.264110191s]
Sep  5 21:54:40.632: INFO: Created: latency-svc-gl57v
Sep  5 21:54:40.687: INFO: Got endpoints: latency-svc-gl57v [1.234810663s]
Sep  5 21:54:40.750: INFO: Created: latency-svc-w5djr
Sep  5 21:54:40.758: INFO: Got endpoints: latency-svc-w5djr [1.228295158s]
Sep  5 21:54:40.818: INFO: Created: latency-svc-jldm4
Sep  5 21:54:40.832: INFO: Got endpoints: latency-svc-jldm4 [1.13288828s]
Sep  5 21:54:40.878: INFO: Created: latency-svc-4jntc
Sep  5 21:54:40.917: INFO: Got endpoints: latency-svc-4jntc [1.106865983s]
Sep  5 21:54:40.954: INFO: Created: latency-svc-7fk6t
Sep  5 21:54:40.964: INFO: Got endpoints: latency-svc-7fk6t [1.085374648s]
Sep  5 21:54:41.037: INFO: Created: latency-svc-sgdtv
Sep  5 21:54:41.087: INFO: Got endpoints: latency-svc-sgdtv [1.159730833s]
Sep  5 21:54:41.106: INFO: Created: latency-svc-z7m7h
Sep  5 21:54:41.122: INFO: Got endpoints: latency-svc-z7m7h [1.114588028s]
Sep  5 21:54:41.202: INFO: Created: latency-svc-9rb4q
Sep  5 21:54:41.231: INFO: Got endpoints: latency-svc-9rb4q [1.141420462s]
Sep  5 21:54:41.303: INFO: Created: latency-svc-h69h5
Sep  5 21:54:41.314: INFO: Got endpoints: latency-svc-h69h5 [1.092248567s]
Sep  5 21:54:41.357: INFO: Created: latency-svc-znpm5
Sep  5 21:54:41.363: INFO: Got endpoints: latency-svc-znpm5 [1.087924082s]
Sep  5 21:54:41.461: INFO: Created: latency-svc-t5njv
Sep  5 21:54:41.476: INFO: Got endpoints: latency-svc-t5njv [1.154901605s]
Sep  5 21:54:41.511: INFO: Created: latency-svc-djdj9
Sep  5 21:54:41.554: INFO: Got endpoints: latency-svc-djdj9 [1.144599799s]
Sep  5 21:54:41.584: INFO: Created: latency-svc-r6g88
Sep  5 21:54:41.639: INFO: Got endpoints: latency-svc-r6g88 [1.157770604s]
Sep  5 21:54:41.679: INFO: Created: latency-svc-ck44x
Sep  5 21:54:41.699: INFO: Got endpoints: latency-svc-ck44x [1.140305643s]
Sep  5 21:54:41.744: INFO: Created: latency-svc-69dph
Sep  5 21:54:41.762: INFO: Got endpoints: latency-svc-69dph [1.16163299s]
Sep  5 21:54:41.812: INFO: Created: latency-svc-bq42x
Sep  5 21:54:41.827: INFO: Got endpoints: latency-svc-bq42x [1.139992617s]
Sep  5 21:54:41.854: INFO: Created: latency-svc-lz2ng
Sep  5 21:54:41.858: INFO: Got endpoints: latency-svc-lz2ng [1.100025977s]
Sep  5 21:54:41.921: INFO: Created: latency-svc-wlddn
Sep  5 21:54:41.935: INFO: Got endpoints: latency-svc-wlddn [1.102716911s]
Sep  5 21:54:41.983: INFO: Created: latency-svc-lkqfw
Sep  5 21:54:42.015: INFO: Got endpoints: latency-svc-lkqfw [1.09746302s]
Sep  5 21:54:42.022: INFO: Created: latency-svc-8hfnl
Sep  5 21:54:42.066: INFO: Got endpoints: latency-svc-8hfnl [1.101941034s]
Sep  5 21:54:42.085: INFO: Created: latency-svc-scs47
Sep  5 21:54:42.085: INFO: Got endpoints: latency-svc-scs47 [998.43438ms]
Sep  5 21:54:42.093: INFO: Created: latency-svc-hpgwn
Sep  5 21:54:42.116: INFO: Got endpoints: latency-svc-hpgwn [993.056608ms]
Sep  5 21:54:42.149: INFO: Created: latency-svc-g9xg9
Sep  5 21:54:42.154: INFO: Got endpoints: latency-svc-g9xg9 [923.226046ms]
Sep  5 21:54:42.183: INFO: Created: latency-svc-85m97
Sep  5 21:54:42.195: INFO: Got endpoints: latency-svc-85m97 [881.423047ms]
Sep  5 21:54:42.261: INFO: Created: latency-svc-z2m4z
Sep  5 21:54:42.272: INFO: Got endpoints: latency-svc-z2m4z [908.894556ms]
Sep  5 21:54:42.314: INFO: Created: latency-svc-qghnw
Sep  5 21:54:42.375: INFO: Got endpoints: latency-svc-qghnw [899.0171ms]
Sep  5 21:54:42.399: INFO: Created: latency-svc-r5dp6
Sep  5 21:54:42.454: INFO: Got endpoints: latency-svc-r5dp6 [900.192927ms]
Sep  5 21:54:42.563: INFO: Created: latency-svc-zmd68
Sep  5 21:54:42.564: INFO: Got endpoints: latency-svc-zmd68 [924.34632ms]
Sep  5 21:54:42.617: INFO: Created: latency-svc-tr847
Sep  5 21:54:42.657: INFO: Got endpoints: latency-svc-tr847 [958.126166ms]
Sep  5 21:54:42.695: INFO: Created: latency-svc-jhcbj
Sep  5 21:54:42.728: INFO: Got endpoints: latency-svc-jhcbj [965.504128ms]
Sep  5 21:54:42.746: INFO: Created: latency-svc-r7x8s
Sep  5 21:54:42.786: INFO: Got endpoints: latency-svc-r7x8s [958.976971ms]
Sep  5 21:54:42.822: INFO: Created: latency-svc-r57nc
Sep  5 21:54:42.862: INFO: Got endpoints: latency-svc-r57nc [1.003805125s]
Sep  5 21:54:42.893: INFO: Created: latency-svc-rzhtv
Sep  5 21:54:42.973: INFO: Got endpoints: latency-svc-rzhtv [1.03782233s]
Sep  5 21:54:43.024: INFO: Created: latency-svc-phcxn
Sep  5 21:54:43.064: INFO: Got endpoints: latency-svc-phcxn [1.049163428s]
Sep  5 21:54:43.191: INFO: Created: latency-svc-79z9c
Sep  5 21:54:43.219: INFO: Got endpoints: latency-svc-79z9c [1.152723323s]
Sep  5 21:54:43.262: INFO: Created: latency-svc-tb9xz
Sep  5 21:54:43.302: INFO: Got endpoints: latency-svc-tb9xz [1.216660375s]
Sep  5 21:54:43.317: INFO: Created: latency-svc-4vvx4
Sep  5 21:54:43.357: INFO: Got endpoints: latency-svc-4vvx4 [1.241777678s]
Sep  5 21:54:43.383: INFO: Created: latency-svc-hz8g2
Sep  5 21:54:43.421: INFO: Got endpoints: latency-svc-hz8g2 [1.266352864s]
Sep  5 21:54:43.463: INFO: Created: latency-svc-9xjw6
Sep  5 21:54:43.539: INFO: Got endpoints: latency-svc-9xjw6 [1.344231825s]
Sep  5 21:54:43.607: INFO: Created: latency-svc-bw459
Sep  5 21:54:43.616: INFO: Got endpoints: latency-svc-bw459 [1.344883568s]
Sep  5 21:54:43.631: INFO: Created: latency-svc-6k72t
Sep  5 21:54:43.654: INFO: Got endpoints: latency-svc-6k72t [1.278692651s]
Sep  5 21:54:43.672: INFO: Created: latency-svc-hzn4c
Sep  5 21:54:43.693: INFO: Got endpoints: latency-svc-hzn4c [1.238588189s]
Sep  5 21:54:43.716: INFO: Created: latency-svc-f5mpn
Sep  5 21:54:43.733: INFO: Got endpoints: latency-svc-f5mpn [1.169688955s]
Sep  5 21:54:43.772: INFO: Created: latency-svc-t95st
Sep  5 21:54:43.789: INFO: Got endpoints: latency-svc-t95st [1.132510215s]
Sep  5 21:54:43.804: INFO: Created: latency-svc-k5ndw
Sep  5 21:54:43.823: INFO: Got endpoints: latency-svc-k5ndw [1.095354922s]
Sep  5 21:54:43.867: INFO: Created: latency-svc-558bb
Sep  5 21:54:43.888: INFO: Got endpoints: latency-svc-558bb [1.101195731s]
Sep  5 21:54:43.946: INFO: Created: latency-svc-gddgg
Sep  5 21:54:43.951: INFO: Got endpoints: latency-svc-gddgg [1.088444771s]
Sep  5 21:54:44.025: INFO: Created: latency-svc-kfp2w
Sep  5 21:54:44.043: INFO: Got endpoints: latency-svc-kfp2w [1.070038561s]
Sep  5 21:54:44.076: INFO: Created: latency-svc-d8gxp
Sep  5 21:54:44.091: INFO: Got endpoints: latency-svc-d8gxp [1.026901906s]
Sep  5 21:54:44.103: INFO: Created: latency-svc-jp9kn
Sep  5 21:54:44.115: INFO: Got endpoints: latency-svc-jp9kn [896.500924ms]
Sep  5 21:54:44.126: INFO: Created: latency-svc-jbv8g
Sep  5 21:54:44.167: INFO: Got endpoints: latency-svc-jbv8g [864.344224ms]
Sep  5 21:54:44.192: INFO: Created: latency-svc-qvntl
Sep  5 21:54:44.215: INFO: Got endpoints: latency-svc-qvntl [857.819524ms]
Sep  5 21:54:44.259: INFO: Created: latency-svc-s9bkk
Sep  5 21:54:44.306: INFO: Got endpoints: latency-svc-s9bkk [885.627093ms]
Sep  5 21:54:44.317: INFO: Created: latency-svc-m94pf
Sep  5 21:54:44.324: INFO: Got endpoints: latency-svc-m94pf [784.741572ms]
Sep  5 21:54:44.336: INFO: Created: latency-svc-chgcm
Sep  5 21:54:44.363: INFO: Got endpoints: latency-svc-chgcm [746.590529ms]
Sep  5 21:54:44.428: INFO: Created: latency-svc-4xxbq
Sep  5 21:54:44.449: INFO: Created: latency-svc-q5mgf
Sep  5 21:54:44.479: INFO: Got endpoints: latency-svc-4xxbq [825.702928ms]
Sep  5 21:54:44.488: INFO: Got endpoints: latency-svc-q5mgf [794.621609ms]
Sep  5 21:54:44.557: INFO: Created: latency-svc-rtx4b
Sep  5 21:54:44.628: INFO: Got endpoints: latency-svc-rtx4b [894.341492ms]
Sep  5 21:54:44.655: INFO: Created: latency-svc-hqh9r
Sep  5 21:54:44.694: INFO: Got endpoints: latency-svc-hqh9r [904.339953ms]
Sep  5 21:54:44.901: INFO: Created: latency-svc-52fsk
Sep  5 21:54:44.912: INFO: Got endpoints: latency-svc-52fsk [1.088698319s]
Sep  5 21:54:44.920: INFO: Created: latency-svc-qk5l4
Sep  5 21:54:44.960: INFO: Got endpoints: latency-svc-qk5l4 [1.072307184s]
Sep  5 21:54:45.016: INFO: Created: latency-svc-nd22t
Sep  5 21:54:45.029: INFO: Got endpoints: latency-svc-nd22t [1.078203071s]
Sep  5 21:54:45.061: INFO: Created: latency-svc-7rvxk
Sep  5 21:54:45.088: INFO: Got endpoints: latency-svc-7rvxk [127.939086ms]
Sep  5 21:54:45.109: INFO: Created: latency-svc-6mw6q
Sep  5 21:54:45.129: INFO: Got endpoints: latency-svc-6mw6q [1.086310414s]
Sep  5 21:54:45.154: INFO: Created: latency-svc-cdlm9
Sep  5 21:54:45.177: INFO: Got endpoints: latency-svc-cdlm9 [1.085755448s]
Sep  5 21:54:45.207: INFO: Created: latency-svc-ddg9h
Sep  5 21:54:45.231: INFO: Got endpoints: latency-svc-ddg9h [1.116111113s]
Sep  5 21:54:45.243: INFO: Created: latency-svc-b29x4
Sep  5 21:54:45.278: INFO: Got endpoints: latency-svc-b29x4 [1.111062438s]
Sep  5 21:54:45.296: INFO: Created: latency-svc-zn8rp
Sep  5 21:54:45.314: INFO: Got endpoints: latency-svc-zn8rp [1.098596248s]
Sep  5 21:54:45.375: INFO: Created: latency-svc-tj2l9
Sep  5 21:54:45.417: INFO: Got endpoints: latency-svc-tj2l9 [1.110140245s]
Sep  5 21:54:45.500: INFO: Created: latency-svc-28jjk
Sep  5 21:54:45.510: INFO: Got endpoints: latency-svc-28jjk [1.185745012s]
Sep  5 21:54:45.542: INFO: Created: latency-svc-q6bmv
Sep  5 21:54:45.581: INFO: Got endpoints: latency-svc-q6bmv [1.218245171s]
Sep  5 21:54:45.624: INFO: Created: latency-svc-h5d2z
Sep  5 21:54:45.646: INFO: Got endpoints: latency-svc-h5d2z [1.166274525s]
Sep  5 21:54:45.663: INFO: Created: latency-svc-blxxk
Sep  5 21:54:45.683: INFO: Got endpoints: latency-svc-blxxk [1.19517887s]
Sep  5 21:54:45.734: INFO: Created: latency-svc-w92cd
Sep  5 21:54:45.768: INFO: Got endpoints: latency-svc-w92cd [1.140488908s]
Sep  5 21:54:45.794: INFO: Created: latency-svc-wxh88
Sep  5 21:54:45.799: INFO: Created: latency-svc-plrkz
Sep  5 21:54:45.816: INFO: Got endpoints: latency-svc-plrkz [1.121846242s]
Sep  5 21:54:45.836: INFO: Got endpoints: latency-svc-wxh88 [923.705699ms]
Sep  5 21:54:45.985: INFO: Created: latency-svc-hm6z5
Sep  5 21:54:46.032: INFO: Created: latency-svc-gls9g
Sep  5 21:54:46.073: INFO: Got endpoints: latency-svc-hm6z5 [1.044030376s]
Sep  5 21:54:46.108: INFO: Got endpoints: latency-svc-gls9g [1.020176057s]
Sep  5 21:54:46.132: INFO: Created: latency-svc-bsl7v
Sep  5 21:54:46.181: INFO: Got endpoints: latency-svc-bsl7v [1.051733203s]
Sep  5 21:54:46.211: INFO: Created: latency-svc-bh5rd
Sep  5 21:54:46.247: INFO: Got endpoints: latency-svc-bh5rd [1.070169334s]
Sep  5 21:54:46.282: INFO: Created: latency-svc-fpx7g
Sep  5 21:54:46.307: INFO: Got endpoints: latency-svc-fpx7g [1.075957851s]
Sep  5 21:54:46.320: INFO: Created: latency-svc-z5mvh
Sep  5 21:54:46.347: INFO: Got endpoints: latency-svc-z5mvh [1.069487433s]
Sep  5 21:54:46.371: INFO: Created: latency-svc-njwfn
Sep  5 21:54:46.402: INFO: Got endpoints: latency-svc-njwfn [1.087633077s]
Sep  5 21:54:46.450: INFO: Created: latency-svc-4b8vg
Sep  5 21:54:46.503: INFO: Got endpoints: latency-svc-4b8vg [1.086018109s]
Sep  5 21:54:46.571: INFO: Created: latency-svc-z7p7g
Sep  5 21:54:46.659: INFO: Got endpoints: latency-svc-z7p7g [1.149036492s]
Sep  5 21:54:46.736: INFO: Created: latency-svc-6mzld
Sep  5 21:54:46.736: INFO: Got endpoints: latency-svc-6mzld [1.154151721s]
Sep  5 21:54:46.764: INFO: Created: latency-svc-r49tt
Sep  5 21:54:46.780: INFO: Got endpoints: latency-svc-r49tt [1.134259306s]
Sep  5 21:54:46.804: INFO: Created: latency-svc-2ncm2
Sep  5 21:54:46.838: INFO: Got endpoints: latency-svc-2ncm2 [1.155575781s]
Sep  5 21:54:46.886: INFO: Created: latency-svc-xxqmf
Sep  5 21:54:46.912: INFO: Got endpoints: latency-svc-xxqmf [1.143505108s]
Sep  5 21:54:46.946: INFO: Created: latency-svc-8sgvm
Sep  5 21:54:46.951: INFO: Got endpoints: latency-svc-8sgvm [1.135096736s]
Sep  5 21:54:46.990: INFO: Created: latency-svc-68t77
Sep  5 21:54:47.025: INFO: Got endpoints: latency-svc-68t77 [1.188682446s]
Sep  5 21:54:47.047: INFO: Created: latency-svc-8447q
Sep  5 21:54:47.064: INFO: Got endpoints: latency-svc-8447q [990.731915ms]
Sep  5 21:54:47.111: INFO: Created: latency-svc-zqrlw
Sep  5 21:54:47.135: INFO: Got endpoints: latency-svc-zqrlw [1.026470135s]
Sep  5 21:54:47.167: INFO: Created: latency-svc-84nbk
Sep  5 21:54:47.184: INFO: Got endpoints: latency-svc-84nbk [1.002776028s]
Sep  5 21:54:47.209: INFO: Created: latency-svc-vd7b7
Sep  5 21:54:47.249: INFO: Got endpoints: latency-svc-vd7b7 [1.002041353s]
Sep  5 21:54:47.262: INFO: Created: latency-svc-l66sj
Sep  5 21:54:47.285: INFO: Got endpoints: latency-svc-l66sj [977.218627ms]
Sep  5 21:54:47.343: INFO: Created: latency-svc-dcpvm
Sep  5 21:54:47.361: INFO: Created: latency-svc-pljpl
Sep  5 21:54:47.366: INFO: Got endpoints: latency-svc-dcpvm [1.018771939s]
Sep  5 21:54:47.395: INFO: Got endpoints: latency-svc-pljpl [993.612451ms]
Sep  5 21:54:47.412: INFO: Created: latency-svc-p897c
Sep  5 21:54:47.440: INFO: Got endpoints: latency-svc-p897c [937.029003ms]
Sep  5 21:54:47.472: INFO: Created: latency-svc-7rp6b
Sep  5 21:54:47.482: INFO: Got endpoints: latency-svc-7rp6b [822.467423ms]
Sep  5 21:54:47.514: INFO: Created: latency-svc-z4sn6
Sep  5 21:54:47.546: INFO: Created: latency-svc-wqlxb
Sep  5 21:54:47.546: INFO: Got endpoints: latency-svc-z4sn6 [810.488164ms]
Sep  5 21:54:47.569: INFO: Got endpoints: latency-svc-wqlxb [788.582282ms]
Sep  5 21:54:47.673: INFO: Created: latency-svc-llh8t
Sep  5 21:54:47.693: INFO: Got endpoints: latency-svc-llh8t [854.536207ms]
Sep  5 21:54:47.713: INFO: Created: latency-svc-9mklf
Sep  5 21:54:47.740: INFO: Got endpoints: latency-svc-9mklf [828.274115ms]
Sep  5 21:54:47.748: INFO: Created: latency-svc-79std
Sep  5 21:54:47.779: INFO: Got endpoints: latency-svc-79std [828.252201ms]
Sep  5 21:54:47.822: INFO: Created: latency-svc-sqdx6
Sep  5 21:54:47.841: INFO: Got endpoints: latency-svc-sqdx6 [816.635648ms]
Sep  5 21:54:47.889: INFO: Created: latency-svc-gfzfq
Sep  5 21:54:47.934: INFO: Got endpoints: latency-svc-gfzfq [870.427052ms]
Sep  5 21:54:47.964: INFO: Created: latency-svc-d996w
Sep  5 21:54:47.979: INFO: Got endpoints: latency-svc-d996w [844.137748ms]
Sep  5 21:54:47.989: INFO: Created: latency-svc-9h5j4
Sep  5 21:54:48.016: INFO: Got endpoints: latency-svc-9h5j4 [831.613223ms]
Sep  5 21:54:48.036: INFO: Created: latency-svc-qngnj
Sep  5 21:54:48.048: INFO: Got endpoints: latency-svc-qngnj [799.133854ms]
Sep  5 21:54:48.079: INFO: Created: latency-svc-zsn5b
Sep  5 21:54:48.101: INFO: Got endpoints: latency-svc-zsn5b [816.101352ms]
Sep  5 21:54:48.128: INFO: Created: latency-svc-s2hkv
Sep  5 21:54:48.151: INFO: Got endpoints: latency-svc-s2hkv [784.464536ms]
Sep  5 21:54:48.170: INFO: Created: latency-svc-ftgfv
Sep  5 21:54:48.182: INFO: Got endpoints: latency-svc-ftgfv [786.4017ms]
Sep  5 21:54:48.233: INFO: Created: latency-svc-vct2g
Sep  5 21:54:48.308: INFO: Got endpoints: latency-svc-vct2g [868.210987ms]
Sep  5 21:54:48.334: INFO: Created: latency-svc-blxz4
Sep  5 21:54:48.401: INFO: Got endpoints: latency-svc-blxz4 [918.97057ms]
Sep  5 21:54:48.439: INFO: Created: latency-svc-5chdk
Sep  5 21:54:48.464: INFO: Got endpoints: latency-svc-5chdk [917.472649ms]
Sep  5 21:54:48.487: INFO: Created: latency-svc-ckstz
Sep  5 21:54:48.529: INFO: Got endpoints: latency-svc-ckstz [960.170482ms]
Sep  5 21:54:48.560: INFO: Created: latency-svc-v6hzn
Sep  5 21:54:48.618: INFO: Created: latency-svc-kkxbn
Sep  5 21:54:48.623: INFO: Got endpoints: latency-svc-v6hzn [930.229971ms]
Sep  5 21:54:48.654: INFO: Got endpoints: latency-svc-kkxbn [913.729279ms]
Sep  5 21:54:48.669: INFO: Created: latency-svc-tswsr
Sep  5 21:54:48.712: INFO: Got endpoints: latency-svc-tswsr [932.744507ms]
Sep  5 21:54:48.775: INFO: Created: latency-svc-6c7mv
Sep  5 21:54:48.871: INFO: Got endpoints: latency-svc-6c7mv [1.029360364s]
Sep  5 21:54:48.897: INFO: Created: latency-svc-s6vdc
Sep  5 21:54:48.907: INFO: Got endpoints: latency-svc-s6vdc [972.309926ms]
Sep  5 21:54:49.019: INFO: Created: latency-svc-zthvs
Sep  5 21:54:49.056: INFO: Got endpoints: latency-svc-zthvs [1.076435528s]
Sep  5 21:54:49.103: INFO: Created: latency-svc-972dr
Sep  5 21:54:49.125: INFO: Got endpoints: latency-svc-972dr [1.109299989s]
Sep  5 21:54:49.141: INFO: Created: latency-svc-x6c8m
Sep  5 21:54:49.163: INFO: Got endpoints: latency-svc-x6c8m [1.114797419s]
Sep  5 21:54:49.206: INFO: Created: latency-svc-hktms
Sep  5 21:54:49.215: INFO: Got endpoints: latency-svc-hktms [1.113746551s]
Sep  5 21:54:49.233: INFO: Created: latency-svc-gmf8k
Sep  5 21:54:49.269: INFO: Got endpoints: latency-svc-gmf8k [1.117890851s]
Sep  5 21:54:49.302: INFO: Created: latency-svc-blr8w
Sep  5 21:54:49.338: INFO: Got endpoints: latency-svc-blr8w [1.156196532s]
Sep  5 21:54:49.346: INFO: Created: latency-svc-2jk92
Sep  5 21:54:49.367: INFO: Got endpoints: latency-svc-2jk92 [1.058652392s]
Sep  5 21:54:49.579: INFO: Created: latency-svc-pcrnn
Sep  5 21:54:49.585: INFO: Got endpoints: latency-svc-pcrnn [1.184064429s]
Sep  5 21:54:49.675: INFO: Created: latency-svc-7x4v6
Sep  5 21:54:49.694: INFO: Got endpoints: latency-svc-7x4v6 [1.230067754s]
Sep  5 21:54:49.715: INFO: Created: latency-svc-bbdp6
Sep  5 21:54:49.728: INFO: Got endpoints: latency-svc-bbdp6 [1.199359722s]
Sep  5 21:54:49.769: INFO: Created: latency-svc-npgld
Sep  5 21:54:49.803: INFO: Got endpoints: latency-svc-npgld [1.17999361s]
Sep  5 21:54:49.846: INFO: Created: latency-svc-26fpp
Sep  5 21:54:49.867: INFO: Got endpoints: latency-svc-26fpp [1.213254715s]
Sep  5 21:54:49.891: INFO: Created: latency-svc-msl8j
Sep  5 21:54:49.930: INFO: Got endpoints: latency-svc-msl8j [1.217950927s]
Sep  5 21:54:49.989: INFO: Created: latency-svc-lhw6s
Sep  5 21:54:50.019: INFO: Got endpoints: latency-svc-lhw6s [1.148002551s]
Sep  5 21:54:50.060: INFO: Created: latency-svc-c6b8q
Sep  5 21:54:50.075: INFO: Got endpoints: latency-svc-c6b8q [1.168533596s]
Sep  5 21:54:50.123: INFO: Created: latency-svc-d5tfk
Sep  5 21:54:50.161: INFO: Got endpoints: latency-svc-d5tfk [1.104953292s]
Sep  5 21:54:50.184: INFO: Created: latency-svc-9dvjz
Sep  5 21:54:50.219: INFO: Got endpoints: latency-svc-9dvjz [1.094263211s]
Sep  5 21:54:50.250: INFO: Created: latency-svc-whvfv
Sep  5 21:54:50.268: INFO: Got endpoints: latency-svc-whvfv [1.104756769s]
Sep  5 21:54:50.299: INFO: Created: latency-svc-ht7j7
Sep  5 21:54:50.303: INFO: Got endpoints: latency-svc-ht7j7 [1.087925799s]
Sep  5 21:54:50.340: INFO: Created: latency-svc-gxmlf
Sep  5 21:54:50.358: INFO: Got endpoints: latency-svc-gxmlf [1.089103955s]
Sep  5 21:54:50.413: INFO: Created: latency-svc-9qjzv
Sep  5 21:54:50.438: INFO: Got endpoints: latency-svc-9qjzv [1.100065031s]
Sep  5 21:54:50.484: INFO: Created: latency-svc-lvpfv
Sep  5 21:54:50.493: INFO: Got endpoints: latency-svc-lvpfv [1.125942929s]
Sep  5 21:54:50.542: INFO: Created: latency-svc-g75gp
Sep  5 21:54:50.594: INFO: Got endpoints: latency-svc-g75gp [1.009203432s]
Sep  5 21:54:50.594: INFO: Latencies: [127.939086ms 176.903795ms 259.63512ms 310.615501ms 388.69176ms 425.130142ms 494.126105ms 745.612171ms 746.590529ms 763.70322ms 784.464536ms 784.741572ms 786.4017ms 788.582282ms 789.763332ms 794.621609ms 799.133854ms 807.936064ms 810.488164ms 816.101352ms 816.635648ms 822.467423ms 825.702928ms 828.252201ms 828.274115ms 831.613223ms 844.137748ms 854.536207ms 857.819524ms 864.344224ms 868.210987ms 870.427052ms 871.475792ms 881.423047ms 882.952875ms 885.627093ms 894.341492ms 896.500924ms 899.0171ms 900.192927ms 904.339953ms 908.894556ms 910.100614ms 913.729279ms 917.472649ms 918.97057ms 923.226046ms 923.705699ms 924.34632ms 930.229971ms 932.744507ms 937.029003ms 943.000485ms 958.126166ms 958.976971ms 960.170482ms 965.504128ms 972.309926ms 977.218627ms 990.731915ms 993.056608ms 993.612451ms 998.43438ms 1.000340631s 1.002041353s 1.002776028s 1.003501579s 1.003805125s 1.009203432s 1.018771939s 1.020176057s 1.022440843s 1.026470135s 1.026901906s 1.029360364s 1.03782233s 1.038427708s 1.044030376s 1.048268288s 1.049163428s 1.051733203s 1.058652392s 1.069487433s 1.070038561s 1.070169334s 1.072031622s 1.072307184s 1.075957851s 1.076435528s 1.078203071s 1.08484796s 1.085374648s 1.085755448s 1.086018109s 1.086310414s 1.087633077s 1.087924082s 1.087925799s 1.088444771s 1.088698319s 1.0889334s 1.089103955s 1.092248567s 1.094263211s 1.095354922s 1.09746302s 1.098596248s 1.100025977s 1.100065031s 1.101195731s 1.101941034s 1.102716911s 1.104756769s 1.104953292s 1.106865983s 1.109299989s 1.110140245s 1.111062438s 1.111221008s 1.111419388s 1.111850022s 1.113746551s 1.114588028s 1.114745442s 1.114797419s 1.116111113s 1.117890851s 1.121846242s 1.125942929s 1.128670244s 1.130799506s 1.132510215s 1.13288828s 1.134259306s 1.135096736s 1.139834893s 1.139992617s 1.140305643s 1.140488908s 1.141420462s 1.143505108s 1.144599799s 1.146013199s 1.148002551s 1.149036492s 1.149707304s 1.151388692s 1.152723323s 1.154151721s 1.154901605s 1.155575781s 1.156196532s 1.157770604s 1.159730833s 1.16163299s 1.164110332s 1.164223606s 1.164803902s 1.166274525s 1.168533596s 1.169688955s 1.17999361s 1.184064429s 1.185745012s 1.188682446s 1.190308148s 1.19517887s 1.197070122s 1.199359722s 1.213254715s 1.216660375s 1.217950927s 1.218245171s 1.218313654s 1.223726387s 1.228295158s 1.230067754s 1.233185217s 1.234810663s 1.236422301s 1.238588189s 1.241777678s 1.243476312s 1.255034861s 1.264110191s 1.264221813s 1.265391063s 1.266352864s 1.275127478s 1.278692651s 1.295427177s 1.311307423s 1.318396547s 1.336785562s 1.338513694s 1.344231825s 1.344883568s 1.346537691s 1.348484856s 1.353931921s]
Sep  5 21:54:50.594: INFO: 50 %ile: 1.0889334s
Sep  5 21:54:50.594: INFO: 90 %ile: 1.238588189s
Sep  5 21:54:50.594: INFO: 99 %ile: 1.348484856s
Sep  5 21:54:50.594: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:54:50.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-858" for this suite.

â€¢ [SLOW TEST:43.679 seconds]
[sig-network] Service endpoints latency
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","total":291,"completed":74,"skipped":1343,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:54:50.922: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9012
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 21:54:51.415: INFO: >>> kubeConfig: ./kconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:54:52.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9012" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","total":291,"completed":75,"skipped":1352,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:54:52.506: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9636
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-9636/configmap-test-150c203a-b58c-4142-b335-d970990deb8d
STEP: Creating a pod to test consume configMaps
Sep  5 21:54:53.889: INFO: Waiting up to 5m0s for pod "pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0" in namespace "configmap-9636" to be "Succeeded or Failed"
Sep  5 21:54:53.959: INFO: Pod "pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0": Phase="Pending", Reason="", readiness=false. Elapsed: 69.863558ms
Sep  5 21:54:56.342: INFO: Pod "pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.452686627s
Sep  5 21:54:58.377: INFO: Pod "pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.487498538s
Sep  5 21:55:01.145: INFO: Pod "pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.255946468s
Sep  5 21:55:03.479: INFO: Pod "pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.589298624s
Sep  5 21:55:05.568: INFO: Pod "pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.678401398s
Sep  5 21:55:07.583: INFO: Pod "pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.693098888s
Sep  5 21:55:09.615: INFO: Pod "pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0": Phase="Pending", Reason="", readiness=false. Elapsed: 15.7253707s
Sep  5 21:55:11.664: INFO: Pod "pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0": Phase="Pending", Reason="", readiness=false. Elapsed: 17.774003353s
Sep  5 21:55:13.682: INFO: Pod "pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0": Phase="Pending", Reason="", readiness=false. Elapsed: 19.792600311s
Sep  5 21:55:15.727: INFO: Pod "pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0": Phase="Pending", Reason="", readiness=false. Elapsed: 21.837729s
Sep  5 21:55:17.783: INFO: Pod "pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0": Phase="Pending", Reason="", readiness=false. Elapsed: 23.893361011s
Sep  5 21:55:19.809: INFO: Pod "pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0": Phase="Pending", Reason="", readiness=false. Elapsed: 25.919859806s
Sep  5 21:55:21.842: INFO: Pod "pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 27.952141057s
STEP: Saw pod success
Sep  5 21:55:21.842: INFO: Pod "pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0" satisfied condition "Succeeded or Failed"
Sep  5 21:55:21.850: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0 container env-test: <nil>
STEP: delete the pod
Sep  5 21:55:26.956: INFO: Waiting for pod pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0 to disappear
Sep  5 21:55:27.014: INFO: Pod pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0 still exists
Sep  5 21:55:29.014: INFO: Waiting for pod pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0 to disappear
Sep  5 21:55:29.035: INFO: Pod pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0 still exists
Sep  5 21:55:31.014: INFO: Waiting for pod pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0 to disappear
Sep  5 21:55:31.029: INFO: Pod pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0 still exists
Sep  5 21:55:33.014: INFO: Waiting for pod pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0 to disappear
Sep  5 21:55:33.033: INFO: Pod pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0 still exists
Sep  5 21:55:35.014: INFO: Waiting for pod pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0 to disappear
Sep  5 21:55:35.026: INFO: Pod pod-configmaps-3cf91b6a-8dc1-48d5-97c8-3d6996de8da0 no longer exists
[AfterEach] [sig-node] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:55:35.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9636" for this suite.

â€¢ [SLOW TEST:42.759 seconds]
[sig-node] ConfigMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via the environment [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","total":291,"completed":76,"skipped":1355,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:55:35.266: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2486
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-downwardapi-jtpz
STEP: Creating a pod to test atomic-volume-subpath
Sep  5 21:55:35.784: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-jtpz" in namespace "subpath-2486" to be "Succeeded or Failed"
Sep  5 21:55:35.802: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Pending", Reason="", readiness=false. Elapsed: 18.160643ms
Sep  5 21:55:37.820: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036490992s
Sep  5 21:55:39.843: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.059269826s
Sep  5 21:55:41.868: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084216476s
Sep  5 21:55:43.898: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Pending", Reason="", readiness=false. Elapsed: 8.113671721s
Sep  5 21:55:45.906: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Pending", Reason="", readiness=false. Elapsed: 10.12172532s
Sep  5 21:55:47.912: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Pending", Reason="", readiness=false. Elapsed: 12.128485433s
Sep  5 21:55:49.919: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Pending", Reason="", readiness=false. Elapsed: 14.134597362s
Sep  5 21:55:51.925: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Pending", Reason="", readiness=false. Elapsed: 16.141319994s
Sep  5 21:55:53.931: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Pending", Reason="", readiness=false. Elapsed: 18.147259167s
Sep  5 21:55:55.939: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Running", Reason="", readiness=true. Elapsed: 20.155103679s
Sep  5 21:55:57.954: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Running", Reason="", readiness=true. Elapsed: 22.16974609s
Sep  5 21:55:59.961: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Running", Reason="", readiness=true. Elapsed: 24.177132397s
Sep  5 21:56:01.970: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Running", Reason="", readiness=true. Elapsed: 26.185686519s
Sep  5 21:56:03.982: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Running", Reason="", readiness=true. Elapsed: 28.198483103s
Sep  5 21:56:05.990: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Running", Reason="", readiness=true. Elapsed: 30.205937504s
Sep  5 21:56:08.004: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Running", Reason="", readiness=true. Elapsed: 32.219989338s
Sep  5 21:56:10.012: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Running", Reason="", readiness=true. Elapsed: 34.228107064s
Sep  5 21:56:12.021: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Running", Reason="", readiness=true. Elapsed: 36.23751348s
Sep  5 21:56:14.029: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Running", Reason="", readiness=true. Elapsed: 38.245000232s
Sep  5 21:56:16.044: INFO: Pod "pod-subpath-test-downwardapi-jtpz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 40.260413126s
STEP: Saw pod success
Sep  5 21:56:16.044: INFO: Pod "pod-subpath-test-downwardapi-jtpz" satisfied condition "Succeeded or Failed"
Sep  5 21:56:16.054: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-subpath-test-downwardapi-jtpz container test-container-subpath-downwardapi-jtpz: <nil>
STEP: delete the pod
Sep  5 21:56:16.114: INFO: Waiting for pod pod-subpath-test-downwardapi-jtpz to disappear
Sep  5 21:56:16.133: INFO: Pod pod-subpath-test-downwardapi-jtpz still exists
Sep  5 21:56:18.133: INFO: Waiting for pod pod-subpath-test-downwardapi-jtpz to disappear
Sep  5 21:56:18.147: INFO: Pod pod-subpath-test-downwardapi-jtpz still exists
Sep  5 21:56:20.135: INFO: Waiting for pod pod-subpath-test-downwardapi-jtpz to disappear
Sep  5 21:56:20.149: INFO: Pod pod-subpath-test-downwardapi-jtpz still exists
Sep  5 21:56:22.134: INFO: Waiting for pod pod-subpath-test-downwardapi-jtpz to disappear
Sep  5 21:56:22.149: INFO: Pod pod-subpath-test-downwardapi-jtpz still exists
Sep  5 21:56:24.134: INFO: Waiting for pod pod-subpath-test-downwardapi-jtpz to disappear
Sep  5 21:56:24.143: INFO: Pod pod-subpath-test-downwardapi-jtpz no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-jtpz
Sep  5 21:56:24.143: INFO: Deleting pod "pod-subpath-test-downwardapi-jtpz" in namespace "subpath-2486"
[AfterEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:56:24.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2486" for this suite.

â€¢ [SLOW TEST:49.155 seconds]
[sig-storage] Subpath
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]","total":291,"completed":77,"skipped":1386,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:56:24.422: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7293
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 21:56:25.767: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  5 21:56:27.794: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:56:29.808: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:56:31.805: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:56:33.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:56:35.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:56:37.801: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:56:39.803: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:56:41.828: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:56:43.802: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 56, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 21:56:46.842: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:56:59.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7293" for this suite.
STEP: Destroying namespace "webhook-7293-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:35.494 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","total":291,"completed":78,"skipped":1452,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:56:59.917: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3363
STEP: Waiting for a default service account to be provisioned in namespace
[It] should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: waiting for pod running
STEP: creating a file in subpath
Sep  5 21:57:22.417: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3363 PodName:var-expansion-6869aaf5-22ff-4785-8b3b-b38ea7da9848 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 21:57:22.417: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: test for file in mounted path
Sep  5 21:57:22.550: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3363 PodName:var-expansion-6869aaf5-22ff-4785-8b3b-b38ea7da9848 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 21:57:22.550: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: updating the annotation value
Sep  5 21:57:23.201: INFO: Successfully updated pod "var-expansion-6869aaf5-22ff-4785-8b3b-b38ea7da9848"
STEP: waiting for annotated pod running
STEP: deleting the pod gracefully
Sep  5 21:57:23.229: INFO: Deleting pod "var-expansion-6869aaf5-22ff-4785-8b3b-b38ea7da9848" in namespace "var-expansion-3363"
Sep  5 21:57:23.252: INFO: Wait up to 5m0s for pod "var-expansion-6869aaf5-22ff-4785-8b3b-b38ea7da9848" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:58:03.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3363" for this suite.

â€¢ [SLOW TEST:63.573 seconds]
[k8s.io] Variable Expansion
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should succeed in writing subpaths in container [sig-storage][Slow] [Conformance]","total":291,"completed":79,"skipped":1466,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
S
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Aggregator
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:58:03.491: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-3271
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Sep  5 21:58:03.955: INFO: >>> kubeConfig: ./kconfig.yaml
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the sample API server.
Sep  5 21:58:04.662: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep  5 21:58:06.834: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:08.864: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:10.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:12.856: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:14.847: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:16.860: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:18.847: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:20.843: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:22.844: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:24.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:26.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:28.846: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:30.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:32.843: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:34.846: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:36.844: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:38.841: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:40.850: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:42.860: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:44.843: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:46.854: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:48.847: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:50.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:52.842: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:54.856: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 21, 58, 4, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-67dc674868\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 21:58:59.040: INFO: Waited 2.172659499s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:59:01.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3271" for this suite.

â€¢ [SLOW TEST:58.890 seconds]
[sig-api-machinery] Aggregator
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","total":291,"completed":80,"skipped":1467,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:59:02.381: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8783
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 21:59:02.881: INFO: >>> kubeConfig: ./kconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:59:05.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8783" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","total":291,"completed":81,"skipped":1469,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:59:06.947: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4239
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's args
Sep  5 21:59:07.632: INFO: Waiting up to 5m0s for pod "var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53" in namespace "var-expansion-4239" to be "Succeeded or Failed"
Sep  5 21:59:07.652: INFO: Pod "var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53": Phase="Pending", Reason="", readiness=false. Elapsed: 20.206244ms
Sep  5 21:59:09.741: INFO: Pod "var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.108960949s
Sep  5 21:59:11.748: INFO: Pod "var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53": Phase="Pending", Reason="", readiness=false. Elapsed: 4.115832728s
Sep  5 21:59:13.777: INFO: Pod "var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53": Phase="Pending", Reason="", readiness=false. Elapsed: 6.145227044s
Sep  5 21:59:15.790: INFO: Pod "var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53": Phase="Pending", Reason="", readiness=false. Elapsed: 8.158219191s
Sep  5 21:59:17.823: INFO: Pod "var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53": Phase="Pending", Reason="", readiness=false. Elapsed: 10.190838209s
Sep  5 21:59:19.843: INFO: Pod "var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53": Phase="Pending", Reason="", readiness=false. Elapsed: 12.211730966s
Sep  5 21:59:21.859: INFO: Pod "var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53": Phase="Pending", Reason="", readiness=false. Elapsed: 14.226974767s
Sep  5 21:59:23.876: INFO: Pod "var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53": Phase="Pending", Reason="", readiness=false. Elapsed: 16.244238865s
Sep  5 21:59:25.884: INFO: Pod "var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53": Phase="Pending", Reason="", readiness=false. Elapsed: 18.252152724s
Sep  5 21:59:27.894: INFO: Pod "var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53": Phase="Pending", Reason="", readiness=false. Elapsed: 20.262107856s
Sep  5 21:59:29.905: INFO: Pod "var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53": Phase="Pending", Reason="", readiness=false. Elapsed: 22.272928754s
Sep  5 21:59:31.922: INFO: Pod "var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.290716522s
STEP: Saw pod success
Sep  5 21:59:31.922: INFO: Pod "var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53" satisfied condition "Succeeded or Failed"
Sep  5 21:59:31.939: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com pod var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53 container dapi-container: <nil>
STEP: delete the pod
Sep  5 21:59:37.014: INFO: Waiting for pod var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53 to disappear
Sep  5 21:59:37.040: INFO: Pod var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53 still exists
Sep  5 21:59:39.041: INFO: Waiting for pod var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53 to disappear
Sep  5 21:59:39.048: INFO: Pod var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53 still exists
Sep  5 21:59:41.041: INFO: Waiting for pod var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53 to disappear
Sep  5 21:59:41.054: INFO: Pod var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53 still exists
Sep  5 21:59:43.040: INFO: Waiting for pod var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53 to disappear
Sep  5 21:59:43.048: INFO: Pod var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53 still exists
Sep  5 21:59:45.042: INFO: Waiting for pod var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53 to disappear
Sep  5 21:59:45.048: INFO: Pod var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53 still exists
Sep  5 21:59:47.041: INFO: Waiting for pod var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53 to disappear
Sep  5 21:59:47.047: INFO: Pod var-expansion-f4bcfa31-98c1-4d63-afd6-69cc2add1b53 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 21:59:47.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4239" for this suite.

â€¢ [SLOW TEST:40.380 seconds]
[k8s.io] Variable Expansion
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","total":291,"completed":82,"skipped":1491,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 21:59:47.327: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4045
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep  5 21:59:47.788: INFO: Waiting up to 5m0s for pod "downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919" in namespace "downward-api-4045" to be "Succeeded or Failed"
Sep  5 21:59:47.803: INFO: Pod "downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919": Phase="Pending", Reason="", readiness=false. Elapsed: 14.369831ms
Sep  5 21:59:49.810: INFO: Pod "downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02180836s
Sep  5 21:59:51.819: INFO: Pod "downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030768292s
Sep  5 21:59:53.831: INFO: Pod "downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043008036s
Sep  5 21:59:55.838: INFO: Pod "downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919": Phase="Pending", Reason="", readiness=false. Elapsed: 8.050053399s
Sep  5 21:59:57.845: INFO: Pod "downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919": Phase="Pending", Reason="", readiness=false. Elapsed: 10.057147706s
Sep  5 21:59:59.859: INFO: Pod "downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919": Phase="Pending", Reason="", readiness=false. Elapsed: 12.071211528s
Sep  5 22:00:01.876: INFO: Pod "downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919": Phase="Pending", Reason="", readiness=false. Elapsed: 14.087630639s
Sep  5 22:00:03.887: INFO: Pod "downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919": Phase="Pending", Reason="", readiness=false. Elapsed: 16.099251218s
Sep  5 22:00:05.897: INFO: Pod "downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919": Phase="Pending", Reason="", readiness=false. Elapsed: 18.108876772s
Sep  5 22:00:07.921: INFO: Pod "downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919": Phase="Pending", Reason="", readiness=false. Elapsed: 20.132351428s
Sep  5 22:00:09.930: INFO: Pod "downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919": Phase="Pending", Reason="", readiness=false. Elapsed: 22.141580738s
Sep  5 22:00:11.939: INFO: Pod "downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919": Phase="Pending", Reason="", readiness=false. Elapsed: 24.150688923s
Sep  5 22:00:13.951: INFO: Pod "downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919": Phase="Pending", Reason="", readiness=false. Elapsed: 26.162488553s
Sep  5 22:00:15.958: INFO: Pod "downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919": Phase="Pending", Reason="", readiness=false. Elapsed: 28.17008647s
Sep  5 22:00:17.975: INFO: Pod "downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.187300882s
STEP: Saw pod success
Sep  5 22:00:17.976: INFO: Pod "downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919" satisfied condition "Succeeded or Failed"
Sep  5 22:00:17.998: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919 container dapi-container: <nil>
STEP: delete the pod
Sep  5 22:00:22.813: INFO: Waiting for pod downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919 to disappear
Sep  5 22:00:22.844: INFO: Pod downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919 still exists
Sep  5 22:00:24.844: INFO: Waiting for pod downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919 to disappear
Sep  5 22:00:24.860: INFO: Pod downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919 still exists
Sep  5 22:00:26.845: INFO: Waiting for pod downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919 to disappear
Sep  5 22:00:26.861: INFO: Pod downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919 still exists
Sep  5 22:00:28.845: INFO: Waiting for pod downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919 to disappear
Sep  5 22:00:28.858: INFO: Pod downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919 still exists
Sep  5 22:00:30.844: INFO: Waiting for pod downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919 to disappear
Sep  5 22:00:30.855: INFO: Pod downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919 still exists
Sep  5 22:00:32.844: INFO: Waiting for pod downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919 to disappear
Sep  5 22:00:32.855: INFO: Pod downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919 still exists
Sep  5 22:00:34.844: INFO: Waiting for pod downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919 to disappear
Sep  5 22:00:34.853: INFO: Pod downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919 still exists
Sep  5 22:00:36.844: INFO: Waiting for pod downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919 to disappear
Sep  5 22:00:36.853: INFO: Pod downward-api-c597047f-528d-4f63-ab7d-3a8f6f994919 no longer exists
[AfterEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:00:36.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4045" for this suite.

â€¢ [SLOW TEST:49.777 seconds]
[sig-node] Downward API
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","total":291,"completed":83,"skipped":1496,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:00:37.104: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-2575
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:00:49.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2575" for this suite.

â€¢ [SLOW TEST:12.889 seconds]
[sig-api-machinery] ResourceQuota
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","total":291,"completed":84,"skipped":1502,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:00:49.993: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1303
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 22:00:50.391: INFO: Creating ReplicaSet my-hostname-basic-4e2e54bb-902d-4e54-ab8a-d42fed5f8525
Sep  5 22:00:50.451: INFO: Pod name my-hostname-basic-4e2e54bb-902d-4e54-ab8a-d42fed5f8525: Found 0 pods out of 1
Sep  5 22:00:55.465: INFO: Pod name my-hostname-basic-4e2e54bb-902d-4e54-ab8a-d42fed5f8525: Found 1 pods out of 1
Sep  5 22:00:55.465: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-4e2e54bb-902d-4e54-ab8a-d42fed5f8525" is running
Sep  5 22:01:17.493: INFO: Pod "my-hostname-basic-4e2e54bb-902d-4e54-ab8a-d42fed5f8525-lxtpf" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2021-09-05 22:00:50 -0700 PDT Reason: Message:}])
Sep  5 22:01:17.494: INFO: Trying to dial the pod
Sep  5 22:01:22.569: INFO: Controller my-hostname-basic-4e2e54bb-902d-4e54-ab8a-d42fed5f8525: Got expected result from replica 1 [my-hostname-basic-4e2e54bb-902d-4e54-ab8a-d42fed5f8525-lxtpf]: "my-hostname-basic-4e2e54bb-902d-4e54-ab8a-d42fed5f8525-lxtpf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:01:22.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1303" for this suite.

â€¢ [SLOW TEST:32.810 seconds]
[sig-apps] ReplicaSet
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","total":291,"completed":85,"skipped":1508,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:01:22.803: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3296
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-map-70753f99-5b6e-46aa-9ae6-c192e7fcef4c
STEP: Creating a pod to test consume secrets
Sep  5 22:01:23.327: INFO: Waiting up to 5m0s for pod "pod-secrets-583791b8-a996-4018-82c7-33427d3415c1" in namespace "secrets-3296" to be "Succeeded or Failed"
Sep  5 22:01:23.347: INFO: Pod "pod-secrets-583791b8-a996-4018-82c7-33427d3415c1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.828841ms
Sep  5 22:01:25.355: INFO: Pod "pod-secrets-583791b8-a996-4018-82c7-33427d3415c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028437579s
Sep  5 22:01:27.363: INFO: Pod "pod-secrets-583791b8-a996-4018-82c7-33427d3415c1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036256178s
Sep  5 22:01:29.384: INFO: Pod "pod-secrets-583791b8-a996-4018-82c7-33427d3415c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056703685s
Sep  5 22:01:31.409: INFO: Pod "pod-secrets-583791b8-a996-4018-82c7-33427d3415c1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082126485s
Sep  5 22:01:33.419: INFO: Pod "pod-secrets-583791b8-a996-4018-82c7-33427d3415c1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.092139117s
Sep  5 22:01:35.429: INFO: Pod "pod-secrets-583791b8-a996-4018-82c7-33427d3415c1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.10185122s
Sep  5 22:01:37.493: INFO: Pod "pod-secrets-583791b8-a996-4018-82c7-33427d3415c1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.166325759s
Sep  5 22:01:39.502: INFO: Pod "pod-secrets-583791b8-a996-4018-82c7-33427d3415c1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.17466128s
Sep  5 22:01:41.525: INFO: Pod "pod-secrets-583791b8-a996-4018-82c7-33427d3415c1": Phase="Running", Reason="", readiness=true. Elapsed: 18.198129273s
Sep  5 22:01:43.535: INFO: Pod "pod-secrets-583791b8-a996-4018-82c7-33427d3415c1": Phase="Running", Reason="", readiness=true. Elapsed: 20.207623253s
Sep  5 22:01:45.543: INFO: Pod "pod-secrets-583791b8-a996-4018-82c7-33427d3415c1": Phase="Running", Reason="", readiness=true. Elapsed: 22.215551315s
Sep  5 22:01:47.559: INFO: Pod "pod-secrets-583791b8-a996-4018-82c7-33427d3415c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.232048491s
STEP: Saw pod success
Sep  5 22:01:47.559: INFO: Pod "pod-secrets-583791b8-a996-4018-82c7-33427d3415c1" satisfied condition "Succeeded or Failed"
Sep  5 22:01:47.575: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-secrets-583791b8-a996-4018-82c7-33427d3415c1 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 22:01:51.770: INFO: Waiting for pod pod-secrets-583791b8-a996-4018-82c7-33427d3415c1 to disappear
Sep  5 22:01:51.793: INFO: Pod pod-secrets-583791b8-a996-4018-82c7-33427d3415c1 still exists
Sep  5 22:01:53.793: INFO: Waiting for pod pod-secrets-583791b8-a996-4018-82c7-33427d3415c1 to disappear
Sep  5 22:01:53.803: INFO: Pod pod-secrets-583791b8-a996-4018-82c7-33427d3415c1 still exists
Sep  5 22:01:55.793: INFO: Waiting for pod pod-secrets-583791b8-a996-4018-82c7-33427d3415c1 to disappear
Sep  5 22:01:55.805: INFO: Pod pod-secrets-583791b8-a996-4018-82c7-33427d3415c1 still exists
Sep  5 22:01:57.794: INFO: Waiting for pod pod-secrets-583791b8-a996-4018-82c7-33427d3415c1 to disappear
Sep  5 22:01:57.805: INFO: Pod pod-secrets-583791b8-a996-4018-82c7-33427d3415c1 still exists
Sep  5 22:01:59.794: INFO: Waiting for pod pod-secrets-583791b8-a996-4018-82c7-33427d3415c1 to disappear
Sep  5 22:01:59.804: INFO: Pod pod-secrets-583791b8-a996-4018-82c7-33427d3415c1 still exists
Sep  5 22:02:01.794: INFO: Waiting for pod pod-secrets-583791b8-a996-4018-82c7-33427d3415c1 to disappear
Sep  5 22:02:01.806: INFO: Pod pod-secrets-583791b8-a996-4018-82c7-33427d3415c1 still exists
Sep  5 22:02:03.793: INFO: Waiting for pod pod-secrets-583791b8-a996-4018-82c7-33427d3415c1 to disappear
Sep  5 22:02:03.815: INFO: Pod pod-secrets-583791b8-a996-4018-82c7-33427d3415c1 still exists
Sep  5 22:02:05.793: INFO: Waiting for pod pod-secrets-583791b8-a996-4018-82c7-33427d3415c1 to disappear
Sep  5 22:02:05.801: INFO: Pod pod-secrets-583791b8-a996-4018-82c7-33427d3415c1 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:02:05.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3296" for this suite.

â€¢ [SLOW TEST:43.268 seconds]
[sig-storage] Secrets
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":291,"completed":86,"skipped":1564,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:02:06.072: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6866
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-a2b7008f-ad99-4f5b-9bae-73766b6a6167
STEP: Creating a pod to test consume configMaps
Sep  5 22:02:06.720: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82" in namespace "projected-6866" to be "Succeeded or Failed"
Sep  5 22:02:06.738: INFO: Pod "pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82": Phase="Pending", Reason="", readiness=false. Elapsed: 18.387024ms
Sep  5 22:02:08.750: INFO: Pod "pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030541725s
Sep  5 22:02:10.773: INFO: Pod "pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82": Phase="Pending", Reason="", readiness=false. Elapsed: 4.053789024s
Sep  5 22:02:12.785: INFO: Pod "pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065262196s
Sep  5 22:02:14.798: INFO: Pod "pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82": Phase="Pending", Reason="", readiness=false. Elapsed: 8.078062521s
Sep  5 22:02:16.809: INFO: Pod "pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82": Phase="Pending", Reason="", readiness=false. Elapsed: 10.089547797s
Sep  5 22:02:18.835: INFO: Pod "pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82": Phase="Pending", Reason="", readiness=false. Elapsed: 12.114903117s
Sep  5 22:02:20.865: INFO: Pod "pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82": Phase="Pending", Reason="", readiness=false. Elapsed: 14.145571926s
Sep  5 22:02:22.878: INFO: Pod "pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82": Phase="Pending", Reason="", readiness=false. Elapsed: 16.158377578s
Sep  5 22:02:24.886: INFO: Pod "pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82": Phase="Pending", Reason="", readiness=false. Elapsed: 18.166647146s
Sep  5 22:02:26.896: INFO: Pod "pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.176281154s
STEP: Saw pod success
Sep  5 22:02:26.896: INFO: Pod "pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82" satisfied condition "Succeeded or Failed"
Sep  5 22:02:26.903: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 22:02:26.955: INFO: Waiting for pod pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82 to disappear
Sep  5 22:02:26.998: INFO: Pod pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82 still exists
Sep  5 22:02:28.999: INFO: Waiting for pod pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82 to disappear
Sep  5 22:02:29.008: INFO: Pod pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82 still exists
Sep  5 22:02:30.998: INFO: Waiting for pod pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82 to disappear
Sep  5 22:02:31.012: INFO: Pod pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82 still exists
Sep  5 22:02:32.999: INFO: Waiting for pod pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82 to disappear
Sep  5 22:02:33.006: INFO: Pod pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82 still exists
Sep  5 22:02:34.999: INFO: Waiting for pod pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82 to disappear
Sep  5 22:02:35.007: INFO: Pod pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82 still exists
Sep  5 22:02:36.999: INFO: Waiting for pod pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82 to disappear
Sep  5 22:02:37.007: INFO: Pod pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82 still exists
Sep  5 22:02:38.999: INFO: Waiting for pod pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82 to disappear
Sep  5 22:02:39.007: INFO: Pod pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82 still exists
Sep  5 22:02:40.999: INFO: Waiting for pod pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82 to disappear
Sep  5 22:02:41.009: INFO: Pod pod-projected-configmaps-d1e99444-34c2-4886-be36-abad9f992b82 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:02:41.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6866" for this suite.

â€¢ [SLOW TEST:35.159 seconds]
[sig-storage] Projected configMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":87,"skipped":1564,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should delete a collection of events [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:02:41.231: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-7234
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should delete a collection of events [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
STEP: get a list of Events with a label in the current namespace
STEP: delete a list of events
Sep  5 22:02:41.683: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
[AfterEach] [sig-instrumentation] Events API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:02:41.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7234" for this suite.
â€¢{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","total":291,"completed":88,"skipped":1574,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:02:41.955: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6070
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:02:58.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6070" for this suite.

â€¢ [SLOW TEST:16.953 seconds]
[sig-api-machinery] ResourceQuota
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","total":291,"completed":89,"skipped":1591,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:02:58.909: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8311
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep  5 22:02:59.259: INFO: Waiting up to 5m0s for pod "pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225" in namespace "emptydir-8311" to be "Succeeded or Failed"
Sep  5 22:02:59.276: INFO: Pod "pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225": Phase="Pending", Reason="", readiness=false. Elapsed: 17.353909ms
Sep  5 22:03:01.286: INFO: Pod "pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027164811s
Sep  5 22:03:03.292: INFO: Pod "pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03347947s
Sep  5 22:03:05.303: INFO: Pod "pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043852228s
Sep  5 22:03:07.310: INFO: Pod "pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225": Phase="Pending", Reason="", readiness=false. Elapsed: 8.051486555s
Sep  5 22:03:09.322: INFO: Pod "pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225": Phase="Pending", Reason="", readiness=false. Elapsed: 10.06306984s
Sep  5 22:03:11.329: INFO: Pod "pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225": Phase="Pending", Reason="", readiness=false. Elapsed: 12.069769132s
Sep  5 22:03:13.357: INFO: Pod "pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225": Phase="Pending", Reason="", readiness=false. Elapsed: 14.097708114s
Sep  5 22:03:15.362: INFO: Pod "pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225": Phase="Pending", Reason="", readiness=false. Elapsed: 16.103434082s
Sep  5 22:03:17.373: INFO: Pod "pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225": Phase="Running", Reason="", readiness=true. Elapsed: 18.113640874s
Sep  5 22:03:19.383: INFO: Pod "pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225": Phase="Running", Reason="", readiness=true. Elapsed: 20.123710118s
Sep  5 22:03:21.393: INFO: Pod "pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225": Phase="Running", Reason="", readiness=true. Elapsed: 22.133558677s
Sep  5 22:03:23.410: INFO: Pod "pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.151440763s
STEP: Saw pod success
Sep  5 22:03:23.410: INFO: Pod "pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225" satisfied condition "Succeeded or Failed"
Sep  5 22:03:23.422: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225 container test-container: <nil>
STEP: delete the pod
Sep  5 22:03:27.502: INFO: Waiting for pod pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225 to disappear
Sep  5 22:03:27.534: INFO: Pod pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225 still exists
Sep  5 22:03:29.534: INFO: Waiting for pod pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225 to disappear
Sep  5 22:03:29.553: INFO: Pod pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225 still exists
Sep  5 22:03:31.534: INFO: Waiting for pod pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225 to disappear
Sep  5 22:03:31.540: INFO: Pod pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225 still exists
Sep  5 22:03:33.535: INFO: Waiting for pod pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225 to disappear
Sep  5 22:03:33.543: INFO: Pod pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225 still exists
Sep  5 22:03:35.535: INFO: Waiting for pod pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225 to disappear
Sep  5 22:03:35.544: INFO: Pod pod-5b4dd1bf-2367-4670-9ed7-cd12799f4225 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:03:35.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8311" for this suite.

â€¢ [SLOW TEST:36.876 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":90,"skipped":1600,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:03:35.785: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-806
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  5 22:03:36.250: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13" in namespace "downward-api-806" to be "Succeeded or Failed"
Sep  5 22:03:36.264: INFO: Pod "downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13": Phase="Pending", Reason="", readiness=false. Elapsed: 14.39604ms
Sep  5 22:03:38.270: INFO: Pod "downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020669306s
Sep  5 22:03:40.290: INFO: Pod "downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040443925s
Sep  5 22:03:42.326: INFO: Pod "downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07624309s
Sep  5 22:03:44.335: INFO: Pod "downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13": Phase="Pending", Reason="", readiness=false. Elapsed: 8.085289788s
Sep  5 22:03:46.341: INFO: Pod "downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13": Phase="Pending", Reason="", readiness=false. Elapsed: 10.091146276s
Sep  5 22:03:48.378: INFO: Pod "downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13": Phase="Pending", Reason="", readiness=false. Elapsed: 12.127853907s
Sep  5 22:03:50.387: INFO: Pod "downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13": Phase="Pending", Reason="", readiness=false. Elapsed: 14.137332911s
Sep  5 22:03:52.393: INFO: Pod "downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13": Phase="Pending", Reason="", readiness=false. Elapsed: 16.143461186s
Sep  5 22:03:54.404: INFO: Pod "downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13": Phase="Pending", Reason="", readiness=false. Elapsed: 18.154103648s
Sep  5 22:03:56.414: INFO: Pod "downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13": Phase="Pending", Reason="", readiness=false. Elapsed: 20.164072324s
Sep  5 22:03:58.424: INFO: Pod "downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.174284953s
STEP: Saw pod success
Sep  5 22:03:58.424: INFO: Pod "downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13" satisfied condition "Succeeded or Failed"
Sep  5 22:03:58.435: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13 container client-container: <nil>
STEP: delete the pod
Sep  5 22:04:02.811: INFO: Waiting for pod downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13 to disappear
Sep  5 22:04:02.834: INFO: Pod downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13 still exists
Sep  5 22:04:04.835: INFO: Waiting for pod downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13 to disappear
Sep  5 22:04:04.843: INFO: Pod downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13 still exists
Sep  5 22:04:06.835: INFO: Waiting for pod downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13 to disappear
Sep  5 22:04:06.852: INFO: Pod downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13 still exists
Sep  5 22:04:08.834: INFO: Waiting for pod downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13 to disappear
Sep  5 22:04:08.846: INFO: Pod downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13 still exists
Sep  5 22:04:10.834: INFO: Waiting for pod downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13 to disappear
Sep  5 22:04:10.846: INFO: Pod downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13 still exists
Sep  5 22:04:12.834: INFO: Waiting for pod downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13 to disappear
Sep  5 22:04:12.850: INFO: Pod downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13 still exists
Sep  5 22:04:14.836: INFO: Waiting for pod downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13 to disappear
Sep  5 22:04:14.846: INFO: Pod downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13 still exists
Sep  5 22:04:16.835: INFO: Waiting for pod downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13 to disappear
Sep  5 22:04:16.841: INFO: Pod downwardapi-volume-f7b2f3c6-056d-4e46-9bfa-c483a5f80f13 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:04:16.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-806" for this suite.

â€¢ [SLOW TEST:41.326 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's cpu limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","total":291,"completed":91,"skipped":1600,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:04:17.111: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1521
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should adopt matching pods on creation [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:04:38.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1521" for this suite.

â€¢ [SLOW TEST:21.947 seconds]
[sig-apps] ReplicationController
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","total":291,"completed":92,"skipped":1600,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected combined
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:04:39.059: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6902
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-projected-all-test-volume-c8ea3c91-bf82-47fa-af11-e31792b01729
STEP: Creating secret with name secret-projected-all-test-volume-a8715296-5964-4bc3-811e-3313e44b832d
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep  5 22:04:39.909: INFO: Waiting up to 5m0s for pod "projected-volume-7822bef8-172d-4885-8fea-a021897345ee" in namespace "projected-6902" to be "Succeeded or Failed"
Sep  5 22:04:39.926: INFO: Pod "projected-volume-7822bef8-172d-4885-8fea-a021897345ee": Phase="Pending", Reason="", readiness=false. Elapsed: 16.469026ms
Sep  5 22:04:41.936: INFO: Pod "projected-volume-7822bef8-172d-4885-8fea-a021897345ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026704696s
Sep  5 22:04:43.953: INFO: Pod "projected-volume-7822bef8-172d-4885-8fea-a021897345ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043943879s
Sep  5 22:04:45.973: INFO: Pod "projected-volume-7822bef8-172d-4885-8fea-a021897345ee": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063719383s
Sep  5 22:04:47.984: INFO: Pod "projected-volume-7822bef8-172d-4885-8fea-a021897345ee": Phase="Pending", Reason="", readiness=false. Elapsed: 8.075125357s
Sep  5 22:04:49.994: INFO: Pod "projected-volume-7822bef8-172d-4885-8fea-a021897345ee": Phase="Pending", Reason="", readiness=false. Elapsed: 10.084654252s
Sep  5 22:04:52.009: INFO: Pod "projected-volume-7822bef8-172d-4885-8fea-a021897345ee": Phase="Pending", Reason="", readiness=false. Elapsed: 12.100265233s
Sep  5 22:04:54.018: INFO: Pod "projected-volume-7822bef8-172d-4885-8fea-a021897345ee": Phase="Pending", Reason="", readiness=false. Elapsed: 14.108345435s
Sep  5 22:04:56.044: INFO: Pod "projected-volume-7822bef8-172d-4885-8fea-a021897345ee": Phase="Pending", Reason="", readiness=false. Elapsed: 16.135043984s
Sep  5 22:04:58.054: INFO: Pod "projected-volume-7822bef8-172d-4885-8fea-a021897345ee": Phase="Pending", Reason="", readiness=false. Elapsed: 18.144853887s
Sep  5 22:05:00.065: INFO: Pod "projected-volume-7822bef8-172d-4885-8fea-a021897345ee": Phase="Pending", Reason="", readiness=false. Elapsed: 20.155612714s
Sep  5 22:05:02.077: INFO: Pod "projected-volume-7822bef8-172d-4885-8fea-a021897345ee": Phase="Pending", Reason="", readiness=false. Elapsed: 22.167713504s
Sep  5 22:05:04.088: INFO: Pod "projected-volume-7822bef8-172d-4885-8fea-a021897345ee": Phase="Pending", Reason="", readiness=false. Elapsed: 24.178945989s
Sep  5 22:05:06.098: INFO: Pod "projected-volume-7822bef8-172d-4885-8fea-a021897345ee": Phase="Pending", Reason="", readiness=false. Elapsed: 26.189125632s
Sep  5 22:05:08.107: INFO: Pod "projected-volume-7822bef8-172d-4885-8fea-a021897345ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.197516725s
STEP: Saw pod success
Sep  5 22:05:08.107: INFO: Pod "projected-volume-7822bef8-172d-4885-8fea-a021897345ee" satisfied condition "Succeeded or Failed"
Sep  5 22:05:08.114: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee container projected-all-volume-test: <nil>
STEP: delete the pod
Sep  5 22:05:08.174: INFO: Waiting for pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee to disappear
Sep  5 22:05:08.186: INFO: Pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee still exists
Sep  5 22:05:10.186: INFO: Waiting for pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee to disappear
Sep  5 22:05:10.199: INFO: Pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee still exists
Sep  5 22:05:12.186: INFO: Waiting for pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee to disappear
Sep  5 22:05:12.192: INFO: Pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee still exists
Sep  5 22:05:14.186: INFO: Waiting for pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee to disappear
Sep  5 22:05:14.196: INFO: Pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee still exists
Sep  5 22:05:16.186: INFO: Waiting for pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee to disappear
Sep  5 22:05:16.191: INFO: Pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee still exists
Sep  5 22:05:18.187: INFO: Waiting for pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee to disappear
Sep  5 22:05:18.194: INFO: Pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee still exists
Sep  5 22:05:20.186: INFO: Waiting for pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee to disappear
Sep  5 22:05:20.193: INFO: Pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee still exists
Sep  5 22:05:22.186: INFO: Waiting for pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee to disappear
Sep  5 22:05:22.197: INFO: Pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee still exists
Sep  5 22:05:24.186: INFO: Waiting for pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee to disappear
Sep  5 22:05:24.203: INFO: Pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee still exists
Sep  5 22:05:26.186: INFO: Waiting for pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee to disappear
Sep  5 22:05:26.194: INFO: Pod projected-volume-7822bef8-172d-4885-8fea-a021897345ee no longer exists
[AfterEach] [sig-storage] Projected combined
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:05:26.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6902" for this suite.

â€¢ [SLOW TEST:47.361 seconds]
[sig-storage] Projected combined
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","total":291,"completed":93,"skipped":1659,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  custom resource defaulting for requests and from storage works  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:05:26.420: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5869
STEP: Waiting for a default service account to be provisioned in namespace
[It] custom resource defaulting for requests and from storage works  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 22:05:26.849: INFO: >>> kubeConfig: ./kconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:05:28.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5869" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","total":291,"completed":94,"skipped":1683,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:05:29.591: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8207
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in volume subpath
Sep  5 22:05:30.770: INFO: Waiting up to 5m0s for pod "var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac" in namespace "var-expansion-8207" to be "Succeeded or Failed"
Sep  5 22:05:30.791: INFO: Pod "var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac": Phase="Pending", Reason="", readiness=false. Elapsed: 20.828901ms
Sep  5 22:05:32.802: INFO: Pod "var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031800591s
Sep  5 22:05:34.835: INFO: Pod "var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065161015s
Sep  5 22:05:36.860: INFO: Pod "var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac": Phase="Pending", Reason="", readiness=false. Elapsed: 6.090408779s
Sep  5 22:05:38.888: INFO: Pod "var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac": Phase="Pending", Reason="", readiness=false. Elapsed: 8.118140691s
Sep  5 22:05:40.922: INFO: Pod "var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac": Phase="Pending", Reason="", readiness=false. Elapsed: 10.151996476s
Sep  5 22:05:42.940: INFO: Pod "var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac": Phase="Pending", Reason="", readiness=false. Elapsed: 12.170384907s
Sep  5 22:05:44.964: INFO: Pod "var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac": Phase="Pending", Reason="", readiness=false. Elapsed: 14.194195391s
Sep  5 22:05:46.975: INFO: Pod "var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac": Phase="Pending", Reason="", readiness=false. Elapsed: 16.205494896s
Sep  5 22:05:48.983: INFO: Pod "var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac": Phase="Pending", Reason="", readiness=false. Elapsed: 18.212671559s
Sep  5 22:05:50.991: INFO: Pod "var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac": Phase="Pending", Reason="", readiness=false. Elapsed: 20.220844645s
Sep  5 22:05:52.999: INFO: Pod "var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac": Phase="Pending", Reason="", readiness=false. Elapsed: 22.229079902s
Sep  5 22:05:55.011: INFO: Pod "var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac": Phase="Pending", Reason="", readiness=false. Elapsed: 24.241627657s
Sep  5 22:05:57.019: INFO: Pod "var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac": Phase="Pending", Reason="", readiness=false. Elapsed: 26.249425694s
Sep  5 22:05:59.027: INFO: Pod "var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.25739006s
STEP: Saw pod success
Sep  5 22:05:59.027: INFO: Pod "var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac" satisfied condition "Succeeded or Failed"
Sep  5 22:05:59.035: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac container dapi-container: <nil>
STEP: delete the pod
Sep  5 22:06:03.391: INFO: Waiting for pod var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac to disappear
Sep  5 22:06:03.425: INFO: Pod var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac still exists
Sep  5 22:06:05.426: INFO: Waiting for pod var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac to disappear
Sep  5 22:06:05.435: INFO: Pod var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac still exists
Sep  5 22:06:07.427: INFO: Waiting for pod var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac to disappear
Sep  5 22:06:07.439: INFO: Pod var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac still exists
Sep  5 22:06:09.427: INFO: Waiting for pod var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac to disappear
Sep  5 22:06:09.434: INFO: Pod var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac still exists
Sep  5 22:06:11.426: INFO: Waiting for pod var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac to disappear
Sep  5 22:06:11.436: INFO: Pod var-expansion-7d314883-5b9e-4919-8907-fa8f73b2dcac no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:06:11.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8207" for this suite.

â€¢ [SLOW TEST:42.074 seconds]
[k8s.io] Variable Expansion
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow substituting values in a volume subpath [sig-storage] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage] [Conformance]","total":291,"completed":95,"skipped":1691,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:06:11.666: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8563
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 22:06:13.212: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  5 22:06:15.237: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:06:17.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:06:19.253: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:06:21.245: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:06:23.249: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:06:25.248: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:06:27.246: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:06:29.244: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:06:31.247: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:06:33.244: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:06:35.249: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:06:37.244: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 6, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 22:06:40.269: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 22:06:40.276: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-588-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:06:41.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8563" for this suite.
STEP: Destroying namespace "webhook-8563-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:32.587 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","total":291,"completed":96,"skipped":1716,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:06:44.253: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2622
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  5 22:06:44.820: INFO: Waiting up to 5m0s for pod "pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80" in namespace "emptydir-2622" to be "Succeeded or Failed"
Sep  5 22:06:44.847: INFO: Pod "pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80": Phase="Pending", Reason="", readiness=false. Elapsed: 27.778612ms
Sep  5 22:06:46.856: INFO: Pod "pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036504497s
Sep  5 22:06:48.867: INFO: Pod "pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047662812s
Sep  5 22:06:50.885: INFO: Pod "pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064832413s
Sep  5 22:06:52.937: INFO: Pod "pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80": Phase="Pending", Reason="", readiness=false. Elapsed: 8.116834437s
Sep  5 22:06:54.946: INFO: Pod "pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80": Phase="Pending", Reason="", readiness=false. Elapsed: 10.126223376s
Sep  5 22:06:56.959: INFO: Pod "pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80": Phase="Pending", Reason="", readiness=false. Elapsed: 12.139257763s
Sep  5 22:06:58.972: INFO: Pod "pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80": Phase="Pending", Reason="", readiness=false. Elapsed: 14.152636445s
Sep  5 22:07:00.983: INFO: Pod "pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80": Phase="Pending", Reason="", readiness=false. Elapsed: 16.163100217s
Sep  5 22:07:02.996: INFO: Pod "pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80": Phase="Pending", Reason="", readiness=false. Elapsed: 18.176653992s
Sep  5 22:07:05.007: INFO: Pod "pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80": Phase="Pending", Reason="", readiness=false. Elapsed: 20.186971477s
Sep  5 22:07:07.021: INFO: Pod "pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80": Phase="Pending", Reason="", readiness=false. Elapsed: 22.200950236s
Sep  5 22:07:09.044: INFO: Pod "pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.224416183s
STEP: Saw pod success
Sep  5 22:07:09.044: INFO: Pod "pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80" satisfied condition "Succeeded or Failed"
Sep  5 22:07:09.064: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80 container test-container: <nil>
STEP: delete the pod
Sep  5 22:07:13.701: INFO: Waiting for pod pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80 to disappear
Sep  5 22:07:13.720: INFO: Pod pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80 still exists
Sep  5 22:07:15.720: INFO: Waiting for pod pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80 to disappear
Sep  5 22:07:15.736: INFO: Pod pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80 still exists
Sep  5 22:07:17.720: INFO: Waiting for pod pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80 to disappear
Sep  5 22:07:17.733: INFO: Pod pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80 still exists
Sep  5 22:07:19.721: INFO: Waiting for pod pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80 to disappear
Sep  5 22:07:19.733: INFO: Pod pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80 still exists
Sep  5 22:07:21.721: INFO: Waiting for pod pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80 to disappear
Sep  5 22:07:21.732: INFO: Pod pod-4a749a56-fa95-4f4e-87c3-0cdccba19d80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:07:21.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2622" for this suite.

â€¢ [SLOW TEST:37.748 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":97,"skipped":1716,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:07:22.002: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9394
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 22:07:23.858: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  5 22:07:25.884: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:07:27.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:07:29.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:07:31.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:07:33.895: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:07:35.893: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:07:37.893: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:07:39.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:07:41.895: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:07:43.900: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:07:45.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:07:47.897: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 7, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 22:07:50.929: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Sep  5 22:07:51.014: INFO: >>> kubeConfig: ./kconfig.yaml
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:07:51.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9394" for this suite.
STEP: Destroying namespace "webhook-9394-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:29.719 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","total":291,"completed":98,"skipped":1720,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
S
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:07:51.721: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  5 22:07:52.209: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c" in namespace "downward-api-5577" to be "Succeeded or Failed"
Sep  5 22:07:52.222: INFO: Pod "downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.349716ms
Sep  5 22:07:54.240: INFO: Pod "downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030991035s
Sep  5 22:07:56.248: INFO: Pod "downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038507303s
Sep  5 22:07:58.261: INFO: Pod "downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.05217808s
Sep  5 22:08:00.278: INFO: Pod "downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.069320885s
Sep  5 22:08:02.289: INFO: Pod "downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.079983211s
Sep  5 22:08:04.299: INFO: Pod "downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.089545963s
Sep  5 22:08:06.307: INFO: Pod "downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.097879837s
Sep  5 22:08:08.318: INFO: Pod "downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.1088059s
Sep  5 22:08:10.330: INFO: Pod "downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.120915605s
Sep  5 22:08:12.342: INFO: Pod "downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.13252994s
Sep  5 22:08:14.350: INFO: Pod "downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.140419232s
STEP: Saw pod success
Sep  5 22:08:14.350: INFO: Pod "downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c" satisfied condition "Succeeded or Failed"
Sep  5 22:08:14.355: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c container client-container: <nil>
STEP: delete the pod
Sep  5 22:08:14.454: INFO: Waiting for pod downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c to disappear
Sep  5 22:08:14.482: INFO: Pod downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c still exists
Sep  5 22:08:16.483: INFO: Waiting for pod downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c to disappear
Sep  5 22:08:16.490: INFO: Pod downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c still exists
Sep  5 22:08:18.482: INFO: Waiting for pod downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c to disappear
Sep  5 22:08:18.491: INFO: Pod downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c still exists
Sep  5 22:08:20.482: INFO: Waiting for pod downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c to disappear
Sep  5 22:08:20.492: INFO: Pod downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c still exists
Sep  5 22:08:22.482: INFO: Waiting for pod downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c to disappear
Sep  5 22:08:22.489: INFO: Pod downwardapi-volume-2856ada7-939e-4aa0-8691-dd94e51b953c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:08:22.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5577" for this suite.

â€¢ [SLOW TEST:31.009 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":99,"skipped":1721,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:08:22.730: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-168
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  5 22:08:23.171: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27" in namespace "downward-api-168" to be "Succeeded or Failed"
Sep  5 22:08:23.177: INFO: Pod "downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27": Phase="Pending", Reason="", readiness=false. Elapsed: 6.219313ms
Sep  5 22:08:25.184: INFO: Pod "downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013744755s
Sep  5 22:08:27.195: INFO: Pod "downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02402752s
Sep  5 22:08:29.218: INFO: Pod "downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27": Phase="Pending", Reason="", readiness=false. Elapsed: 6.047296358s
Sep  5 22:08:31.227: INFO: Pod "downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27": Phase="Pending", Reason="", readiness=false. Elapsed: 8.056175003s
Sep  5 22:08:33.238: INFO: Pod "downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27": Phase="Pending", Reason="", readiness=false. Elapsed: 10.067705749s
Sep  5 22:08:35.257: INFO: Pod "downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27": Phase="Pending", Reason="", readiness=false. Elapsed: 12.085925763s
Sep  5 22:08:37.266: INFO: Pod "downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27": Phase="Pending", Reason="", readiness=false. Elapsed: 14.094856282s
Sep  5 22:08:39.277: INFO: Pod "downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27": Phase="Pending", Reason="", readiness=false. Elapsed: 16.105962997s
Sep  5 22:08:41.285: INFO: Pod "downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27": Phase="Pending", Reason="", readiness=false. Elapsed: 18.114822709s
Sep  5 22:08:43.298: INFO: Pod "downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.12779314s
STEP: Saw pod success
Sep  5 22:08:43.299: INFO: Pod "downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27" satisfied condition "Succeeded or Failed"
Sep  5 22:08:43.324: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27 container client-container: <nil>
STEP: delete the pod
Sep  5 22:08:43.367: INFO: Waiting for pod downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27 to disappear
Sep  5 22:08:43.378: INFO: Pod downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27 still exists
Sep  5 22:08:45.379: INFO: Waiting for pod downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27 to disappear
Sep  5 22:08:45.386: INFO: Pod downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27 still exists
Sep  5 22:08:47.380: INFO: Waiting for pod downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27 to disappear
Sep  5 22:08:47.388: INFO: Pod downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27 still exists
Sep  5 22:08:49.379: INFO: Waiting for pod downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27 to disappear
Sep  5 22:08:49.394: INFO: Pod downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27 still exists
Sep  5 22:08:51.379: INFO: Waiting for pod downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27 to disappear
Sep  5 22:08:51.388: INFO: Pod downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27 still exists
Sep  5 22:08:53.379: INFO: Waiting for pod downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27 to disappear
Sep  5 22:08:53.387: INFO: Pod downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27 still exists
Sep  5 22:08:55.380: INFO: Waiting for pod downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27 to disappear
Sep  5 22:08:55.388: INFO: Pod downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27 still exists
Sep  5 22:08:57.380: INFO: Waiting for pod downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27 to disappear
Sep  5 22:08:57.391: INFO: Pod downwardapi-volume-b1bd03ae-9da0-43bb-a35a-50cbc48c5f27 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:08:57.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-168" for this suite.

â€¢ [SLOW TEST:34.917 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide podname only [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","total":291,"completed":100,"skipped":1745,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:08:57.649: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5828
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should delete old replica sets [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 22:08:58.129: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep  5 22:09:03.139: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  5 22:09:19.154: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep  5 22:09:19.217: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-5828 /apis/apps/v1/namespaces/deployment-5828/deployments/test-cleanup-deployment e2bfbf24-1c49-4b2a-a38a-b002c5c0261c 116516 1 2021-09-05 22:09:19 -0700 PDT <nil> <nil> map[name:cleanup-pod] map[] [] []  [{e2e.test Update apps/v1 2021-09-05 22:09:19 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00531bb58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Sep  5 22:09:19.244: INFO: New ReplicaSet "test-cleanup-deployment-5d446bdd47" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-5d446bdd47  deployment-5828 /apis/apps/v1/namespaces/deployment-5828/replicasets/test-cleanup-deployment-5d446bdd47 12beb76d-13ef-4d5c-9e7c-746438965abb 116518 1 2021-09-05 22:09:19 -0700 PDT <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment e2bfbf24-1c49-4b2a-a38a-b002c5c0261c 0xc008412017 0xc008412018}] []  [{kube-controller-manager Update apps/v1 2021-09-05 22:09:19 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e2bfbf24-1c49-4b2a-a38a-b002c5c0261c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 5d446bdd47,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0084120a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  5 22:09:19.244: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Sep  5 22:09:19.244: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-5828 /apis/apps/v1/namespaces/deployment-5828/replicasets/test-cleanup-controller bcee6eaf-0ca5-4f43-9bae-65624b49a01b 116517 1 2021-09-05 22:08:58 -0700 PDT <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment e2bfbf24-1c49-4b2a-a38a-b002c5c0261c 0xc00531bef7 0xc00531bef8}] []  [{e2e.test Update apps/v1 2021-09-05 22:08:58 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-05 22:09:19 -0700 PDT FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"e2bfbf24-1c49-4b2a-a38a-b002c5c0261c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00531bfa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  5 22:09:19.282: INFO: Pod "test-cleanup-controller-lfwbp" is available:
&Pod{ObjectMeta:{test-cleanup-controller-lfwbp test-cleanup-controller- deployment-5828 /api/v1/namespaces/deployment-5828/pods/test-cleanup-controller-lfwbp 8e99321f-b7ed-4153-ae78-ca15c4738f49 116509 0 2021-09-05 22:08:58 -0700 PDT <nil> <nil> map[name:cleanup-pod pod:httpd] map[attachment_id:71d9d9ac-2232-4f17-9e18-511327d1e0b9 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:60:18 vlan:None vmware-system-ephemeral-disk-uuid:6000C29b-191e-87a5-8ea1-c64c0de9c70d vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v18777"} vmware-system-vm-moid:vm-727:7badc608-dd2f-42a9-952d-0d4c30d5f283 vmware-system-vm-uuid:5017ad9e-8eb2-c16f-beb1-0a92cd171c8a] [{apps/v1 ReplicaSet test-cleanup-controller bcee6eaf-0ca5-4f43-9bae-65624b49a01b 0xc0023c27e7 0xc0023c27e8}] [lifecycle-controller/system.vmware.com]  [{image-controller Update v1 2021-09-05 22:08:58 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {kube-controller-manager Update v1 2021-09-05 22:08:58 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bcee6eaf-0ca5-4f43-9bae-65624b49a01b\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-9mqvr Update v1 2021-09-05 22:09:08 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-05 22:09:11 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-05 22:09:18 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cmrkh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cmrkh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cmrkh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-43-208.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 22:08:58 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 22:09:18 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 22:09:18 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 22:09:18 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.43.208,PodIP:172.26.1.210,StartTime:2021-09-05 22:09:16 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-05 22:09:16 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v18777,ContainerID:4007e3eb-08ad-48ab-9d15-ff354ee7ff92,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.210,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  5 22:09:19.282: INFO: Pod "test-cleanup-deployment-5d446bdd47-q977h" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-5d446bdd47-q977h test-cleanup-deployment-5d446bdd47- deployment-5828 /api/v1/namespaces/deployment-5828/pods/test-cleanup-deployment-5d446bdd47-q977h ba90a88e-9efc-4136-89d8-e8b547ae48dd 116521 0 2021-09-05 22:09:19 -0700 PDT <nil> <nil> map[name:cleanup-pod pod-template-hash:5d446bdd47] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-cleanup-deployment-5d446bdd47 12beb76d-13ef-4d5c-9e7c-746438965abb 0xc0023c2bd7 0xc0023c2bd8}] []  [{kube-controller-manager Update v1 2021-09-05 22:09:19 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12beb76d-13ef-4d5c-9e7c-746438965abb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-cmrkh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-cmrkh,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-cmrkh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:09:19.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5828" for this suite.

â€¢ [SLOW TEST:21.873 seconds]
[sig-apps] Deployment
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","total":291,"completed":101,"skipped":1803,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:09:19.523: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8300
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 22:09:21.661: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  5 22:09:23.686: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:09:25.702: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:09:27.695: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:09:29.697: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:09:31.693: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:09:33.694: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:09:35.696: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:09:37.692: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:09:39.697: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:09:41.695: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:09:43.696: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:09:45.697: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:09:47.694: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 9, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 22:09:50.715: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:09:50.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8300" for this suite.
STEP: Destroying namespace "webhook-8300-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:32.101 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","total":291,"completed":102,"skipped":1821,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:09:51.624: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2914
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  5 22:09:52.260: INFO: Waiting up to 5m0s for pod "pod-a4ced868-727b-4542-93f6-4ad05ec4ad95" in namespace "emptydir-2914" to be "Succeeded or Failed"
Sep  5 22:09:52.272: INFO: Pod "pod-a4ced868-727b-4542-93f6-4ad05ec4ad95": Phase="Pending", Reason="", readiness=false. Elapsed: 11.634104ms
Sep  5 22:09:54.279: INFO: Pod "pod-a4ced868-727b-4542-93f6-4ad05ec4ad95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019298587s
Sep  5 22:09:56.307: INFO: Pod "pod-a4ced868-727b-4542-93f6-4ad05ec4ad95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047296337s
Sep  5 22:09:58.320: INFO: Pod "pod-a4ced868-727b-4542-93f6-4ad05ec4ad95": Phase="Pending", Reason="", readiness=false. Elapsed: 6.059750566s
Sep  5 22:10:00.328: INFO: Pod "pod-a4ced868-727b-4542-93f6-4ad05ec4ad95": Phase="Pending", Reason="", readiness=false. Elapsed: 8.067989675s
Sep  5 22:10:02.342: INFO: Pod "pod-a4ced868-727b-4542-93f6-4ad05ec4ad95": Phase="Pending", Reason="", readiness=false. Elapsed: 10.082406746s
Sep  5 22:10:04.360: INFO: Pod "pod-a4ced868-727b-4542-93f6-4ad05ec4ad95": Phase="Pending", Reason="", readiness=false. Elapsed: 12.100038969s
Sep  5 22:10:06.401: INFO: Pod "pod-a4ced868-727b-4542-93f6-4ad05ec4ad95": Phase="Pending", Reason="", readiness=false. Elapsed: 14.141125452s
Sep  5 22:10:08.409: INFO: Pod "pod-a4ced868-727b-4542-93f6-4ad05ec4ad95": Phase="Pending", Reason="", readiness=false. Elapsed: 16.149343019s
Sep  5 22:10:10.417: INFO: Pod "pod-a4ced868-727b-4542-93f6-4ad05ec4ad95": Phase="Pending", Reason="", readiness=false. Elapsed: 18.157014694s
Sep  5 22:10:12.430: INFO: Pod "pod-a4ced868-727b-4542-93f6-4ad05ec4ad95": Phase="Pending", Reason="", readiness=false. Elapsed: 20.17008461s
Sep  5 22:10:14.439: INFO: Pod "pod-a4ced868-727b-4542-93f6-4ad05ec4ad95": Phase="Pending", Reason="", readiness=false. Elapsed: 22.178429429s
Sep  5 22:10:16.448: INFO: Pod "pod-a4ced868-727b-4542-93f6-4ad05ec4ad95": Phase="Pending", Reason="", readiness=false. Elapsed: 24.187549587s
Sep  5 22:10:18.464: INFO: Pod "pod-a4ced868-727b-4542-93f6-4ad05ec4ad95": Phase="Pending", Reason="", readiness=false. Elapsed: 26.203562492s
Sep  5 22:10:20.489: INFO: Pod "pod-a4ced868-727b-4542-93f6-4ad05ec4ad95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.22873413s
STEP: Saw pod success
Sep  5 22:10:20.489: INFO: Pod "pod-a4ced868-727b-4542-93f6-4ad05ec4ad95" satisfied condition "Succeeded or Failed"
Sep  5 22:10:20.495: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-a4ced868-727b-4542-93f6-4ad05ec4ad95 container test-container: <nil>
STEP: delete the pod
Sep  5 22:10:24.550: INFO: Waiting for pod pod-a4ced868-727b-4542-93f6-4ad05ec4ad95 to disappear
Sep  5 22:10:24.571: INFO: Pod pod-a4ced868-727b-4542-93f6-4ad05ec4ad95 still exists
Sep  5 22:10:26.572: INFO: Waiting for pod pod-a4ced868-727b-4542-93f6-4ad05ec4ad95 to disappear
Sep  5 22:10:26.579: INFO: Pod pod-a4ced868-727b-4542-93f6-4ad05ec4ad95 still exists
Sep  5 22:10:28.572: INFO: Waiting for pod pod-a4ced868-727b-4542-93f6-4ad05ec4ad95 to disappear
Sep  5 22:10:28.584: INFO: Pod pod-a4ced868-727b-4542-93f6-4ad05ec4ad95 still exists
Sep  5 22:10:30.572: INFO: Waiting for pod pod-a4ced868-727b-4542-93f6-4ad05ec4ad95 to disappear
Sep  5 22:10:30.581: INFO: Pod pod-a4ced868-727b-4542-93f6-4ad05ec4ad95 still exists
Sep  5 22:10:32.573: INFO: Waiting for pod pod-a4ced868-727b-4542-93f6-4ad05ec4ad95 to disappear
Sep  5 22:10:32.581: INFO: Pod pod-a4ced868-727b-4542-93f6-4ad05ec4ad95 still exists
Sep  5 22:10:34.573: INFO: Waiting for pod pod-a4ced868-727b-4542-93f6-4ad05ec4ad95 to disappear
Sep  5 22:10:34.579: INFO: Pod pod-a4ced868-727b-4542-93f6-4ad05ec4ad95 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:10:34.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2914" for this suite.

â€¢ [SLOW TEST:43.258 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":103,"skipped":1826,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSS
------------------------------
[sig-network] Services 
  should test the lifecycle of an Endpoint [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:10:34.882: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6523
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should test the lifecycle of an Endpoint [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating an Endpoint
STEP: waiting for available Endpoint
STEP: listing all Endpoints
STEP: updating the Endpoint
STEP: fetching the Endpoint
STEP: patching the Endpoint
STEP: fetching the Endpoint
STEP: deleting the Endpoint by Collection
STEP: waiting for Endpoint deletion
STEP: fetching the Endpoint
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:10:35.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6523" for this suite.
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
â€¢{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","total":291,"completed":104,"skipped":1829,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:10:35.609: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3749
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 22:10:37.303: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  5 22:10:39.328: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:10:41.347: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:10:43.335: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:10:45.386: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:10:47.347: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:10:49.342: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:10:51.336: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:10:53.335: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:10:55.389: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:10:57.340: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:10:59.337: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:11:01.338: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:11:03.337: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 10, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 22:11:06.364: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:11:07.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3749" for this suite.
STEP: Destroying namespace "webhook-3749-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:32.293 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","total":291,"completed":105,"skipped":1843,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:11:07.902: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-258
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 22:11:30.535: INFO: Waiting up to 5m0s for pod "client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86" in namespace "pods-258" to be "Succeeded or Failed"
Sep  5 22:11:30.553: INFO: Pod "client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86": Phase="Pending", Reason="", readiness=false. Elapsed: 18.166549ms
Sep  5 22:11:32.561: INFO: Pod "client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026196082s
Sep  5 22:11:34.569: INFO: Pod "client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03393902s
Sep  5 22:11:36.575: INFO: Pod "client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040317966s
Sep  5 22:11:38.597: INFO: Pod "client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86": Phase="Pending", Reason="", readiness=false. Elapsed: 8.062690601s
Sep  5 22:11:40.609: INFO: Pod "client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86": Phase="Pending", Reason="", readiness=false. Elapsed: 10.074058397s
Sep  5 22:11:42.619: INFO: Pod "client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86": Phase="Pending", Reason="", readiness=false. Elapsed: 12.084456383s
Sep  5 22:11:44.633: INFO: Pod "client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86": Phase="Pending", Reason="", readiness=false. Elapsed: 14.09810427s
Sep  5 22:11:46.671: INFO: Pod "client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86": Phase="Pending", Reason="", readiness=false. Elapsed: 16.135930167s
Sep  5 22:11:48.684: INFO: Pod "client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86": Phase="Pending", Reason="", readiness=false. Elapsed: 18.149773465s
Sep  5 22:11:50.707: INFO: Pod "client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86": Phase="Pending", Reason="", readiness=false. Elapsed: 20.172677184s
Sep  5 22:11:52.713: INFO: Pod "client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86": Phase="Pending", Reason="", readiness=false. Elapsed: 22.178768294s
Sep  5 22:11:54.722: INFO: Pod "client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.186888367s
STEP: Saw pod success
Sep  5 22:11:54.722: INFO: Pod "client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86" satisfied condition "Succeeded or Failed"
Sep  5 22:11:54.734: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86 container env3cont: <nil>
STEP: delete the pod
Sep  5 22:11:54.859: INFO: Waiting for pod client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86 to disappear
Sep  5 22:11:54.870: INFO: Pod client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86 still exists
Sep  5 22:11:56.871: INFO: Waiting for pod client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86 to disappear
Sep  5 22:11:56.882: INFO: Pod client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86 still exists
Sep  5 22:11:58.871: INFO: Waiting for pod client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86 to disappear
Sep  5 22:11:58.886: INFO: Pod client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86 still exists
Sep  5 22:12:00.871: INFO: Waiting for pod client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86 to disappear
Sep  5 22:12:00.883: INFO: Pod client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86 still exists
Sep  5 22:12:02.871: INFO: Waiting for pod client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86 to disappear
Sep  5 22:12:02.885: INFO: Pod client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86 still exists
Sep  5 22:12:04.871: INFO: Waiting for pod client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86 to disappear
Sep  5 22:12:04.892: INFO: Pod client-envvars-a88e4b8b-f467-49b3-a540-0fdcec90de86 no longer exists
[AfterEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:12:04.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-258" for this suite.

â€¢ [SLOW TEST:57.277 seconds]
[k8s.io] Pods
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should contain environment variables for services [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]","total":291,"completed":106,"skipped":1843,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:12:05.179: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-4977
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 22:12:05.706: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69" in namespace "security-context-test-4977" to be "Succeeded or Failed"
Sep  5 22:12:05.716: INFO: Pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69": Phase="Pending", Reason="", readiness=false. Elapsed: 10.584104ms
Sep  5 22:12:07.724: INFO: Pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018061573s
Sep  5 22:12:09.736: INFO: Pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030237698s
Sep  5 22:12:11.746: INFO: Pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040129327s
Sep  5 22:12:13.758: INFO: Pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69": Phase="Pending", Reason="", readiness=false. Elapsed: 8.052511138s
Sep  5 22:12:15.766: INFO: Pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69": Phase="Pending", Reason="", readiness=false. Elapsed: 10.060189993s
Sep  5 22:12:17.782: INFO: Pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69": Phase="Pending", Reason="", readiness=false. Elapsed: 12.07568755s
Sep  5 22:12:19.806: INFO: Pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69": Phase="Pending", Reason="", readiness=false. Elapsed: 14.10053144s
Sep  5 22:12:21.815: INFO: Pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69": Phase="Pending", Reason="", readiness=false. Elapsed: 16.109041052s
Sep  5 22:12:23.822: INFO: Pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69": Phase="Pending", Reason="", readiness=false. Elapsed: 18.116305517s
Sep  5 22:12:25.830: INFO: Pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69": Phase="Pending", Reason="", readiness=false. Elapsed: 20.124152424s
Sep  5 22:12:27.843: INFO: Pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69": Phase="Pending", Reason="", readiness=false. Elapsed: 22.136972322s
Sep  5 22:12:29.864: INFO: Pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69": Phase="Pending", Reason="", readiness=false. Elapsed: 24.157803735s
Sep  5 22:12:31.874: INFO: Pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69": Phase="Pending", Reason="", readiness=false. Elapsed: 26.168271727s
Sep  5 22:12:33.882: INFO: Pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69": Phase="Pending", Reason="", readiness=false. Elapsed: 28.175702873s
Sep  5 22:12:35.891: INFO: Pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69": Phase="Pending", Reason="", readiness=false. Elapsed: 30.185601665s
Sep  5 22:12:37.905: INFO: Pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69": Phase="Pending", Reason="", readiness=false. Elapsed: 32.199289972s
Sep  5 22:12:39.914: INFO: Pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 34.20784736s
Sep  5 22:12:39.914: INFO: Pod "alpine-nnp-false-3d72d569-3b6e-4529-9ac6-8250e4589d69" satisfied condition "Succeeded or Failed"
[AfterEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:12:39.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4977" for this suite.

â€¢ [SLOW TEST:35.063 seconds]
[k8s.io] Security Context
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when creating containers with AllowPrivilegeEscalation
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:291
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":107,"skipped":1867,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:12:40.243: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-276
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-868affeb-cbaf-432a-a2a1-e2544f0cbdc6
STEP: Creating a pod to test consume configMaps
Sep  5 22:12:40.889: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0" in namespace "projected-276" to be "Succeeded or Failed"
Sep  5 22:12:40.914: INFO: Pod "pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0": Phase="Pending", Reason="", readiness=false. Elapsed: 25.519114ms
Sep  5 22:12:42.924: INFO: Pod "pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034950397s
Sep  5 22:12:44.931: INFO: Pod "pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042083128s
Sep  5 22:12:46.940: INFO: Pod "pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051038182s
Sep  5 22:12:48.949: INFO: Pod "pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.060820353s
Sep  5 22:12:50.958: INFO: Pod "pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.069105796s
Sep  5 22:12:52.967: INFO: Pod "pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.077867118s
Sep  5 22:12:54.975: INFO: Pod "pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.086054074s
Sep  5 22:12:56.983: INFO: Pod "pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.094184114s
Sep  5 22:12:58.994: INFO: Pod "pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.10538387s
Sep  5 22:13:01.003: INFO: Pod "pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.113873234s
Sep  5 22:13:03.016: INFO: Pod "pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.127744635s
Sep  5 22:13:05.024: INFO: Pod "pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.135724423s
Sep  5 22:13:07.031: INFO: Pod "pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.142720934s
STEP: Saw pod success
Sep  5 22:13:07.031: INFO: Pod "pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0" satisfied condition "Succeeded or Failed"
Sep  5 22:13:07.038: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com pod pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 22:13:07.134: INFO: Waiting for pod pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0 to disappear
Sep  5 22:13:07.149: INFO: Pod pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0 still exists
Sep  5 22:13:09.150: INFO: Waiting for pod pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0 to disappear
Sep  5 22:13:09.160: INFO: Pod pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0 still exists
Sep  5 22:13:11.150: INFO: Waiting for pod pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0 to disappear
Sep  5 22:13:11.163: INFO: Pod pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0 still exists
Sep  5 22:13:13.150: INFO: Waiting for pod pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0 to disappear
Sep  5 22:13:13.163: INFO: Pod pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0 still exists
Sep  5 22:13:15.151: INFO: Waiting for pod pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0 to disappear
Sep  5 22:13:15.168: INFO: Pod pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0 still exists
Sep  5 22:13:17.149: INFO: Waiting for pod pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0 to disappear
Sep  5 22:13:17.162: INFO: Pod pod-projected-configmaps-1f47f4d9-4717-427f-8695-ea3cf07543a0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:13:17.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-276" for this suite.

â€¢ [SLOW TEST:37.137 seconds]
[sig-storage] Projected configMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","total":291,"completed":108,"skipped":1893,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should patch a Namespace [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:13:17.380: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-8613
STEP: Waiting for a default service account to be provisioned in namespace
[It] should patch a Namespace [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a Namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nspatchtest-ebe9a7d9-0204-450b-9da5-7da9abdd72ac-9264
STEP: patching the Namespace
STEP: get the Namespace and ensuring it has the label
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:13:18.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8613" for this suite.
STEP: Destroying namespace "nspatchtest-ebe9a7d9-0204-450b-9da5-7da9abdd72ac-9264" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","total":291,"completed":109,"skipped":1928,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:13:18.877: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2677
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Sep  5 22:13:25.445: INFO: 10 pods remaining
Sep  5 22:13:25.445: INFO: 10 pods has nil DeletionTimestamp
Sep  5 22:13:25.445: INFO: 
Sep  5 22:13:26.461: INFO: 10 pods remaining
Sep  5 22:13:26.461: INFO: 10 pods has nil DeletionTimestamp
Sep  5 22:13:26.461: INFO: 
STEP: Gathering metrics
Sep  5 22:15:31.539: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:15:31.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2677" for this suite.

â€¢ [SLOW TEST:133.133 seconds]
[sig-api-machinery] Garbage collector
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","total":291,"completed":110,"skipped":1929,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:15:32.010: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8864
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 22:15:34.088: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  5 22:15:36.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:15:38.139: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:15:40.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:15:42.167: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:15:44.160: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:15:46.142: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:15:48.135: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:15:50.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:15:52.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:15:54.136: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:15:56.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:15:58.128: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 15, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 22:16:01.186: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 22:16:01.200: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5526-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:16:02.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8864" for this suite.
STEP: Destroying namespace "webhook-8864-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:33.039 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","total":291,"completed":111,"skipped":1929,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
S
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:16:05.049: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9635
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 22:16:05.509: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml create -f - --namespace=kubectl-9635'
Sep  5 22:16:06.728: INFO: stderr: ""
Sep  5 22:16:06.728: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Sep  5 22:16:06.728: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml create -f - --namespace=kubectl-9635'
Sep  5 22:16:07.381: INFO: stderr: ""
Sep  5 22:16:07.381: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep  5 22:16:08.392: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:08.392: INFO: Found 0 / 1
Sep  5 22:16:09.432: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:09.433: INFO: Found 0 / 1
Sep  5 22:16:10.393: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:10.393: INFO: Found 0 / 1
Sep  5 22:16:11.415: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:11.415: INFO: Found 0 / 1
Sep  5 22:16:12.401: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:12.401: INFO: Found 0 / 1
Sep  5 22:16:13.392: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:13.393: INFO: Found 0 / 1
Sep  5 22:16:14.399: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:14.399: INFO: Found 0 / 1
Sep  5 22:16:15.394: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:15.394: INFO: Found 0 / 1
Sep  5 22:16:16.390: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:16.390: INFO: Found 0 / 1
Sep  5 22:16:17.410: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:17.410: INFO: Found 0 / 1
Sep  5 22:16:18.397: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:18.397: INFO: Found 0 / 1
Sep  5 22:16:19.420: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:19.420: INFO: Found 0 / 1
Sep  5 22:16:20.389: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:20.389: INFO: Found 0 / 1
Sep  5 22:16:21.392: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:21.392: INFO: Found 0 / 1
Sep  5 22:16:22.388: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:22.388: INFO: Found 0 / 1
Sep  5 22:16:23.390: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:23.390: INFO: Found 0 / 1
Sep  5 22:16:24.389: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:24.389: INFO: Found 0 / 1
Sep  5 22:16:25.392: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:25.392: INFO: Found 0 / 1
Sep  5 22:16:26.392: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:26.392: INFO: Found 0 / 1
Sep  5 22:16:27.393: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:27.393: INFO: Found 0 / 1
Sep  5 22:16:28.389: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:28.389: INFO: Found 0 / 1
Sep  5 22:16:29.390: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:29.390: INFO: Found 0 / 1
Sep  5 22:16:30.406: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:30.406: INFO: Found 1 / 1
Sep  5 22:16:30.406: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep  5 22:16:30.423: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 22:16:30.423: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  5 22:16:30.423: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml describe pod agnhost-primary-qdvwl --namespace=kubectl-9635'
Sep  5 22:16:30.608: INFO: stderr: ""
Sep  5 22:16:30.608: INFO: stdout: "Name:         agnhost-primary-qdvwl\nNamespace:    kubectl-9635\nPriority:     0\nNode:         sc2-rdops-vm09-dhcp-43-208.eng.vmware.com/10.193.43.208\nStart Time:   Sun, 05 Sep 2021 22:16:27 -0700\nLabels:       app=agnhost\n              role=primary\nAnnotations:  attachment_id: 126c2ee3-0390-4c2b-b08e-ad6dff746344\n              kubernetes.io/psp: e2e-test-privileged-psp\n              mac: 04:50:56:00:60:0b\n              vlan: None\n              vmware-system-ephemeral-disk-uuid: 6000C298-48dd-4408-90d7-7d26464deadd\n              vmware-system-image-references: {\"agnhost-primary\":\"agnhost-e4bbf9f419e02f0332193ee525b99e9c3c927d53-v31706\"}\n              vmware-system-vm-moid: vm-792:7badc608-dd2f-42a9-952d-0d4c30d5f283\n              vmware-system-vm-uuid: 5017da77-8bd6-12e1-a411-7ba5bdb0381d\nStatus:       Running\nIP:           172.26.1.226\nIPs:\n  IP:           172.26.1.226\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   dff0bfdd-e552-465b-bc8a-22a460b74b87\n    Image:          k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Image ID:       agnhost-e4bbf9f419e02f0332193ee525b99e9c3c927d53-v31706\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sun, 05 Sep 2021 22:16:27 -0700\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-swqsd (ro)\nConditions:\n  Type              Status\n  PodScheduled      True \n  ContainersReady   True \n  Ready             True \n  Initialized       True \nVolumes:\n  default-token-swqsd:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-swqsd\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason                        Age   From               Message\n  ----    ------                        ----  ----               -------\n  Normal  Scheduled                     23s   default-scheduler  Successfully assigned kubectl-9635/agnhost-primary-qdvwl to sc2-rdops-vm09-dhcp-43-208.eng.vmware.com\n  Normal  Image                         23s   image-controller   Image agnhost-e4bbf9f419e02f0332193ee525b99e9c3c927d53-v31706 bound successfully\n  Normal  SuccessfulRealizeNSXResource  16s   nsx-container-ncp  Successfully realized NSX resource for Pod\n  Normal  Pulling                       7s    kubelet            Waiting for Image kubectl-9635/agnhost-e4bbf9f419e02f0332193ee525b99e9c3c927d53-v31706\n  Normal  Pulled                        7s    kubelet            Image kubectl-9635/agnhost-e4bbf9f419e02f0332193ee525b99e9c3c927d53-v31706 is ready\n  Normal  SuccessfulMountVolume         1s    kubelet            Successfully mounted volume default-token-swqsd\n  Normal  Created                       1s    kubelet            Created container agnhost-primary\n  Normal  Started                       1s    kubelet            Started container agnhost-primary\n"
Sep  5 22:16:30.608: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml describe rc agnhost-primary --namespace=kubectl-9635'
Sep  5 22:16:30.746: INFO: stderr: ""
Sep  5 22:16:30.746: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9635\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        k8s.gcr.io/e2e-test-images/agnhost:2.20\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  24s   replication-controller  Created pod: agnhost-primary-qdvwl\n"
Sep  5 22:16:30.747: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml describe service agnhost-primary --namespace=kubectl-9635'
Sep  5 22:16:30.862: INFO: stderr: ""
Sep  5 22:16:30.863: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9635\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Families:       <none>\nIP:                172.24.153.28\nIPs:               <none>\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.26.1.226:6379\nSession Affinity:  None\nEvents:\n  Type    Reason                        Age   From               Message\n  ----    ------                        ----  ----               -------\n  Normal  SuccessfulRealizeNSXResource  22s   nsx-container-ncp  Successful to update NSX resource for DLB Service\n"
Sep  5 22:16:30.872: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml describe node 42174da95d8c532b15b7283e9031a350'
Sep  5 22:16:31.319: INFO: stderr: ""
Sep  5 22:16:31.319: INFO: stdout: "Name:               42174da95d8c532b15b7283e9031a350\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=42174da95d8c532b15b7283e9031a350\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    vmware-system-workload-ip-configured: \n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sun, 05 Sep 2021 19:32:14 -0700\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  42174da95d8c532b15b7283e9031a350\n  AcquireTime:     <unset>\n  RenewTime:       Sun, 05 Sep 2021 22:16:27 -0700\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sun, 05 Sep 2021 22:16:16 -0700   Sun, 05 Sep 2021 19:32:09 -0700   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sun, 05 Sep 2021 22:16:16 -0700   Sun, 05 Sep 2021 19:32:09 -0700   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sun, 05 Sep 2021 22:16:16 -0700   Sun, 05 Sep 2021 19:32:09 -0700   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sun, 05 Sep 2021 22:16:16 -0700   Sun, 05 Sep 2021 19:41:02 -0700   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  172.26.0.2\n  Hostname:    42174da95d8c532b15b7283e9031a350\nCapacity:\n  cpu:                2\n  ephemeral-storage:  32881404Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8136972Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  32881404Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8136972Ki\n  pods:               110\nSystem Info:\n  Machine ID:                                f229f6663c3f4882864365c2b5551463\n  System UUID:                               a94d1742-8c5d-2b53-15b7-283e9031a350\n  Boot ID:                                   0ebd5374-2dbd-462c-9f09-004c258259ab\n  Kernel Version:                            4.19.198-1.ph3-esx\n  OS Image:                                  VMware Photon OS/Linux\n  Operating System:                          linux\n  Architecture:                              amd64\n  Container Runtime Version:                 containerd://1.4.4\n  Kubelet Version:                           v1.19.12+vmware.wcp.1\n  Kube-Proxy Version:                        v1.19.12+vmware.wcp.1\nNon-terminated Pods:                         (35 in total)\n  Namespace                                  Name                                                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                                  ----                                                               ------------  ----------  ---------------  -------------  ---\n  kube-system                                coredns-f5549f4bd-z75xc                                            100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     163m\n  kube-system                                docker-registry-42174da95d8c532b15b7283e9031a350                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         163m\n  kube-system                                etcd-42174da95d8c532b15b7283e9031a350                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         162m\n  kube-system                                kube-apiserver-42174da95d8c532b15b7283e9031a350                    250m (12%)    0 (0%)      0 (0%)           0 (0%)         155m\n  kube-system                                kube-controller-manager-42174da95d8c532b15b7283e9031a350           200m (10%)    0 (0%)      0 (0%)           0 (0%)         163m\n  kube-system                                kube-proxy-h7qwz                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         163m\n  kube-system                                kube-scheduler-42174da95d8c532b15b7283e9031a350                    100m (5%)     0 (0%)      0 (0%)           0 (0%)         163m\n  kube-system                                kubectl-plugin-vsphere-42174da95d8c532b15b7283e9031a350            0 (0%)        0 (0%)      0 (0%)           0 (0%)         163m\n  kube-system                                wcp-authproxy-42174da95d8c532b15b7283e9031a350                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         157m\n  kube-system                                wcp-fip-42174da95d8c532b15b7283e9031a350                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         163m\n  vmware-system-appplatform-operator-system  vmware-system-appplatform-operator-mgr-0                           100m (5%)     800m (40%)  20Mi (0%)        300Mi (3%)     163m\n  vmware-system-appplatform-operator-system  vmware-system-psp-operator-mgr-7888d487d7-lpn42                    100m (5%)     800m (40%)  20Mi (0%)        1536Mi (19%)   159m\n  vmware-system-capw                         capi-controller-manager-d586f4c8-6lfgm                             0 (0%)        0 (0%)      150Mi (1%)       1800Mi (22%)   159m\n  vmware-system-capw                         capi-kubeadm-bootstrap-controller-manager-5f5774d559-nsw45         0 (0%)        0 (0%)      150Mi (1%)       1200Mi (15%)   159m\n  vmware-system-capw                         capi-kubeadm-bootstrap-webhook-6766c687f9-mnx8s                    0 (0%)        0 (0%)      150Mi (1%)       1200Mi (15%)   159m\n  vmware-system-capw                         capi-kubeadm-control-plane-controller-manager-7c8d4b456f-lpq8r     0 (0%)        0 (0%)      150Mi (1%)       1200Mi (15%)   159m\n  vmware-system-capw                         capi-kubeadm-control-plane-webhook-5f4c87b8b9-tjwwb                0 (0%)        0 (0%)      150Mi (1%)       1200Mi (15%)   159m\n  vmware-system-capw                         capi-webhook-69769f4c68-wkf2l                                      0 (0%)        0 (0%)      150Mi (1%)       1200Mi (15%)   159m\n  vmware-system-capw                         capw-controller-manager-85b7cbb4bf-qkcm5                           10m (0%)      100m (5%)   150Mi (1%)       800Mi (10%)    159m\n  vmware-system-capw                         capw-webhook-58b86fb8b-x4w4j                                       10m (0%)      100m (5%)   150Mi (1%)       800Mi (10%)    159m\n  vmware-system-cert-manager                 cert-manager-799b5bbfdf-p4z4n                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         163m\n  vmware-system-cert-manager                 cert-manager-cainjector-69c886766f-6p68t                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         163m\n  vmware-system-cert-manager                 cert-manager-webhook-74488f47f-gbggg                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         162m\n  vmware-system-csi                          vsphere-csi-controller-7ff5f98858-57hcg                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         158m\n  vmware-system-kubeimage                    image-controller-597bd95bc9-bnsfj                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         163m\n  vmware-system-license-operator             vmware-system-license-operator-controller-manager-8cd89d68xw886    100m (5%)     150m (7%)   20Mi (0%)        300Mi (3%)     158m\n  vmware-system-logging                      fluentbit-775kx                                                    100m (5%)     500m (25%)  0 (0%)           0 (0%)         163m\n  vmware-system-nsop                         vmware-system-nsop-controller-manager-6fcd64f5f8-sxrhx             20m (1%)      100m (5%)   20Mi (0%)        300Mi (3%)     157m\n  vmware-system-nsx                          nsx-ncp-6d7f7bf559-9mqvr                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         163m\n  vmware-system-tkg                          masterproxy-tkgs-plugin-tbvb9                                      10m (0%)      250m (12%)  75Mi (0%)        200Mi (2%)     157m\n  vmware-system-tkg                          tkgs-plugin-server-57df5fcfbf-s6x9p                                100m (5%)     800m (40%)  256Mi (3%)       512Mi (6%)     158m\n  vmware-system-tkg                          vmware-system-tkg-controller-manager-575d95fb57-kpc65              110m (5%)     400m (20%)  120Mi (1%)       460Mi (5%)     158m\n  vmware-system-tkg                          vmware-system-tkg-webhook-64fd4868cb-mfbg8                         20m (1%)      400m (20%)  90Mi (1%)        120Mi (1%)     158m\n  vmware-system-ucs                          upgrade-compatibility-service-6849c664fd-6zgh2                     100m (5%)     1 (50%)     20Mi (0%)        600Mi (7%)     163m\n  vmware-system-vmop                         vmware-system-vmop-controller-manager-6649dd65b-hpvsz              100m (5%)     250m (12%)  75Mi (0%)        400Mi (5%)     158m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests      Limits\n  --------           --------      ------\n  cpu                1530m (76%)   5650m (282%)\n  memory             1986Mi (24%)  14298Mi (179%)\n  ephemeral-storage  0 (0%)        0 (0%)\n  hugepages-1Gi      0 (0%)        0 (0%)\n  hugepages-2Mi      0 (0%)        0 (0%)\nEvents:              <none>\n"
Sep  5 22:16:31.319: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml describe namespace kubectl-9635'
Sep  5 22:16:31.448: INFO: stderr: ""
Sep  5 22:16:31.448: INFO: stdout: "Name:         kubectl-9635\nLabels:       e2e-framework=kubectl\n              e2e-run=7fae1b16-c36f-46f4-8750-b3f5dd961598\n              vSphereClusterID=domain-c9\nAnnotations:  ls_id-0: a22e78e0-f8b3-4335-a517-b1718fd935fb\n              ncp/extpoolid: domain-c9:7badc608-dd2f-42a9-952d-0d4c30d5f283-ippool-192-168-124-1-192-168-124-254\n              ncp/router_id: t1_be87758b-2dcb-4a56-991d-6bbd1120a83f_rtr\n              ncp/snat_ip: 192.168.124.31\n              ncp/subnet-0: 172.26.1.224/28\n              vmware-system-namespace-owner-count: 1\n              vmware-system-resource-pool: resgroup-790\n              vmware-system-resource-pool-cpu-limit: 20.8850\n              vmware-system-resource-pool-memory-limit: 5000Mi\n              vmware-system-self-service-namespace: true\n              vmware-system-vm-folder: group-v791\nStatus:       Active\n\nResource Quotas\n  Name:                                                                   kubectl-9635\n  Resource                                                                Used  Hard\n  --------                                                                ---   ---\n  requests.storage                                                        0     10000Mi\n  Name:                                                                   kubectl-9635-storagequota\n  Resource                                                                Used  Hard\n  --------                                                                ---   ---\n  wcpglobal-storage-profile.storageclass.storage.k8s.io/requests.storage  0     9223372036854775807\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:16:31.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9635" for this suite.

â€¢ [SLOW TEST:26.589 seconds]
[sig-cli] Kubectl client
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","total":291,"completed":112,"skipped":1930,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:16:31.638: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-4190
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override arguments
Sep  5 22:16:32.038: INFO: Waiting up to 5m0s for pod "client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc" in namespace "containers-4190" to be "Succeeded or Failed"
Sep  5 22:16:32.045: INFO: Pod "client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.997769ms
Sep  5 22:16:34.057: INFO: Pod "client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018650209s
Sep  5 22:16:36.067: INFO: Pod "client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028903393s
Sep  5 22:16:38.109: INFO: Pod "client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.071128497s
Sep  5 22:16:40.118: INFO: Pod "client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc": Phase="Pending", Reason="", readiness=false. Elapsed: 8.079634036s
Sep  5 22:16:42.124: INFO: Pod "client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.086499998s
Sep  5 22:16:44.131: INFO: Pod "client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc": Phase="Pending", Reason="", readiness=false. Elapsed: 12.093487129s
Sep  5 22:16:46.141: INFO: Pod "client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc": Phase="Pending", Reason="", readiness=false. Elapsed: 14.103201296s
Sep  5 22:16:48.151: INFO: Pod "client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc": Phase="Pending", Reason="", readiness=false. Elapsed: 16.112953481s
Sep  5 22:16:50.165: INFO: Pod "client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc": Phase="Pending", Reason="", readiness=false. Elapsed: 18.126649515s
Sep  5 22:16:52.172: INFO: Pod "client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc": Phase="Pending", Reason="", readiness=false. Elapsed: 20.133714984s
Sep  5 22:16:54.179: INFO: Pod "client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc": Phase="Pending", Reason="", readiness=false. Elapsed: 22.141242414s
Sep  5 22:16:56.188: INFO: Pod "client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.150512195s
STEP: Saw pod success
Sep  5 22:16:56.189: INFO: Pod "client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc" satisfied condition "Succeeded or Failed"
Sep  5 22:16:56.196: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc container test-container: <nil>
STEP: delete the pod
Sep  5 22:17:00.447: INFO: Waiting for pod client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc to disappear
Sep  5 22:17:00.470: INFO: Pod client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc still exists
Sep  5 22:17:02.472: INFO: Waiting for pod client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc to disappear
Sep  5 22:17:02.497: INFO: Pod client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc still exists
Sep  5 22:17:04.472: INFO: Waiting for pod client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc to disappear
Sep  5 22:17:04.481: INFO: Pod client-containers-8a642fa3-c312-4e07-b27e-fe198aea93bc no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:17:04.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4190" for this suite.

â€¢ [SLOW TEST:33.073 seconds]
[k8s.io] Docker Containers
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]","total":291,"completed":113,"skipped":1943,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:17:04.712: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3738
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  5 22:17:30.650: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:17:30.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3738" for this suite.

â€¢ [SLOW TEST:26.163 seconds]
[k8s.io] Container Runtime
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","total":291,"completed":114,"skipped":1953,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:17:30.875: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2346
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-secret-97cj
STEP: Creating a pod to test atomic-volume-subpath
Sep  5 22:17:31.279: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-97cj" in namespace "subpath-2346" to be "Succeeded or Failed"
Sep  5 22:17:31.287: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Pending", Reason="", readiness=false. Elapsed: 7.475765ms
Sep  5 22:17:33.296: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017303897s
Sep  5 22:17:35.306: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026393319s
Sep  5 22:17:37.320: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040465121s
Sep  5 22:17:39.331: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.051440843s
Sep  5 22:17:41.336: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Pending", Reason="", readiness=false. Elapsed: 10.056769105s
Sep  5 22:17:43.343: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Pending", Reason="", readiness=false. Elapsed: 12.063906562s
Sep  5 22:17:45.352: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Pending", Reason="", readiness=false. Elapsed: 14.072929509s
Sep  5 22:17:48.635: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Pending", Reason="", readiness=false. Elapsed: 17.355843413s
Sep  5 22:17:50.646: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Pending", Reason="", readiness=false. Elapsed: 19.366673641s
Sep  5 22:17:52.655: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Pending", Reason="", readiness=false. Elapsed: 21.375981155s
Sep  5 22:17:54.664: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Pending", Reason="", readiness=false. Elapsed: 23.385152245s
Sep  5 22:17:56.674: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Running", Reason="", readiness=true. Elapsed: 25.394367056s
Sep  5 22:17:58.684: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Running", Reason="", readiness=true. Elapsed: 27.404323274s
Sep  5 22:18:00.694: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Running", Reason="", readiness=true. Elapsed: 29.415265101s
Sep  5 22:18:02.706: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Running", Reason="", readiness=true. Elapsed: 31.426533791s
Sep  5 22:18:04.715: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Running", Reason="", readiness=true. Elapsed: 33.435762309s
Sep  5 22:18:06.722: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Running", Reason="", readiness=true. Elapsed: 35.442805404s
Sep  5 22:18:08.736: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Running", Reason="", readiness=true. Elapsed: 37.456585231s
Sep  5 22:18:10.750: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Running", Reason="", readiness=true. Elapsed: 39.470713406s
Sep  5 22:18:12.765: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Running", Reason="", readiness=true. Elapsed: 41.48577622s
Sep  5 22:18:14.772: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Running", Reason="", readiness=true. Elapsed: 43.493008867s
Sep  5 22:18:16.786: INFO: Pod "pod-subpath-test-secret-97cj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 45.506334032s
STEP: Saw pod success
Sep  5 22:18:16.786: INFO: Pod "pod-subpath-test-secret-97cj" satisfied condition "Succeeded or Failed"
Sep  5 22:18:16.798: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-subpath-test-secret-97cj container test-container-subpath-secret-97cj: <nil>
STEP: delete the pod
Sep  5 22:18:16.855: INFO: Waiting for pod pod-subpath-test-secret-97cj to disappear
Sep  5 22:18:16.866: INFO: Pod pod-subpath-test-secret-97cj still exists
Sep  5 22:18:18.867: INFO: Waiting for pod pod-subpath-test-secret-97cj to disappear
Sep  5 22:18:18.875: INFO: Pod pod-subpath-test-secret-97cj still exists
Sep  5 22:18:20.867: INFO: Waiting for pod pod-subpath-test-secret-97cj to disappear
Sep  5 22:18:20.894: INFO: Pod pod-subpath-test-secret-97cj still exists
Sep  5 22:18:22.866: INFO: Waiting for pod pod-subpath-test-secret-97cj to disappear
Sep  5 22:18:22.873: INFO: Pod pod-subpath-test-secret-97cj still exists
Sep  5 22:18:24.867: INFO: Waiting for pod pod-subpath-test-secret-97cj to disappear
Sep  5 22:18:24.876: INFO: Pod pod-subpath-test-secret-97cj still exists
Sep  5 22:18:26.867: INFO: Waiting for pod pod-subpath-test-secret-97cj to disappear
Sep  5 22:18:26.874: INFO: Pod pod-subpath-test-secret-97cj still exists
Sep  5 22:18:28.867: INFO: Waiting for pod pod-subpath-test-secret-97cj to disappear
Sep  5 22:18:28.875: INFO: Pod pod-subpath-test-secret-97cj still exists
Sep  5 22:18:30.866: INFO: Waiting for pod pod-subpath-test-secret-97cj to disappear
Sep  5 22:18:30.880: INFO: Pod pod-subpath-test-secret-97cj still exists
Sep  5 22:18:32.866: INFO: Waiting for pod pod-subpath-test-secret-97cj to disappear
Sep  5 22:18:32.872: INFO: Pod pod-subpath-test-secret-97cj still exists
Sep  5 22:18:34.867: INFO: Waiting for pod pod-subpath-test-secret-97cj to disappear
Sep  5 22:18:34.887: INFO: Pod pod-subpath-test-secret-97cj still exists
Sep  5 22:18:36.866: INFO: Waiting for pod pod-subpath-test-secret-97cj to disappear
Sep  5 22:18:36.874: INFO: Pod pod-subpath-test-secret-97cj no longer exists
STEP: Deleting pod pod-subpath-test-secret-97cj
Sep  5 22:18:36.874: INFO: Deleting pod "pod-subpath-test-secret-97cj" in namespace "subpath-2346"
[AfterEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:18:36.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2346" for this suite.

â€¢ [SLOW TEST:66.200 seconds]
[sig-storage] Subpath
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]","total":291,"completed":115,"skipped":1963,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:18:37.075: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8783
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-d0b4b3d8-15b4-4a34-acda-6de2e3e05c98
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-d0b4b3d8-15b4-4a34-acda-6de2e3e05c98
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:19:10.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8783" for this suite.

â€¢ [SLOW TEST:33.333 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","total":291,"completed":116,"skipped":1964,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:19:10.409: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8425
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep  5 22:19:10.963: INFO: Waiting up to 5m0s for pod "pod-1709844e-4030-487f-b7d2-9588122224e0" in namespace "emptydir-8425" to be "Succeeded or Failed"
Sep  5 22:19:10.986: INFO: Pod "pod-1709844e-4030-487f-b7d2-9588122224e0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.68376ms
Sep  5 22:19:13.000: INFO: Pod "pod-1709844e-4030-487f-b7d2-9588122224e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036591542s
Sep  5 22:19:15.024: INFO: Pod "pod-1709844e-4030-487f-b7d2-9588122224e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060705582s
Sep  5 22:19:17.101: INFO: Pod "pod-1709844e-4030-487f-b7d2-9588122224e0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.137903929s
Sep  5 22:19:19.126: INFO: Pod "pod-1709844e-4030-487f-b7d2-9588122224e0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.162287929s
Sep  5 22:19:21.139: INFO: Pod "pod-1709844e-4030-487f-b7d2-9588122224e0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.175149405s
Sep  5 22:19:23.148: INFO: Pod "pod-1709844e-4030-487f-b7d2-9588122224e0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.18419656s
Sep  5 22:19:25.187: INFO: Pod "pod-1709844e-4030-487f-b7d2-9588122224e0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.22332776s
Sep  5 22:19:27.201: INFO: Pod "pod-1709844e-4030-487f-b7d2-9588122224e0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.237177425s
Sep  5 22:19:29.207: INFO: Pod "pod-1709844e-4030-487f-b7d2-9588122224e0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.244104413s
Sep  5 22:19:31.216: INFO: Pod "pod-1709844e-4030-487f-b7d2-9588122224e0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.253003604s
Sep  5 22:19:33.227: INFO: Pod "pod-1709844e-4030-487f-b7d2-9588122224e0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.263408064s
Sep  5 22:19:35.235: INFO: Pod "pod-1709844e-4030-487f-b7d2-9588122224e0": Phase="Pending", Reason="", readiness=false. Elapsed: 24.271156906s
Sep  5 22:19:37.241: INFO: Pod "pod-1709844e-4030-487f-b7d2-9588122224e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.277778466s
STEP: Saw pod success
Sep  5 22:19:37.241: INFO: Pod "pod-1709844e-4030-487f-b7d2-9588122224e0" satisfied condition "Succeeded or Failed"
Sep  5 22:19:37.248: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-1709844e-4030-487f-b7d2-9588122224e0 container test-container: <nil>
STEP: delete the pod
Sep  5 22:19:41.503: INFO: Waiting for pod pod-1709844e-4030-487f-b7d2-9588122224e0 to disappear
Sep  5 22:19:41.524: INFO: Pod pod-1709844e-4030-487f-b7d2-9588122224e0 still exists
Sep  5 22:19:43.525: INFO: Waiting for pod pod-1709844e-4030-487f-b7d2-9588122224e0 to disappear
Sep  5 22:19:43.534: INFO: Pod pod-1709844e-4030-487f-b7d2-9588122224e0 still exists
Sep  5 22:19:45.524: INFO: Waiting for pod pod-1709844e-4030-487f-b7d2-9588122224e0 to disappear
Sep  5 22:19:45.533: INFO: Pod pod-1709844e-4030-487f-b7d2-9588122224e0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:19:45.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8425" for this suite.

â€¢ [SLOW TEST:35.335 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":117,"skipped":1984,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:19:45.744: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8574
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep  5 22:19:46.275: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8574 /api/v1/namespaces/watch-8574/configmaps/e2e-watch-test-watch-closed 9ff08c5a-616b-4f0c-aef4-2053dc56b23f 124847 0 2021-09-05 22:19:46 -0700 PDT <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-05 22:19:46 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  5 22:19:46.275: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8574 /api/v1/namespaces/watch-8574/configmaps/e2e-watch-test-watch-closed 9ff08c5a-616b-4f0c-aef4-2053dc56b23f 124848 0 2021-09-05 22:19:46 -0700 PDT <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-05 22:19:46 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep  5 22:19:46.373: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8574 /api/v1/namespaces/watch-8574/configmaps/e2e-watch-test-watch-closed 9ff08c5a-616b-4f0c-aef4-2053dc56b23f 124849 0 2021-09-05 22:19:46 -0700 PDT <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-05 22:19:46 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  5 22:19:46.373: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-8574 /api/v1/namespaces/watch-8574/configmaps/e2e-watch-test-watch-closed 9ff08c5a-616b-4f0c-aef4-2053dc56b23f 124852 0 2021-09-05 22:19:46 -0700 PDT <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  [{e2e.test Update v1 2021-09-05 22:19:46 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:19:46.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8574" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","total":291,"completed":118,"skipped":2035,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:19:46.586: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1514
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  5 22:20:27.187: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 22:20:27.194: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 22:20:29.194: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 22:20:29.207: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 22:20:31.194: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 22:20:31.201: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 22:20:33.194: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 22:20:33.201: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 22:20:35.195: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 22:20:35.202: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 22:20:37.194: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 22:20:37.205: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 22:20:39.195: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 22:20:39.203: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 22:20:41.195: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 22:20:41.209: INFO: Pod pod-with-prestop-exec-hook still exists
Sep  5 22:20:43.194: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep  5 22:20:43.204: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:20:43.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1514" for this suite.

â€¢ [SLOW TEST:56.897 seconds]
[k8s.io] Container Lifecycle Hook
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","total":291,"completed":119,"skipped":2039,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:20:43.483: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-2029
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3773
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6158
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:21:44.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2029" for this suite.
STEP: Destroying namespace "nsdeletetest-3773" for this suite.
Sep  5 22:21:45.196: INFO: Namespace nsdeletetest-3773 was already deleted
STEP: Destroying namespace "nsdeletetest-6158" for this suite.

â€¢ [SLOW TEST:61.894 seconds]
[sig-api-machinery] Namespaces [Serial]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","total":291,"completed":120,"skipped":2054,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:21:45.378: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3613
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  5 22:21:45.935: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955" in namespace "projected-3613" to be "Succeeded or Failed"
Sep  5 22:21:45.999: INFO: Pod "downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955": Phase="Pending", Reason="", readiness=false. Elapsed: 63.905753ms
Sep  5 22:21:48.007: INFO: Pod "downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955": Phase="Pending", Reason="", readiness=false. Elapsed: 2.071647022s
Sep  5 22:21:50.015: INFO: Pod "downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955": Phase="Pending", Reason="", readiness=false. Elapsed: 4.079946781s
Sep  5 22:21:52.024: INFO: Pod "downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955": Phase="Pending", Reason="", readiness=false. Elapsed: 6.089129671s
Sep  5 22:21:54.032: INFO: Pod "downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955": Phase="Pending", Reason="", readiness=false. Elapsed: 8.097071779s
Sep  5 22:21:56.049: INFO: Pod "downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955": Phase="Pending", Reason="", readiness=false. Elapsed: 10.114355525s
Sep  5 22:21:58.058: INFO: Pod "downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955": Phase="Pending", Reason="", readiness=false. Elapsed: 12.123409016s
Sep  5 22:22:00.098: INFO: Pod "downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955": Phase="Pending", Reason="", readiness=false. Elapsed: 14.162611509s
Sep  5 22:22:02.108: INFO: Pod "downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955": Phase="Pending", Reason="", readiness=false. Elapsed: 16.172802329s
Sep  5 22:22:04.117: INFO: Pod "downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955": Phase="Pending", Reason="", readiness=false. Elapsed: 18.182256864s
Sep  5 22:22:06.130: INFO: Pod "downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955": Phase="Running", Reason="", readiness=true. Elapsed: 20.195141147s
Sep  5 22:22:08.161: INFO: Pod "downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955": Phase="Running", Reason="", readiness=true. Elapsed: 22.22624562s
Sep  5 22:22:10.186: INFO: Pod "downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955": Phase="Running", Reason="", readiness=true. Elapsed: 24.251295353s
Sep  5 22:22:12.205: INFO: Pod "downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.269591022s
STEP: Saw pod success
Sep  5 22:22:12.205: INFO: Pod "downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955" satisfied condition "Succeeded or Failed"
Sep  5 22:22:12.213: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955 container client-container: <nil>
STEP: delete the pod
Sep  5 22:22:12.277: INFO: Waiting for pod downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955 to disappear
Sep  5 22:22:12.289: INFO: Pod downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955 still exists
Sep  5 22:22:14.290: INFO: Waiting for pod downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955 to disappear
Sep  5 22:22:14.298: INFO: Pod downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955 still exists
Sep  5 22:22:16.290: INFO: Waiting for pod downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955 to disappear
Sep  5 22:22:16.296: INFO: Pod downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955 still exists
Sep  5 22:22:18.290: INFO: Waiting for pod downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955 to disappear
Sep  5 22:22:18.297: INFO: Pod downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955 still exists
Sep  5 22:22:20.289: INFO: Waiting for pod downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955 to disappear
Sep  5 22:22:20.302: INFO: Pod downwardapi-volume-1529a6ef-7ded-4b51-ba6c-91aec8f35955 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:22:20.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3613" for this suite.

â€¢ [SLOW TEST:35.163 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":121,"skipped":2062,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:22:20.541: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7973
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl logs
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1415
STEP: creating an pod
Sep  5 22:22:20.928: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml run logs-generator --image=k8s.gcr.io/e2e-test-images/agnhost:2.20 --namespace=kubectl-7973 --restart=Never -- logs-generator --log-lines-total 100 --run-duration 20s'
Sep  5 22:22:21.060: INFO: stderr: ""
Sep  5 22:22:21.060: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Waiting for log generator to start.
Sep  5 22:22:21.064: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Sep  5 22:22:21.064: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7973" to be "running and ready, or succeeded"
Sep  5 22:22:21.075: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.823636ms
Sep  5 22:22:23.096: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031575365s
Sep  5 22:22:25.106: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041853034s
Sep  5 22:22:27.114: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049705819s
Sep  5 22:22:29.125: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 8.061363233s
Sep  5 22:22:31.146: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 10.081509623s
Sep  5 22:22:33.157: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 12.092736048s
Sep  5 22:22:35.174: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 14.109559271s
Sep  5 22:22:37.180: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 16.115747937s
Sep  5 22:22:39.189: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 18.124660413s
Sep  5 22:22:41.197: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 20.132652097s
Sep  5 22:22:43.207: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 22.143095563s
Sep  5 22:22:45.215: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 24.151216587s
Sep  5 22:22:47.227: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 26.162605871s
Sep  5 22:22:47.227: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Sep  5 22:22:47.227: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Sep  5 22:22:47.227: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml logs logs-generator logs-generator --namespace=kubectl-7973'
Sep  5 22:22:47.400: INFO: stderr: ""
Sep  5 22:22:47.400: INFO: stdout: "I0906 05:22:42.776781       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/r4js 501\nI0906 05:22:42.969267       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/7kgs 479\nI0906 05:22:43.169254       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/fmn 374\nI0906 05:22:43.369058       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/mjs8 210\nI0906 05:22:43.569140       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/t7j 526\nI0906 05:22:43.769175       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/42d 265\nI0906 05:22:43.969056       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/k7n5 395\nI0906 05:22:44.169116       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/2brv 352\nI0906 05:22:44.369075       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/ph62 548\nI0906 05:22:44.568891       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/p4z 425\nI0906 05:22:44.769448       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/mbr 216\nI0906 05:22:44.969206       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/stc 537\nI0906 05:22:45.169038       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/2vjm 382\nI0906 05:22:45.369249       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/spxq 467\nI0906 05:22:45.569194       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/djcl 306\nI0906 05:22:45.769615       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/74lb 494\nI0906 05:22:45.969224       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/ln2b 233\nI0906 05:22:46.169099       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/pqd 511\nI0906 05:22:46.369051       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/dxq 259\nI0906 05:22:46.569083       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/8krk 560\nI0906 05:22:46.769073       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/9b4 325\nI0906 05:22:46.969218       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/lsq 287\nI0906 05:22:47.169257       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/wdr 500\nI0906 05:22:47.369049       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/nhc 405\nI0906 05:22:47.569152       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/prlf 279\n"
STEP: limiting log lines
Sep  5 22:22:47.400: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml logs logs-generator logs-generator --namespace=kubectl-7973 --tail=1'
Sep  5 22:22:47.565: INFO: stderr: ""
Sep  5 22:22:47.565: INFO: stdout: "I0906 05:22:47.769055       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/tbt 218\n"
Sep  5 22:22:47.565: INFO: got output "I0906 05:22:47.769055       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/tbt 218\n"
STEP: limiting log bytes
Sep  5 22:22:47.566: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml logs logs-generator logs-generator --namespace=kubectl-7973 --limit-bytes=1'
Sep  5 22:22:47.704: INFO: stderr: ""
Sep  5 22:22:47.704: INFO: stdout: "I"
Sep  5 22:22:47.704: INFO: got output "I"
STEP: exposing timestamps
Sep  5 22:22:47.704: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml logs logs-generator logs-generator --namespace=kubectl-7973 --tail=1 --timestamps'
Sep  5 22:22:47.857: INFO: stderr: ""
Sep  5 22:22:47.857: INFO: stdout: "2021-09-06T05:22:48.169740853Z I0906 05:22:48.169500       1 logs_generator.go:76] 27 GET /api/v1/namespaces/ns/pods/478 447\n"
Sep  5 22:22:47.857: INFO: got output "2021-09-06T05:22:48.169740853Z I0906 05:22:48.169500       1 logs_generator.go:76] 27 GET /api/v1/namespaces/ns/pods/478 447\n"
STEP: restricting to a time range
Sep  5 22:22:50.361: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml logs logs-generator logs-generator --namespace=kubectl-7973 --since=1s'
Sep  5 22:22:50.531: INFO: stderr: ""
Sep  5 22:22:50.532: INFO: stdout: "I0906 05:22:49.969236       1 logs_generator.go:76] 36 POST /api/v1/namespaces/kube-system/pods/b6sx 214\nI0906 05:22:50.169067       1 logs_generator.go:76] 37 POST /api/v1/namespaces/default/pods/ghw 343\nI0906 05:22:50.368989       1 logs_generator.go:76] 38 POST /api/v1/namespaces/ns/pods/6wh2 262\nI0906 05:22:50.568878       1 logs_generator.go:76] 39 POST /api/v1/namespaces/ns/pods/p24 436\nI0906 05:22:50.769272       1 logs_generator.go:76] 40 GET /api/v1/namespaces/default/pods/8r6 335\n"
Sep  5 22:22:50.532: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml logs logs-generator logs-generator --namespace=kubectl-7973 --since=24h'
Sep  5 22:22:50.706: INFO: stderr: ""
Sep  5 22:22:50.706: INFO: stdout: "I0906 05:22:42.776781       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/r4js 501\nI0906 05:22:42.969267       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/7kgs 479\nI0906 05:22:43.169254       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/fmn 374\nI0906 05:22:43.369058       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/kube-system/pods/mjs8 210\nI0906 05:22:43.569140       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/t7j 526\nI0906 05:22:43.769175       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/default/pods/42d 265\nI0906 05:22:43.969056       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/k7n5 395\nI0906 05:22:44.169116       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/2brv 352\nI0906 05:22:44.369075       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/ph62 548\nI0906 05:22:44.568891       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/p4z 425\nI0906 05:22:44.769448       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/mbr 216\nI0906 05:22:44.969206       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/stc 537\nI0906 05:22:45.169038       1 logs_generator.go:76] 12 POST /api/v1/namespaces/kube-system/pods/2vjm 382\nI0906 05:22:45.369249       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/spxq 467\nI0906 05:22:45.569194       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/djcl 306\nI0906 05:22:45.769615       1 logs_generator.go:76] 15 POST /api/v1/namespaces/default/pods/74lb 494\nI0906 05:22:45.969224       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/ln2b 233\nI0906 05:22:46.169099       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/pqd 511\nI0906 05:22:46.369051       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/dxq 259\nI0906 05:22:46.569083       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/8krk 560\nI0906 05:22:46.769073       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/9b4 325\nI0906 05:22:46.969218       1 logs_generator.go:76] 21 GET /api/v1/namespaces/ns/pods/lsq 287\nI0906 05:22:47.169257       1 logs_generator.go:76] 22 POST /api/v1/namespaces/default/pods/wdr 500\nI0906 05:22:47.369049       1 logs_generator.go:76] 23 POST /api/v1/namespaces/kube-system/pods/nhc 405\nI0906 05:22:47.569152       1 logs_generator.go:76] 24 GET /api/v1/namespaces/default/pods/prlf 279\nI0906 05:22:47.769055       1 logs_generator.go:76] 25 PUT /api/v1/namespaces/ns/pods/tbt 218\nI0906 05:22:47.969239       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/npg 599\nI0906 05:22:48.169500       1 logs_generator.go:76] 27 GET /api/v1/namespaces/ns/pods/478 447\nI0906 05:22:48.369495       1 logs_generator.go:76] 28 POST /api/v1/namespaces/default/pods/f5hs 291\nI0906 05:22:48.569048       1 logs_generator.go:76] 29 POST /api/v1/namespaces/kube-system/pods/qr8 402\nI0906 05:22:48.769061       1 logs_generator.go:76] 30 GET /api/v1/namespaces/default/pods/lfn6 359\nI0906 05:22:48.969077       1 logs_generator.go:76] 31 PUT /api/v1/namespaces/kube-system/pods/7hx 281\nI0906 05:22:49.169229       1 logs_generator.go:76] 32 GET /api/v1/namespaces/ns/pods/2ch 532\nI0906 05:22:49.369047       1 logs_generator.go:76] 33 POST /api/v1/namespaces/default/pods/bsf 514\nI0906 05:22:49.569143       1 logs_generator.go:76] 34 PUT /api/v1/namespaces/default/pods/nszj 251\nI0906 05:22:49.768884       1 logs_generator.go:76] 35 PUT /api/v1/namespaces/ns/pods/57w 340\nI0906 05:22:49.969236       1 logs_generator.go:76] 36 POST /api/v1/namespaces/kube-system/pods/b6sx 214\nI0906 05:22:50.169067       1 logs_generator.go:76] 37 POST /api/v1/namespaces/default/pods/ghw 343\nI0906 05:22:50.368989       1 logs_generator.go:76] 38 POST /api/v1/namespaces/ns/pods/6wh2 262\nI0906 05:22:50.568878       1 logs_generator.go:76] 39 POST /api/v1/namespaces/ns/pods/p24 436\nI0906 05:22:50.769272       1 logs_generator.go:76] 40 GET /api/v1/namespaces/default/pods/8r6 335\nI0906 05:22:50.969249       1 logs_generator.go:76] 41 POST /api/v1/namespaces/default/pods/q4pv 584\n"
[AfterEach] Kubectl logs
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
Sep  5 22:22:50.706: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml delete pod logs-generator --namespace=kubectl-7973'
Sep  5 22:23:06.329: INFO: stderr: ""
Sep  5 22:23:06.329: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:23:06.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7973" for this suite.

â€¢ [SLOW TEST:46.015 seconds]
[sig-cli] Kubectl client
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1411
    should be able to retrieve and filter logs  [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","total":291,"completed":122,"skipped":2084,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:23:06.557: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  5 22:23:07.043: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd" in namespace "projected-3647" to be "Succeeded or Failed"
Sep  5 22:23:07.068: INFO: Pod "downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd": Phase="Pending", Reason="", readiness=false. Elapsed: 24.182746ms
Sep  5 22:23:09.082: INFO: Pod "downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038722334s
Sep  5 22:23:11.100: INFO: Pod "downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056882753s
Sep  5 22:23:13.114: INFO: Pod "downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.070540678s
Sep  5 22:23:15.129: INFO: Pod "downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.085169338s
Sep  5 22:23:17.148: INFO: Pod "downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.104812358s
Sep  5 22:23:19.188: INFO: Pod "downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.144144739s
Sep  5 22:23:21.195: INFO: Pod "downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.151412661s
Sep  5 22:23:23.203: INFO: Pod "downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.159269412s
Sep  5 22:23:25.248: INFO: Pod "downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.205004828s
Sep  5 22:23:27.259: INFO: Pod "downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd": Phase="Pending", Reason="", readiness=false. Elapsed: 20.215408238s
Sep  5 22:23:29.266: INFO: Pod "downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd": Phase="Pending", Reason="", readiness=false. Elapsed: 22.222867064s
Sep  5 22:23:31.273: INFO: Pod "downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd": Phase="Pending", Reason="", readiness=false. Elapsed: 24.229354937s
Sep  5 22:23:33.284: INFO: Pod "downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.240337898s
STEP: Saw pod success
Sep  5 22:23:33.284: INFO: Pod "downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd" satisfied condition "Succeeded or Failed"
Sep  5 22:23:33.294: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd container client-container: <nil>
STEP: delete the pod
Sep  5 22:23:39.038: INFO: Waiting for pod downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd to disappear
Sep  5 22:23:39.052: INFO: Pod downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd still exists
Sep  5 22:23:41.053: INFO: Waiting for pod downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd to disappear
Sep  5 22:23:41.067: INFO: Pod downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd still exists
Sep  5 22:23:43.053: INFO: Waiting for pod downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd to disappear
Sep  5 22:23:43.065: INFO: Pod downwardapi-volume-e2a66f87-d105-4215-a525-8091b0d892dd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:23:43.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3647" for this suite.

â€¢ [SLOW TEST:36.718 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's cpu request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","total":291,"completed":123,"skipped":2095,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:23:43.275: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5152
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should create and stop a replication controller  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Sep  5 22:23:43.630: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml create -f - --namespace=kubectl-5152'
Sep  5 22:23:44.199: INFO: stderr: ""
Sep  5 22:23:44.199: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  5 22:23:44.199: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5152'
Sep  5 22:23:44.312: INFO: stderr: ""
Sep  5 22:23:44.312: INFO: stdout: "update-demo-nautilus-4thl6 update-demo-nautilus-j6g8l "
Sep  5 22:23:44.312: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-4thl6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5152'
Sep  5 22:23:44.611: INFO: stderr: ""
Sep  5 22:23:44.611: INFO: stdout: ""
Sep  5 22:23:44.611: INFO: update-demo-nautilus-4thl6 is created but not running
Sep  5 22:23:49.611: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5152'
Sep  5 22:23:49.705: INFO: stderr: ""
Sep  5 22:23:49.705: INFO: stdout: "update-demo-nautilus-4thl6 update-demo-nautilus-j6g8l "
Sep  5 22:23:49.706: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-4thl6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5152'
Sep  5 22:23:49.796: INFO: stderr: ""
Sep  5 22:23:49.796: INFO: stdout: ""
Sep  5 22:23:49.796: INFO: update-demo-nautilus-4thl6 is created but not running
Sep  5 22:23:54.796: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5152'
Sep  5 22:23:54.903: INFO: stderr: ""
Sep  5 22:23:54.903: INFO: stdout: "update-demo-nautilus-4thl6 update-demo-nautilus-j6g8l "
Sep  5 22:23:54.903: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-4thl6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5152'
Sep  5 22:23:54.998: INFO: stderr: ""
Sep  5 22:23:54.998: INFO: stdout: ""
Sep  5 22:23:54.998: INFO: update-demo-nautilus-4thl6 is created but not running
Sep  5 22:23:59.998: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5152'
Sep  5 22:24:00.097: INFO: stderr: ""
Sep  5 22:24:00.097: INFO: stdout: "update-demo-nautilus-4thl6 update-demo-nautilus-j6g8l "
Sep  5 22:24:00.097: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-4thl6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5152'
Sep  5 22:24:00.193: INFO: stderr: ""
Sep  5 22:24:00.193: INFO: stdout: ""
Sep  5 22:24:00.193: INFO: update-demo-nautilus-4thl6 is created but not running
Sep  5 22:24:05.196: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5152'
Sep  5 22:24:05.330: INFO: stderr: ""
Sep  5 22:24:05.330: INFO: stdout: "update-demo-nautilus-4thl6 update-demo-nautilus-j6g8l "
Sep  5 22:24:05.330: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-4thl6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5152'
Sep  5 22:24:05.431: INFO: stderr: ""
Sep  5 22:24:05.432: INFO: stdout: ""
Sep  5 22:24:05.432: INFO: update-demo-nautilus-4thl6 is created but not running
Sep  5 22:24:10.434: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5152'
Sep  5 22:24:10.550: INFO: stderr: ""
Sep  5 22:24:10.550: INFO: stdout: "update-demo-nautilus-4thl6 update-demo-nautilus-j6g8l "
Sep  5 22:24:10.550: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-4thl6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5152'
Sep  5 22:24:10.647: INFO: stderr: ""
Sep  5 22:24:10.647: INFO: stdout: ""
Sep  5 22:24:10.647: INFO: update-demo-nautilus-4thl6 is created but not running
Sep  5 22:24:15.648: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5152'
Sep  5 22:24:15.753: INFO: stderr: ""
Sep  5 22:24:15.754: INFO: stdout: "update-demo-nautilus-4thl6 update-demo-nautilus-j6g8l "
Sep  5 22:24:15.754: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-4thl6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5152'
Sep  5 22:24:15.859: INFO: stderr: ""
Sep  5 22:24:15.859: INFO: stdout: ""
Sep  5 22:24:15.859: INFO: update-demo-nautilus-4thl6 is created but not running
Sep  5 22:24:20.859: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5152'
Sep  5 22:24:20.970: INFO: stderr: ""
Sep  5 22:24:20.970: INFO: stdout: "update-demo-nautilus-4thl6 update-demo-nautilus-j6g8l "
Sep  5 22:24:20.970: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-4thl6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5152'
Sep  5 22:24:21.423: INFO: stderr: ""
Sep  5 22:24:21.423: INFO: stdout: "true"
Sep  5 22:24:21.423: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-4thl6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5152'
Sep  5 22:24:21.526: INFO: stderr: ""
Sep  5 22:24:21.526: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 22:24:21.526: INFO: validating pod update-demo-nautilus-4thl6
Sep  5 22:24:21.574: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 22:24:21.574: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 22:24:21.574: INFO: update-demo-nautilus-4thl6 is verified up and running
Sep  5 22:24:21.574: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-j6g8l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5152'
Sep  5 22:24:21.686: INFO: stderr: ""
Sep  5 22:24:21.686: INFO: stdout: "true"
Sep  5 22:24:21.686: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-j6g8l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5152'
Sep  5 22:24:21.786: INFO: stderr: ""
Sep  5 22:24:21.786: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  5 22:24:21.786: INFO: validating pod update-demo-nautilus-j6g8l
Sep  5 22:24:21.835: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  5 22:24:21.835: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  5 22:24:21.835: INFO: update-demo-nautilus-j6g8l is verified up and running
STEP: using delete to clean up resources
Sep  5 22:24:21.835: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml delete --grace-period=0 --force -f - --namespace=kubectl-5152'
Sep  5 22:24:21.952: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 22:24:21.952: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  5 22:24:21.952: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5152'
Sep  5 22:24:22.068: INFO: stderr: "No resources found in kubectl-5152 namespace.\n"
Sep  5 22:24:22.068: INFO: stdout: ""
Sep  5 22:24:22.068: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -l name=update-demo --namespace=kubectl-5152 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 22:24:22.172: INFO: stderr: ""
Sep  5 22:24:22.172: INFO: stdout: "update-demo-nautilus-4thl6\nupdate-demo-nautilus-j6g8l\n"
Sep  5 22:24:22.673: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5152'
Sep  5 22:24:22.797: INFO: stderr: "No resources found in kubectl-5152 namespace.\n"
Sep  5 22:24:22.797: INFO: stdout: ""
Sep  5 22:24:22.797: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -l name=update-demo --namespace=kubectl-5152 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 22:24:22.892: INFO: stderr: ""
Sep  5 22:24:22.892: INFO: stdout: "update-demo-nautilus-4thl6\nupdate-demo-nautilus-j6g8l\n"
Sep  5 22:24:23.172: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5152'
Sep  5 22:24:23.288: INFO: stderr: "No resources found in kubectl-5152 namespace.\n"
Sep  5 22:24:23.288: INFO: stdout: ""
Sep  5 22:24:23.288: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -l name=update-demo --namespace=kubectl-5152 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 22:24:23.395: INFO: stderr: ""
Sep  5 22:24:23.395: INFO: stdout: "update-demo-nautilus-4thl6\nupdate-demo-nautilus-j6g8l\n"
Sep  5 22:24:23.673: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5152'
Sep  5 22:24:23.780: INFO: stderr: "No resources found in kubectl-5152 namespace.\n"
Sep  5 22:24:23.780: INFO: stdout: ""
Sep  5 22:24:23.780: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -l name=update-demo --namespace=kubectl-5152 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 22:24:23.880: INFO: stderr: ""
Sep  5 22:24:23.880: INFO: stdout: "update-demo-nautilus-4thl6\nupdate-demo-nautilus-j6g8l\n"
Sep  5 22:24:24.173: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5152'
Sep  5 22:24:24.278: INFO: stderr: "No resources found in kubectl-5152 namespace.\n"
Sep  5 22:24:24.278: INFO: stdout: ""
Sep  5 22:24:24.278: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -l name=update-demo --namespace=kubectl-5152 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 22:24:24.414: INFO: stderr: ""
Sep  5 22:24:24.414: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:24:24.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5152" for this suite.

â€¢ [SLOW TEST:41.376 seconds]
[sig-cli] Kubectl client
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should create and stop a replication controller  [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","total":291,"completed":124,"skipped":2105,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:24:24.652: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7854
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:24:25.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7854" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","total":291,"completed":125,"skipped":2113,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:24:25.460: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2595
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should get a host IP [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating pod
Sep  5 22:24:48.013: INFO: Pod pod-hostip-1618e22f-eecf-4ae9-a3a5-61f0f4895d81 has hostIP: 10.193.34.149
[AfterEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:24:48.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2595" for this suite.

â€¢ [SLOW TEST:22.771 seconds]
[k8s.io] Pods
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should get a host IP [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]","total":291,"completed":126,"skipped":2134,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:24:48.232: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9012
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide podname only [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  5 22:24:48.664: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57" in namespace "projected-9012" to be "Succeeded or Failed"
Sep  5 22:24:48.681: INFO: Pod "downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57": Phase="Pending", Reason="", readiness=false. Elapsed: 17.225659ms
Sep  5 22:24:50.695: INFO: Pod "downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031184172s
Sep  5 22:24:52.702: INFO: Pod "downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038074138s
Sep  5 22:24:54.711: INFO: Pod "downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57": Phase="Pending", Reason="", readiness=false. Elapsed: 6.047209081s
Sep  5 22:24:56.718: INFO: Pod "downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57": Phase="Pending", Reason="", readiness=false. Elapsed: 8.054515731s
Sep  5 22:24:58.728: INFO: Pod "downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57": Phase="Pending", Reason="", readiness=false. Elapsed: 10.06447125s
Sep  5 22:25:00.756: INFO: Pod "downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57": Phase="Pending", Reason="", readiness=false. Elapsed: 12.09179902s
Sep  5 22:25:02.767: INFO: Pod "downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57": Phase="Pending", Reason="", readiness=false. Elapsed: 14.102712257s
Sep  5 22:25:04.777: INFO: Pod "downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57": Phase="Pending", Reason="", readiness=false. Elapsed: 16.113315833s
Sep  5 22:25:06.787: INFO: Pod "downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57": Phase="Pending", Reason="", readiness=false. Elapsed: 18.123080785s
Sep  5 22:25:08.812: INFO: Pod "downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57": Phase="Pending", Reason="", readiness=false. Elapsed: 20.147858061s
Sep  5 22:25:10.821: INFO: Pod "downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57": Phase="Pending", Reason="", readiness=false. Elapsed: 22.156983894s
Sep  5 22:25:12.829: INFO: Pod "downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.165024347s
STEP: Saw pod success
Sep  5 22:25:12.829: INFO: Pod "downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57" satisfied condition "Succeeded or Failed"
Sep  5 22:25:12.834: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57 container client-container: <nil>
STEP: delete the pod
Sep  5 22:25:12.893: INFO: Waiting for pod downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57 to disappear
Sep  5 22:25:12.903: INFO: Pod downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57 still exists
Sep  5 22:25:14.904: INFO: Waiting for pod downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57 to disappear
Sep  5 22:25:14.912: INFO: Pod downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57 still exists
Sep  5 22:25:16.903: INFO: Waiting for pod downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57 to disappear
Sep  5 22:25:16.912: INFO: Pod downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57 still exists
Sep  5 22:25:18.904: INFO: Waiting for pod downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57 to disappear
Sep  5 22:25:18.914: INFO: Pod downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57 still exists
Sep  5 22:25:20.903: INFO: Waiting for pod downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57 to disappear
Sep  5 22:25:20.911: INFO: Pod downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57 still exists
Sep  5 22:25:22.903: INFO: Waiting for pod downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57 to disappear
Sep  5 22:25:22.911: INFO: Pod downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57 still exists
Sep  5 22:25:24.904: INFO: Waiting for pod downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57 to disappear
Sep  5 22:25:24.911: INFO: Pod downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57 still exists
Sep  5 22:25:26.904: INFO: Waiting for pod downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57 to disappear
Sep  5 22:25:26.944: INFO: Pod downwardapi-volume-e1ae6340-e59a-4120-8547-8ba6fae39b57 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:25:26.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9012" for this suite.

â€¢ [SLOW TEST:38.973 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide podname only [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","total":291,"completed":127,"skipped":2136,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:25:27.205: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-372
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  5 22:26:13.788: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 22:26:13.801: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 22:26:15.801: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 22:26:15.808: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 22:26:17.802: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 22:26:17.809: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 22:26:19.803: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 22:26:19.811: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 22:26:21.802: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 22:26:21.810: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 22:26:23.802: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 22:26:23.817: INFO: Pod pod-with-poststart-exec-hook still exists
Sep  5 22:26:25.802: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep  5 22:26:25.810: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:26:25.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-372" for this suite.

â€¢ [SLOW TEST:58.810 seconds]
[k8s.io] Container Lifecycle Hook
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","total":291,"completed":128,"skipped":2148,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:26:26.015: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8089
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Sep  5 22:28:40.596: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:28:40.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8089" for this suite.

â€¢ [SLOW TEST:134.914 seconds]
[sig-api-machinery] Garbage collector
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","total":291,"completed":129,"skipped":2178,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:28:40.929: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7110
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap that has name configmap-test-emptyKey-477d6b85-0f9f-4d6e-8f9a-89fd480a2596
[AfterEach] [sig-node] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:28:41.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7110" for this suite.
â€¢{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","total":291,"completed":130,"skipped":2182,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:28:42.024: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2774
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep  5 22:28:42.439: INFO: Waiting up to 5m0s for pod "pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c" in namespace "emptydir-2774" to be "Succeeded or Failed"
Sep  5 22:28:42.447: INFO: Pod "pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.25652ms
Sep  5 22:28:44.464: INFO: Pod "pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024253735s
Sep  5 22:28:46.481: INFO: Pod "pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041870472s
Sep  5 22:28:48.489: INFO: Pod "pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.049785975s
Sep  5 22:28:50.501: INFO: Pod "pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.061135299s
Sep  5 22:28:52.510: INFO: Pod "pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.070803102s
Sep  5 22:28:54.519: INFO: Pod "pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.07942962s
Sep  5 22:28:56.531: INFO: Pod "pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.091821479s
Sep  5 22:28:58.539: INFO: Pod "pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.099152623s
Sep  5 22:29:00.553: INFO: Pod "pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.113748001s
Sep  5 22:29:02.561: INFO: Pod "pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.121763467s
STEP: Saw pod success
Sep  5 22:29:02.561: INFO: Pod "pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c" satisfied condition "Succeeded or Failed"
Sep  5 22:29:02.569: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c container test-container: <nil>
STEP: delete the pod
Sep  5 22:29:02.640: INFO: Waiting for pod pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c to disappear
Sep  5 22:29:02.653: INFO: Pod pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c still exists
Sep  5 22:29:04.655: INFO: Waiting for pod pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c to disappear
Sep  5 22:29:04.663: INFO: Pod pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c still exists
Sep  5 22:29:06.654: INFO: Waiting for pod pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c to disappear
Sep  5 22:29:06.665: INFO: Pod pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c still exists
Sep  5 22:29:08.654: INFO: Waiting for pod pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c to disappear
Sep  5 22:29:08.665: INFO: Pod pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c still exists
Sep  5 22:29:10.654: INFO: Waiting for pod pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c to disappear
Sep  5 22:29:10.662: INFO: Pod pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c still exists
Sep  5 22:29:12.655: INFO: Waiting for pod pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c to disappear
Sep  5 22:29:12.670: INFO: Pod pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c still exists
Sep  5 22:29:14.655: INFO: Waiting for pod pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c to disappear
Sep  5 22:29:14.664: INFO: Pod pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c still exists
Sep  5 22:29:16.655: INFO: Waiting for pod pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c to disappear
Sep  5 22:29:16.661: INFO: Pod pod-6dc33c84-ca90-4edf-bcf0-ecf030aeaa7c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:29:16.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2774" for this suite.

â€¢ [SLOW TEST:34.818 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":131,"skipped":2194,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:29:16.842: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9267
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Sep  5 22:29:17.210: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:32:02.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9267" for this suite.

â€¢ [SLOW TEST:165.864 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartAlways pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","total":291,"completed":132,"skipped":2211,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:32:02.707: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6116
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod liveness-ae9b7138-a3c1-4dd7-8a02-eef569c8e2c4 in namespace container-probe-6116
Sep  5 22:32:31.247: INFO: Started pod liveness-ae9b7138-a3c1-4dd7-8a02-eef569c8e2c4 in namespace container-probe-6116
STEP: checking the pod's current state and verifying that restartCount is present
Sep  5 22:32:31.256: INFO: Initial restart count of pod liveness-ae9b7138-a3c1-4dd7-8a02-eef569c8e2c4 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:36:32.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6116" for this suite.

â€¢ [SLOW TEST:270.051 seconds]
[k8s.io] Probing container
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","total":291,"completed":133,"skipped":2253,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:36:32.759: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3646
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if v1 is in available api versions  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: validating api versions
Sep  5 22:36:33.186: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml api-versions'
Sep  5 22:36:33.289: INFO: stderr: ""
Sep  5 22:36:33.289: INFO: stdout: "acme.cert-manager.io/v1alpha2\nacme.cert-manager.io/v1alpha3\naddons.cluster.x-k8s.io/v1alpha3\nadmissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\nappplatform.wcp.vmware.com/v1alpha1\nappplatform.wcp.vmware.com/v1alpha2\nappplatform.wcp.vmware.com/v1beta1\napps/v1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbootstrap.cluster.x-k8s.io/v1alpha2\nbootstrap.cluster.x-k8s.io/v1alpha3\ncert-manager.io/v1alpha2\ncert-manager.io/v1alpha3\ncertificates.k8s.io/v1\ncertificates.k8s.io/v1beta1\ncluster.x-k8s.io/v1alpha2\ncluster.x-k8s.io/v1alpha3\ncns.vmware.com/v1alpha1\ncontrolplane.cluster.x-k8s.io/v1alpha3\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1beta1\nevents.k8s.io/v1\nevents.k8s.io/v1beta1\nexp.cluster.x-k8s.io/v1alpha3\nextensions/v1beta1\nimagecontroller.vmware.com/v1\ninfrastructure.cluster.vmware.com/v1alpha2\ninfrastructure.cluster.vmware.com/v1alpha3\ninstallers.tmc.cloud.vmware.com/v1alpha1\nk8s.cni.cncf.io/v1\nlicenseoperator.vmware.com/v1alpha1\nnetoperator.vmware.com/v1alpha1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnetworking.x-k8s.io/v1alpha1pre1\nnode.k8s.io/v1beta1\nnsx.vmware.com/v1\nnsx.vmware.com/v1alpha1\npolicy/v1beta1\npsp.wcp.vmware.com/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nregistryagent.vmware.com/v1alpha1\nrun.tanzu.vmware.com/v1alpha1\nrun.tanzu.vmware.com/v1alpha2\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntopology.tanzu.vmware.com/v1alpha1\nv1\nvmoperator.vmware.com/v1alpha1\nvmware.com/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:36:33.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3646" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","total":291,"completed":134,"skipped":2276,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}

------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:36:33.491: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9325
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep  5 22:36:33.916: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9325 /api/v1/namespaces/watch-9325/configmaps/e2e-watch-test-configmap-a a1cc9140-0db5-44a9-9e76-6d3fb9c89eed 136837 0 2021-09-05 22:36:33 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-05 22:36:33 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  5 22:36:33.917: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9325 /api/v1/namespaces/watch-9325/configmaps/e2e-watch-test-configmap-a a1cc9140-0db5-44a9-9e76-6d3fb9c89eed 136837 0 2021-09-05 22:36:33 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-05 22:36:33 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep  5 22:36:43.941: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9325 /api/v1/namespaces/watch-9325/configmaps/e2e-watch-test-configmap-a a1cc9140-0db5-44a9-9e76-6d3fb9c89eed 136979 0 2021-09-05 22:36:33 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-05 22:36:43 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  5 22:36:43.941: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9325 /api/v1/namespaces/watch-9325/configmaps/e2e-watch-test-configmap-a a1cc9140-0db5-44a9-9e76-6d3fb9c89eed 136979 0 2021-09-05 22:36:33 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-05 22:36:43 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep  5 22:36:53.974: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9325 /api/v1/namespaces/watch-9325/configmaps/e2e-watch-test-configmap-a a1cc9140-0db5-44a9-9e76-6d3fb9c89eed 137088 0 2021-09-05 22:36:33 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-05 22:36:43 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  5 22:36:53.974: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9325 /api/v1/namespaces/watch-9325/configmaps/e2e-watch-test-configmap-a a1cc9140-0db5-44a9-9e76-6d3fb9c89eed 137088 0 2021-09-05 22:36:33 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-05 22:36:43 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep  5 22:37:04.018: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9325 /api/v1/namespaces/watch-9325/configmaps/e2e-watch-test-configmap-a a1cc9140-0db5-44a9-9e76-6d3fb9c89eed 137193 0 2021-09-05 22:36:33 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-05 22:36:43 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  5 22:37:04.019: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9325 /api/v1/namespaces/watch-9325/configmaps/e2e-watch-test-configmap-a a1cc9140-0db5-44a9-9e76-6d3fb9c89eed 137193 0 2021-09-05 22:36:33 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  [{e2e.test Update v1 2021-09-05 22:36:43 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep  5 22:37:14.037: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9325 /api/v1/namespaces/watch-9325/configmaps/e2e-watch-test-configmap-b 1032fc17-4776-455a-87f3-e5c9ae748772 137318 0 2021-09-05 22:37:14 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-05 22:37:14 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  5 22:37:14.037: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9325 /api/v1/namespaces/watch-9325/configmaps/e2e-watch-test-configmap-b 1032fc17-4776-455a-87f3-e5c9ae748772 137318 0 2021-09-05 22:37:14 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-05 22:37:14 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep  5 22:37:24.073: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9325 /api/v1/namespaces/watch-9325/configmaps/e2e-watch-test-configmap-b 1032fc17-4776-455a-87f3-e5c9ae748772 137420 0 2021-09-05 22:37:14 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-05 22:37:14 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  5 22:37:24.074: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9325 /api/v1/namespaces/watch-9325/configmaps/e2e-watch-test-configmap-b 1032fc17-4776-455a-87f3-e5c9ae748772 137420 0 2021-09-05 22:37:14 -0700 PDT <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  [{e2e.test Update v1 2021-09-05 22:37:14 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:37:34.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9325" for this suite.

â€¢ [SLOW TEST:60.862 seconds]
[sig-api-machinery] Watchers
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","total":291,"completed":135,"skipped":2276,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:37:34.354: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-855
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 22:37:34.769: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep  5 22:37:34.807: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep  5 22:37:39.820: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  5 22:37:59.839: INFO: Creating deployment "test-rolling-update-deployment"
Sep  5 22:37:59.851: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep  5 22:37:59.870: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep  5 22:38:01.890: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep  5 22:38:01.903: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 38, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:38:03.910: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 38, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:38:05.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 38, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:38:07.911: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 38, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:38:09.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 38, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:38:11.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 38, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:38:13.911: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 38, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:38:15.912: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 38, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:38:17.914: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 38, 0, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 37, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-c4cb8d6d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:38:19.915: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep  5 22:38:19.930: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-855 /apis/apps/v1/namespaces/deployment-855/deployments/test-rolling-update-deployment 5bf14af2-50c0-4335-a4e2-625759d9a1d3 138118 1 2021-09-05 22:37:59 -0700 PDT <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  [{e2e.test Update apps/v1 2021-09-05 22:37:59 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-05 22:38:19 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00319d788 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-09-05 22:37:59 -0700 PDT,LastTransitionTime:2021-09-05 22:37:59 -0700 PDT,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" has successfully progressed.,LastUpdateTime:2021-09-05 22:38:19 -0700 PDT,LastTransitionTime:2021-09-05 22:37:59 -0700 PDT,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  5 22:38:19.937: INFO: New ReplicaSet "test-rolling-update-deployment-c4cb8d6d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9  deployment-855 /apis/apps/v1/namespaces/deployment-855/replicasets/test-rolling-update-deployment-c4cb8d6d9 76ad9d48-9108-4e0f-995c-aaca43ab8bb4 138108 1 2021-09-05 22:37:59 -0700 PDT <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 5bf14af2-50c0-4335-a4e2-625759d9a1d3 0xc00319dce0 0xc00319dce1}] []  [{kube-controller-manager Update apps/v1 2021-09-05 22:38:19 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5bf14af2-50c0-4335-a4e2-625759d9a1d3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: c4cb8d6d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00319dd58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  5 22:38:19.937: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep  5 22:38:19.937: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-855 /apis/apps/v1/namespaces/deployment-855/replicasets/test-rolling-update-controller 77c56be0-7003-4892-8f54-469a9ec25b23 138116 2 2021-09-05 22:37:34 -0700 PDT <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 5bf14af2-50c0-4335-a4e2-625759d9a1d3 0xc00319dbd7 0xc00319dbd8}] []  [{e2e.test Update apps/v1 2021-09-05 22:37:34 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-05 22:38:19 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5bf14af2-50c0-4335-a4e2-625759d9a1d3\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00319dc78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  5 22:38:19.942: INFO: Pod "test-rolling-update-controller-f4xp5" is available:
&Pod{ObjectMeta:{test-rolling-update-controller-f4xp5 test-rolling-update-controller- deployment-855 /api/v1/namespaces/deployment-855/pods/test-rolling-update-controller-f4xp5 fcd2326a-0834-4cfd-9043-eb6d2f89d872 138113 0 2021-09-05 22:37:34 -0700 PDT 2021-09-05 22:38:19 -0700 PDT 0xc007da4568 map[name:sample-pod pod:httpd] map[attachment_id:35551360-9004-4be4-9685-715f799958cc kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:60:0b vlan:None vmware-system-ephemeral-disk-uuid:6000C297-9884-578b-1736-7b99aa143286 vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38682"} vmware-system-vm-moid:vm-893:7badc608-dd2f-42a9-952d-0d4c30d5f283 vmware-system-vm-uuid:50174f66-f037-d609-0c16-ecd90f5eddd5] [{apps/v1 ReplicaSet test-rolling-update-controller 77c56be0-7003-4892-8f54-469a9ec25b23 0xc007da4617 0xc007da4618}] [lifecycle-controller/system.vmware.com]  [{image-controller Update v1 2021-09-05 22:37:34 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {kube-controller-manager Update v1 2021-09-05 22:37:34 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"77c56be0-7003-4892-8f54-469a9ec25b23\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-9mqvr Update v1 2021-09-05 22:37:46 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-05 22:37:51 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-05 22:37:59 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xr2z8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xr2z8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xr2z8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-43-208.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 22:37:34 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 22:37:59 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 22:37:59 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 22:37:59 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.43.208,PodIP:172.26.1.194,StartTime:2021-09-05 22:37:55 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-05 22:37:56 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38682,ContainerID:582b7bbc-7b63-4e73-9d82-147b849f618e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  5 22:38:19.943: INFO: Pod "test-rolling-update-deployment-c4cb8d6d9-wtwcb" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-c4cb8d6d9-wtwcb test-rolling-update-deployment-c4cb8d6d9- deployment-855 /api/v1/namespaces/deployment-855/pods/test-rolling-update-deployment-c4cb8d6d9-wtwcb 4001c178-caac-4ad2-86fc-9a885a9670a3 138106 0 2021-09-05 22:37:59 -0700 PDT <nil> <nil> map[name:sample-pod pod-template-hash:c4cb8d6d9] map[attachment_id:1c0dca59-d768-4a45-ad2a-033ba0f67382 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:60:10 vlan:None vmware-system-ephemeral-disk-uuid:6000C29c-9b8d-b85c-59c9-d455d34012ce vmware-system-image-references:{"agnhost":"agnhost-e4bbf9f419e02f0332193ee525b99e9c3c927d53-v59234"} vmware-system-vm-moid:vm-895:7badc608-dd2f-42a9-952d-0d4c30d5f283 vmware-system-vm-uuid:5017a85a-3629-02bd-7e9e-082db30ca424] [{apps/v1 ReplicaSet test-rolling-update-deployment-c4cb8d6d9 76ad9d48-9108-4e0f-995c-aaca43ab8bb4 0xc007da4bd7 0xc007da4bd8}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-05 22:37:59 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76ad9d48-9108-4e0f-995c-aaca43ab8bb4\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-05 22:38:00 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {nsx-ncp-6d7f7bf559-9mqvr Update v1 2021-09-05 22:38:01 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-05 22:38:12 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-05 22:38:19 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.195\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-xr2z8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-xr2z8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-xr2z8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-43-208.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 22:38:00 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 22:38:19 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 22:38:19 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-05 22:38:19 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.43.208,PodIP:172.26.1.195,StartTime:2021-09-05 22:38:15 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-05 22:38:16 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:agnhost-e4bbf9f419e02f0332193ee525b99e9c3c927d53-v59234,ContainerID:8659f24b-be09-4d42-ae55-5a81f7f64d61,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.195,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:38:19.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-855" for this suite.

â€¢ [SLOW TEST:45.845 seconds]
[sig-apps] Deployment
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","total":291,"completed":136,"skipped":2288,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:38:20.199: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1086
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:38:32.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1086" for this suite.

â€¢ [SLOW TEST:13.179 seconds]
[sig-api-machinery] ResourceQuota
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","total":291,"completed":137,"skipped":2318,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:38:33.379: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-6155
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-6155, will wait for the garbage collector to delete the pods
Sep  5 22:38:57.936: INFO: Deleting Job.batch foo took: 17.387546ms
Sep  5 22:39:00.037: INFO: Terminating Job.batch foo pods took: 2.100653966s
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:39:46.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6155" for this suite.

â€¢ [SLOW TEST:72.927 seconds]
[sig-apps] Job
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","total":291,"completed":138,"skipped":2331,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:39:46.307: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3960
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Sep  5 22:39:46.749: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:40:24.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3960" for this suite.

â€¢ [SLOW TEST:38.819 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","total":291,"completed":139,"skipped":2350,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:40:25.127: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-157
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-157
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a new StatefulSet
Sep  5 22:40:25.694: INFO: Found 0 stateful pods, waiting for 3
Sep  5 22:40:35.724: INFO: Found 1 stateful pods, waiting for 3
Sep  5 22:40:45.702: INFO: Found 1 stateful pods, waiting for 3
Sep  5 22:40:55.703: INFO: Found 2 stateful pods, waiting for 3
Sep  5 22:41:05.701: INFO: Found 2 stateful pods, waiting for 3
Sep  5 22:41:15.848: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 22:41:15.848: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 22:41:15.848: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 22:41:25.704: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 22:41:25.705: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 22:41:25.705: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 22:41:35.702: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 22:41:35.702: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 22:41:35.702: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 22:41:35.721: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-157 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 22:41:36.504: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 22:41:36.504: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 22:41:36.504: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from mirror.gcr.io/library/httpd:2.4.38-alpine to mirror.gcr.io/library/httpd:2.4.39-alpine
Sep  5 22:41:46.567: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep  5 22:41:56.614: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-157 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 22:41:56.843: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  5 22:41:56.843: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  5 22:41:56.843: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  5 22:42:06.909: INFO: Waiting for StatefulSet statefulset-157/ss2 to complete update
Sep  5 22:42:06.909: INFO: Waiting for Pod statefulset-157/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 22:42:06.909: INFO: Waiting for Pod statefulset-157/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 22:42:06.909: INFO: Waiting for Pod statefulset-157/ss2-2 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 22:42:16.949: INFO: Waiting for StatefulSet statefulset-157/ss2 to complete update
Sep  5 22:42:16.949: INFO: Waiting for Pod statefulset-157/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 22:42:16.949: INFO: Waiting for Pod statefulset-157/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 22:42:26.960: INFO: Waiting for StatefulSet statefulset-157/ss2 to complete update
Sep  5 22:42:26.960: INFO: Waiting for Pod statefulset-157/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 22:42:26.960: INFO: Waiting for Pod statefulset-157/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 22:42:36.927: INFO: Waiting for StatefulSet statefulset-157/ss2 to complete update
Sep  5 22:42:36.927: INFO: Waiting for Pod statefulset-157/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 22:42:36.927: INFO: Waiting for Pod statefulset-157/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 22:42:46.941: INFO: Waiting for StatefulSet statefulset-157/ss2 to complete update
Sep  5 22:42:46.941: INFO: Waiting for Pod statefulset-157/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 22:42:46.941: INFO: Waiting for Pod statefulset-157/ss2-1 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 22:42:56.925: INFO: Waiting for StatefulSet statefulset-157/ss2 to complete update
Sep  5 22:42:56.925: INFO: Waiting for Pod statefulset-157/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 22:43:06.924: INFO: Waiting for StatefulSet statefulset-157/ss2 to complete update
Sep  5 22:43:06.924: INFO: Waiting for Pod statefulset-157/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 22:43:16.924: INFO: Waiting for StatefulSet statefulset-157/ss2 to complete update
Sep  5 22:43:16.924: INFO: Waiting for Pod statefulset-157/ss2-0 to have revision ss2-6dc56fb9cb update revision ss2-6969d67667
Sep  5 22:43:26.926: INFO: Waiting for StatefulSet statefulset-157/ss2 to complete update
Sep  5 22:43:36.945: INFO: Waiting for StatefulSet statefulset-157/ss2 to complete update
STEP: Rolling back to a previous revision
Sep  5 22:43:46.950: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-157 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 22:43:47.253: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 22:43:47.253: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 22:43:47.253: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  5 22:43:57.317: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep  5 22:44:07.366: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-157 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 22:44:07.853: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  5 22:44:07.853: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  5 22:44:07.853: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  5 22:44:17.916: INFO: Waiting for StatefulSet statefulset-157/ss2 to complete update
Sep  5 22:44:17.916: INFO: Waiting for Pod statefulset-157/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 22:44:17.916: INFO: Waiting for Pod statefulset-157/ss2-1 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 22:44:17.916: INFO: Waiting for Pod statefulset-157/ss2-2 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 22:44:27.940: INFO: Waiting for StatefulSet statefulset-157/ss2 to complete update
Sep  5 22:44:27.940: INFO: Waiting for Pod statefulset-157/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 22:44:27.940: INFO: Waiting for Pod statefulset-157/ss2-1 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 22:44:37.940: INFO: Waiting for StatefulSet statefulset-157/ss2 to complete update
Sep  5 22:44:37.940: INFO: Waiting for Pod statefulset-157/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 22:44:37.940: INFO: Waiting for Pod statefulset-157/ss2-1 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 22:44:47.932: INFO: Waiting for StatefulSet statefulset-157/ss2 to complete update
Sep  5 22:44:47.932: INFO: Waiting for Pod statefulset-157/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 22:44:47.932: INFO: Waiting for Pod statefulset-157/ss2-1 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 22:44:57.941: INFO: Waiting for StatefulSet statefulset-157/ss2 to complete update
Sep  5 22:44:57.941: INFO: Waiting for Pod statefulset-157/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 22:45:07.942: INFO: Waiting for StatefulSet statefulset-157/ss2 to complete update
Sep  5 22:45:07.942: INFO: Waiting for Pod statefulset-157/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 22:45:17.932: INFO: Waiting for StatefulSet statefulset-157/ss2 to complete update
Sep  5 22:45:17.932: INFO: Waiting for Pod statefulset-157/ss2-0 to have revision ss2-6969d67667 update revision ss2-6dc56fb9cb
Sep  5 22:45:27.938: INFO: Waiting for StatefulSet statefulset-157/ss2 to complete update
Sep  5 22:45:37.957: INFO: Waiting for StatefulSet statefulset-157/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep  5 22:45:47.948: INFO: Deleting all statefulset in ns statefulset-157
Sep  5 22:45:47.966: INFO: Scaling statefulset ss2 to 0
Sep  5 22:46:28.039: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 22:46:28.056: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:46:28.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-157" for this suite.

â€¢ [SLOW TEST:363.350 seconds]
[sig-apps] StatefulSet
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    should perform rolling updates and roll backs of template modifications [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","total":291,"completed":140,"skipped":2362,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:46:28.477: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3124
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  5 22:46:51.647: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:46:51.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3124" for this suite.

â€¢ [SLOW TEST:23.487 seconds]
[k8s.io] Container Runtime
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":291,"completed":141,"skipped":2371,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:46:51.965: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3855
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  5 22:46:52.486: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f" in namespace "downward-api-3855" to be "Succeeded or Failed"
Sep  5 22:46:52.495: INFO: Pod "downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.905292ms
Sep  5 22:46:54.507: INFO: Pod "downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020984865s
Sep  5 22:46:56.524: INFO: Pod "downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0379788s
Sep  5 22:46:58.532: INFO: Pod "downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.046084932s
Sep  5 22:47:00.544: INFO: Pod "downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.058148325s
Sep  5 22:47:02.553: INFO: Pod "downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.066504068s
Sep  5 22:47:04.566: INFO: Pod "downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.080146119s
Sep  5 22:47:06.574: INFO: Pod "downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.088071656s
Sep  5 22:47:08.584: INFO: Pod "downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.098351339s
Sep  5 22:47:10.598: INFO: Pod "downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.111975443s
Sep  5 22:47:12.607: INFO: Pod "downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.120402601s
Sep  5 22:47:14.620: INFO: Pod "downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.134085114s
Sep  5 22:47:16.632: INFO: Pod "downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.146285347s
STEP: Saw pod success
Sep  5 22:47:16.632: INFO: Pod "downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f" satisfied condition "Succeeded or Failed"
Sep  5 22:47:16.640: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f container client-container: <nil>
STEP: delete the pod
Sep  5 22:47:16.718: INFO: Waiting for pod downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f to disappear
Sep  5 22:47:16.736: INFO: Pod downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f still exists
Sep  5 22:47:18.736: INFO: Waiting for pod downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f to disappear
Sep  5 22:47:18.756: INFO: Pod downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f still exists
Sep  5 22:47:20.736: INFO: Waiting for pod downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f to disappear
Sep  5 22:47:20.752: INFO: Pod downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f still exists
Sep  5 22:47:22.736: INFO: Waiting for pod downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f to disappear
Sep  5 22:47:22.746: INFO: Pod downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f still exists
Sep  5 22:47:24.737: INFO: Waiting for pod downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f to disappear
Sep  5 22:47:24.745: INFO: Pod downwardapi-volume-1600f9ba-8564-4923-a69c-f9968ad9f63f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:47:24.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3855" for this suite.

â€¢ [SLOW TEST:33.149 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's cpu request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","total":291,"completed":142,"skipped":2409,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:47:25.114: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9915
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide container's memory request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  5 22:47:25.555: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c" in namespace "downward-api-9915" to be "Succeeded or Failed"
Sep  5 22:47:25.584: INFO: Pod "downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c": Phase="Pending", Reason="", readiness=false. Elapsed: 28.351849ms
Sep  5 22:47:27.593: INFO: Pod "downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037084065s
Sep  5 22:47:29.608: INFO: Pod "downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052277492s
Sep  5 22:47:31.616: INFO: Pod "downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.060684545s
Sep  5 22:47:33.632: INFO: Pod "downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.076246051s
Sep  5 22:47:35.656: INFO: Pod "downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.100785331s
Sep  5 22:47:37.664: INFO: Pod "downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.10812725s
Sep  5 22:47:39.676: INFO: Pod "downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.12013744s
Sep  5 22:47:41.706: INFO: Pod "downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.150233518s
Sep  5 22:47:43.717: INFO: Pod "downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.161393925s
Sep  5 22:47:45.729: INFO: Pod "downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.173395979s
Sep  5 22:47:47.742: INFO: Pod "downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c": Phase="Running", Reason="", readiness=true. Elapsed: 22.186835279s
Sep  5 22:47:49.753: INFO: Pod "downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c": Phase="Running", Reason="", readiness=true. Elapsed: 24.19720464s
Sep  5 22:47:51.763: INFO: Pod "downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.207693115s
STEP: Saw pod success
Sep  5 22:47:51.763: INFO: Pod "downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c" satisfied condition "Succeeded or Failed"
Sep  5 22:47:51.774: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c container client-container: <nil>
STEP: delete the pod
Sep  5 22:47:51.837: INFO: Waiting for pod downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c to disappear
Sep  5 22:47:51.848: INFO: Pod downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c still exists
Sep  5 22:47:53.848: INFO: Waiting for pod downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c to disappear
Sep  5 22:47:53.858: INFO: Pod downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c still exists
Sep  5 22:47:55.849: INFO: Waiting for pod downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c to disappear
Sep  5 22:47:55.866: INFO: Pod downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c still exists
Sep  5 22:47:57.849: INFO: Waiting for pod downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c to disappear
Sep  5 22:47:57.886: INFO: Pod downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c still exists
Sep  5 22:47:59.849: INFO: Waiting for pod downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c to disappear
Sep  5 22:47:59.860: INFO: Pod downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c still exists
Sep  5 22:48:01.849: INFO: Waiting for pod downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c to disappear
Sep  5 22:48:01.859: INFO: Pod downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c still exists
Sep  5 22:48:03.849: INFO: Waiting for pod downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c to disappear
Sep  5 22:48:03.861: INFO: Pod downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c still exists
Sep  5 22:48:05.849: INFO: Waiting for pod downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c to disappear
Sep  5 22:48:05.857: INFO: Pod downwardapi-volume-bddb6eb0-7d24-4916-a5a3-d7b7dee4722c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:48:05.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9915" for this suite.

â€¢ [SLOW TEST:40.987 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide container's memory request [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","total":291,"completed":143,"skipped":2426,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:48:06.101: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5989
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-878
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3301
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:48:16.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5989" for this suite.
STEP: Destroying namespace "nsdeletetest-878" for this suite.
Sep  5 22:48:17.440: INFO: Namespace nsdeletetest-878 was already deleted
STEP: Destroying namespace "nsdeletetest-3301" for this suite.

â€¢ [SLOW TEST:11.584 seconds]
[sig-api-machinery] Namespaces [Serial]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","total":291,"completed":144,"skipped":2442,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:48:17.686: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-2115
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 22:48:19.653: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  5 22:48:21.689: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:48:23.723: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:48:25.698: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:48:27.706: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:48:29.713: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:48:31.698: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:48:33.730: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:48:35.699: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:48:37.695: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:48:39.698: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 22:48:41.768: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 22, 48, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 22:48:44.741: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:48:45.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2115" for this suite.
STEP: Destroying namespace "webhook-2115-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:28.730 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","total":291,"completed":145,"skipped":2462,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:48:46.416: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5882
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Sep  5 22:50:52.686: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:50:52.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5882" for this suite.

â€¢ [SLOW TEST:126.535 seconds]
[sig-api-machinery] Garbage collector
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","total":291,"completed":146,"skipped":2462,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:50:52.952: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-932
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 22:50:53.409: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:51:19.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-932" for this suite.

â€¢ [SLOW TEST:26.893 seconds]
[k8s.io] Pods
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","total":291,"completed":147,"skipped":2499,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:51:19.845: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-938
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  5 22:51:20.513: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe" in namespace "projected-938" to be "Succeeded or Failed"
Sep  5 22:51:20.553: INFO: Pod "downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009247ms
Sep  5 22:51:22.565: INFO: Pod "downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051847509s
Sep  5 22:51:24.576: INFO: Pod "downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062701548s
Sep  5 22:51:26.591: INFO: Pod "downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.078013057s
Sep  5 22:51:28.599: INFO: Pod "downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 8.086389907s
Sep  5 22:51:30.607: INFO: Pod "downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 10.094188728s
Sep  5 22:51:32.615: INFO: Pod "downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 12.101531051s
Sep  5 22:51:34.623: INFO: Pod "downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 14.110113523s
Sep  5 22:51:36.640: INFO: Pod "downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe": Phase="Pending", Reason="", readiness=false. Elapsed: 16.126517601s
Sep  5 22:51:38.652: INFO: Pod "downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe": Phase="Running", Reason="", readiness=true. Elapsed: 18.139163321s
Sep  5 22:51:40.665: INFO: Pod "downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe": Phase="Running", Reason="", readiness=true. Elapsed: 20.152265472s
Sep  5 22:51:42.678: INFO: Pod "downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.165043446s
STEP: Saw pod success
Sep  5 22:51:42.678: INFO: Pod "downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe" satisfied condition "Succeeded or Failed"
Sep  5 22:51:42.689: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe container client-container: <nil>
STEP: delete the pod
Sep  5 22:51:42.754: INFO: Waiting for pod downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe to disappear
Sep  5 22:51:42.773: INFO: Pod downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe still exists
Sep  5 22:51:44.774: INFO: Waiting for pod downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe to disappear
Sep  5 22:51:44.789: INFO: Pod downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe still exists
Sep  5 22:51:46.774: INFO: Waiting for pod downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe to disappear
Sep  5 22:51:46.785: INFO: Pod downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe still exists
Sep  5 22:51:48.774: INFO: Waiting for pod downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe to disappear
Sep  5 22:51:48.781: INFO: Pod downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe still exists
Sep  5 22:51:50.774: INFO: Waiting for pod downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe to disappear
Sep  5 22:51:50.794: INFO: Pod downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe still exists
Sep  5 22:51:52.775: INFO: Waiting for pod downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe to disappear
Sep  5 22:51:52.783: INFO: Pod downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe still exists
Sep  5 22:51:54.775: INFO: Waiting for pod downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe to disappear
Sep  5 22:51:54.783: INFO: Pod downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe still exists
Sep  5 22:51:56.774: INFO: Waiting for pod downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe to disappear
Sep  5 22:51:56.789: INFO: Pod downwardapi-volume-cb630949-27e8-4167-83c6-5e33d5ad3dfe no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:51:56.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-938" for this suite.

â€¢ [SLOW TEST:37.742 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":291,"completed":148,"skipped":2508,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should support configurable pod DNS nameservers [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:51:57.588: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-247
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support configurable pod DNS nameservers [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig...
Sep  5 22:51:58.079: INFO: Created pod &Pod{ObjectMeta:{dns-247  dns-247 /api/v1/namespaces/dns-247/pods/dns-247 6adcc34a-34fe-44b5-8464-8806e5440631 148216 0 2021-09-05 22:51:58 -0700 PDT <nil> <nil> map[] map[kubernetes.io/psp:e2e-test-privileged-psp] [] []  [{e2e.test Update v1 2021-09-05 22:51:57 -0700 PDT FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2ht5l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2ht5l,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2ht5l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  5 22:51:58.093: INFO: The status of Pod dns-247 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:52:00.112: INFO: The status of Pod dns-247 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:52:02.205: INFO: The status of Pod dns-247 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:52:04.107: INFO: The status of Pod dns-247 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:52:06.103: INFO: The status of Pod dns-247 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:52:08.105: INFO: The status of Pod dns-247 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:52:10.154: INFO: The status of Pod dns-247 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:52:12.105: INFO: The status of Pod dns-247 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:52:14.101: INFO: The status of Pod dns-247 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:52:16.105: INFO: The status of Pod dns-247 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:52:18.105: INFO: The status of Pod dns-247 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:52:20.107: INFO: The status of Pod dns-247 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:52:22.101: INFO: The status of Pod dns-247 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 22:52:24.104: INFO: The status of Pod dns-247 is Running (Ready = true)
STEP: Verifying customized DNS suffix list is configured on pod...
Sep  5 22:52:24.104: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-247 PodName:dns-247 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 22:52:24.104: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Verifying customized DNS server is configured on pod...
Sep  5 22:52:24.271: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-247 PodName:dns-247 ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 22:52:24.271: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 22:52:24.433: INFO: Deleting pod dns-247...
[AfterEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:52:24.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-247" for this suite.

â€¢ [SLOW TEST:27.186 seconds]
[sig-network] DNS
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","total":291,"completed":149,"skipped":2517,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:52:24.774: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7678
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep  5 22:53:09.309: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 22:53:09.324: INFO: Pod pod-with-prestop-http-hook still exists
Sep  5 22:53:11.324: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 22:53:11.336: INFO: Pod pod-with-prestop-http-hook still exists
Sep  5 22:53:13.325: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 22:53:13.336: INFO: Pod pod-with-prestop-http-hook still exists
Sep  5 22:53:15.326: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 22:53:15.338: INFO: Pod pod-with-prestop-http-hook still exists
Sep  5 22:53:17.325: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 22:53:17.332: INFO: Pod pod-with-prestop-http-hook still exists
Sep  5 22:53:19.325: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 22:53:19.333: INFO: Pod pod-with-prestop-http-hook still exists
Sep  5 22:53:21.324: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 22:53:21.333: INFO: Pod pod-with-prestop-http-hook still exists
Sep  5 22:53:23.325: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 22:53:23.332: INFO: Pod pod-with-prestop-http-hook still exists
Sep  5 22:53:25.324: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep  5 22:53:25.337: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:53:25.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7678" for this suite.

â€¢ [SLOW TEST:60.845 seconds]
[k8s.io] Container Lifecycle Hook
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","total":291,"completed":150,"skipped":2519,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:53:25.620: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3422
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop complex daemon [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 22:53:26.246: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep  5 22:53:26.278: INFO: Number of nodes with available pods: 0
Sep  5 22:53:26.278: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep  5 22:53:26.350: INFO: Number of nodes with available pods: 0
Sep  5 22:53:26.350: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:27.361: INFO: Number of nodes with available pods: 0
Sep  5 22:53:27.361: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:28.356: INFO: Number of nodes with available pods: 0
Sep  5 22:53:28.356: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:29.361: INFO: Number of nodes with available pods: 0
Sep  5 22:53:29.361: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:30.373: INFO: Number of nodes with available pods: 0
Sep  5 22:53:30.373: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:31.360: INFO: Number of nodes with available pods: 0
Sep  5 22:53:31.360: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:32.361: INFO: Number of nodes with available pods: 0
Sep  5 22:53:32.361: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:33.366: INFO: Number of nodes with available pods: 0
Sep  5 22:53:33.366: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:34.361: INFO: Number of nodes with available pods: 0
Sep  5 22:53:34.361: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:35.358: INFO: Number of nodes with available pods: 0
Sep  5 22:53:35.359: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:36.357: INFO: Number of nodes with available pods: 0
Sep  5 22:53:36.357: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:37.357: INFO: Number of nodes with available pods: 0
Sep  5 22:53:37.357: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:38.359: INFO: Number of nodes with available pods: 0
Sep  5 22:53:38.359: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:39.360: INFO: Number of nodes with available pods: 0
Sep  5 22:53:39.360: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:40.363: INFO: Number of nodes with available pods: 0
Sep  5 22:53:40.363: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:41.358: INFO: Number of nodes with available pods: 0
Sep  5 22:53:41.358: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:42.371: INFO: Number of nodes with available pods: 0
Sep  5 22:53:42.371: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:43.360: INFO: Number of nodes with available pods: 0
Sep  5 22:53:43.360: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:44.359: INFO: Number of nodes with available pods: 0
Sep  5 22:53:44.359: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:45.358: INFO: Number of nodes with available pods: 0
Sep  5 22:53:45.358: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:46.423: INFO: Number of nodes with available pods: 0
Sep  5 22:53:46.423: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:47.357: INFO: Number of nodes with available pods: 0
Sep  5 22:53:47.357: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:48.357: INFO: Number of nodes with available pods: 0
Sep  5 22:53:48.357: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:49.359: INFO: Number of nodes with available pods: 0
Sep  5 22:53:49.359: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:50.360: INFO: Number of nodes with available pods: 0
Sep  5 22:53:50.360: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:51.375: INFO: Number of nodes with available pods: 1
Sep  5 22:53:51.375: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep  5 22:53:51.447: INFO: Number of nodes with available pods: 1
Sep  5 22:53:51.447: INFO: Number of running nodes: 0, number of available pods: 1
Sep  5 22:53:52.455: INFO: Number of nodes with available pods: 0
Sep  5 22:53:52.455: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep  5 22:53:52.487: INFO: Number of nodes with available pods: 0
Sep  5 22:53:52.487: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:53.494: INFO: Number of nodes with available pods: 0
Sep  5 22:53:53.494: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:54.496: INFO: Number of nodes with available pods: 0
Sep  5 22:53:54.496: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:55.494: INFO: Number of nodes with available pods: 0
Sep  5 22:53:55.494: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:56.502: INFO: Number of nodes with available pods: 0
Sep  5 22:53:56.502: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:57.495: INFO: Number of nodes with available pods: 0
Sep  5 22:53:57.495: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:58.495: INFO: Number of nodes with available pods: 0
Sep  5 22:53:58.495: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:53:59.495: INFO: Number of nodes with available pods: 0
Sep  5 22:53:59.495: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:00.495: INFO: Number of nodes with available pods: 0
Sep  5 22:54:00.495: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:01.500: INFO: Number of nodes with available pods: 0
Sep  5 22:54:01.500: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:02.496: INFO: Number of nodes with available pods: 0
Sep  5 22:54:02.496: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:03.498: INFO: Number of nodes with available pods: 0
Sep  5 22:54:03.498: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:04.497: INFO: Number of nodes with available pods: 0
Sep  5 22:54:04.497: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:05.495: INFO: Number of nodes with available pods: 0
Sep  5 22:54:05.495: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:06.494: INFO: Number of nodes with available pods: 0
Sep  5 22:54:06.494: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:07.498: INFO: Number of nodes with available pods: 0
Sep  5 22:54:07.498: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:08.497: INFO: Number of nodes with available pods: 0
Sep  5 22:54:08.497: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:09.496: INFO: Number of nodes with available pods: 0
Sep  5 22:54:09.496: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:10.495: INFO: Number of nodes with available pods: 0
Sep  5 22:54:10.495: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:11.516: INFO: Number of nodes with available pods: 0
Sep  5 22:54:11.516: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:12.509: INFO: Number of nodes with available pods: 0
Sep  5 22:54:12.509: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:13.497: INFO: Number of nodes with available pods: 0
Sep  5 22:54:13.497: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:14.501: INFO: Number of nodes with available pods: 0
Sep  5 22:54:14.501: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:15.496: INFO: Number of nodes with available pods: 0
Sep  5 22:54:15.496: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:16.495: INFO: Number of nodes with available pods: 0
Sep  5 22:54:16.495: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:17.541: INFO: Number of nodes with available pods: 0
Sep  5 22:54:17.542: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:18.494: INFO: Number of nodes with available pods: 0
Sep  5 22:54:18.494: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:19.496: INFO: Number of nodes with available pods: 0
Sep  5 22:54:19.496: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:20.503: INFO: Number of nodes with available pods: 0
Sep  5 22:54:20.503: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:54:21.494: INFO: Number of nodes with available pods: 1
Sep  5 22:54:21.494: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3422, will wait for the garbage collector to delete the pods
Sep  5 22:54:21.605: INFO: Deleting DaemonSet.extensions daemon-set took: 43.562709ms
Sep  5 22:54:23.705: INFO: Terminating DaemonSet.extensions daemon-set pods took: 2.100248667s
Sep  5 22:54:38.912: INFO: Number of nodes with available pods: 0
Sep  5 22:54:38.912: INFO: Number of running nodes: 0, number of available pods: 0
Sep  5 22:54:38.918: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3422/daemonsets","resourceVersion":"150276"},"items":null}

Sep  5 22:54:38.925: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3422/pods","resourceVersion":"150276"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:54:38.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3422" for this suite.

â€¢ [SLOW TEST:73.539 seconds]
[sig-apps] Daemon set [Serial]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","total":291,"completed":151,"skipped":2559,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:54:39.159: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9421
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:54:39.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9421" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","total":291,"completed":152,"skipped":2585,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:54:39.809: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8715
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8715.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8715.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8715.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8715.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8715.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8715.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8715.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8715.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8715.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8715.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8715.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8715.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8715.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8715.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8715.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8715.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8715.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8715.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 22:55:14.413: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8715.svc.cluster.local from pod dns-8715/dns-test-74b06189-2ef3-4d5c-bf87-98f40cc96e8d: the server could not find the requested resource (get pods dns-test-74b06189-2ef3-4d5c-bf87-98f40cc96e8d)
Sep  5 22:55:14.429: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8715.svc.cluster.local from pod dns-8715/dns-test-74b06189-2ef3-4d5c-bf87-98f40cc96e8d: the server could not find the requested resource (get pods dns-test-74b06189-2ef3-4d5c-bf87-98f40cc96e8d)
Sep  5 22:55:14.442: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8715.svc.cluster.local from pod dns-8715/dns-test-74b06189-2ef3-4d5c-bf87-98f40cc96e8d: the server could not find the requested resource (get pods dns-test-74b06189-2ef3-4d5c-bf87-98f40cc96e8d)
Sep  5 22:55:14.452: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8715.svc.cluster.local from pod dns-8715/dns-test-74b06189-2ef3-4d5c-bf87-98f40cc96e8d: the server could not find the requested resource (get pods dns-test-74b06189-2ef3-4d5c-bf87-98f40cc96e8d)
Sep  5 22:55:14.509: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8715.svc.cluster.local from pod dns-8715/dns-test-74b06189-2ef3-4d5c-bf87-98f40cc96e8d: the server could not find the requested resource (get pods dns-test-74b06189-2ef3-4d5c-bf87-98f40cc96e8d)
Sep  5 22:55:14.522: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8715.svc.cluster.local from pod dns-8715/dns-test-74b06189-2ef3-4d5c-bf87-98f40cc96e8d: the server could not find the requested resource (get pods dns-test-74b06189-2ef3-4d5c-bf87-98f40cc96e8d)
Sep  5 22:55:14.539: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8715.svc.cluster.local from pod dns-8715/dns-test-74b06189-2ef3-4d5c-bf87-98f40cc96e8d: the server could not find the requested resource (get pods dns-test-74b06189-2ef3-4d5c-bf87-98f40cc96e8d)
Sep  5 22:55:14.550: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8715.svc.cluster.local from pod dns-8715/dns-test-74b06189-2ef3-4d5c-bf87-98f40cc96e8d: the server could not find the requested resource (get pods dns-test-74b06189-2ef3-4d5c-bf87-98f40cc96e8d)
Sep  5 22:55:14.576: INFO: Lookups using dns-8715/dns-test-74b06189-2ef3-4d5c-bf87-98f40cc96e8d failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8715.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8715.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8715.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8715.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8715.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8715.svc.cluster.local jessie_udp@dns-test-service-2.dns-8715.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8715.svc.cluster.local]

Sep  5 22:55:19.763: INFO: DNS probes using dns-8715/dns-test-74b06189-2ef3-4d5c-bf87-98f40cc96e8d succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:55:19.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8715" for this suite.

â€¢ [SLOW TEST:40.395 seconds]
[sig-network] DNS
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","total":291,"completed":153,"skipped":2589,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:55:20.205: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5743
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-04eda510-41f4-41fc-a909-6fb967e09f46
STEP: Creating a pod to test consume secrets
Sep  5 22:55:20.753: INFO: Waiting up to 5m0s for pod "pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4" in namespace "secrets-5743" to be "Succeeded or Failed"
Sep  5 22:55:20.763: INFO: Pod "pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.809577ms
Sep  5 22:55:22.781: INFO: Pod "pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027692817s
Sep  5 22:55:24.787: INFO: Pod "pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034069566s
Sep  5 22:55:26.798: INFO: Pod "pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.045311447s
Sep  5 22:55:28.808: INFO: Pod "pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.054842208s
Sep  5 22:55:30.817: INFO: Pod "pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.064053643s
Sep  5 22:55:32.837: INFO: Pod "pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.083760957s
Sep  5 22:55:34.845: INFO: Pod "pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.091760461s
Sep  5 22:55:36.857: INFO: Pod "pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.103546456s
Sep  5 22:55:38.864: INFO: Pod "pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.110467945s
Sep  5 22:55:40.871: INFO: Pod "pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.118106877s
STEP: Saw pod success
Sep  5 22:55:40.871: INFO: Pod "pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4" satisfied condition "Succeeded or Failed"
Sep  5 22:55:40.878: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com pod pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 22:55:40.945: INFO: Waiting for pod pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4 to disappear
Sep  5 22:55:40.964: INFO: Pod pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4 still exists
Sep  5 22:55:42.964: INFO: Waiting for pod pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4 to disappear
Sep  5 22:55:42.974: INFO: Pod pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4 still exists
Sep  5 22:55:44.965: INFO: Waiting for pod pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4 to disappear
Sep  5 22:55:44.974: INFO: Pod pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4 still exists
Sep  5 22:55:46.966: INFO: Waiting for pod pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4 to disappear
Sep  5 22:55:46.992: INFO: Pod pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4 still exists
Sep  5 22:55:48.965: INFO: Waiting for pod pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4 to disappear
Sep  5 22:55:48.976: INFO: Pod pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4 still exists
Sep  5 22:55:50.965: INFO: Waiting for pod pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4 to disappear
Sep  5 22:55:50.999: INFO: Pod pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4 still exists
Sep  5 22:55:52.966: INFO: Waiting for pod pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4 to disappear
Sep  5 22:55:52.977: INFO: Pod pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4 still exists
Sep  5 22:55:54.965: INFO: Waiting for pod pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4 to disappear
Sep  5 22:55:54.980: INFO: Pod pod-secrets-757891f9-7718-4b25-900b-28baa4c280b4 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:55:54.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5743" for this suite.

â€¢ [SLOW TEST:35.050 seconds]
[sig-storage] Secrets
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":291,"completed":154,"skipped":2607,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:55:55.256: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9415
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9415.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9415.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9415.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9415.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9415.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9415.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 22:56:26.049: INFO: DNS probes using dns-9415/dns-test-c77bdfe5-1b73-4330-81f8-979a0e27b8b0 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:56:26.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9415" for this suite.

â€¢ [SLOW TEST:31.181 seconds]
[sig-network] DNS
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]","total":291,"completed":155,"skipped":2646,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:56:26.437: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2351
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  5 22:56:26.852: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b" in namespace "downward-api-2351" to be "Succeeded or Failed"
Sep  5 22:56:26.869: INFO: Pod "downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.515137ms
Sep  5 22:56:28.877: INFO: Pod "downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025057454s
Sep  5 22:56:30.887: INFO: Pod "downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034865477s
Sep  5 22:56:32.901: INFO: Pod "downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048975342s
Sep  5 22:56:34.912: INFO: Pod "downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.060445897s
Sep  5 22:56:36.920: INFO: Pod "downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.06848583s
Sep  5 22:56:38.930: INFO: Pod "downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.078631035s
Sep  5 22:56:40.946: INFO: Pod "downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.093973562s
Sep  5 22:56:42.955: INFO: Pod "downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.103454509s
Sep  5 22:56:44.964: INFO: Pod "downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.112035856s
Sep  5 22:56:46.978: INFO: Pod "downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.12632845s
Sep  5 22:56:48.988: INFO: Pod "downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.136180394s
STEP: Saw pod success
Sep  5 22:56:48.988: INFO: Pod "downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b" satisfied condition "Succeeded or Failed"
Sep  5 22:56:48.999: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b container client-container: <nil>
STEP: delete the pod
Sep  5 22:56:53.401: INFO: Waiting for pod downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b to disappear
Sep  5 22:56:53.411: INFO: Pod downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b still exists
Sep  5 22:56:55.411: INFO: Waiting for pod downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b to disappear
Sep  5 22:56:55.419: INFO: Pod downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b still exists
Sep  5 22:56:57.411: INFO: Waiting for pod downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b to disappear
Sep  5 22:56:57.433: INFO: Pod downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b still exists
Sep  5 22:56:59.412: INFO: Waiting for pod downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b to disappear
Sep  5 22:56:59.422: INFO: Pod downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b still exists
Sep  5 22:57:01.411: INFO: Waiting for pod downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b to disappear
Sep  5 22:57:01.422: INFO: Pod downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b still exists
Sep  5 22:57:03.411: INFO: Waiting for pod downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b to disappear
Sep  5 22:57:03.419: INFO: Pod downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b still exists
Sep  5 22:57:05.411: INFO: Waiting for pod downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b to disappear
Sep  5 22:57:05.426: INFO: Pod downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b still exists
Sep  5 22:57:07.411: INFO: Waiting for pod downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b to disappear
Sep  5 22:57:07.420: INFO: Pod downwardapi-volume-a59dfdef-85d5-4cfe-94d5-44d7379a718b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:57:07.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2351" for this suite.

â€¢ [SLOW TEST:41.238 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":156,"skipped":2649,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:57:07.675: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1039
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 22:57:08.367: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep  5 22:57:08.401: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:08.402: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:08.402: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:08.416: INFO: Number of nodes with available pods: 0
Sep  5 22:57:08.416: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:09.445: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:09.445: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:09.445: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:09.467: INFO: Number of nodes with available pods: 0
Sep  5 22:57:09.467: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:10.489: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:10.489: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:10.489: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:10.502: INFO: Number of nodes with available pods: 0
Sep  5 22:57:10.502: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:11.430: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:11.430: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:11.430: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:11.440: INFO: Number of nodes with available pods: 0
Sep  5 22:57:11.440: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:12.428: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:12.428: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:12.428: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:12.437: INFO: Number of nodes with available pods: 0
Sep  5 22:57:12.437: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:13.448: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:13.448: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:13.448: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:13.468: INFO: Number of nodes with available pods: 0
Sep  5 22:57:13.468: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:14.460: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:14.460: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:14.460: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:14.471: INFO: Number of nodes with available pods: 0
Sep  5 22:57:14.471: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:15.430: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:15.430: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:15.430: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:15.441: INFO: Number of nodes with available pods: 0
Sep  5 22:57:15.441: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:16.429: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:16.429: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:16.429: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:16.437: INFO: Number of nodes with available pods: 0
Sep  5 22:57:16.437: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:17.431: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:17.431: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:17.431: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:17.438: INFO: Number of nodes with available pods: 0
Sep  5 22:57:17.439: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:18.466: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:18.466: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:18.466: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:18.488: INFO: Number of nodes with available pods: 0
Sep  5 22:57:18.488: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:19.434: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:19.434: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:19.434: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:19.462: INFO: Number of nodes with available pods: 0
Sep  5 22:57:19.462: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:20.434: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:20.435: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:20.435: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:20.441: INFO: Number of nodes with available pods: 0
Sep  5 22:57:20.442: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:21.436: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:21.436: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:21.436: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:21.445: INFO: Number of nodes with available pods: 0
Sep  5 22:57:21.445: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:22.431: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:22.431: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:22.431: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:22.440: INFO: Number of nodes with available pods: 0
Sep  5 22:57:22.440: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:23.444: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:23.444: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:23.444: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:23.453: INFO: Number of nodes with available pods: 0
Sep  5 22:57:23.453: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:24.428: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:24.428: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:24.428: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:24.436: INFO: Number of nodes with available pods: 0
Sep  5 22:57:24.436: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:25.436: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:25.437: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:25.437: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:25.458: INFO: Number of nodes with available pods: 0
Sep  5 22:57:25.458: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:26.430: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:26.430: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:26.430: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:26.440: INFO: Number of nodes with available pods: 0
Sep  5 22:57:26.440: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:27.431: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:27.431: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:27.431: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:27.437: INFO: Number of nodes with available pods: 0
Sep  5 22:57:27.437: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:28.456: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:28.456: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:28.456: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:28.464: INFO: Number of nodes with available pods: 0
Sep  5 22:57:28.464: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:29.428: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:29.428: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:29.428: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:29.442: INFO: Number of nodes with available pods: 1
Sep  5 22:57:29.442: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:30.430: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:30.430: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:30.430: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:30.439: INFO: Number of nodes with available pods: 1
Sep  5 22:57:30.439: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:31.428: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:31.428: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:31.428: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:31.445: INFO: Number of nodes with available pods: 2
Sep  5 22:57:31.445: INFO: Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com is running more than one daemon pod
Sep  5 22:57:32.428: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:32.428: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:32.428: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:32.437: INFO: Number of nodes with available pods: 3
Sep  5 22:57:32.437: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep  5 22:57:32.524: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:32.524: INFO: Wrong image for pod: daemon-set-8rd6d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:32.524: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:32.532: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:32.532: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:32.532: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:33.540: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:33.540: INFO: Wrong image for pod: daemon-set-8rd6d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:33.540: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:33.553: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:33.553: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:33.553: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:34.552: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:34.552: INFO: Wrong image for pod: daemon-set-8rd6d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:34.552: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:34.580: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:34.580: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:34.580: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:35.546: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:35.546: INFO: Wrong image for pod: daemon-set-8rd6d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:35.546: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:35.558: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:35.558: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:35.558: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:36.542: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:36.542: INFO: Wrong image for pod: daemon-set-8rd6d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:36.542: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:36.553: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:36.553: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:36.553: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:37.547: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:37.547: INFO: Wrong image for pod: daemon-set-8rd6d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:37.547: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:37.562: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:37.562: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:37.562: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:38.598: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:38.598: INFO: Wrong image for pod: daemon-set-8rd6d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:38.598: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:38.610: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:38.610: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:38.610: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:39.548: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:39.548: INFO: Wrong image for pod: daemon-set-8rd6d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:39.548: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:39.558: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:39.559: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:39.559: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:40.544: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:40.544: INFO: Wrong image for pod: daemon-set-8rd6d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:40.544: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:40.557: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:40.557: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:40.557: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:41.541: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:41.541: INFO: Wrong image for pod: daemon-set-8rd6d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:41.541: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:41.550: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:41.550: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:41.550: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:42.544: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:42.544: INFO: Wrong image for pod: daemon-set-8rd6d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:42.544: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:42.564: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:42.564: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:42.564: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:43.542: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:43.542: INFO: Wrong image for pod: daemon-set-8rd6d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:43.542: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:43.552: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:43.552: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:43.552: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:44.550: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:44.551: INFO: Wrong image for pod: daemon-set-8rd6d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:44.551: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:44.570: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:44.570: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:44.570: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:45.549: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:45.549: INFO: Wrong image for pod: daemon-set-8rd6d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:45.549: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:45.564: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:45.564: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:45.564: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:46.545: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:46.545: INFO: Wrong image for pod: daemon-set-8rd6d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:46.545: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:46.563: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:46.563: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:46.563: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:47.543: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:47.543: INFO: Wrong image for pod: daemon-set-8rd6d. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:47.543: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:47.578: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:47.578: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:47.578: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:48.541: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:48.541: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:48.541: INFO: Pod daemon-set-qdlrd is not available
Sep  5 22:57:48.550: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:48.550: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:48.550: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:49.545: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:49.545: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:49.545: INFO: Pod daemon-set-qdlrd is not available
Sep  5 22:57:49.555: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:49.556: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:49.556: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:50.551: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:50.551: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:50.551: INFO: Pod daemon-set-qdlrd is not available
Sep  5 22:57:50.576: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:50.576: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:50.576: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:51.545: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:51.545: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:51.545: INFO: Pod daemon-set-qdlrd is not available
Sep  5 22:57:51.559: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:51.559: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:51.559: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:52.554: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:52.554: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:52.554: INFO: Pod daemon-set-qdlrd is not available
Sep  5 22:57:52.568: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:52.569: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:52.569: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:53.541: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:53.541: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:53.541: INFO: Pod daemon-set-qdlrd is not available
Sep  5 22:57:53.559: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:53.559: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:53.559: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:54.554: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:54.554: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:54.554: INFO: Pod daemon-set-qdlrd is not available
Sep  5 22:57:54.585: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:54.585: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:54.586: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:55.542: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:55.542: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:55.542: INFO: Pod daemon-set-qdlrd is not available
Sep  5 22:57:55.560: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:55.560: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:55.561: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:56.543: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:56.543: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:56.543: INFO: Pod daemon-set-qdlrd is not available
Sep  5 22:57:56.571: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:56.572: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:56.572: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:57.565: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:57.565: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:57.565: INFO: Pod daemon-set-qdlrd is not available
Sep  5 22:57:57.588: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:57.588: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:57.588: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:58.543: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:58.543: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:58.543: INFO: Pod daemon-set-qdlrd is not available
Sep  5 22:57:58.555: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:58.555: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:58.555: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:59.544: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:59.544: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:57:59.544: INFO: Pod daemon-set-qdlrd is not available
Sep  5 22:57:59.553: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:59.553: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:57:59.553: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:00.546: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:00.546: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:00.546: INFO: Pod daemon-set-qdlrd is not available
Sep  5 22:58:00.561: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:00.561: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:00.561: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:01.541: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:01.541: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:01.541: INFO: Pod daemon-set-qdlrd is not available
Sep  5 22:58:01.559: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:01.559: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:01.559: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:02.542: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:02.542: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:02.542: INFO: Pod daemon-set-qdlrd is not available
Sep  5 22:58:02.562: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:02.562: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:02.562: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:03.551: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:03.551: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:03.551: INFO: Pod daemon-set-qdlrd is not available
Sep  5 22:58:03.572: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:03.573: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:03.573: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:04.557: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:04.557: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:04.557: INFO: Pod daemon-set-qdlrd is not available
Sep  5 22:58:04.583: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:04.584: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:04.584: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:05.547: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:05.547: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:05.547: INFO: Pod daemon-set-qdlrd is not available
Sep  5 22:58:05.564: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:05.565: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:05.565: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:06.544: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:06.544: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:06.545: INFO: Pod daemon-set-qdlrd is not available
Sep  5 22:58:06.562: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:06.563: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:06.563: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:07.550: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:07.550: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:07.559: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:07.559: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:07.560: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:08.544: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:08.544: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:08.554: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:08.554: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:08.554: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:09.552: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:09.552: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:09.568: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:09.568: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:09.568: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:10.543: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:10.543: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:10.558: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:10.558: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:10.558: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:11.550: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:11.550: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:11.567: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:11.567: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:11.567: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:12.562: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:12.562: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:12.573: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:12.573: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:12.573: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:13.553: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:13.553: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:13.566: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:13.566: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:13.567: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:14.542: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:14.542: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:14.552: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:14.552: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:14.552: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:15.542: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:15.543: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:15.558: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:15.558: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:15.558: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:16.542: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:16.542: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:16.552: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:16.552: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:16.552: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:17.543: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:17.543: INFO: Wrong image for pod: daemon-set-jx4kj. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:17.558: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:17.558: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:17.558: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:18.539: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:18.539: INFO: Pod daemon-set-tnglk is not available
Sep  5 22:58:18.548: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:18.548: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:18.549: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:19.549: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:19.549: INFO: Pod daemon-set-tnglk is not available
Sep  5 22:58:19.559: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:19.559: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:19.559: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:20.544: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:20.544: INFO: Pod daemon-set-tnglk is not available
Sep  5 22:58:20.554: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:20.555: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:20.555: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:21.571: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:21.571: INFO: Pod daemon-set-tnglk is not available
Sep  5 22:58:21.580: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:21.580: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:21.580: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:22.545: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:22.545: INFO: Pod daemon-set-tnglk is not available
Sep  5 22:58:22.556: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:22.556: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:22.556: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:23.541: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:23.541: INFO: Pod daemon-set-tnglk is not available
Sep  5 22:58:23.551: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:23.551: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:23.552: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:24.543: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:24.543: INFO: Pod daemon-set-tnglk is not available
Sep  5 22:58:24.551: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:24.551: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:24.551: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:25.542: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:25.542: INFO: Pod daemon-set-tnglk is not available
Sep  5 22:58:25.551: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:25.552: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:25.552: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:26.545: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:26.545: INFO: Pod daemon-set-tnglk is not available
Sep  5 22:58:26.554: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:26.555: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:26.555: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:27.544: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:27.544: INFO: Pod daemon-set-tnglk is not available
Sep  5 22:58:27.554: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:27.554: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:27.554: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:28.540: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:28.540: INFO: Pod daemon-set-tnglk is not available
Sep  5 22:58:28.549: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:28.549: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:28.549: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:29.542: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:29.543: INFO: Pod daemon-set-tnglk is not available
Sep  5 22:58:29.552: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:29.552: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:29.552: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:30.542: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:30.542: INFO: Pod daemon-set-tnglk is not available
Sep  5 22:58:30.553: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:30.553: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:30.553: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:31.544: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:31.544: INFO: Pod daemon-set-tnglk is not available
Sep  5 22:58:31.559: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:31.559: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:31.559: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:32.539: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:32.551: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:32.551: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:32.551: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:33.544: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:33.565: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:33.565: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:33.565: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:34.547: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:34.557: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:34.557: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:34.557: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:35.541: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:35.553: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:35.553: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:35.553: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:36.543: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:36.572: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:36.572: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:36.572: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:37.541: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:37.551: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:37.551: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:37.551: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:38.555: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:38.569: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:38.569: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:38.569: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:39.548: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:39.562: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:39.562: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:39.562: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:40.546: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:40.577: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:40.577: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:40.577: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:41.547: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:41.560: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:41.560: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:41.560: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:42.545: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:42.561: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:42.561: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:42.561: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:43.541: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:43.550: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:43.550: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:43.550: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:44.541: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:44.552: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:44.552: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:44.552: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:45.542: INFO: Wrong image for pod: daemon-set-4gjtc. Expected: k8s.gcr.io/e2e-test-images/agnhost:2.20, got: mirror.gcr.io/library/httpd:2.4.38-alpine.
Sep  5 22:58:45.553: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:45.553: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:45.553: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:46.581: INFO: Pod daemon-set-fjbhn is not available
Sep  5 22:58:46.629: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:46.629: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:46.629: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Sep  5 22:58:46.651: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:46.651: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:46.651: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:46.693: INFO: Number of nodes with available pods: 2
Sep  5 22:58:46.693: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 22:58:47.708: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:47.708: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:47.708: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:47.719: INFO: Number of nodes with available pods: 2
Sep  5 22:58:47.719: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 22:58:48.704: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:48.704: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:48.704: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:48.725: INFO: Number of nodes with available pods: 2
Sep  5 22:58:48.725: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 22:58:49.702: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:49.702: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:49.703: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:49.724: INFO: Number of nodes with available pods: 2
Sep  5 22:58:49.725: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 22:58:50.709: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:50.709: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:50.709: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:50.724: INFO: Number of nodes with available pods: 2
Sep  5 22:58:50.725: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 22:58:51.704: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:51.704: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:51.704: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:51.714: INFO: Number of nodes with available pods: 2
Sep  5 22:58:51.714: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 22:58:52.702: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:52.702: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:52.702: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:52.721: INFO: Number of nodes with available pods: 2
Sep  5 22:58:52.721: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 22:58:53.705: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:53.705: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:53.705: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:53.715: INFO: Number of nodes with available pods: 2
Sep  5 22:58:53.716: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 22:58:54.705: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:54.705: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:54.705: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:54.724: INFO: Number of nodes with available pods: 2
Sep  5 22:58:54.724: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 22:58:55.702: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:55.702: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:55.702: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:55.723: INFO: Number of nodes with available pods: 2
Sep  5 22:58:55.723: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 22:58:56.707: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:56.707: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:56.707: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:56.714: INFO: Number of nodes with available pods: 2
Sep  5 22:58:56.714: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 22:58:57.711: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:57.711: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:57.711: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:57.734: INFO: Number of nodes with available pods: 2
Sep  5 22:58:57.734: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 22:58:58.705: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:58.705: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:58.705: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:58.713: INFO: Number of nodes with available pods: 2
Sep  5 22:58:58.713: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 22:58:59.704: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:59.705: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:59.705: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:58:59.712: INFO: Number of nodes with available pods: 2
Sep  5 22:58:59.713: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 22:59:00.701: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:59:00.701: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:59:00.702: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:59:00.719: INFO: Number of nodes with available pods: 2
Sep  5 22:59:00.719: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 22:59:01.704: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:59:01.704: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:59:01.704: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:59:01.733: INFO: Number of nodes with available pods: 2
Sep  5 22:59:01.733: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 22:59:02.703: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:59:02.703: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:59:02.703: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:59:02.720: INFO: Number of nodes with available pods: 2
Sep  5 22:59:02.720: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 22:59:03.709: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:59:03.709: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:59:03.709: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:59:03.717: INFO: Number of nodes with available pods: 2
Sep  5 22:59:03.717: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 22:59:04.703: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:59:04.704: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:59:04.704: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 22:59:04.729: INFO: Number of nodes with available pods: 3
Sep  5 22:59:04.729: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1039, will wait for the garbage collector to delete the pods
Sep  5 22:59:04.847: INFO: Deleting DaemonSet.extensions daemon-set took: 23.936276ms
Sep  5 22:59:07.048: INFO: Terminating DaemonSet.extensions daemon-set pods took: 2.201123159s
Sep  5 22:59:18.062: INFO: Number of nodes with available pods: 0
Sep  5 22:59:18.062: INFO: Number of running nodes: 0, number of available pods: 0
Sep  5 22:59:18.071: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1039/daemonsets","resourceVersion":"153907"},"items":null}

Sep  5 22:59:18.080: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1039/pods","resourceVersion":"153907"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:59:18.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1039" for this suite.

â€¢ [SLOW TEST:130.840 seconds]
[sig-apps] Daemon set [Serial]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","total":291,"completed":157,"skipped":2659,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:59:18.515: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-6930
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 22:59:37.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6930" for this suite.

â€¢ [SLOW TEST:19.103 seconds]
[sig-api-machinery] ResourceQuota
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","total":291,"completed":158,"skipped":2663,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 22:59:37.618: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8407
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-c7f5
STEP: Creating a pod to test atomic-volume-subpath
Sep  5 22:59:38.162: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-c7f5" in namespace "subpath-8407" to be "Succeeded or Failed"
Sep  5 22:59:38.178: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.101233ms
Sep  5 22:59:40.194: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031723436s
Sep  5 22:59:42.204: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042324944s
Sep  5 22:59:44.213: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050436497s
Sep  5 22:59:46.226: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.064320014s
Sep  5 22:59:48.243: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.080761146s
Sep  5 22:59:50.264: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.101955644s
Sep  5 22:59:52.285: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.122469332s
Sep  5 22:59:54.300: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.137524792s
Sep  5 22:59:56.313: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.150782656s
Sep  5 22:59:58.327: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.164718252s
Sep  5 23:00:00.337: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Running", Reason="", readiness=true. Elapsed: 22.174418379s
Sep  5 23:00:02.362: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Running", Reason="", readiness=true. Elapsed: 24.199658196s
Sep  5 23:00:04.381: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Running", Reason="", readiness=true. Elapsed: 26.218845707s
Sep  5 23:00:06.391: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Running", Reason="", readiness=true. Elapsed: 28.228509032s
Sep  5 23:00:08.416: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Running", Reason="", readiness=true. Elapsed: 30.254169757s
Sep  5 23:00:10.427: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Running", Reason="", readiness=true. Elapsed: 32.265052276s
Sep  5 23:00:12.439: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Running", Reason="", readiness=true. Elapsed: 34.276573053s
Sep  5 23:00:14.448: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Running", Reason="", readiness=true. Elapsed: 36.28540766s
Sep  5 23:00:16.463: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Running", Reason="", readiness=true. Elapsed: 38.300764244s
Sep  5 23:00:18.473: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Running", Reason="", readiness=true. Elapsed: 40.310656106s
Sep  5 23:00:20.500: INFO: Pod "pod-subpath-test-configmap-c7f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 42.338227802s
STEP: Saw pod success
Sep  5 23:00:20.500: INFO: Pod "pod-subpath-test-configmap-c7f5" satisfied condition "Succeeded or Failed"
Sep  5 23:00:20.524: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-subpath-test-configmap-c7f5 container test-container-subpath-configmap-c7f5: <nil>
STEP: delete the pod
Sep  5 23:00:20.660: INFO: Waiting for pod pod-subpath-test-configmap-c7f5 to disappear
Sep  5 23:00:20.681: INFO: Pod pod-subpath-test-configmap-c7f5 still exists
Sep  5 23:00:22.682: INFO: Waiting for pod pod-subpath-test-configmap-c7f5 to disappear
Sep  5 23:00:22.691: INFO: Pod pod-subpath-test-configmap-c7f5 still exists
Sep  5 23:00:24.682: INFO: Waiting for pod pod-subpath-test-configmap-c7f5 to disappear
Sep  5 23:00:24.721: INFO: Pod pod-subpath-test-configmap-c7f5 still exists
Sep  5 23:00:26.682: INFO: Waiting for pod pod-subpath-test-configmap-c7f5 to disappear
Sep  5 23:00:26.691: INFO: Pod pod-subpath-test-configmap-c7f5 still exists
Sep  5 23:00:28.681: INFO: Waiting for pod pod-subpath-test-configmap-c7f5 to disappear
Sep  5 23:00:28.689: INFO: Pod pod-subpath-test-configmap-c7f5 still exists
Sep  5 23:00:30.682: INFO: Waiting for pod pod-subpath-test-configmap-c7f5 to disappear
Sep  5 23:00:30.703: INFO: Pod pod-subpath-test-configmap-c7f5 still exists
Sep  5 23:00:32.683: INFO: Waiting for pod pod-subpath-test-configmap-c7f5 to disappear
Sep  5 23:00:32.691: INFO: Pod pod-subpath-test-configmap-c7f5 still exists
Sep  5 23:00:34.682: INFO: Waiting for pod pod-subpath-test-configmap-c7f5 to disappear
Sep  5 23:00:34.692: INFO: Pod pod-subpath-test-configmap-c7f5 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-c7f5
Sep  5 23:00:34.692: INFO: Deleting pod "pod-subpath-test-configmap-c7f5" in namespace "subpath-8407"
[AfterEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:00:34.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8407" for this suite.

â€¢ [SLOW TEST:57.381 seconds]
[sig-storage] Subpath
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]","total":291,"completed":159,"skipped":2666,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API 
  should support creating Ingress API operations [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Ingress API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:00:35.000: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename ingress
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingress-1566
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support creating Ingress API operations [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep  5 23:00:35.579: INFO: starting watch
STEP: cluster-wide listing
STEP: cluster-wide watching
Sep  5 23:00:35.588: INFO: starting watch
STEP: patching
STEP: updating
Sep  5 23:00:35.620: INFO: waiting for watch events with expected annotations
Sep  5 23:00:35.620: INFO: saw patched and updated annotations
STEP: patching /status
STEP: updating /status
STEP: get /status
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] Ingress API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:00:35.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-1566" for this suite.
â€¢{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","total":291,"completed":160,"skipped":2694,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:00:35.993: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9882
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-7ef85fe5-01c5-4a62-85cd-6cb3517bbcf9
STEP: Creating a pod to test consume configMaps
Sep  5 23:00:36.502: INFO: Waiting up to 5m0s for pod "pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227" in namespace "configmap-9882" to be "Succeeded or Failed"
Sep  5 23:00:36.522: INFO: Pod "pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227": Phase="Pending", Reason="", readiness=false. Elapsed: 19.271564ms
Sep  5 23:00:38.537: INFO: Pod "pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034831824s
Sep  5 23:00:40.569: INFO: Pod "pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067055246s
Sep  5 23:00:42.587: INFO: Pod "pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227": Phase="Pending", Reason="", readiness=false. Elapsed: 6.084821457s
Sep  5 23:00:44.614: INFO: Pod "pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227": Phase="Pending", Reason="", readiness=false. Elapsed: 8.111306138s
Sep  5 23:00:46.625: INFO: Pod "pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227": Phase="Pending", Reason="", readiness=false. Elapsed: 10.122624562s
Sep  5 23:00:48.637: INFO: Pod "pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227": Phase="Pending", Reason="", readiness=false. Elapsed: 12.134257024s
Sep  5 23:00:50.646: INFO: Pod "pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227": Phase="Pending", Reason="", readiness=false. Elapsed: 14.143307558s
Sep  5 23:00:52.655: INFO: Pod "pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227": Phase="Pending", Reason="", readiness=false. Elapsed: 16.152461524s
Sep  5 23:00:54.676: INFO: Pod "pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227": Phase="Pending", Reason="", readiness=false. Elapsed: 18.173985767s
Sep  5 23:00:56.692: INFO: Pod "pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227": Phase="Pending", Reason="", readiness=false. Elapsed: 20.189737272s
Sep  5 23:00:58.714: INFO: Pod "pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227": Phase="Pending", Reason="", readiness=false. Elapsed: 22.211475809s
Sep  5 23:01:00.722: INFO: Pod "pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227": Phase="Pending", Reason="", readiness=false. Elapsed: 24.219768268s
Sep  5 23:01:02.741: INFO: Pod "pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227": Phase="Pending", Reason="", readiness=false. Elapsed: 26.238261793s
Sep  5 23:01:04.752: INFO: Pod "pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.249784041s
STEP: Saw pod success
Sep  5 23:01:04.752: INFO: Pod "pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227" satisfied condition "Succeeded or Failed"
Sep  5 23:01:04.761: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 23:01:04.836: INFO: Waiting for pod pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227 to disappear
Sep  5 23:01:04.858: INFO: Pod pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227 still exists
Sep  5 23:01:06.858: INFO: Waiting for pod pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227 to disappear
Sep  5 23:01:06.881: INFO: Pod pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227 still exists
Sep  5 23:01:08.858: INFO: Waiting for pod pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227 to disappear
Sep  5 23:01:08.864: INFO: Pod pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227 still exists
Sep  5 23:01:10.859: INFO: Waiting for pod pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227 to disappear
Sep  5 23:01:10.867: INFO: Pod pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227 still exists
Sep  5 23:01:12.860: INFO: Waiting for pod pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227 to disappear
Sep  5 23:01:12.872: INFO: Pod pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227 still exists
Sep  5 23:01:14.858: INFO: Waiting for pod pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227 to disappear
Sep  5 23:01:14.869: INFO: Pod pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227 still exists
Sep  5 23:01:16.858: INFO: Waiting for pod pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227 to disappear
Sep  5 23:01:16.867: INFO: Pod pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227 still exists
Sep  5 23:01:18.858: INFO: Waiting for pod pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227 to disappear
Sep  5 23:01:18.865: INFO: Pod pod-configmaps-0a4d6524-c6bf-4ba9-97fa-074f44f93227 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:01:18.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9882" for this suite.

â€¢ [SLOW TEST:43.144 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":291,"completed":161,"skipped":2695,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:01:19.137: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-5006
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:01:31.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5006" for this suite.

â€¢ [SLOW TEST:12.900 seconds]
[sig-api-machinery] ResourceQuota
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","total":291,"completed":162,"skipped":2697,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:01:32.038: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4822
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep  5 23:02:14.624: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 23:02:14.635: INFO: Pod pod-with-poststart-http-hook still exists
Sep  5 23:02:16.635: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 23:02:16.654: INFO: Pod pod-with-poststart-http-hook still exists
Sep  5 23:02:18.635: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 23:02:18.645: INFO: Pod pod-with-poststart-http-hook still exists
Sep  5 23:02:20.636: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 23:02:20.643: INFO: Pod pod-with-poststart-http-hook still exists
Sep  5 23:02:22.635: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 23:02:22.647: INFO: Pod pod-with-poststart-http-hook still exists
Sep  5 23:02:24.636: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep  5 23:02:24.646: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:02:24.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4822" for this suite.

â€¢ [SLOW TEST:52.922 seconds]
[k8s.io] Container Lifecycle Hook
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when create a pod with lifecycle hook
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","total":291,"completed":163,"skipped":2719,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:02:24.960: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9996
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep  5 23:02:25.421: INFO: Waiting up to 5m0s for pod "downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e" in namespace "downward-api-9996" to be "Succeeded or Failed"
Sep  5 23:02:25.432: INFO: Pod "downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.634279ms
Sep  5 23:02:27.439: INFO: Pod "downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018207198s
Sep  5 23:02:29.458: INFO: Pod "downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036898026s
Sep  5 23:02:31.482: INFO: Pod "downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06047592s
Sep  5 23:02:33.499: INFO: Pod "downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.078007068s
Sep  5 23:02:35.508: INFO: Pod "downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.087183201s
Sep  5 23:02:37.531: INFO: Pod "downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.110042055s
Sep  5 23:02:39.545: INFO: Pod "downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.124365565s
Sep  5 23:02:41.555: INFO: Pod "downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.134287087s
Sep  5 23:02:43.563: INFO: Pod "downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.141531905s
Sep  5 23:02:45.570: INFO: Pod "downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.148755545s
Sep  5 23:02:47.579: INFO: Pod "downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.158027889s
Sep  5 23:02:49.592: INFO: Pod "downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.170675623s
STEP: Saw pod success
Sep  5 23:02:49.592: INFO: Pod "downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e" satisfied condition "Succeeded or Failed"
Sep  5 23:02:49.603: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e container dapi-container: <nil>
STEP: delete the pod
Sep  5 23:02:49.727: INFO: Waiting for pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e to disappear
Sep  5 23:02:49.821: INFO: Pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e still exists
Sep  5 23:02:51.821: INFO: Waiting for pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e to disappear
Sep  5 23:02:51.830: INFO: Pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e still exists
Sep  5 23:02:53.821: INFO: Waiting for pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e to disappear
Sep  5 23:02:53.829: INFO: Pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e still exists
Sep  5 23:02:55.821: INFO: Waiting for pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e to disappear
Sep  5 23:02:55.828: INFO: Pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e still exists
Sep  5 23:02:57.822: INFO: Waiting for pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e to disappear
Sep  5 23:02:57.830: INFO: Pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e still exists
Sep  5 23:02:59.822: INFO: Waiting for pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e to disappear
Sep  5 23:02:59.830: INFO: Pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e still exists
Sep  5 23:03:01.822: INFO: Waiting for pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e to disappear
Sep  5 23:03:01.833: INFO: Pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e still exists
Sep  5 23:03:03.822: INFO: Waiting for pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e to disappear
Sep  5 23:03:03.842: INFO: Pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e still exists
Sep  5 23:03:05.822: INFO: Waiting for pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e to disappear
Sep  5 23:03:05.832: INFO: Pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e still exists
Sep  5 23:03:07.821: INFO: Waiting for pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e to disappear
Sep  5 23:03:07.829: INFO: Pod downward-api-6d85aec0-22f7-46de-a031-3d8145b8a01e no longer exists
[AfterEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:03:07.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9996" for this suite.

â€¢ [SLOW TEST:43.151 seconds]
[sig-node] Downward API
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","total":291,"completed":164,"skipped":2726,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:03:08.111: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-2333
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Sep  5 23:03:08.522: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 23:03:17.569: INFO: >>> kubeConfig: ./kconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:03:46.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2333" for this suite.

â€¢ [SLOW TEST:38.193 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","total":291,"completed":165,"skipped":2740,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:03:46.305: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6420
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 23:03:46.703: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:04:10.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6420" for this suite.

â€¢ [SLOW TEST:24.802 seconds]
[k8s.io] Pods
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","total":291,"completed":166,"skipped":2757,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:04:11.108: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8644
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-3a7aba60-12bf-48de-8af0-a7436e01481e
STEP: Creating a pod to test consume secrets
Sep  5 23:04:11.645: INFO: Waiting up to 5m0s for pod "pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51" in namespace "secrets-8644" to be "Succeeded or Failed"
Sep  5 23:04:11.711: INFO: Pod "pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51": Phase="Pending", Reason="", readiness=false. Elapsed: 65.623847ms
Sep  5 23:04:13.719: INFO: Pod "pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.073568058s
Sep  5 23:04:15.726: INFO: Pod "pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51": Phase="Pending", Reason="", readiness=false. Elapsed: 4.080824823s
Sep  5 23:04:17.738: INFO: Pod "pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51": Phase="Pending", Reason="", readiness=false. Elapsed: 6.093064171s
Sep  5 23:04:19.753: INFO: Pod "pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51": Phase="Pending", Reason="", readiness=false. Elapsed: 8.108085608s
Sep  5 23:04:21.768: INFO: Pod "pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51": Phase="Pending", Reason="", readiness=false. Elapsed: 10.12277585s
Sep  5 23:04:23.784: INFO: Pod "pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51": Phase="Pending", Reason="", readiness=false. Elapsed: 12.138858545s
Sep  5 23:04:25.792: INFO: Pod "pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51": Phase="Pending", Reason="", readiness=false. Elapsed: 14.146829136s
Sep  5 23:04:27.800: INFO: Pod "pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51": Phase="Pending", Reason="", readiness=false. Elapsed: 16.155406064s
Sep  5 23:04:29.810: INFO: Pod "pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51": Phase="Pending", Reason="", readiness=false. Elapsed: 18.165093404s
Sep  5 23:04:31.819: INFO: Pod "pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51": Phase="Pending", Reason="", readiness=false. Elapsed: 20.174470722s
Sep  5 23:04:33.828: INFO: Pod "pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51": Phase="Pending", Reason="", readiness=false. Elapsed: 22.183210179s
Sep  5 23:04:35.838: INFO: Pod "pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.192985505s
STEP: Saw pod success
Sep  5 23:04:35.838: INFO: Pod "pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51" satisfied condition "Succeeded or Failed"
Sep  5 23:04:35.845: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 23:04:40.074: INFO: Waiting for pod pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51 to disappear
Sep  5 23:04:40.094: INFO: Pod pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51 still exists
Sep  5 23:04:42.095: INFO: Waiting for pod pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51 to disappear
Sep  5 23:04:42.102: INFO: Pod pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51 still exists
Sep  5 23:04:44.095: INFO: Waiting for pod pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51 to disappear
Sep  5 23:04:44.102: INFO: Pod pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51 still exists
Sep  5 23:04:46.095: INFO: Waiting for pod pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51 to disappear
Sep  5 23:04:46.109: INFO: Pod pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51 still exists
Sep  5 23:04:48.095: INFO: Waiting for pod pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51 to disappear
Sep  5 23:04:48.102: INFO: Pod pod-secrets-ef6e6e46-bca9-409d-8a9f-2c4378af5e51 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:04:48.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8644" for this suite.

â€¢ [SLOW TEST:37.228 seconds]
[sig-storage] Secrets
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","total":291,"completed":167,"skipped":2768,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:04:48.336: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8003
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should provide secure master service  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:04:48.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8003" for this suite.
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786
â€¢{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","total":291,"completed":168,"skipped":2801,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:04:49.514: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9053
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-map-8c2fd4a3-6beb-4bb6-8b7a-a8e0288baafb
STEP: Creating a pod to test consume secrets
Sep  5 23:04:50.026: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e" in namespace "projected-9053" to be "Succeeded or Failed"
Sep  5 23:04:50.060: INFO: Pod "pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e": Phase="Pending", Reason="", readiness=false. Elapsed: 33.657929ms
Sep  5 23:04:52.070: INFO: Pod "pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043601275s
Sep  5 23:04:54.081: INFO: Pod "pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05545844s
Sep  5 23:04:56.099: INFO: Pod "pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072697193s
Sep  5 23:04:58.109: INFO: Pod "pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082585448s
Sep  5 23:05:00.140: INFO: Pod "pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.113896455s
Sep  5 23:05:02.161: INFO: Pod "pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.134924913s
Sep  5 23:05:04.182: INFO: Pod "pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.15572367s
Sep  5 23:05:06.196: INFO: Pod "pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.169780455s
Sep  5 23:05:08.206: INFO: Pod "pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.180207287s
Sep  5 23:05:10.214: INFO: Pod "pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.187709283s
Sep  5 23:05:12.231: INFO: Pod "pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.204932872s
Sep  5 23:05:14.249: INFO: Pod "pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e": Phase="Pending", Reason="", readiness=false. Elapsed: 24.222596507s
Sep  5 23:05:16.257: INFO: Pod "pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.23077623s
STEP: Saw pod success
Sep  5 23:05:16.257: INFO: Pod "pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e" satisfied condition "Succeeded or Failed"
Sep  5 23:05:16.263: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  5 23:05:16.403: INFO: Waiting for pod pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e to disappear
Sep  5 23:05:16.440: INFO: Pod pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e still exists
Sep  5 23:05:18.440: INFO: Waiting for pod pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e to disappear
Sep  5 23:05:18.451: INFO: Pod pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e still exists
Sep  5 23:05:20.440: INFO: Waiting for pod pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e to disappear
Sep  5 23:05:20.447: INFO: Pod pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e still exists
Sep  5 23:05:22.440: INFO: Waiting for pod pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e to disappear
Sep  5 23:05:22.447: INFO: Pod pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e still exists
Sep  5 23:05:24.440: INFO: Waiting for pod pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e to disappear
Sep  5 23:05:24.449: INFO: Pod pod-projected-secrets-16947b8b-1a7c-4238-b620-2efdc98ed50e no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:05:24.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9053" for this suite.

â€¢ [SLOW TEST:35.202 seconds]
[sig-storage] Projected secret
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":169,"skipped":2809,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:05:24.716: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-598
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a service externalname-service with the type=ExternalName in namespace services-598
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-598
Sep  5 23:05:52.404: INFO: Creating new exec pod
Sep  5 23:06:11.468: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=services-598 execpodcns7s -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Sep  5 23:06:12.280: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Sep  5 23:06:12.280: INFO: stdout: ""
Sep  5 23:06:12.281: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=services-598 execpodcns7s -- /bin/sh -x -c nc -zv -t -w 2 172.24.252.241 80'
Sep  5 23:06:12.545: INFO: stderr: "+ nc -zv -t -w 2 172.24.252.241 80\nConnection to 172.24.252.241 80 port [tcp/http] succeeded!\n"
Sep  5 23:06:12.545: INFO: stdout: ""
Sep  5 23:06:12.545: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:06:12.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-598" for this suite.
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:48.182 seconds]
[sig-network] Services
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","total":291,"completed":170,"skipped":2812,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:06:12.899: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-5859
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 23:06:15.150: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  5 23:06:17.215: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:06:19.232: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:06:21.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:06:23.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:06:25.222: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:06:27.223: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:06:29.238: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:06:31.225: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:06:33.225: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:06:35.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:06:37.223: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:06:39.223: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 6, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 23:06:42.288: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:06:42.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5859" for this suite.
STEP: Destroying namespace "webhook-5859-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:30.253 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","total":291,"completed":171,"skipped":2858,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:06:43.153: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5532
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 23:06:43.508: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Sep  5 23:06:45.610: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:06:46.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5532" for this suite.
â€¢{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","total":291,"completed":172,"skipped":2946,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:06:46.939: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2163
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2163.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2163.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2163.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2163.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 23:07:21.702: INFO: DNS probes using dns-test-76fe4579-5893-4751-b4f7-0087ad05a69b succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2163.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-2163.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2163.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-2163.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 23:07:41.966: INFO: DNS probes using dns-test-6e1b8bff-9ad8-4683-9f5f-053e8bb94b64 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2163.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-2163.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-2163.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-2163.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  5 23:08:02.293: INFO: DNS probes using dns-test-60b7615c-e296-44e5-a306-ff934faedcf8 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:08:02.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2163" for this suite.

â€¢ [SLOW TEST:75.747 seconds]
[sig-network] DNS
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","total":291,"completed":173,"skipped":2954,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:08:02.687: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2431
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-2431
STEP: creating service affinity-clusterip-transition in namespace services-2431
STEP: creating replication controller affinity-clusterip-transition in namespace services-2431
Sep  5 23:08:33.568: INFO: Creating new exec pod
Sep  5 23:08:52.637: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=services-2431 execpod-affinitybmktn -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip-transition 80'
Sep  5 23:08:53.113: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Sep  5 23:08:53.113: INFO: stdout: ""
Sep  5 23:08:53.114: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=services-2431 execpod-affinitybmktn -- /bin/sh -x -c nc -zv -t -w 2 172.24.50.141 80'
Sep  5 23:08:53.387: INFO: stderr: "+ nc -zv -t -w 2 172.24.50.141 80\nConnection to 172.24.50.141 80 port [tcp/http] succeeded!\n"
Sep  5 23:08:53.387: INFO: stdout: ""
Sep  5 23:08:53.429: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=services-2431 execpod-affinitybmktn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.24.50.141:80/ ; done'
Sep  5 23:08:54.063: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n"
Sep  5 23:08:54.063: INFO: stdout: "\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5"
Sep  5 23:08:54.063: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:08:54.063: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:08:54.063: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:08:54.063: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:08:54.063: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:08:54.063: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:08:54.063: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:08:54.063: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:08:54.063: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:08:54.063: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:08:54.063: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:08:54.063: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:08:54.063: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:08:54.063: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:08:54.063: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:08:54.063: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:24.064: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=services-2431 execpod-affinitybmktn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.24.50.141:80/ ; done'
Sep  5 23:09:24.613: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n"
Sep  5 23:09:24.613: INFO: stdout: "\naffinity-clusterip-transition-phhvx\naffinity-clusterip-transition-st2b7\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-phhvx\naffinity-clusterip-transition-st2b7\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-phhvx\naffinity-clusterip-transition-st2b7\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-phhvx\naffinity-clusterip-transition-st2b7\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-phhvx\naffinity-clusterip-transition-st2b7\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-phhvx"
Sep  5 23:09:24.613: INFO: Received response from host: affinity-clusterip-transition-phhvx
Sep  5 23:09:24.613: INFO: Received response from host: affinity-clusterip-transition-st2b7
Sep  5 23:09:24.613: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:24.613: INFO: Received response from host: affinity-clusterip-transition-phhvx
Sep  5 23:09:24.613: INFO: Received response from host: affinity-clusterip-transition-st2b7
Sep  5 23:09:24.613: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:24.613: INFO: Received response from host: affinity-clusterip-transition-phhvx
Sep  5 23:09:24.613: INFO: Received response from host: affinity-clusterip-transition-st2b7
Sep  5 23:09:24.613: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:24.613: INFO: Received response from host: affinity-clusterip-transition-phhvx
Sep  5 23:09:24.613: INFO: Received response from host: affinity-clusterip-transition-st2b7
Sep  5 23:09:24.613: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:24.613: INFO: Received response from host: affinity-clusterip-transition-phhvx
Sep  5 23:09:24.613: INFO: Received response from host: affinity-clusterip-transition-st2b7
Sep  5 23:09:24.613: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:24.613: INFO: Received response from host: affinity-clusterip-transition-phhvx
Sep  5 23:09:24.687: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=services-2431 execpod-affinitybmktn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.24.50.141:80/ ; done'
Sep  5 23:09:25.166: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n"
Sep  5 23:09:25.166: INFO: stdout: "\naffinity-clusterip-transition-st2b7\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-phhvx\naffinity-clusterip-transition-st2b7\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-phhvx\naffinity-clusterip-transition-st2b7\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-phhvx\naffinity-clusterip-transition-st2b7\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-phhvx\naffinity-clusterip-transition-st2b7\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-phhvx\naffinity-clusterip-transition-st2b7"
Sep  5 23:09:25.166: INFO: Received response from host: affinity-clusterip-transition-st2b7
Sep  5 23:09:25.166: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:25.166: INFO: Received response from host: affinity-clusterip-transition-phhvx
Sep  5 23:09:25.166: INFO: Received response from host: affinity-clusterip-transition-st2b7
Sep  5 23:09:25.166: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:25.166: INFO: Received response from host: affinity-clusterip-transition-phhvx
Sep  5 23:09:25.166: INFO: Received response from host: affinity-clusterip-transition-st2b7
Sep  5 23:09:25.166: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:25.166: INFO: Received response from host: affinity-clusterip-transition-phhvx
Sep  5 23:09:25.166: INFO: Received response from host: affinity-clusterip-transition-st2b7
Sep  5 23:09:25.166: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:25.166: INFO: Received response from host: affinity-clusterip-transition-phhvx
Sep  5 23:09:25.166: INFO: Received response from host: affinity-clusterip-transition-st2b7
Sep  5 23:09:25.166: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:25.166: INFO: Received response from host: affinity-clusterip-transition-phhvx
Sep  5 23:09:25.166: INFO: Received response from host: affinity-clusterip-transition-st2b7
Sep  5 23:09:55.167: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=services-2431 execpod-affinitybmktn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.24.50.141:80/ ; done'
Sep  5 23:09:55.666: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.50.141:80/\n"
Sep  5 23:09:55.666: INFO: stdout: "\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5\naffinity-clusterip-transition-9s5m5"
Sep  5 23:09:55.666: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:55.666: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:55.666: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:55.666: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:55.666: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:55.666: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:55.666: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:55.666: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:55.666: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:55.666: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:55.666: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:55.666: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:55.666: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:55.666: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:55.666: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:55.666: INFO: Received response from host: affinity-clusterip-transition-9s5m5
Sep  5 23:09:55.666: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-2431, will wait for the garbage collector to delete the pods
Sep  5 23:09:55.777: INFO: Deleting ReplicationController affinity-clusterip-transition took: 19.648228ms
Sep  5 23:09:57.878: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 2.100817426s
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:10:18.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2431" for this suite.
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:135.981 seconds]
[sig-network] Services
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","total":291,"completed":174,"skipped":2964,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:10:18.668: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5098
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:10:24.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5098" for this suite.

â€¢ [SLOW TEST:5.996 seconds]
[sig-api-machinery] Watchers
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","total":291,"completed":175,"skipped":2994,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:10:24.664: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2948
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:10:47.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2948" for this suite.

â€¢ [SLOW TEST:22.911 seconds]
[k8s.io] Kubelet
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a busybox Pod with hostAliases
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:137
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":176,"skipped":2999,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:10:47.576: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6639
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-8110455a-680e-4068-a95e-b41c41387e23
STEP: Creating a pod to test consume configMaps
Sep  5 23:10:48.286: INFO: Waiting up to 5m0s for pod "pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84" in namespace "configmap-6639" to be "Succeeded or Failed"
Sep  5 23:10:48.297: INFO: Pod "pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84": Phase="Pending", Reason="", readiness=false. Elapsed: 11.381133ms
Sep  5 23:10:50.306: INFO: Pod "pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019945704s
Sep  5 23:10:52.318: INFO: Pod "pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031580751s
Sep  5 23:10:54.325: INFO: Pod "pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84": Phase="Pending", Reason="", readiness=false. Elapsed: 6.03888448s
Sep  5 23:10:56.351: INFO: Pod "pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84": Phase="Pending", Reason="", readiness=false. Elapsed: 8.065143245s
Sep  5 23:10:58.383: INFO: Pod "pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84": Phase="Pending", Reason="", readiness=false. Elapsed: 10.09684549s
Sep  5 23:11:00.395: INFO: Pod "pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84": Phase="Pending", Reason="", readiness=false. Elapsed: 12.108589844s
Sep  5 23:11:02.405: INFO: Pod "pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84": Phase="Pending", Reason="", readiness=false. Elapsed: 14.11932716s
Sep  5 23:11:04.416: INFO: Pod "pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84": Phase="Pending", Reason="", readiness=false. Elapsed: 16.129506866s
Sep  5 23:11:06.424: INFO: Pod "pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84": Phase="Pending", Reason="", readiness=false. Elapsed: 18.13778392s
Sep  5 23:11:08.438: INFO: Pod "pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84": Phase="Pending", Reason="", readiness=false. Elapsed: 20.152027576s
Sep  5 23:11:10.447: INFO: Pod "pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84": Phase="Pending", Reason="", readiness=false. Elapsed: 22.161382305s
Sep  5 23:11:12.456: INFO: Pod "pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.170443666s
STEP: Saw pod success
Sep  5 23:11:12.457: INFO: Pod "pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84" satisfied condition "Succeeded or Failed"
Sep  5 23:11:12.464: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 23:11:12.524: INFO: Waiting for pod pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84 to disappear
Sep  5 23:11:12.538: INFO: Pod pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84 still exists
Sep  5 23:11:14.539: INFO: Waiting for pod pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84 to disappear
Sep  5 23:11:14.547: INFO: Pod pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84 still exists
Sep  5 23:11:16.538: INFO: Waiting for pod pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84 to disappear
Sep  5 23:11:16.545: INFO: Pod pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84 still exists
Sep  5 23:11:18.538: INFO: Waiting for pod pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84 to disappear
Sep  5 23:11:18.546: INFO: Pod pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84 still exists
Sep  5 23:11:20.538: INFO: Waiting for pod pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84 to disappear
Sep  5 23:11:20.546: INFO: Pod pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84 still exists
Sep  5 23:11:22.538: INFO: Waiting for pod pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84 to disappear
Sep  5 23:11:22.546: INFO: Pod pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84 still exists
Sep  5 23:11:24.539: INFO: Waiting for pod pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84 to disappear
Sep  5 23:11:24.546: INFO: Pod pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84 still exists
Sep  5 23:11:26.538: INFO: Waiting for pod pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84 to disappear
Sep  5 23:11:26.547: INFO: Pod pod-configmaps-f05fe697-1aaa-4c4c-bb1e-b4c533d09b84 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:11:26.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6639" for this suite.

â€¢ [SLOW TEST:39.226 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":291,"completed":177,"skipped":3003,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:11:26.802: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9404
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep  5 23:11:27.299: INFO: Waiting up to 5m0s for pod "downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42" in namespace "downward-api-9404" to be "Succeeded or Failed"
Sep  5 23:11:27.334: INFO: Pod "downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42": Phase="Pending", Reason="", readiness=false. Elapsed: 35.05573ms
Sep  5 23:11:29.349: INFO: Pod "downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049672658s
Sep  5 23:11:31.356: INFO: Pod "downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42": Phase="Pending", Reason="", readiness=false. Elapsed: 4.057376406s
Sep  5 23:11:33.368: INFO: Pod "downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42": Phase="Pending", Reason="", readiness=false. Elapsed: 6.06850939s
Sep  5 23:11:35.377: INFO: Pod "downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42": Phase="Pending", Reason="", readiness=false. Elapsed: 8.078215399s
Sep  5 23:11:37.387: INFO: Pod "downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42": Phase="Pending", Reason="", readiness=false. Elapsed: 10.087552618s
Sep  5 23:11:39.397: INFO: Pod "downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42": Phase="Pending", Reason="", readiness=false. Elapsed: 12.0983369s
Sep  5 23:11:41.409: INFO: Pod "downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42": Phase="Pending", Reason="", readiness=false. Elapsed: 14.109584617s
Sep  5 23:11:43.419: INFO: Pod "downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42": Phase="Pending", Reason="", readiness=false. Elapsed: 16.119496216s
Sep  5 23:11:45.431: INFO: Pod "downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42": Phase="Pending", Reason="", readiness=false. Elapsed: 18.131738883s
Sep  5 23:11:47.439: INFO: Pod "downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.140083631s
STEP: Saw pod success
Sep  5 23:11:47.439: INFO: Pod "downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42" satisfied condition "Succeeded or Failed"
Sep  5 23:11:47.445: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com pod downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42 container dapi-container: <nil>
STEP: delete the pod
Sep  5 23:11:47.516: INFO: Waiting for pod downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42 to disappear
Sep  5 23:11:47.529: INFO: Pod downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42 still exists
Sep  5 23:11:49.530: INFO: Waiting for pod downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42 to disappear
Sep  5 23:11:49.542: INFO: Pod downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42 still exists
Sep  5 23:11:51.530: INFO: Waiting for pod downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42 to disappear
Sep  5 23:11:51.540: INFO: Pod downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42 still exists
Sep  5 23:11:53.529: INFO: Waiting for pod downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42 to disappear
Sep  5 23:11:53.540: INFO: Pod downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42 still exists
Sep  5 23:11:55.530: INFO: Waiting for pod downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42 to disappear
Sep  5 23:11:55.541: INFO: Pod downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42 still exists
Sep  5 23:11:57.529: INFO: Waiting for pod downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42 to disappear
Sep  5 23:11:57.536: INFO: Pod downward-api-39e556b1-2b3a-47b7-a29e-b418a25a8e42 no longer exists
[AfterEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:11:57.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9404" for this suite.

â€¢ [SLOW TEST:30.974 seconds]
[sig-node] Downward API
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","total":291,"completed":178,"skipped":3011,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:11:57.777: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4323
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test substitution in container's command
Sep  5 23:11:58.248: INFO: Waiting up to 5m0s for pod "var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680" in namespace "var-expansion-4323" to be "Succeeded or Failed"
Sep  5 23:11:58.254: INFO: Pod "var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680": Phase="Pending", Reason="", readiness=false. Elapsed: 6.151857ms
Sep  5 23:12:00.262: INFO: Pod "var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01481167s
Sep  5 23:12:02.279: INFO: Pod "var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031031894s
Sep  5 23:12:04.304: INFO: Pod "var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056421156s
Sep  5 23:12:06.316: INFO: Pod "var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680": Phase="Pending", Reason="", readiness=false. Elapsed: 8.068654611s
Sep  5 23:12:08.326: INFO: Pod "var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680": Phase="Pending", Reason="", readiness=false. Elapsed: 10.078750783s
Sep  5 23:12:10.337: INFO: Pod "var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680": Phase="Pending", Reason="", readiness=false. Elapsed: 12.089738838s
Sep  5 23:12:12.354: INFO: Pod "var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680": Phase="Pending", Reason="", readiness=false. Elapsed: 14.106296418s
Sep  5 23:12:14.367: INFO: Pod "var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680": Phase="Pending", Reason="", readiness=false. Elapsed: 16.119552442s
Sep  5 23:12:16.377: INFO: Pod "var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680": Phase="Pending", Reason="", readiness=false. Elapsed: 18.12895526s
Sep  5 23:12:18.385: INFO: Pod "var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680": Phase="Pending", Reason="", readiness=false. Elapsed: 20.137891914s
Sep  5 23:12:20.396: INFO: Pod "var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680": Phase="Pending", Reason="", readiness=false. Elapsed: 22.148019617s
Sep  5 23:12:22.406: INFO: Pod "var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.158145661s
STEP: Saw pod success
Sep  5 23:12:22.406: INFO: Pod "var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680" satisfied condition "Succeeded or Failed"
Sep  5 23:12:22.419: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680 container dapi-container: <nil>
STEP: delete the pod
Sep  5 23:12:22.505: INFO: Waiting for pod var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680 to disappear
Sep  5 23:12:22.516: INFO: Pod var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680 still exists
Sep  5 23:12:24.516: INFO: Waiting for pod var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680 to disappear
Sep  5 23:12:24.529: INFO: Pod var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680 still exists
Sep  5 23:12:26.516: INFO: Waiting for pod var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680 to disappear
Sep  5 23:12:26.536: INFO: Pod var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680 still exists
Sep  5 23:12:28.516: INFO: Waiting for pod var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680 to disappear
Sep  5 23:12:28.529: INFO: Pod var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680 still exists
Sep  5 23:12:30.516: INFO: Waiting for pod var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680 to disappear
Sep  5 23:12:30.535: INFO: Pod var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680 still exists
Sep  5 23:12:32.516: INFO: Waiting for pod var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680 to disappear
Sep  5 23:12:32.527: INFO: Pod var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680 still exists
Sep  5 23:12:34.517: INFO: Waiting for pod var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680 to disappear
Sep  5 23:12:34.525: INFO: Pod var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680 still exists
Sep  5 23:12:36.517: INFO: Waiting for pod var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680 to disappear
Sep  5 23:12:36.525: INFO: Pod var-expansion-b349fffe-e99b-49bc-8d35-c3b3d0bdb680 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:12:36.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4323" for this suite.

â€¢ [SLOW TEST:39.036 seconds]
[k8s.io] Variable Expansion
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","total":291,"completed":179,"skipped":3011,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:12:36.813: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-8178
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:12:53.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8178" for this suite.

â€¢ [SLOW TEST:17.025 seconds]
[sig-api-machinery] ResourceQuota
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","total":291,"completed":180,"skipped":3015,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:12:53.838: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1624
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service in namespace services-1624
STEP: creating service affinity-clusterip in namespace services-1624
STEP: creating replication controller affinity-clusterip in namespace services-1624
Sep  5 23:13:24.471: INFO: Creating new exec pod
Sep  5 23:13:45.519: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=services-1624 execpod-affinitywkr4h -- /bin/sh -x -c nc -zv -t -w 2 affinity-clusterip 80'
Sep  5 23:13:45.932: INFO: stderr: "+ nc -zv -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Sep  5 23:13:45.932: INFO: stdout: ""
Sep  5 23:13:45.933: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=services-1624 execpod-affinitywkr4h -- /bin/sh -x -c nc -zv -t -w 2 172.24.12.9 80'
Sep  5 23:13:46.170: INFO: stderr: "+ nc -zv -t -w 2 172.24.12.9 80\nConnection to 172.24.12.9 80 port [tcp/http] succeeded!\n"
Sep  5 23:13:46.170: INFO: stdout: ""
Sep  5 23:13:46.170: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=services-1624 execpod-affinitywkr4h -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.24.12.9:80/ ; done'
Sep  5 23:13:46.696: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.12.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.12.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.12.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.12.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.12.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.12.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.12.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.12.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.12.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.12.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.12.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.12.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.12.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.12.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.12.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.24.12.9:80/\n"
Sep  5 23:13:46.696: INFO: stdout: "\naffinity-clusterip-clm7h\naffinity-clusterip-clm7h\naffinity-clusterip-clm7h\naffinity-clusterip-clm7h\naffinity-clusterip-clm7h\naffinity-clusterip-clm7h\naffinity-clusterip-clm7h\naffinity-clusterip-clm7h\naffinity-clusterip-clm7h\naffinity-clusterip-clm7h\naffinity-clusterip-clm7h\naffinity-clusterip-clm7h\naffinity-clusterip-clm7h\naffinity-clusterip-clm7h\naffinity-clusterip-clm7h\naffinity-clusterip-clm7h"
Sep  5 23:13:46.696: INFO: Received response from host: affinity-clusterip-clm7h
Sep  5 23:13:46.696: INFO: Received response from host: affinity-clusterip-clm7h
Sep  5 23:13:46.696: INFO: Received response from host: affinity-clusterip-clm7h
Sep  5 23:13:46.696: INFO: Received response from host: affinity-clusterip-clm7h
Sep  5 23:13:46.696: INFO: Received response from host: affinity-clusterip-clm7h
Sep  5 23:13:46.696: INFO: Received response from host: affinity-clusterip-clm7h
Sep  5 23:13:46.696: INFO: Received response from host: affinity-clusterip-clm7h
Sep  5 23:13:46.696: INFO: Received response from host: affinity-clusterip-clm7h
Sep  5 23:13:46.696: INFO: Received response from host: affinity-clusterip-clm7h
Sep  5 23:13:46.696: INFO: Received response from host: affinity-clusterip-clm7h
Sep  5 23:13:46.696: INFO: Received response from host: affinity-clusterip-clm7h
Sep  5 23:13:46.696: INFO: Received response from host: affinity-clusterip-clm7h
Sep  5 23:13:46.696: INFO: Received response from host: affinity-clusterip-clm7h
Sep  5 23:13:46.696: INFO: Received response from host: affinity-clusterip-clm7h
Sep  5 23:13:46.696: INFO: Received response from host: affinity-clusterip-clm7h
Sep  5 23:13:46.696: INFO: Received response from host: affinity-clusterip-clm7h
Sep  5 23:13:46.696: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-1624, will wait for the garbage collector to delete the pods
Sep  5 23:13:46.806: INFO: Deleting ReplicationController affinity-clusterip took: 13.928321ms
Sep  5 23:13:48.907: INFO: Terminating ReplicationController affinity-clusterip pods took: 2.100520476s
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:14:07.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1624" for this suite.
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:73.994 seconds]
[sig-network] Services
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","total":291,"completed":181,"skipped":3057,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:14:07.832: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7938
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 23:14:08.270: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Sep  5 23:14:16.902: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7938 create -f -'
Sep  5 23:14:18.006: INFO: stderr: ""
Sep  5 23:14:18.006: INFO: stdout: "e2e-test-crd-publish-openapi-7806-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  5 23:14:18.006: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7938 delete e2e-test-crd-publish-openapi-7806-crds test-foo'
Sep  5 23:14:18.274: INFO: stderr: ""
Sep  5 23:14:18.274: INFO: stdout: "e2e-test-crd-publish-openapi-7806-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Sep  5 23:14:18.274: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7938 apply -f -'
Sep  5 23:14:18.727: INFO: stderr: ""
Sep  5 23:14:18.727: INFO: stdout: "e2e-test-crd-publish-openapi-7806-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Sep  5 23:14:18.727: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7938 delete e2e-test-crd-publish-openapi-7806-crds test-foo'
Sep  5 23:14:18.937: INFO: stderr: ""
Sep  5 23:14:18.937: INFO: stdout: "e2e-test-crd-publish-openapi-7806-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Sep  5 23:14:18.941: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7938 create -f -'
Sep  5 23:14:19.334: INFO: rc: 1
Sep  5 23:14:19.334: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7938 apply -f -'
Sep  5 23:14:19.689: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Sep  5 23:14:19.689: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7938 create -f -'
Sep  5 23:14:20.088: INFO: rc: 1
Sep  5 23:14:20.088: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7938 apply -f -'
Sep  5 23:14:20.476: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Sep  5 23:14:20.476: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml explain e2e-test-crd-publish-openapi-7806-crds'
Sep  5 23:14:20.812: INFO: stderr: ""
Sep  5 23:14:20.812: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7806-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Sep  5 23:14:20.813: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml explain e2e-test-crd-publish-openapi-7806-crds.metadata'
Sep  5 23:14:21.198: INFO: stderr: ""
Sep  5 23:14:21.198: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7806-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     NOT return a 409 - instead, it will either return 201 Created or 500 with\n     Reason ServerTimeout indicating a unique name could not be found in the\n     time allotted, and the client should retry (optionally after the time\n     indicated in the Retry-After header).\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only.\n\n     DEPRECATED Kubernetes will stop propagating this field in 1.20 release and\n     the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Sep  5 23:14:21.198: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml explain e2e-test-crd-publish-openapi-7806-crds.spec'
Sep  5 23:14:21.549: INFO: stderr: ""
Sep  5 23:14:21.549: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7806-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Sep  5 23:14:21.549: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml explain e2e-test-crd-publish-openapi-7806-crds.spec.bars'
Sep  5 23:14:21.922: INFO: stderr: ""
Sep  5 23:14:21.922: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-7806-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Sep  5 23:14:21.922: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml explain e2e-test-crd-publish-openapi-7806-crds.spec.bars2'
Sep  5 23:14:22.311: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:14:30.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7938" for this suite.

â€¢ [SLOW TEST:23.343 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","total":291,"completed":182,"skipped":3058,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:14:31.175: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8583
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  5 23:14:31.717: INFO: Waiting up to 5m0s for pod "downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd" in namespace "projected-8583" to be "Succeeded or Failed"
Sep  5 23:14:31.727: INFO: Pod "downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.711482ms
Sep  5 23:14:33.740: INFO: Pod "downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023110644s
Sep  5 23:14:35.761: INFO: Pod "downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044066657s
Sep  5 23:14:37.783: INFO: Pod "downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.065867047s
Sep  5 23:14:39.795: INFO: Pod "downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.077443458s
Sep  5 23:14:41.814: INFO: Pod "downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.096420254s
Sep  5 23:14:43.821: INFO: Pod "downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.104125216s
Sep  5 23:14:45.832: INFO: Pod "downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.115259679s
Sep  5 23:14:47.839: INFO: Pod "downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.121627838s
Sep  5 23:14:49.854: INFO: Pod "downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.136787571s
Sep  5 23:14:51.867: INFO: Pod "downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd": Phase="Pending", Reason="", readiness=false. Elapsed: 20.14970988s
Sep  5 23:14:53.876: INFO: Pod "downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.158983363s
STEP: Saw pod success
Sep  5 23:14:53.876: INFO: Pod "downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd" satisfied condition "Succeeded or Failed"
Sep  5 23:14:53.882: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd container client-container: <nil>
STEP: delete the pod
Sep  5 23:14:58.693: INFO: Waiting for pod downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd to disappear
Sep  5 23:14:58.713: INFO: Pod downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd still exists
Sep  5 23:15:00.713: INFO: Waiting for pod downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd to disappear
Sep  5 23:15:00.728: INFO: Pod downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd still exists
Sep  5 23:15:02.714: INFO: Waiting for pod downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd to disappear
Sep  5 23:15:02.732: INFO: Pod downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd still exists
Sep  5 23:15:04.713: INFO: Waiting for pod downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd to disappear
Sep  5 23:15:04.737: INFO: Pod downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd still exists
Sep  5 23:15:06.713: INFO: Waiting for pod downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd to disappear
Sep  5 23:15:06.720: INFO: Pod downwardapi-volume-52082ce6-2293-44e1-8333-e3421b8049fd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:15:06.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8583" for this suite.

â€¢ [SLOW TEST:35.782 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide container's memory limit [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","total":291,"completed":183,"skipped":3091,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:15:06.958: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7814
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-b109b509-fa26-4a5b-950c-f8343802cda5
STEP: Creating a pod to test consume configMaps
Sep  5 23:15:07.566: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359" in namespace "projected-7814" to be "Succeeded or Failed"
Sep  5 23:15:07.575: INFO: Pod "pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359": Phase="Pending", Reason="", readiness=false. Elapsed: 8.835209ms
Sep  5 23:15:09.590: INFO: Pod "pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023255603s
Sep  5 23:15:11.604: INFO: Pod "pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03727168s
Sep  5 23:15:13.615: INFO: Pod "pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048918646s
Sep  5 23:15:15.625: INFO: Pod "pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359": Phase="Pending", Reason="", readiness=false. Elapsed: 8.058265029s
Sep  5 23:15:17.636: INFO: Pod "pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359": Phase="Pending", Reason="", readiness=false. Elapsed: 10.069798396s
Sep  5 23:15:19.646: INFO: Pod "pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359": Phase="Pending", Reason="", readiness=false. Elapsed: 12.079529361s
Sep  5 23:15:21.655: INFO: Pod "pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359": Phase="Pending", Reason="", readiness=false. Elapsed: 14.088388782s
Sep  5 23:15:23.667: INFO: Pod "pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359": Phase="Pending", Reason="", readiness=false. Elapsed: 16.100810532s
Sep  5 23:15:25.674: INFO: Pod "pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359": Phase="Pending", Reason="", readiness=false. Elapsed: 18.108060897s
Sep  5 23:15:27.694: INFO: Pod "pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359": Phase="Pending", Reason="", readiness=false. Elapsed: 20.12748231s
Sep  5 23:15:29.703: INFO: Pod "pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359": Phase="Pending", Reason="", readiness=false. Elapsed: 22.136322927s
Sep  5 23:15:31.722: INFO: Pod "pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359": Phase="Pending", Reason="", readiness=false. Elapsed: 24.155345649s
Sep  5 23:15:33.733: INFO: Pod "pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.167107198s
STEP: Saw pod success
Sep  5 23:15:33.734: INFO: Pod "pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359" satisfied condition "Succeeded or Failed"
Sep  5 23:15:33.747: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 23:15:38.970: INFO: Waiting for pod pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359 to disappear
Sep  5 23:15:38.994: INFO: Pod pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359 still exists
Sep  5 23:15:40.994: INFO: Waiting for pod pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359 to disappear
Sep  5 23:15:41.003: INFO: Pod pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359 still exists
Sep  5 23:15:42.994: INFO: Waiting for pod pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359 to disappear
Sep  5 23:15:43.002: INFO: Pod pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359 still exists
Sep  5 23:15:44.994: INFO: Waiting for pod pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359 to disappear
Sep  5 23:15:45.002: INFO: Pod pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359 still exists
Sep  5 23:15:46.994: INFO: Waiting for pod pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359 to disappear
Sep  5 23:15:47.002: INFO: Pod pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359 still exists
Sep  5 23:15:48.994: INFO: Waiting for pod pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359 to disappear
Sep  5 23:15:49.003: INFO: Pod pod-projected-configmaps-1d14bb26-4a8d-4a40-9574-a3d6ee644359 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:15:49.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7814" for this suite.

â€¢ [SLOW TEST:42.307 seconds]
[sig-storage] Projected configMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","total":291,"completed":184,"skipped":3099,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:15:49.265: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4023
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-upd-74b8e38f-00ec-4308-99ff-8ac597274f7b
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:16:17.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4023" for this suite.

â€¢ [SLOW TEST:28.888 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","total":291,"completed":185,"skipped":3099,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:16:18.154: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-5181
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: set up a multi version CRD
Sep  5 23:16:18.643: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:16:57.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5181" for this suite.

â€¢ [SLOW TEST:39.719 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","total":291,"completed":186,"skipped":3106,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:16:57.873: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-314
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing all events in all namespaces
STEP: patching the test event
STEP: fetching the test event
STEP: deleting the test event
STEP: listing all events in all namespaces
[AfterEach] [sig-api-machinery] Events
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:16:58.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-314" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Events should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":291,"completed":187,"skipped":3124,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:16:59.210: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6001
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-50cc7616-8d59-4256-9033-8b7cc2cf0acd
STEP: Creating a pod to test consume configMaps
Sep  5 23:16:59.647: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b" in namespace "projected-6001" to be "Succeeded or Failed"
Sep  5 23:16:59.668: INFO: Pod "pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.240471ms
Sep  5 23:17:01.680: INFO: Pod "pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032597581s
Sep  5 23:17:03.690: INFO: Pod "pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042954924s
Sep  5 23:17:05.720: INFO: Pod "pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072415243s
Sep  5 23:17:07.754: INFO: Pod "pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.106740073s
Sep  5 23:17:09.787: INFO: Pod "pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.140272244s
Sep  5 23:17:11.795: INFO: Pod "pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.147691454s
Sep  5 23:17:13.802: INFO: Pod "pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.155312898s
Sep  5 23:17:15.821: INFO: Pod "pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.173760599s
Sep  5 23:17:17.831: INFO: Pod "pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.184161109s
Sep  5 23:17:19.843: INFO: Pod "pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.1958452s
Sep  5 23:17:21.853: INFO: Pod "pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.205538374s
Sep  5 23:17:23.861: INFO: Pod "pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.214142949s
STEP: Saw pod success
Sep  5 23:17:23.861: INFO: Pod "pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b" satisfied condition "Succeeded or Failed"
Sep  5 23:17:23.867: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 23:17:28.701: INFO: Waiting for pod pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b to disappear
Sep  5 23:17:28.710: INFO: Pod pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b still exists
Sep  5 23:17:30.711: INFO: Waiting for pod pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b to disappear
Sep  5 23:17:30.721: INFO: Pod pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b still exists
Sep  5 23:17:32.712: INFO: Waiting for pod pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b to disappear
Sep  5 23:17:32.724: INFO: Pod pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b still exists
Sep  5 23:17:34.711: INFO: Waiting for pod pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b to disappear
Sep  5 23:17:34.727: INFO: Pod pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b still exists
Sep  5 23:17:36.711: INFO: Waiting for pod pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b to disappear
Sep  5 23:17:36.717: INFO: Pod pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b still exists
Sep  5 23:17:38.711: INFO: Waiting for pod pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b to disappear
Sep  5 23:17:38.720: INFO: Pod pod-projected-configmaps-629fbdc3-6204-44d4-ba0e-4397b964f71b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:17:38.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6001" for this suite.

â€¢ [SLOW TEST:39.725 seconds]
[sig-storage] Projected configMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","total":291,"completed":188,"skipped":3129,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:17:38.935: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1356
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should run and stop simple daemon [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep  5 23:17:39.621: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:39.621: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:39.621: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:39.632: INFO: Number of nodes with available pods: 0
Sep  5 23:17:39.632: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:40.646: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:40.646: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:40.646: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:40.661: INFO: Number of nodes with available pods: 0
Sep  5 23:17:40.661: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:41.647: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:41.647: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:41.647: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:41.670: INFO: Number of nodes with available pods: 0
Sep  5 23:17:41.670: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:42.646: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:42.647: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:42.647: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:42.664: INFO: Number of nodes with available pods: 0
Sep  5 23:17:42.665: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:43.662: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:43.662: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:43.662: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:43.670: INFO: Number of nodes with available pods: 0
Sep  5 23:17:43.670: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:44.645: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:44.645: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:44.645: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:44.657: INFO: Number of nodes with available pods: 0
Sep  5 23:17:44.658: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:45.685: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:45.685: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:45.685: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:45.695: INFO: Number of nodes with available pods: 0
Sep  5 23:17:45.695: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:46.646: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:46.646: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:46.646: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:46.654: INFO: Number of nodes with available pods: 0
Sep  5 23:17:46.654: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:47.642: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:47.642: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:47.642: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:47.673: INFO: Number of nodes with available pods: 0
Sep  5 23:17:47.673: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:48.647: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:48.647: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:48.647: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:48.681: INFO: Number of nodes with available pods: 0
Sep  5 23:17:48.681: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:49.647: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:49.647: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:49.647: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:49.654: INFO: Number of nodes with available pods: 0
Sep  5 23:17:49.654: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:50.661: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:50.662: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:50.662: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:50.773: INFO: Number of nodes with available pods: 0
Sep  5 23:17:50.773: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:51.647: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:51.647: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:51.647: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:51.660: INFO: Number of nodes with available pods: 0
Sep  5 23:17:51.660: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:52.641: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:52.641: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:52.641: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:52.649: INFO: Number of nodes with available pods: 0
Sep  5 23:17:52.649: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:53.643: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:53.643: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:53.643: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:53.653: INFO: Number of nodes with available pods: 0
Sep  5 23:17:53.653: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:54.650: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:54.650: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:54.650: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:54.675: INFO: Number of nodes with available pods: 0
Sep  5 23:17:54.675: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:55.647: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:55.647: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:55.647: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:55.665: INFO: Number of nodes with available pods: 0
Sep  5 23:17:55.665: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:56.647: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:56.647: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:56.647: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:56.655: INFO: Number of nodes with available pods: 0
Sep  5 23:17:56.655: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:57.647: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:57.647: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:57.647: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:57.655: INFO: Number of nodes with available pods: 0
Sep  5 23:17:57.655: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:58.650: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:58.650: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:58.650: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:58.661: INFO: Number of nodes with available pods: 0
Sep  5 23:17:58.661: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:17:59.661: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:59.661: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:59.661: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:17:59.671: INFO: Number of nodes with available pods: 0
Sep  5 23:17:59.671: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:00.642: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:00.642: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:00.642: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:00.657: INFO: Number of nodes with available pods: 0
Sep  5 23:18:00.657: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:01.649: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:01.649: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:01.649: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:01.665: INFO: Number of nodes with available pods: 0
Sep  5 23:18:01.665: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:02.664: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:02.664: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:02.664: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:02.675: INFO: Number of nodes with available pods: 0
Sep  5 23:18:02.675: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:03.642: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:03.642: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:03.642: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:03.652: INFO: Number of nodes with available pods: 0
Sep  5 23:18:03.652: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:04.646: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:04.646: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:04.646: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:04.663: INFO: Number of nodes with available pods: 0
Sep  5 23:18:04.663: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:05.656: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:05.656: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:05.656: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:05.663: INFO: Number of nodes with available pods: 0
Sep  5 23:18:05.663: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:06.686: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:06.686: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:06.686: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:06.736: INFO: Number of nodes with available pods: 0
Sep  5 23:18:06.736: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:07.644: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:07.644: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:07.644: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:07.655: INFO: Number of nodes with available pods: 1
Sep  5 23:18:07.655: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:08.651: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:08.651: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:08.651: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:08.683: INFO: Number of nodes with available pods: 2
Sep  5 23:18:08.683: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:09.644: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:09.644: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:09.644: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:09.653: INFO: Number of nodes with available pods: 2
Sep  5 23:18:09.653: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:10.643: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:10.643: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:10.643: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:10.651: INFO: Number of nodes with available pods: 3
Sep  5 23:18:10.651: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep  5 23:18:10.711: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:10.711: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:10.711: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:10.722: INFO: Number of nodes with available pods: 2
Sep  5 23:18:10.722: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:11.733: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:11.733: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:11.733: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:11.740: INFO: Number of nodes with available pods: 2
Sep  5 23:18:11.740: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:12.735: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:12.735: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:12.735: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:12.745: INFO: Number of nodes with available pods: 2
Sep  5 23:18:12.746: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:13.731: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:13.731: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:13.731: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:13.740: INFO: Number of nodes with available pods: 2
Sep  5 23:18:13.740: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:14.731: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:14.731: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:14.731: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:14.740: INFO: Number of nodes with available pods: 2
Sep  5 23:18:14.740: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:15.731: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:15.731: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:15.731: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:15.739: INFO: Number of nodes with available pods: 2
Sep  5 23:18:15.739: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:16.740: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:16.740: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:16.740: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:16.757: INFO: Number of nodes with available pods: 2
Sep  5 23:18:16.757: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:17.735: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:17.735: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:17.735: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:17.741: INFO: Number of nodes with available pods: 2
Sep  5 23:18:17.742: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:18.743: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:18.743: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:18.743: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:18.831: INFO: Number of nodes with available pods: 2
Sep  5 23:18:18.831: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:19.737: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:19.737: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:19.737: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:19.752: INFO: Number of nodes with available pods: 2
Sep  5 23:18:19.752: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:20.732: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:20.732: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:20.732: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:20.738: INFO: Number of nodes with available pods: 2
Sep  5 23:18:20.738: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:21.735: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:21.735: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:21.735: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:21.742: INFO: Number of nodes with available pods: 2
Sep  5 23:18:21.742: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:22.735: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:22.735: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:22.735: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:22.741: INFO: Number of nodes with available pods: 2
Sep  5 23:18:22.741: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:23.734: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:23.734: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:23.734: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:23.748: INFO: Number of nodes with available pods: 2
Sep  5 23:18:23.748: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:24.733: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:24.733: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:24.733: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:24.743: INFO: Number of nodes with available pods: 2
Sep  5 23:18:24.743: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:25.734: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:25.734: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:25.734: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:25.744: INFO: Number of nodes with available pods: 2
Sep  5 23:18:25.744: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:26.730: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:26.730: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:26.730: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:26.737: INFO: Number of nodes with available pods: 2
Sep  5 23:18:26.737: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:27.737: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:27.737: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:27.737: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:27.755: INFO: Number of nodes with available pods: 2
Sep  5 23:18:27.755: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:28.731: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:28.731: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:28.731: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:28.738: INFO: Number of nodes with available pods: 2
Sep  5 23:18:28.738: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:29.733: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:29.733: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:29.733: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:29.739: INFO: Number of nodes with available pods: 2
Sep  5 23:18:29.739: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:30.736: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:30.736: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:30.736: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:30.742: INFO: Number of nodes with available pods: 2
Sep  5 23:18:30.742: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:31.734: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:31.734: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:31.734: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:31.755: INFO: Number of nodes with available pods: 2
Sep  5 23:18:31.755: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:32.738: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:32.739: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:32.739: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:32.749: INFO: Number of nodes with available pods: 2
Sep  5 23:18:32.749: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:33.732: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:33.732: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:33.732: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:33.740: INFO: Number of nodes with available pods: 2
Sep  5 23:18:33.740: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:34.733: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:34.733: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:34.733: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:34.739: INFO: Number of nodes with available pods: 2
Sep  5 23:18:34.739: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:35.739: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:35.739: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:35.739: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:35.767: INFO: Number of nodes with available pods: 2
Sep  5 23:18:35.767: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:36.733: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:36.733: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:36.734: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:36.741: INFO: Number of nodes with available pods: 2
Sep  5 23:18:36.742: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:37.733: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:37.733: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:37.733: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:37.742: INFO: Number of nodes with available pods: 2
Sep  5 23:18:37.742: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:38.735: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:38.735: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:38.735: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:38.746: INFO: Number of nodes with available pods: 2
Sep  5 23:18:38.746: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:39.732: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:39.732: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:39.732: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:39.739: INFO: Number of nodes with available pods: 2
Sep  5 23:18:39.739: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:40.732: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:40.732: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:40.732: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:40.739: INFO: Number of nodes with available pods: 2
Sep  5 23:18:40.739: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:41.732: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:41.732: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:41.732: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:41.739: INFO: Number of nodes with available pods: 2
Sep  5 23:18:41.739: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:42.739: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:42.739: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:42.739: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:42.748: INFO: Number of nodes with available pods: 2
Sep  5 23:18:42.748: INFO: Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com is running more than one daemon pod
Sep  5 23:18:43.735: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:43.735: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:43.735: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  5 23:18:43.746: INFO: Number of nodes with available pods: 3
Sep  5 23:18:43.746: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1356, will wait for the garbage collector to delete the pods
Sep  5 23:18:43.837: INFO: Deleting DaemonSet.extensions daemon-set took: 21.842172ms
Sep  5 23:18:45.937: INFO: Terminating DaemonSet.extensions daemon-set pods took: 2.100153087s
Sep  5 23:19:00.952: INFO: Number of nodes with available pods: 0
Sep  5 23:19:00.952: INFO: Number of running nodes: 0, number of available pods: 0
Sep  5 23:19:00.957: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1356/daemonsets","resourceVersion":"169354"},"items":null}

Sep  5 23:19:00.966: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1356/pods","resourceVersion":"169354"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:19:01.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1356" for this suite.

â€¢ [SLOW TEST:82.328 seconds]
[sig-apps] Daemon set [Serial]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","total":291,"completed":189,"skipped":3134,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:19:01.263: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6300
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name secret-emptykey-test-92858162-3b8e-4094-ab45-ed225e90bfbf
[AfterEach] [sig-api-machinery] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:19:01.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6300" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]","total":291,"completed":190,"skipped":3136,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:19:02.016: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8162
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep  5 23:19:02.513: INFO: Waiting up to 5m0s for pod "pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a" in namespace "emptydir-8162" to be "Succeeded or Failed"
Sep  5 23:19:02.528: INFO: Pod "pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.405499ms
Sep  5 23:19:04.541: INFO: Pod "pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027436882s
Sep  5 23:19:06.560: INFO: Pod "pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046084165s
Sep  5 23:19:08.702: INFO: Pod "pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.18865962s
Sep  5 23:19:10.718: INFO: Pod "pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.204161437s
Sep  5 23:19:12.726: INFO: Pod "pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.212338807s
Sep  5 23:19:14.735: INFO: Pod "pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.221577275s
Sep  5 23:19:16.745: INFO: Pod "pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.23129569s
Sep  5 23:19:18.755: INFO: Pod "pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.241044925s
Sep  5 23:19:20.766: INFO: Pod "pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.252559434s
Sep  5 23:19:22.775: INFO: Pod "pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.261531808s
Sep  5 23:19:24.787: INFO: Pod "pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.273089939s
Sep  5 23:19:26.821: INFO: Pod "pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a": Phase="Pending", Reason="", readiness=false. Elapsed: 24.307785878s
Sep  5 23:19:28.829: INFO: Pod "pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.315605003s
STEP: Saw pod success
Sep  5 23:19:28.829: INFO: Pod "pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a" satisfied condition "Succeeded or Failed"
Sep  5 23:19:28.835: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a container test-container: <nil>
STEP: delete the pod
Sep  5 23:19:28.927: INFO: Waiting for pod pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a to disappear
Sep  5 23:19:28.976: INFO: Pod pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a still exists
Sep  5 23:19:30.977: INFO: Waiting for pod pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a to disappear
Sep  5 23:19:31.014: INFO: Pod pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a still exists
Sep  5 23:19:32.977: INFO: Waiting for pod pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a to disappear
Sep  5 23:19:32.988: INFO: Pod pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a still exists
Sep  5 23:19:34.976: INFO: Waiting for pod pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a to disappear
Sep  5 23:19:34.983: INFO: Pod pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a still exists
Sep  5 23:19:36.976: INFO: Waiting for pod pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a to disappear
Sep  5 23:19:36.986: INFO: Pod pod-9b05a748-d93d-4b88-80b1-eb6ceb27403a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:19:36.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8162" for this suite.

â€¢ [SLOW TEST:35.375 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":191,"skipped":3146,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:19:37.392: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8390
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Sep  5 23:19:59.128: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:19:59.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8390" for this suite.

â€¢ [SLOW TEST:21.982 seconds]
[k8s.io] Container Runtime
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    on terminated container
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:134
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","total":291,"completed":192,"skipped":3173,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:19:59.375: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5861
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Sep  5 23:22:06.891: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
[AfterEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:22:06.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5861" for this suite.

â€¢ [SLOW TEST:127.838 seconds]
[sig-api-machinery] Garbage collector
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","total":291,"completed":193,"skipped":3201,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Job
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:22:07.213: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-7138
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Sep  5 23:22:34.574: INFO: Successfully updated pod "adopt-release-72hqh"
STEP: Checking that the Job readopts the Pod
Sep  5 23:22:34.574: INFO: Waiting up to 15m0s for pod "adopt-release-72hqh" in namespace "job-7138" to be "adopted"
Sep  5 23:22:34.646: INFO: Pod "adopt-release-72hqh": Phase="Running", Reason="", readiness=true. Elapsed: 71.357652ms
Sep  5 23:22:36.657: INFO: Pod "adopt-release-72hqh": Phase="Running", Reason="", readiness=true. Elapsed: 2.08253314s
Sep  5 23:22:36.657: INFO: Pod "adopt-release-72hqh" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Sep  5 23:22:37.213: INFO: Successfully updated pod "adopt-release-72hqh"
STEP: Checking that the Job releases the Pod
Sep  5 23:22:37.213: INFO: Waiting up to 15m0s for pod "adopt-release-72hqh" in namespace "job-7138" to be "released"
Sep  5 23:22:37.270: INFO: Pod "adopt-release-72hqh": Phase="Running", Reason="", readiness=true. Elapsed: 56.951327ms
Sep  5 23:22:39.279: INFO: Pod "adopt-release-72hqh": Phase="Running", Reason="", readiness=true. Elapsed: 2.065306294s
Sep  5 23:22:39.279: INFO: Pod "adopt-release-72hqh" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:22:39.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7138" for this suite.

â€¢ [SLOW TEST:32.260 seconds]
[sig-apps] Job
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","total":291,"completed":194,"skipped":3219,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:22:39.473: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update labels on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Sep  5 23:23:02.734: INFO: Successfully updated pod "labelsupdateb8d3deb4-81e8-4294-81a3-c274c61b2b28"
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:23:04.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1230" for this suite.

â€¢ [SLOW TEST:25.548 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update labels on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","total":291,"completed":195,"skipped":3220,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:23:05.022: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8518
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-f0f6ba98-eda6-4b1d-afc2-1ee802c1bf94
STEP: Creating a pod to test consume secrets
Sep  5 23:23:05.482: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb" in namespace "projected-8518" to be "Succeeded or Failed"
Sep  5 23:23:05.499: INFO: Pod "pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.71503ms
Sep  5 23:23:07.528: INFO: Pod "pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045662731s
Sep  5 23:23:09.549: INFO: Pod "pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.066507579s
Sep  5 23:23:11.682: INFO: Pod "pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.199942713s
Sep  5 23:23:13.699: INFO: Pod "pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.216652143s
Sep  5 23:23:15.706: INFO: Pod "pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb": Phase="Pending", Reason="", readiness=false. Elapsed: 10.223465965s
Sep  5 23:23:17.714: INFO: Pod "pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.231556939s
Sep  5 23:23:19.723: INFO: Pod "pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb": Phase="Pending", Reason="", readiness=false. Elapsed: 14.240410601s
Sep  5 23:23:21.735: INFO: Pod "pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.252595298s
Sep  5 23:23:23.744: INFO: Pod "pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb": Phase="Pending", Reason="", readiness=false. Elapsed: 18.261545648s
Sep  5 23:23:25.759: INFO: Pod "pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb": Phase="Pending", Reason="", readiness=false. Elapsed: 20.277001205s
Sep  5 23:23:27.766: INFO: Pod "pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb": Phase="Pending", Reason="", readiness=false. Elapsed: 22.28437553s
Sep  5 23:23:29.785: INFO: Pod "pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb": Phase="Running", Reason="", readiness=true. Elapsed: 24.303166094s
Sep  5 23:23:31.794: INFO: Pod "pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb": Phase="Running", Reason="", readiness=true. Elapsed: 26.311910002s
Sep  5 23:23:33.802: INFO: Pod "pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb": Phase="Running", Reason="", readiness=true. Elapsed: 28.319445684s
Sep  5 23:23:35.810: INFO: Pod "pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.327772678s
STEP: Saw pod success
Sep  5 23:23:35.810: INFO: Pod "pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb" satisfied condition "Succeeded or Failed"
Sep  5 23:23:35.817: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  5 23:23:40.556: INFO: Waiting for pod pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb to disappear
Sep  5 23:23:40.569: INFO: Pod pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb still exists
Sep  5 23:23:42.570: INFO: Waiting for pod pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb to disappear
Sep  5 23:23:42.580: INFO: Pod pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb still exists
Sep  5 23:23:44.569: INFO: Waiting for pod pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb to disappear
Sep  5 23:23:44.577: INFO: Pod pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb still exists
Sep  5 23:23:46.569: INFO: Waiting for pod pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb to disappear
Sep  5 23:23:46.583: INFO: Pod pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb still exists
Sep  5 23:23:48.569: INFO: Waiting for pod pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb to disappear
Sep  5 23:23:48.579: INFO: Pod pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb still exists
Sep  5 23:23:50.569: INFO: Waiting for pod pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb to disappear
Sep  5 23:23:50.579: INFO: Pod pod-projected-secrets-5c4272ca-7f6e-4455-90d0-5a73b83336eb no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:23:50.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8518" for this suite.

â€¢ [SLOW TEST:45.742 seconds]
[sig-storage] Projected secret
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","total":291,"completed":196,"skipped":3223,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:23:50.764: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-1069
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:24:08.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1069" for this suite.

â€¢ [SLOW TEST:17.772 seconds]
[sig-api-machinery] ResourceQuota
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","total":291,"completed":197,"skipped":3256,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:24:08.537: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  5 23:24:09.082: INFO: Waiting up to 5m0s for pod "pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3" in namespace "emptydir-9374" to be "Succeeded or Failed"
Sep  5 23:24:09.104: INFO: Pod "pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3": Phase="Pending", Reason="", readiness=false. Elapsed: 21.692772ms
Sep  5 23:24:11.114: INFO: Pod "pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031690663s
Sep  5 23:24:13.132: INFO: Pod "pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050628281s
Sep  5 23:24:15.143: INFO: Pod "pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.060831763s
Sep  5 23:24:17.158: INFO: Pod "pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.075852335s
Sep  5 23:24:19.174: INFO: Pod "pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.091850742s
Sep  5 23:24:21.182: INFO: Pod "pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.100401016s
Sep  5 23:24:23.260: INFO: Pod "pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.178462731s
Sep  5 23:24:25.268: INFO: Pod "pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.186467555s
Sep  5 23:24:27.275: INFO: Pod "pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.193617988s
Sep  5 23:24:29.291: INFO: Pod "pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3": Phase="Running", Reason="", readiness=true. Elapsed: 20.209244794s
Sep  5 23:24:31.298: INFO: Pod "pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3": Phase="Running", Reason="", readiness=true. Elapsed: 22.216383833s
Sep  5 23:24:33.310: INFO: Pod "pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3": Phase="Running", Reason="", readiness=true. Elapsed: 24.2278555s
Sep  5 23:24:35.319: INFO: Pod "pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.237274246s
STEP: Saw pod success
Sep  5 23:24:35.319: INFO: Pod "pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3" satisfied condition "Succeeded or Failed"
Sep  5 23:24:35.326: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3 container test-container: <nil>
STEP: delete the pod
Sep  5 23:24:39.740: INFO: Waiting for pod pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3 to disappear
Sep  5 23:24:39.751: INFO: Pod pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3 still exists
Sep  5 23:24:41.751: INFO: Waiting for pod pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3 to disappear
Sep  5 23:24:41.772: INFO: Pod pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3 still exists
Sep  5 23:24:43.752: INFO: Waiting for pod pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3 to disappear
Sep  5 23:24:43.766: INFO: Pod pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3 still exists
Sep  5 23:24:45.752: INFO: Waiting for pod pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3 to disappear
Sep  5 23:24:45.760: INFO: Pod pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3 still exists
Sep  5 23:24:47.753: INFO: Waiting for pod pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3 to disappear
Sep  5 23:24:47.760: INFO: Pod pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3 still exists
Sep  5 23:24:49.752: INFO: Waiting for pod pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3 to disappear
Sep  5 23:24:49.760: INFO: Pod pod-efa78f9b-f657-4c2e-b9d8-67459f8633e3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:24:49.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9374" for this suite.

â€¢ [SLOW TEST:41.457 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":198,"skipped":3259,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery 
  should validate PreferredVersion for each APIGroup [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Discovery
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:24:49.994: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename discovery
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in discovery-9760
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Discovery
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/discovery.go:39
STEP: Setting up server cert
[It] should validate PreferredVersion for each APIGroup [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 23:24:51.152: INFO: Checking APIGroup: apiregistration.k8s.io
Sep  5 23:24:51.154: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Sep  5 23:24:51.154: INFO: Versions found [{apiregistration.k8s.io/v1 v1} {apiregistration.k8s.io/v1beta1 v1beta1}]
Sep  5 23:24:51.154: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Sep  5 23:24:51.154: INFO: Checking APIGroup: extensions
Sep  5 23:24:51.156: INFO: PreferredVersion.GroupVersion: extensions/v1beta1
Sep  5 23:24:51.156: INFO: Versions found [{extensions/v1beta1 v1beta1}]
Sep  5 23:24:51.156: INFO: extensions/v1beta1 matches extensions/v1beta1
Sep  5 23:24:51.156: INFO: Checking APIGroup: apps
Sep  5 23:24:51.158: INFO: PreferredVersion.GroupVersion: apps/v1
Sep  5 23:24:51.158: INFO: Versions found [{apps/v1 v1}]
Sep  5 23:24:51.158: INFO: apps/v1 matches apps/v1
Sep  5 23:24:51.158: INFO: Checking APIGroup: events.k8s.io
Sep  5 23:24:51.160: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Sep  5 23:24:51.160: INFO: Versions found [{events.k8s.io/v1 v1} {events.k8s.io/v1beta1 v1beta1}]
Sep  5 23:24:51.160: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Sep  5 23:24:51.160: INFO: Checking APIGroup: authentication.k8s.io
Sep  5 23:24:51.161: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Sep  5 23:24:51.161: INFO: Versions found [{authentication.k8s.io/v1 v1} {authentication.k8s.io/v1beta1 v1beta1}]
Sep  5 23:24:51.161: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Sep  5 23:24:51.161: INFO: Checking APIGroup: authorization.k8s.io
Sep  5 23:24:51.164: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Sep  5 23:24:51.164: INFO: Versions found [{authorization.k8s.io/v1 v1} {authorization.k8s.io/v1beta1 v1beta1}]
Sep  5 23:24:51.164: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Sep  5 23:24:51.164: INFO: Checking APIGroup: autoscaling
Sep  5 23:24:51.166: INFO: PreferredVersion.GroupVersion: autoscaling/v1
Sep  5 23:24:51.166: INFO: Versions found [{autoscaling/v1 v1} {autoscaling/v2beta1 v2beta1} {autoscaling/v2beta2 v2beta2}]
Sep  5 23:24:51.166: INFO: autoscaling/v1 matches autoscaling/v1
Sep  5 23:24:51.166: INFO: Checking APIGroup: batch
Sep  5 23:24:51.168: INFO: PreferredVersion.GroupVersion: batch/v1
Sep  5 23:24:51.168: INFO: Versions found [{batch/v1 v1} {batch/v1beta1 v1beta1}]
Sep  5 23:24:51.168: INFO: batch/v1 matches batch/v1
Sep  5 23:24:51.168: INFO: Checking APIGroup: certificates.k8s.io
Sep  5 23:24:51.170: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Sep  5 23:24:51.170: INFO: Versions found [{certificates.k8s.io/v1 v1} {certificates.k8s.io/v1beta1 v1beta1}]
Sep  5 23:24:51.170: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Sep  5 23:24:51.170: INFO: Checking APIGroup: networking.k8s.io
Sep  5 23:24:51.172: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Sep  5 23:24:51.172: INFO: Versions found [{networking.k8s.io/v1 v1} {networking.k8s.io/v1beta1 v1beta1}]
Sep  5 23:24:51.172: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Sep  5 23:24:51.172: INFO: Checking APIGroup: policy
Sep  5 23:24:51.174: INFO: PreferredVersion.GroupVersion: policy/v1beta1
Sep  5 23:24:51.174: INFO: Versions found [{policy/v1beta1 v1beta1}]
Sep  5 23:24:51.174: INFO: policy/v1beta1 matches policy/v1beta1
Sep  5 23:24:51.174: INFO: Checking APIGroup: rbac.authorization.k8s.io
Sep  5 23:24:51.176: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Sep  5 23:24:51.176: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1} {rbac.authorization.k8s.io/v1beta1 v1beta1}]
Sep  5 23:24:51.176: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Sep  5 23:24:51.176: INFO: Checking APIGroup: storage.k8s.io
Sep  5 23:24:51.178: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Sep  5 23:24:51.178: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Sep  5 23:24:51.178: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Sep  5 23:24:51.178: INFO: Checking APIGroup: admissionregistration.k8s.io
Sep  5 23:24:51.181: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Sep  5 23:24:51.181: INFO: Versions found [{admissionregistration.k8s.io/v1 v1} {admissionregistration.k8s.io/v1beta1 v1beta1}]
Sep  5 23:24:51.181: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Sep  5 23:24:51.181: INFO: Checking APIGroup: apiextensions.k8s.io
Sep  5 23:24:51.184: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Sep  5 23:24:51.184: INFO: Versions found [{apiextensions.k8s.io/v1 v1} {apiextensions.k8s.io/v1beta1 v1beta1}]
Sep  5 23:24:51.184: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Sep  5 23:24:51.184: INFO: Checking APIGroup: scheduling.k8s.io
Sep  5 23:24:51.186: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Sep  5 23:24:51.186: INFO: Versions found [{scheduling.k8s.io/v1 v1} {scheduling.k8s.io/v1beta1 v1beta1}]
Sep  5 23:24:51.186: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Sep  5 23:24:51.186: INFO: Checking APIGroup: coordination.k8s.io
Sep  5 23:24:51.190: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Sep  5 23:24:51.190: INFO: Versions found [{coordination.k8s.io/v1 v1} {coordination.k8s.io/v1beta1 v1beta1}]
Sep  5 23:24:51.190: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Sep  5 23:24:51.190: INFO: Checking APIGroup: node.k8s.io
Sep  5 23:24:51.191: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1beta1
Sep  5 23:24:51.191: INFO: Versions found [{node.k8s.io/v1beta1 v1beta1}]
Sep  5 23:24:51.191: INFO: node.k8s.io/v1beta1 matches node.k8s.io/v1beta1
Sep  5 23:24:51.191: INFO: Checking APIGroup: discovery.k8s.io
Sep  5 23:24:51.193: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1beta1
Sep  5 23:24:51.193: INFO: Versions found [{discovery.k8s.io/v1beta1 v1beta1}]
Sep  5 23:24:51.193: INFO: discovery.k8s.io/v1beta1 matches discovery.k8s.io/v1beta1
Sep  5 23:24:51.193: INFO: Checking APIGroup: crd.projectcalico.org
Sep  5 23:24:51.194: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Sep  5 23:24:51.194: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Sep  5 23:24:51.194: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Sep  5 23:24:51.194: INFO: Checking APIGroup: imagecontroller.vmware.com
Sep  5 23:24:51.195: INFO: PreferredVersion.GroupVersion: imagecontroller.vmware.com/v1
Sep  5 23:24:51.195: INFO: Versions found [{imagecontroller.vmware.com/v1 v1}]
Sep  5 23:24:51.195: INFO: imagecontroller.vmware.com/v1 matches imagecontroller.vmware.com/v1
Sep  5 23:24:51.195: INFO: Checking APIGroup: k8s.cni.cncf.io
Sep  5 23:24:51.196: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Sep  5 23:24:51.196: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Sep  5 23:24:51.196: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Sep  5 23:24:51.196: INFO: Checking APIGroup: nsx.vmware.com
Sep  5 23:24:51.198: INFO: PreferredVersion.GroupVersion: nsx.vmware.com/v1
Sep  5 23:24:51.198: INFO: Versions found [{nsx.vmware.com/v1 v1} {nsx.vmware.com/v1alpha1 v1alpha1}]
Sep  5 23:24:51.198: INFO: nsx.vmware.com/v1 matches nsx.vmware.com/v1
Sep  5 23:24:51.198: INFO: Checking APIGroup: appplatform.wcp.vmware.com
Sep  5 23:24:51.200: INFO: PreferredVersion.GroupVersion: appplatform.wcp.vmware.com/v1beta1
Sep  5 23:24:51.200: INFO: Versions found [{appplatform.wcp.vmware.com/v1beta1 v1beta1} {appplatform.wcp.vmware.com/v1alpha2 v1alpha2} {appplatform.wcp.vmware.com/v1alpha1 v1alpha1}]
Sep  5 23:24:51.200: INFO: appplatform.wcp.vmware.com/v1beta1 matches appplatform.wcp.vmware.com/v1beta1
Sep  5 23:24:51.200: INFO: Checking APIGroup: cns.vmware.com
Sep  5 23:24:51.201: INFO: PreferredVersion.GroupVersion: cns.vmware.com/v1alpha1
Sep  5 23:24:51.201: INFO: Versions found [{cns.vmware.com/v1alpha1 v1alpha1}]
Sep  5 23:24:51.201: INFO: cns.vmware.com/v1alpha1 matches cns.vmware.com/v1alpha1
Sep  5 23:24:51.201: INFO: Checking APIGroup: installers.tmc.cloud.vmware.com
Sep  5 23:24:51.203: INFO: PreferredVersion.GroupVersion: installers.tmc.cloud.vmware.com/v1alpha1
Sep  5 23:24:51.203: INFO: Versions found [{installers.tmc.cloud.vmware.com/v1alpha1 v1alpha1}]
Sep  5 23:24:51.203: INFO: installers.tmc.cloud.vmware.com/v1alpha1 matches installers.tmc.cloud.vmware.com/v1alpha1
Sep  5 23:24:51.203: INFO: Checking APIGroup: licenseoperator.vmware.com
Sep  5 23:24:51.205: INFO: PreferredVersion.GroupVersion: licenseoperator.vmware.com/v1alpha1
Sep  5 23:24:51.205: INFO: Versions found [{licenseoperator.vmware.com/v1alpha1 v1alpha1}]
Sep  5 23:24:51.205: INFO: licenseoperator.vmware.com/v1alpha1 matches licenseoperator.vmware.com/v1alpha1
Sep  5 23:24:51.205: INFO: Checking APIGroup: netoperator.vmware.com
Sep  5 23:24:51.209: INFO: PreferredVersion.GroupVersion: netoperator.vmware.com/v1alpha1
Sep  5 23:24:51.209: INFO: Versions found [{netoperator.vmware.com/v1alpha1 v1alpha1}]
Sep  5 23:24:51.209: INFO: netoperator.vmware.com/v1alpha1 matches netoperator.vmware.com/v1alpha1
Sep  5 23:24:51.209: INFO: Checking APIGroup: registryagent.vmware.com
Sep  5 23:24:51.210: INFO: PreferredVersion.GroupVersion: registryagent.vmware.com/v1alpha1
Sep  5 23:24:51.210: INFO: Versions found [{registryagent.vmware.com/v1alpha1 v1alpha1}]
Sep  5 23:24:51.210: INFO: registryagent.vmware.com/v1alpha1 matches registryagent.vmware.com/v1alpha1
Sep  5 23:24:51.210: INFO: Checking APIGroup: run.tanzu.vmware.com
Sep  5 23:24:51.212: INFO: PreferredVersion.GroupVersion: run.tanzu.vmware.com/v1alpha2
Sep  5 23:24:51.212: INFO: Versions found [{run.tanzu.vmware.com/v1alpha2 v1alpha2} {run.tanzu.vmware.com/v1alpha1 v1alpha1}]
Sep  5 23:24:51.212: INFO: run.tanzu.vmware.com/v1alpha2 matches run.tanzu.vmware.com/v1alpha2
Sep  5 23:24:51.212: INFO: Checking APIGroup: topology.tanzu.vmware.com
Sep  5 23:24:51.216: INFO: PreferredVersion.GroupVersion: topology.tanzu.vmware.com/v1alpha1
Sep  5 23:24:51.216: INFO: Versions found [{topology.tanzu.vmware.com/v1alpha1 v1alpha1}]
Sep  5 23:24:51.216: INFO: topology.tanzu.vmware.com/v1alpha1 matches topology.tanzu.vmware.com/v1alpha1
Sep  5 23:24:51.216: INFO: Checking APIGroup: vmoperator.vmware.com
Sep  5 23:24:51.218: INFO: PreferredVersion.GroupVersion: vmoperator.vmware.com/v1alpha1
Sep  5 23:24:51.218: INFO: Versions found [{vmoperator.vmware.com/v1alpha1 v1alpha1}]
Sep  5 23:24:51.218: INFO: vmoperator.vmware.com/v1alpha1 matches vmoperator.vmware.com/v1alpha1
Sep  5 23:24:51.218: INFO: Checking APIGroup: vmware.com
Sep  5 23:24:51.220: INFO: PreferredVersion.GroupVersion: vmware.com/v1alpha1
Sep  5 23:24:51.220: INFO: Versions found [{vmware.com/v1alpha1 v1alpha1}]
Sep  5 23:24:51.220: INFO: vmware.com/v1alpha1 matches vmware.com/v1alpha1
Sep  5 23:24:51.220: INFO: Checking APIGroup: networking.x-k8s.io
Sep  5 23:24:51.223: INFO: PreferredVersion.GroupVersion: networking.x-k8s.io/v1alpha1pre1
Sep  5 23:24:51.223: INFO: Versions found [{networking.x-k8s.io/v1alpha1pre1 v1alpha1pre1}]
Sep  5 23:24:51.223: INFO: networking.x-k8s.io/v1alpha1pre1 matches networking.x-k8s.io/v1alpha1pre1
Sep  5 23:24:51.223: INFO: Checking APIGroup: acme.cert-manager.io
Sep  5 23:24:51.225: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1alpha3
Sep  5 23:24:51.225: INFO: Versions found [{acme.cert-manager.io/v1alpha3 v1alpha3} {acme.cert-manager.io/v1alpha2 v1alpha2}]
Sep  5 23:24:51.225: INFO: acme.cert-manager.io/v1alpha3 matches acme.cert-manager.io/v1alpha3
Sep  5 23:24:51.225: INFO: Checking APIGroup: bootstrap.cluster.x-k8s.io
Sep  5 23:24:51.226: INFO: PreferredVersion.GroupVersion: bootstrap.cluster.x-k8s.io/v1alpha3
Sep  5 23:24:51.226: INFO: Versions found [{bootstrap.cluster.x-k8s.io/v1alpha3 v1alpha3} {bootstrap.cluster.x-k8s.io/v1alpha2 v1alpha2}]
Sep  5 23:24:51.226: INFO: bootstrap.cluster.x-k8s.io/v1alpha3 matches bootstrap.cluster.x-k8s.io/v1alpha3
Sep  5 23:24:51.226: INFO: Checking APIGroup: cert-manager.io
Sep  5 23:24:51.228: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1alpha3
Sep  5 23:24:51.228: INFO: Versions found [{cert-manager.io/v1alpha3 v1alpha3} {cert-manager.io/v1alpha2 v1alpha2}]
Sep  5 23:24:51.228: INFO: cert-manager.io/v1alpha3 matches cert-manager.io/v1alpha3
Sep  5 23:24:51.228: INFO: Checking APIGroup: cluster.x-k8s.io
Sep  5 23:24:51.230: INFO: PreferredVersion.GroupVersion: cluster.x-k8s.io/v1alpha3
Sep  5 23:24:51.230: INFO: Versions found [{cluster.x-k8s.io/v1alpha3 v1alpha3} {cluster.x-k8s.io/v1alpha2 v1alpha2}]
Sep  5 23:24:51.230: INFO: cluster.x-k8s.io/v1alpha3 matches cluster.x-k8s.io/v1alpha3
Sep  5 23:24:51.230: INFO: Checking APIGroup: infrastructure.cluster.vmware.com
Sep  5 23:24:51.231: INFO: PreferredVersion.GroupVersion: infrastructure.cluster.vmware.com/v1alpha3
Sep  5 23:24:51.231: INFO: Versions found [{infrastructure.cluster.vmware.com/v1alpha3 v1alpha3} {infrastructure.cluster.vmware.com/v1alpha2 v1alpha2}]
Sep  5 23:24:51.231: INFO: infrastructure.cluster.vmware.com/v1alpha3 matches infrastructure.cluster.vmware.com/v1alpha3
Sep  5 23:24:51.231: INFO: Checking APIGroup: addons.cluster.x-k8s.io
Sep  5 23:24:51.233: INFO: PreferredVersion.GroupVersion: addons.cluster.x-k8s.io/v1alpha3
Sep  5 23:24:51.233: INFO: Versions found [{addons.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Sep  5 23:24:51.233: INFO: addons.cluster.x-k8s.io/v1alpha3 matches addons.cluster.x-k8s.io/v1alpha3
Sep  5 23:24:51.233: INFO: Checking APIGroup: controlplane.cluster.x-k8s.io
Sep  5 23:24:51.234: INFO: PreferredVersion.GroupVersion: controlplane.cluster.x-k8s.io/v1alpha3
Sep  5 23:24:51.235: INFO: Versions found [{controlplane.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Sep  5 23:24:51.235: INFO: controlplane.cluster.x-k8s.io/v1alpha3 matches controlplane.cluster.x-k8s.io/v1alpha3
Sep  5 23:24:51.235: INFO: Checking APIGroup: exp.cluster.x-k8s.io
Sep  5 23:24:51.236: INFO: PreferredVersion.GroupVersion: exp.cluster.x-k8s.io/v1alpha3
Sep  5 23:24:51.236: INFO: Versions found [{exp.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Sep  5 23:24:51.236: INFO: exp.cluster.x-k8s.io/v1alpha3 matches exp.cluster.x-k8s.io/v1alpha3
Sep  5 23:24:51.236: INFO: Checking APIGroup: psp.wcp.vmware.com
Sep  5 23:24:51.239: INFO: PreferredVersion.GroupVersion: psp.wcp.vmware.com/v1beta1
Sep  5 23:24:51.239: INFO: Versions found [{psp.wcp.vmware.com/v1beta1 v1beta1}]
Sep  5 23:24:51.239: INFO: psp.wcp.vmware.com/v1beta1 matches psp.wcp.vmware.com/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:24:51.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-9760" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","total":291,"completed":199,"skipped":3269,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:24:51.410: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7333
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name projected-secret-test-3cfea219-09fc-48e6-b4aa-867937c0ced9
STEP: Creating a pod to test consume secrets
Sep  5 23:24:51.897: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd" in namespace "projected-7333" to be "Succeeded or Failed"
Sep  5 23:24:51.909: INFO: Pod "pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.000629ms
Sep  5 23:24:53.924: INFO: Pod "pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027248887s
Sep  5 23:24:55.943: INFO: Pod "pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045855492s
Sep  5 23:24:57.958: INFO: Pod "pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.060835541s
Sep  5 23:24:59.965: INFO: Pod "pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.067558723s
Sep  5 23:25:01.973: INFO: Pod "pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd": Phase="Pending", Reason="", readiness=false. Elapsed: 10.07618503s
Sep  5 23:25:03.981: INFO: Pod "pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd": Phase="Pending", Reason="", readiness=false. Elapsed: 12.084048857s
Sep  5 23:25:05.991: INFO: Pod "pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd": Phase="Pending", Reason="", readiness=false. Elapsed: 14.094065127s
Sep  5 23:25:08.002: INFO: Pod "pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.104605603s
Sep  5 23:25:10.014: INFO: Pod "pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd": Phase="Pending", Reason="", readiness=false. Elapsed: 18.116559998s
Sep  5 23:25:12.024: INFO: Pod "pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd": Phase="Pending", Reason="", readiness=false. Elapsed: 20.127216838s
Sep  5 23:25:14.047: INFO: Pod "pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd": Phase="Pending", Reason="", readiness=false. Elapsed: 22.14958136s
Sep  5 23:25:16.056: INFO: Pod "pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd": Phase="Running", Reason="", readiness=true. Elapsed: 24.159078476s
Sep  5 23:25:18.068: INFO: Pod "pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd": Phase="Running", Reason="", readiness=true. Elapsed: 26.171381573s
Sep  5 23:25:20.083: INFO: Pod "pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.185535972s
STEP: Saw pod success
Sep  5 23:25:20.083: INFO: Pod "pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd" satisfied condition "Succeeded or Failed"
Sep  5 23:25:20.089: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 23:25:20.169: INFO: Waiting for pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd to disappear
Sep  5 23:25:20.198: INFO: Pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd still exists
Sep  5 23:25:22.199: INFO: Waiting for pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd to disappear
Sep  5 23:25:22.209: INFO: Pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd still exists
Sep  5 23:25:24.198: INFO: Waiting for pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd to disappear
Sep  5 23:25:24.213: INFO: Pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd still exists
Sep  5 23:25:26.198: INFO: Waiting for pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd to disappear
Sep  5 23:25:26.217: INFO: Pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd still exists
Sep  5 23:25:28.199: INFO: Waiting for pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd to disappear
Sep  5 23:25:28.213: INFO: Pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd still exists
Sep  5 23:25:30.199: INFO: Waiting for pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd to disappear
Sep  5 23:25:30.208: INFO: Pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd still exists
Sep  5 23:25:32.198: INFO: Waiting for pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd to disappear
Sep  5 23:25:32.207: INFO: Pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd still exists
Sep  5 23:25:34.198: INFO: Waiting for pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd to disappear
Sep  5 23:25:34.212: INFO: Pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd still exists
Sep  5 23:25:36.199: INFO: Waiting for pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd to disappear
Sep  5 23:25:36.206: INFO: Pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd still exists
Sep  5 23:25:38.199: INFO: Waiting for pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd to disappear
Sep  5 23:25:38.207: INFO: Pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd still exists
Sep  5 23:25:40.199: INFO: Waiting for pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd to disappear
Sep  5 23:25:40.207: INFO: Pod pod-projected-secrets-c94d9035-dc36-4e49-bef7-b9b6bcbd40cd no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:25:40.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7333" for this suite.

â€¢ [SLOW TEST:49.014 seconds]
[sig-storage] Projected secret
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","total":291,"completed":200,"skipped":3283,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:25:40.424: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8961
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test override command
Sep  5 23:25:40.856: INFO: Waiting up to 5m0s for pod "client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec" in namespace "containers-8961" to be "Succeeded or Failed"
Sep  5 23:25:40.890: INFO: Pod "client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 33.866178ms
Sep  5 23:25:42.900: INFO: Pod "client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04411556s
Sep  5 23:25:44.909: INFO: Pod "client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05360874s
Sep  5 23:25:46.930: INFO: Pod "client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 6.07477154s
Sep  5 23:25:48.947: INFO: Pod "client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091293007s
Sep  5 23:25:50.959: INFO: Pod "client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 10.103790076s
Sep  5 23:25:52.967: INFO: Pod "client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 12.111526604s
Sep  5 23:25:54.976: INFO: Pod "client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 14.120794904s
Sep  5 23:25:56.982: INFO: Pod "client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 16.126587717s
Sep  5 23:25:58.990: INFO: Pod "client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 18.134319266s
Sep  5 23:26:00.999: INFO: Pod "client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 20.143259633s
Sep  5 23:26:03.007: INFO: Pod "client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec": Phase="Pending", Reason="", readiness=false. Elapsed: 22.150829157s
Sep  5 23:26:05.018: INFO: Pod "client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.162321909s
STEP: Saw pod success
Sep  5 23:26:05.018: INFO: Pod "client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec" satisfied condition "Succeeded or Failed"
Sep  5 23:26:05.026: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec container test-container: <nil>
STEP: delete the pod
Sep  5 23:26:09.499: INFO: Waiting for pod client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec to disappear
Sep  5 23:26:09.514: INFO: Pod client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec still exists
Sep  5 23:26:11.514: INFO: Waiting for pod client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec to disappear
Sep  5 23:26:11.556: INFO: Pod client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec still exists
Sep  5 23:26:13.514: INFO: Waiting for pod client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec to disappear
Sep  5 23:26:13.522: INFO: Pod client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec still exists
Sep  5 23:26:15.514: INFO: Waiting for pod client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec to disappear
Sep  5 23:26:15.521: INFO: Pod client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec still exists
Sep  5 23:26:17.514: INFO: Waiting for pod client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec to disappear
Sep  5 23:26:17.519: INFO: Pod client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec still exists
Sep  5 23:26:19.514: INFO: Waiting for pod client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec to disappear
Sep  5 23:26:19.521: INFO: Pod client-containers-b69b61b9-8d70-4524-b8d7-5baa0230e0ec no longer exists
[AfterEach] [k8s.io] Docker Containers
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:26:19.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8961" for this suite.

â€¢ [SLOW TEST:39.328 seconds]
[k8s.io] Docker Containers
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]","total":291,"completed":201,"skipped":3288,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:26:19.752: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-3907
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:26:28.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3907" for this suite.

â€¢ [SLOW TEST:8.789 seconds]
[sig-api-machinery] ResourceQuota
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","total":291,"completed":202,"skipped":3289,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:26:28.541: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-818
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 23:26:28.910: INFO: >>> kubeConfig: ./kconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:27:02.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-818" for this suite.

â€¢ [SLOW TEST:35.242 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:48
    listing custom resource definition objects works  [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","total":291,"completed":203,"skipped":3361,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:27:03.784: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5500
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check is all data is printed  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 23:27:04.412: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml version'
Sep  5 23:27:04.530: INFO: stderr: "WARNING: version difference between client (1.21) and server (1.19) exceeds the supported minor version skew of +/-1\n"
Sep  5 23:27:04.530: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"21\", GitVersion:\"v1.21.0+vmware.wcp.2\", GitCommit:\"d5bb17833505d15ce5f40815bb14fede978fe8c1\", GitTreeState:\"clean\", BuildDate:\"2021-08-14T16:46:51Z\", GoVersion:\"go1.16.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.12+vmware.wcp.1\", GitCommit:\"d3fb7ee0072361328577d64a012d30fe27b1a889\", GitTreeState:\"clean\", BuildDate:\"2021-08-14T16:41:30Z\", GoVersion:\"go1.15.13\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:27:04.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5500" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","total":291,"completed":204,"skipped":3365,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:27:05.334: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9350
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should invoke init containers on a RestartNever pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Sep  5 23:27:05.907: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:27:24.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9350" for this suite.

â€¢ [SLOW TEST:19.634 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should invoke init containers on a RestartNever pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","total":291,"completed":205,"skipped":3368,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:27:24.969: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4622
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve a basic endpoint from pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service endpoint-test2 in namespace services-4622
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4622 to expose endpoints map[]
Sep  5 23:27:25.524: INFO: successfully validated that service endpoint-test2 in namespace services-4622 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4622
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4622 to expose endpoints map[pod1:[80]]
Sep  5 23:27:29.626: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]], will retry
Sep  5 23:27:34.624: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]], will retry
Sep  5 23:27:39.620: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]], will retry
Sep  5 23:27:44.623: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]], will retry
Sep  5 23:27:49.630: INFO: successfully validated that service endpoint-test2 in namespace services-4622 exposes endpoints map[pod1:[80]]
STEP: Creating pod pod2 in namespace services-4622
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4622 to expose endpoints map[pod1:[80] pod2:[80]]
Sep  5 23:27:53.688: INFO: Unexpected endpoints: found map[407fb5b4-23e1-4ea7-b561-65114108dc22:[80]], expected map[pod1:[80] pod2:[80]], will retry
Sep  5 23:27:58.686: INFO: Unexpected endpoints: found map[407fb5b4-23e1-4ea7-b561-65114108dc22:[80]], expected map[pod1:[80] pod2:[80]], will retry
Sep  5 23:28:03.688: INFO: Unexpected endpoints: found map[407fb5b4-23e1-4ea7-b561-65114108dc22:[80]], expected map[pod1:[80] pod2:[80]], will retry
Sep  5 23:28:05.698: INFO: successfully validated that service endpoint-test2 in namespace services-4622 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Deleting pod pod1 in namespace services-4622
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4622 to expose endpoints map[pod2:[80]]
Sep  5 23:28:05.770: INFO: successfully validated that service endpoint-test2 in namespace services-4622 exposes endpoints map[pod2:[80]]
STEP: Deleting pod pod2 in namespace services-4622
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4622 to expose endpoints map[]
Sep  5 23:28:05.912: INFO: successfully validated that service endpoint-test2 in namespace services-4622 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:28:06.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4622" for this suite.
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:41.384 seconds]
[sig-network] Services
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","total":291,"completed":206,"skipped":3373,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:28:06.354: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4629
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name projected-configmap-test-volume-map-6582e105-048e-4682-b754-9bec5df0f216
STEP: Creating a pod to test consume configMaps
Sep  5 23:28:06.833: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52" in namespace "projected-4629" to be "Succeeded or Failed"
Sep  5 23:28:06.844: INFO: Pod "pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52": Phase="Pending", Reason="", readiness=false. Elapsed: 11.064133ms
Sep  5 23:28:08.862: INFO: Pod "pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028126283s
Sep  5 23:28:10.898: INFO: Pod "pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065074182s
Sep  5 23:28:12.937: INFO: Pod "pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52": Phase="Pending", Reason="", readiness=false. Elapsed: 6.103229426s
Sep  5 23:28:14.949: INFO: Pod "pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52": Phase="Pending", Reason="", readiness=false. Elapsed: 8.115258785s
Sep  5 23:28:17.009: INFO: Pod "pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52": Phase="Pending", Reason="", readiness=false. Elapsed: 10.175720689s
Sep  5 23:28:19.042: INFO: Pod "pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52": Phase="Pending", Reason="", readiness=false. Elapsed: 12.20856461s
Sep  5 23:28:21.051: INFO: Pod "pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52": Phase="Pending", Reason="", readiness=false. Elapsed: 14.217754867s
Sep  5 23:28:23.064: INFO: Pod "pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52": Phase="Pending", Reason="", readiness=false. Elapsed: 16.230258395s
Sep  5 23:28:25.111: INFO: Pod "pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52": Phase="Pending", Reason="", readiness=false. Elapsed: 18.277894395s
Sep  5 23:28:27.119: INFO: Pod "pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52": Phase="Pending", Reason="", readiness=false. Elapsed: 20.28555841s
Sep  5 23:28:29.130: INFO: Pod "pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52": Phase="Pending", Reason="", readiness=false. Elapsed: 22.296723175s
Sep  5 23:28:31.143: INFO: Pod "pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52": Phase="Running", Reason="", readiness=true. Elapsed: 24.310013379s
Sep  5 23:28:33.149: INFO: Pod "pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52": Phase="Running", Reason="", readiness=true. Elapsed: 26.315236204s
Sep  5 23:28:35.179: INFO: Pod "pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.345688213s
STEP: Saw pod success
Sep  5 23:28:35.179: INFO: Pod "pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52" satisfied condition "Succeeded or Failed"
Sep  5 23:28:35.187: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 23:28:35.277: INFO: Waiting for pod pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52 to disappear
Sep  5 23:28:35.296: INFO: Pod pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52 still exists
Sep  5 23:28:37.296: INFO: Waiting for pod pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52 to disappear
Sep  5 23:28:37.313: INFO: Pod pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52 still exists
Sep  5 23:28:39.296: INFO: Waiting for pod pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52 to disappear
Sep  5 23:28:39.305: INFO: Pod pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52 still exists
Sep  5 23:28:41.296: INFO: Waiting for pod pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52 to disappear
Sep  5 23:28:41.325: INFO: Pod pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52 still exists
Sep  5 23:28:43.298: INFO: Waiting for pod pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52 to disappear
Sep  5 23:28:43.306: INFO: Pod pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52 still exists
Sep  5 23:28:45.296: INFO: Waiting for pod pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52 to disappear
Sep  5 23:28:45.304: INFO: Pod pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52 still exists
Sep  5 23:28:47.297: INFO: Waiting for pod pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52 to disappear
Sep  5 23:28:47.320: INFO: Pod pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52 still exists
Sep  5 23:28:49.296: INFO: Waiting for pod pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52 to disappear
Sep  5 23:28:49.303: INFO: Pod pod-projected-configmaps-146533f8-60ca-4e9a-8f45-22b2eb626d52 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:28:49.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4629" for this suite.

â€¢ [SLOW TEST:43.259 seconds]
[sig-storage] Projected configMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":291,"completed":207,"skipped":3417,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:28:49.627: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3385
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep  5 23:28:50.182: INFO: Waiting up to 5m0s for pod "pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1" in namespace "emptydir-3385" to be "Succeeded or Failed"
Sep  5 23:28:50.205: INFO: Pod "pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1": Phase="Pending", Reason="", readiness=false. Elapsed: 23.749309ms
Sep  5 23:28:52.260: INFO: Pod "pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.078075548s
Sep  5 23:28:54.267: INFO: Pod "pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.085065595s
Sep  5 23:28:56.284: INFO: Pod "pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.102885564s
Sep  5 23:28:58.299: INFO: Pod "pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.117511731s
Sep  5 23:29:00.311: INFO: Pod "pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.129071306s
Sep  5 23:29:02.343: INFO: Pod "pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.161182692s
Sep  5 23:29:04.352: INFO: Pod "pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.170064305s
Sep  5 23:29:06.362: INFO: Pod "pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.180520098s
Sep  5 23:29:08.375: INFO: Pod "pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.193231156s
Sep  5 23:29:10.396: INFO: Pod "pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1": Phase="Running", Reason="", readiness=true. Elapsed: 20.214169222s
Sep  5 23:29:12.407: INFO: Pod "pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1": Phase="Running", Reason="", readiness=true. Elapsed: 22.225003948s
Sep  5 23:29:14.417: INFO: Pod "pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1": Phase="Running", Reason="", readiness=true. Elapsed: 24.235018538s
Sep  5 23:29:16.425: INFO: Pod "pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.242922563s
STEP: Saw pod success
Sep  5 23:29:16.425: INFO: Pod "pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1" satisfied condition "Succeeded or Failed"
Sep  5 23:29:16.431: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1 container test-container: <nil>
STEP: delete the pod
Sep  5 23:29:16.532: INFO: Waiting for pod pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1 to disappear
Sep  5 23:29:16.549: INFO: Pod pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1 still exists
Sep  5 23:29:18.549: INFO: Waiting for pod pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1 to disappear
Sep  5 23:29:18.558: INFO: Pod pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1 still exists
Sep  5 23:29:20.550: INFO: Waiting for pod pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1 to disappear
Sep  5 23:29:20.558: INFO: Pod pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1 still exists
Sep  5 23:29:22.550: INFO: Waiting for pod pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1 to disappear
Sep  5 23:29:22.558: INFO: Pod pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1 still exists
Sep  5 23:29:24.550: INFO: Waiting for pod pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1 to disappear
Sep  5 23:29:24.557: INFO: Pod pod-66298913-2cdf-4013-bda4-e7f1e3eac0e1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:29:24.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3385" for this suite.

â€¢ [SLOW TEST:35.169 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":208,"skipped":3420,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should run the lifecycle of PodTemplates [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:29:24.797: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-3302
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run the lifecycle of PodTemplates [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-node] PodTemplates
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:29:25.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3302" for this suite.
â€¢{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","total":291,"completed":209,"skipped":3431,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:29:25.505: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7170
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-9e1a7b4c-2d3c-4906-b6ec-4ba40e1cba9b
STEP: Creating secret with name s-test-opt-upd-97acca7e-811d-43c8-b411-401cf168d611
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9e1a7b4c-2d3c-4906-b6ec-4ba40e1cba9b
STEP: Updating secret s-test-opt-upd-97acca7e-811d-43c8-b411-401cf168d611
STEP: Creating secret with name s-test-opt-create-b8354b19-9528-4406-ac5e-ab032b8798fb
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:30:08.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7170" for this suite.

â€¢ [SLOW TEST:43.767 seconds]
[sig-storage] Projected secret
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","total":291,"completed":210,"skipped":3476,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:30:09.273: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8642
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support --unix-socket=/path  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Starting the proxy
Sep  5 23:30:09.825: INFO: Asynchronously running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml proxy --unix-socket=/tmp/kubectl-proxy-unix1648726959/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:30:10.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8642" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","total":291,"completed":211,"skipped":3516,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:30:10.229: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-3272
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 23:30:11.888: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  5 23:30:13.909: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:30:15.941: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:30:17.947: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:30:19.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:30:21.918: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:30:23.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:30:25.919: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:30:27.919: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:30:29.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:30:31.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:30:33.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 30, 12, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 30, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 23:30:36.965: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:30:37.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3272" for this suite.
STEP: Destroying namespace "webhook-3272-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:27.724 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","total":291,"completed":212,"skipped":3519,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:30:37.953: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1693
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl label
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1333
STEP: creating the pod
Sep  5 23:30:38.354: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml create -f - --namespace=kubectl-1693'
Sep  5 23:30:39.950: INFO: stderr: ""
Sep  5 23:30:39.950: INFO: stdout: "pod/pause created\n"
Sep  5 23:30:39.950: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep  5 23:30:39.952: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1693" to be "running and ready"
Sep  5 23:30:39.969: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 17.193803ms
Sep  5 23:30:41.982: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029809726s
Sep  5 23:30:43.997: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044843415s
Sep  5 23:30:46.007: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055137266s
Sep  5 23:30:48.014: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 8.06212526s
Sep  5 23:30:50.026: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 10.073322874s
Sep  5 23:30:52.106: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 12.154177219s
Sep  5 23:30:54.115: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 14.162505503s
Sep  5 23:30:56.123: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 16.170513438s
Sep  5 23:30:58.133: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 18.180593579s
Sep  5 23:31:00.159: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 20.206830592s
Sep  5 23:31:02.169: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 22.21708704s
Sep  5 23:31:04.182: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 24.229437243s
Sep  5 23:31:06.190: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 26.23759073s
Sep  5 23:31:08.199: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 28.247213966s
Sep  5 23:31:10.210: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 30.25813341s
Sep  5 23:31:12.218: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 32.266296061s
Sep  5 23:31:12.221: INFO: Pod "pause" satisfied condition "running and ready"
Sep  5 23:31:12.221: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: adding the label testing-label with value testing-label-value to a pod
Sep  5 23:31:12.227: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml label pods pause testing-label=testing-label-value --namespace=kubectl-1693'
Sep  5 23:31:12.368: INFO: stderr: ""
Sep  5 23:31:12.368: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep  5 23:31:12.369: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pod pause -L testing-label --namespace=kubectl-1693'
Sep  5 23:31:12.504: INFO: stderr: ""
Sep  5 23:31:12.504: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          33s   testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep  5 23:31:12.504: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml label pods pause testing-label- --namespace=kubectl-1693'
Sep  5 23:31:12.656: INFO: stderr: ""
Sep  5 23:31:12.656: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep  5 23:31:12.656: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pod pause -L testing-label --namespace=kubectl-1693'
Sep  5 23:31:12.762: INFO: stderr: ""
Sep  5 23:31:12.762: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          33s   \n"
[AfterEach] Kubectl label
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1340
STEP: using delete to clean up resources
Sep  5 23:31:12.762: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml delete --grace-period=0 --force -f - --namespace=kubectl-1693'
Sep  5 23:31:25.016: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 23:31:25.016: INFO: stdout: "pod \"pause\" force deleted\n"
Sep  5 23:31:25.017: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get rc,svc -l name=pause --no-headers --namespace=kubectl-1693'
Sep  5 23:31:25.142: INFO: stderr: "No resources found in kubectl-1693 namespace.\n"
Sep  5 23:31:25.142: INFO: stdout: ""
Sep  5 23:31:25.142: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -l name=pause --namespace=kubectl-1693 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  5 23:31:25.251: INFO: stderr: ""
Sep  5 23:31:25.251: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:31:25.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1693" for this suite.

â€¢ [SLOW TEST:47.894 seconds]
[sig-cli] Kubectl client
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1330
    should update the label on a resource  [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","total":291,"completed":213,"skipped":3522,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:31:25.848: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1038
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod test-webserver-12319699-3242-40ac-946b-27e8a09ac3a8 in namespace container-probe-1038
Sep  5 23:31:46.440: INFO: Started pod test-webserver-12319699-3242-40ac-946b-27e8a09ac3a8 in namespace container-probe-1038
STEP: checking the pod's current state and verifying that restartCount is present
Sep  5 23:31:46.453: INFO: Initial restart count of pod test-webserver-12319699-3242-40ac-946b-27e8a09ac3a8 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:35:47.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1038" for this suite.

â€¢ [SLOW TEST:261.757 seconds]
[k8s.io] Probing container
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","total":291,"completed":214,"skipped":3563,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:35:47.613: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8771
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name cm-test-opt-del-49aade98-4c0b-4c12-971a-1224cfd9a4c4
STEP: Creating configMap with name cm-test-opt-upd-af0905c5-08dd-4462-bb9f-d8c62ee5af98
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-49aade98-4c0b-4c12-971a-1224cfd9a4c4
STEP: Updating configmap cm-test-opt-upd-af0905c5-08dd-4462-bb9f-d8c62ee5af98
STEP: Creating configMap with name cm-test-opt-create-f04dd51b-1ce3-458c-bfe9-d748f38cc068
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:37:07.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8771" for this suite.

â€¢ [SLOW TEST:80.362 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","total":291,"completed":215,"skipped":3581,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:37:07.976: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1456
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:103
STEP: Creating service test in namespace statefulset-1456
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating stateful set ss in namespace statefulset-1456
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1456
Sep  5 23:37:08.603: INFO: Found 0 stateful pods, waiting for 1
Sep  5 23:37:18.620: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 23:37:28.609: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Sep  5 23:37:38.614: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep  5 23:37:38.630: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 23:37:39.027: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 23:37:39.027: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 23:37:39.027: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  5 23:37:39.043: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep  5 23:37:49.051: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 23:37:49.051: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 23:37:49.094: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep  5 23:37:49.094: INFO: ss-0  sc2-rdops-vm09-dhcp-34-149.eng.vmware.com  Running         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:08 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:32 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:43 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:43 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:37:49.094: INFO: 
Sep  5 23:37:49.094: INFO: StatefulSet ss has not reached scale 3, at 1
Sep  5 23:37:50.109: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.981401643s
Sep  5 23:37:51.118: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.966836694s
Sep  5 23:37:52.144: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.957517481s
Sep  5 23:37:53.153: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.931789877s
Sep  5 23:37:54.166: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.922251158s
Sep  5 23:37:55.187: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.909771792s
Sep  5 23:37:56.199: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.888364645s
Sep  5 23:37:57.211: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.876761218s
Sep  5 23:37:58.221: INFO: Verifying statefulset ss doesn't scale past 3 for another 864.8743ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1456
Sep  5 23:37:59.282: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:37:59.624: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Sep  5 23:37:59.624: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  5 23:37:59.624: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  5 23:37:59.624: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:37:59.861: INFO: rc: 1
Sep  5 23:37:59.861: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server: 

error:
exit status 1
Sep  5 23:38:09.863: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:38:10.188: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  5 23:38:10.188: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  5 23:38:10.188: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  5 23:38:10.188: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:38:10.586: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep  5 23:38:10.586: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Sep  5 23:38:10.586: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Sep  5 23:38:10.597: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:38:10.597: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=false
Sep  5 23:38:20.655: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:38:20.655: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep  5 23:38:20.655: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep  5 23:38:20.675: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 23:38:20.898: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 23:38:20.898: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 23:38:20.898: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  5 23:38:20.898: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 23:38:21.141: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 23:38:21.141: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 23:38:21.141: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  5 23:38:21.141: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Sep  5 23:38:21.382: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Sep  5 23:38:21.382: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Sep  5 23:38:21.382: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Sep  5 23:38:21.382: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 23:38:21.389: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep  5 23:38:31.401: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 23:38:31.401: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 23:38:31.401: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep  5 23:38:31.434: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep  5 23:38:31.435: INFO: ss-0  sc2-rdops-vm09-dhcp-34-149.eng.vmware.com  Running         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:08 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:32 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:31.435: INFO: ss-1  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:31.435: INFO: ss-2  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  }]
Sep  5 23:38:31.435: INFO: 
Sep  5 23:38:31.435: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 23:38:32.443: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep  5 23:38:32.444: INFO: ss-0  sc2-rdops-vm09-dhcp-34-149.eng.vmware.com  Running  30s    [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:08 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:32 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:32.446: INFO: ss-1  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running  30s    [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:32.446: INFO: ss-2  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running  30s    [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  }]
Sep  5 23:38:32.446: INFO: 
Sep  5 23:38:32.446: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 23:38:33.455: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep  5 23:38:33.455: INFO: ss-0  sc2-rdops-vm09-dhcp-34-149.eng.vmware.com  Running  30s    [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:08 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:32 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:33.455: INFO: ss-1  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running  30s    [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:33.455: INFO: ss-2  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running  30s    [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  }]
Sep  5 23:38:33.455: INFO: 
Sep  5 23:38:33.455: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 23:38:34.462: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep  5 23:38:34.463: INFO: ss-0  sc2-rdops-vm09-dhcp-34-149.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:08 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:32 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:34.463: INFO: ss-1  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:34.463: INFO: ss-2  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  }]
Sep  5 23:38:34.463: INFO: 
Sep  5 23:38:34.463: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 23:38:35.473: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep  5 23:38:35.473: INFO: ss-0  sc2-rdops-vm09-dhcp-34-149.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:08 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:32 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:35.473: INFO: ss-1  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:35.473: INFO: ss-2  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  }]
Sep  5 23:38:35.473: INFO: 
Sep  5 23:38:35.473: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 23:38:36.480: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep  5 23:38:36.480: INFO: ss-0  sc2-rdops-vm09-dhcp-34-149.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:08 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:32 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:36.480: INFO: ss-1  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:36.480: INFO: ss-2  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  }]
Sep  5 23:38:36.480: INFO: 
Sep  5 23:38:36.480: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 23:38:37.519: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep  5 23:38:37.519: INFO: ss-0  sc2-rdops-vm09-dhcp-34-149.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:08 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:32 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:37.519: INFO: ss-1  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:37.519: INFO: ss-2  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  }]
Sep  5 23:38:37.519: INFO: 
Sep  5 23:38:37.519: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 23:38:38.528: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep  5 23:38:38.528: INFO: ss-0  sc2-rdops-vm09-dhcp-34-149.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:08 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:32 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:38.528: INFO: ss-1  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:38.528: INFO: ss-2  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  }]
Sep  5 23:38:38.528: INFO: 
Sep  5 23:38:38.528: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 23:38:39.542: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep  5 23:38:39.542: INFO: ss-0  sc2-rdops-vm09-dhcp-34-149.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:08 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:32 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:39.542: INFO: ss-1  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:39.542: INFO: ss-2  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  }]
Sep  5 23:38:39.542: INFO: 
Sep  5 23:38:39.542: INFO: StatefulSet ss has not reached scale 0, at 3
Sep  5 23:38:40.570: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Sep  5 23:38:40.570: INFO: ss-0  sc2-rdops-vm09-dhcp-34-149.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:08 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:32 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:23 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:40.570: INFO: ss-1  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:21 -0700 PDT ContainersNotReady containers with unready status: [webserver]}]
Sep  5 23:38:40.570: INFO: ss-2  sc2-rdops-vm09-dhcp-43-208.eng.vmware.com  Running  0s     [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:37:49 -0700 PDT  } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Ready False 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:22 -0700 PDT ContainersNotReady containers with unready status: [webserver]} {Initialized True 0001-01-01 00:00:00 +0000 UTC 2021-09-05 23:38:06 -0700 PDT  }]
Sep  5 23:38:40.571: INFO: 
Sep  5 23:38:40.571: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1456
Sep  5 23:38:41.578: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:38:41.741: INFO: rc: 1
Sep  5 23:38:41.741: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server: 

error:
exit status 1
Sep  5 23:38:51.743: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:38:51.863: INFO: rc: 1
Sep  5 23:38:51.863: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:39:01.865: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:39:01.993: INFO: rc: 1
Sep  5 23:39:01.993: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:39:11.995: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:39:12.144: INFO: rc: 1
Sep  5 23:39:12.144: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:39:22.145: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:39:22.271: INFO: rc: 1
Sep  5 23:39:22.271: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:39:32.272: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:39:32.361: INFO: rc: 1
Sep  5 23:39:32.361: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:39:42.361: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:39:42.488: INFO: rc: 1
Sep  5 23:39:42.488: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:39:52.488: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:39:52.659: INFO: rc: 1
Sep  5 23:39:52.659: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:40:02.662: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:40:02.783: INFO: rc: 1
Sep  5 23:40:02.783: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:40:12.784: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:40:12.883: INFO: rc: 1
Sep  5 23:40:12.883: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:40:22.884: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:40:23.000: INFO: rc: 1
Sep  5 23:40:23.000: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:40:33.002: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:40:33.134: INFO: rc: 1
Sep  5 23:40:33.134: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:40:43.136: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:40:43.839: INFO: rc: 1
Sep  5 23:40:43.839: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:40:53.840: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:40:53.991: INFO: rc: 1
Sep  5 23:40:53.991: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:41:03.991: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:41:04.125: INFO: rc: 1
Sep  5 23:41:04.125: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:41:14.125: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:41:14.237: INFO: rc: 1
Sep  5 23:41:14.237: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:41:24.238: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:41:24.372: INFO: rc: 1
Sep  5 23:41:24.372: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:41:34.373: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:41:34.487: INFO: rc: 1
Sep  5 23:41:34.487: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:41:44.488: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:41:44.609: INFO: rc: 1
Sep  5 23:41:44.609: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:41:54.610: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:41:54.719: INFO: rc: 1
Sep  5 23:41:54.719: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:42:04.720: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:42:04.832: INFO: rc: 1
Sep  5 23:42:04.833: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:42:14.833: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:42:14.950: INFO: rc: 1
Sep  5 23:42:14.950: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:42:24.951: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:42:25.055: INFO: rc: 1
Sep  5 23:42:25.055: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:42:35.056: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:42:35.194: INFO: rc: 1
Sep  5 23:42:35.194: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:42:45.195: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:42:45.316: INFO: rc: 1
Sep  5 23:42:45.316: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:42:55.317: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:42:55.438: INFO: rc: 1
Sep  5 23:42:55.438: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:43:05.439: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:43:05.551: INFO: rc: 1
Sep  5 23:43:05.551: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:43:15.551: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:43:15.692: INFO: rc: 1
Sep  5 23:43:15.692: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:43:25.693: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:43:25.803: INFO: rc: 1
Sep  5 23:43:25.803: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:43:35.804: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:43:35.914: INFO: rc: 1
Sep  5 23:43:35.914: INFO: Waiting 10s to retry failed RunHostCmd: error running /home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Sep  5 23:43:45.914: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml exec --namespace=statefulset-1456 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Sep  5 23:43:46.053: INFO: rc: 1
Sep  5 23:43:46.053: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Sep  5 23:43:46.053: INFO: Scaling statefulset ss to 0
Sep  5 23:43:46.103: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:114
Sep  5 23:43:46.110: INFO: Deleting all statefulset in ns statefulset-1456
Sep  5 23:43:46.116: INFO: Scaling statefulset ss to 0
Sep  5 23:43:46.137: INFO: Waiting for statefulset status.replicas updated to 0
Sep  5 23:43:46.143: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:43:46.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1456" for this suite.

â€¢ [SLOW TEST:398.685 seconds]
[sig-apps] StatefulSet
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","total":291,"completed":216,"skipped":3601,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:43:46.661: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-8637
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  5 23:43:49.311: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  5 23:43:51.343: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:43:53.352: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:43:55.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:43:57.367: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:43:59.349: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:44:01.362: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:44:03.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:44:05.352: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:44:07.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:44:09.353: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:44:11.352: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:44:13.355: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:44:15.350: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:44:17.354: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 43, 49, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  5 23:44:20.404: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:44:20.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8637" for this suite.
STEP: Destroying namespace "webhook-8637-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:35.000 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","total":291,"completed":217,"skipped":3604,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-auth] ServiceAccounts
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:44:21.662: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6992
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep  5 23:44:48.752: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl exec --namespace=svcaccounts-6992 pod-service-account-2f363c16-b7c8-4273-a934-a897500160f0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep  5 23:44:48.976: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl exec --namespace=svcaccounts-6992 pod-service-account-2f363c16-b7c8-4273-a934-a897500160f0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep  5 23:44:49.209: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl exec --namespace=svcaccounts-6992 pod-service-account-2f363c16-b7c8-4273-a934-a897500160f0 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:44:49.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6992" for this suite.

â€¢ [SLOW TEST:28.027 seconds]
[sig-auth] ServiceAccounts
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","total":291,"completed":218,"skipped":3644,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:44:49.691: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1895
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:162
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
Sep  5 23:44:50.112: INFO: PodSpec: initContainers in spec.initContainers
Sep  5 23:49:04.946: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-dc33fc6b-9476-471b-87fc-42baeced1d32", GenerateName:"", Namespace:"init-container-1895", SelfLink:"/api/v1/namespaces/init-container-1895/pods/pod-init-dc33fc6b-9476-471b-87fc-42baeced1d32", UID:"783c0d23-6f81-43e8-bf44-d746be0f4027", ResourceVersion:"190844", Generation:0, CreationTimestamp:time.Date(2021, time.September, 5, 23, 44, 50, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"112173288"}, Annotations:map[string]string{"attachment_id":"77c10d39-10d5-45ed-92b1-2f15f549e4a2", "kubernetes.io/psp":"e2e-test-privileged-psp", "mac":"04:50:56:00:60:15", "vlan":"None", "vmware-system-ephemeral-disk-uuid":"6000C29b-c326-412a-fda2-168b4c8b6e41", "vmware-system-image-references":"{\"init1\":\"busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v56263\",\"init2\":\"busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v56263\",\"run1\":\"pause-54467452d21a00c28aa4cce37d0850173b528ca6-v60388\"}", "vmware-system-vm-moid":"vm-1264:7badc608-dd2f-42a9-952d-0d4c30d5f283", "vmware-system-vm-uuid":"501783b0-e756-9c5e-5fcf-664556e7b5f6"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string{"lifecycle-controller/system.vmware.com"}, ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2021, time.September, 5, 23, 44, 50, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002de0090)}, v1.ManagedFieldsEntry{Manager:"image-controller", Operation:"Update", APIVersion:"v1", Time:time.Date(2021, time.September, 5, 23, 44, 50, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002de00d8)}, v1.ManagedFieldsEntry{Manager:"nsx-ncp-6d7f7bf559-9mqvr", Operation:"Update", APIVersion:"v1", Time:time.Date(2021, time.September, 5, 23, 45, 0, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002de0138)}, v1.ManagedFieldsEntry{Manager:"scheduler-extender", Operation:"Update", APIVersion:"v1", Time:time.Date(2021, time.September, 5, 23, 45, 27, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002de0180)}, v1.ManagedFieldsEntry{Manager:"spherelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2021, time.September, 5, 23, 46, 33, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc002de01c8)}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-z2zm4", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc004294040), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"mirror.gcr.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-z2zm4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"mirror.gcr.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-z2zm4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.2", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-z2zm4", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc001f8e118), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"sc2-rdops-vm09-dhcp-43-208.eng.vmware.com", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002ba4000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001f8e1b0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc001f8e1d0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc001f8e1d8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001f8e1dc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc002c18050), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2021, time.September, 5, 23, 44, 50, 0, time.Local), Reason:"", Message:""}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2021, time.September, 5, 23, 46, 10, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2021, time.September, 5, 23, 46, 10, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2021, time.September, 5, 23, 46, 10, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.193.43.208", PodIP:"172.26.1.194", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.26.1.194"}}, StartTime:time.Date(2021, time.September, 5, 23, 45, 59, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0006366a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002ba40e0)}, Ready:false, RestartCount:3, Image:"mirror.gcr.io/library/busybox:1.29", ImageID:"busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v56263", ContainerID:"4b04f64a-6083-4880-a3e2-100c1baf0312", Started:(*bool)(0xc001f8e295)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000636780), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"mirror.gcr.io/library/busybox:1.29", ImageID:"busybox-c954c04429e8f5873d6cf81884df42470d97ef3b-v56263", ContainerID:"", Started:(*bool)(0xc001f8e29b)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000636240), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.2", ImageID:"pause-54467452d21a00c28aa4cce37d0850173b528ca6-v60388", ContainerID:"", Started:(*bool)(0xc001f8e23c)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:49:04.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1895" for this suite.

â€¢ [SLOW TEST:255.619 seconds]
[k8s.io] InitContainer [NodeConformance]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","total":291,"completed":219,"skipped":3663,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:49:05.312: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-4200
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep  5 23:49:05.970: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  5 23:49:06.007: INFO: Waiting for terminating namespaces to be deleted...
Sep  5 23:49:06.032: INFO: 
Logging pods the apiserver thinks is on node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com before test
Sep  5 23:49:06.075: INFO: podwithpersistentvolume from storage-class-test-2 started at 2021-09-05 20:05:57 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.075: INFO: 	Container hello ready: true, restart count 0
Sep  5 23:49:06.075: INFO: hello-web-6b97664bd5-f5452 from test-cluster-ip-service started at 2021-09-05 20:10:51 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.075: INFO: 	Container hello-app ready: true, restart count 0
Sep  5 23:49:06.075: INFO: wcp-sanity-busybox-6f999d6849-45jct from test-dataprovider-podvms-ns started at 2021-09-05 19:57:27 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.075: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 23:49:06.075: INFO: wcp-sanity-busybox-6f999d6849-c27n2 from test-dataprovider-podvms-ns started at 2021-09-05 21:11:14 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.075: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 23:49:06.075: INFO: nginx-private from test-image-pull-secrets-ns started at 2021-09-05 19:59:11 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.075: INFO: 	Container nginx-private-container ready: true, restart count 0
Sep  5 23:49:06.075: INFO: curl-pod from test-network-policy started at 2021-09-05 20:14:04 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.075: INFO: 	Container curl-container ready: true, restart count 0
Sep  5 23:49:06.075: INFO: hello-web-1-6b97664bd5-cl9tj from test-network-policy started at 2021-09-05 21:09:17 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.075: INFO: 	Container hello-app ready: true, restart count 0
Sep  5 23:49:06.075: INFO: schedext-test-node-selector-1 from test-node-selector started at 2021-09-05 20:00:20 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.075: INFO: 	Container hello ready: true, restart count 0
Sep  5 23:49:06.075: INFO: busybox from test-pod-external-nw-access started at 2021-09-05 20:15:25 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.075: INFO: 	Container busybox ready: true, restart count 0
Sep  5 23:49:06.075: INFO: busybox-annotation from test-podvm-annotations started at 2021-09-05 20:03:14 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.075: INFO: 	Container busybox-annotation ready: true, restart count 0
Sep  5 23:49:06.075: INFO: helloworld from test-telemetry started at 2021-09-05 20:09:22 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.075: INFO: 	Container hello ready: true, restart count 0
Sep  5 23:49:06.075: INFO: wcp-sanity-busybox-6f999d6849-46njm from test-update-workload-ns started at 2021-09-05 20:07:43 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.075: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 23:49:06.075: INFO: 
Logging pods the apiserver thinks is on node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com before test
Sep  5 23:49:06.100: INFO: curl-pod from test-cluster-ip-service started at 2021-09-05 20:10:11 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.100: INFO: 	Container curl-container ready: true, restart count 0
Sep  5 23:49:06.100: INFO: helloworld from test-exec-ns started at 2021-09-05 19:58:27 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.100: INFO: 	Container hello ready: true, restart count 0
Sep  5 23:49:06.100: INFO: schedext-test-node-selector-2 from test-node-selector started at 2021-09-05 20:00:22 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.100: INFO: 	Container hello ready: true, restart count 0
Sep  5 23:49:06.100: INFO: schedext-test-affinity-1 from test-pod-affinity started at 2021-09-05 20:00:51 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.100: INFO: 	Container hello ready: true, restart count 0
Sep  5 23:49:06.100: INFO: schedext-test-affinity-2 from test-pod-affinity started at 2021-09-05 20:01:16 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.100: INFO: 	Container hello ready: true, restart count 0
Sep  5 23:49:06.100: INFO: test-docker-registry from test-private-image-registry-ns started at 2021-09-05 20:16:11 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.100: INFO: 	Container test-docker-registry ready: true, restart count 0
Sep  5 23:49:06.100: INFO: helloworld from test-update-workload-ns started at 2021-09-05 20:08:34 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.100: INFO: 	Container hello ready: true, restart count 0
Sep  5 23:49:06.100: INFO: 
Logging pods the apiserver thinks is on node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com before test
Sep  5 23:49:06.130: INFO: pod-init-dc33fc6b-9476-471b-87fc-42baeced1d32 from init-container-1895 started at 2021-09-05 23:45:59 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.130: INFO: 	Container run1 ready: false, restart count 0
Sep  5 23:49:06.130: INFO: hello-web-2-f779cbdff-hffpj from test-network-policy started at 2021-09-05 21:09:20 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.130: INFO: 	Container hello-app ready: true, restart count 0
Sep  5 23:49:06.130: INFO: wcp-sanity-busybox-6f999d6849-856nv from test-update-workload-ns started at 2021-09-05 21:10:00 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.130: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  5 23:49:06.130: INFO: wcp-sanity-busybox-6f999d6849-mts92 from test-update-workload-ns started at 2021-09-05 21:10:00 -0700 PDT (1 container statuses recorded)
Sep  5 23:49:06.130: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16a228dd5ac3fdcc], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.16a228dd5c0ec87c], Reason = [FailedScheduling], Message = [0/6 nodes are available: 6 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:49:07.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4200" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
â€¢{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","total":291,"completed":220,"skipped":3713,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:49:07.412: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3914
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:54
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 23:49:07.829: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:49:09.837: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:49:11.895: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:49:13.845: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:49:15.838: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:49:17.842: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:49:19.836: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:49:21.840: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:49:23.838: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:49:25.839: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:49:27.838: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:49:29.839: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:49:31.838: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Running (Ready = false)
Sep  5 23:49:33.836: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Running (Ready = false)
Sep  5 23:49:35.835: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Running (Ready = false)
Sep  5 23:49:37.842: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Running (Ready = false)
Sep  5 23:49:39.853: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Running (Ready = false)
Sep  5 23:49:41.843: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Running (Ready = false)
Sep  5 23:49:43.840: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Running (Ready = false)
Sep  5 23:49:45.837: INFO: The status of Pod test-webserver-4a1da844-5279-4163-ab97-21319726bf5d is Running (Ready = true)
Sep  5 23:49:45.843: INFO: Container started at 2021-09-05 23:49:30 -0700 PDT, pod became ready at 2021-09-05 23:49:46 -0700 PDT
[AfterEach] [k8s.io] Probing container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:49:45.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3914" for this suite.

â€¢ [SLOW TEST:38.661 seconds]
[k8s.io] Probing container
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","total":291,"completed":221,"skipped":3715,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:49:46.073: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3314
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Sep  5 23:52:00.957: INFO: MetricsGrabber failed grab metrics. Skipping metrics gathering.
Sep  5 23:52:00.957: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mvzp" in namespace "gc-3314"
Sep  5 23:52:00.975: INFO: Deleting pod "simpletest-rc-to-be-deleted-726rc" in namespace "gc-3314"
Sep  5 23:52:01.018: INFO: Deleting pod "simpletest-rc-to-be-deleted-7x5mf" in namespace "gc-3314"
Sep  5 23:52:01.050: INFO: Deleting pod "simpletest-rc-to-be-deleted-82whb" in namespace "gc-3314"
Sep  5 23:52:01.098: INFO: Deleting pod "simpletest-rc-to-be-deleted-bxdhd" in namespace "gc-3314"
[AfterEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:52:01.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3314" for this suite.

â€¢ [SLOW TEST:135.436 seconds]
[sig-api-machinery] Garbage collector
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","total":291,"completed":222,"skipped":3716,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API 
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-instrumentation] Events API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:52:01.510: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6896
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-instrumentation] Events API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/instrumentation/events.go:81
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
STEP: listing events with field selection filtering on source
STEP: listing events with field selection filtering on reportingController
STEP: getting the test event
STEP: patching the test event
STEP: getting the test event
STEP: updating the test event
STEP: getting the test event
STEP: deleting the test event
STEP: listing events in all namespaces
STEP: listing events in test namespace
[AfterEach] [sig-instrumentation] Events API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:52:02.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6896" for this suite.
â€¢{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","total":291,"completed":223,"skipped":3748,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:52:02.989: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8243
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should create and stop a working application  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating all guestbook components
Sep  5 23:52:03.448: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Sep  5 23:52:03.448: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml create -f - --namespace=kubectl-8243'
Sep  5 23:52:04.392: INFO: stderr: ""
Sep  5 23:52:04.393: INFO: stdout: "service/agnhost-replica created\n"
Sep  5 23:52:04.393: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Sep  5 23:52:04.393: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml create -f - --namespace=kubectl-8243'
Sep  5 23:52:04.905: INFO: stderr: ""
Sep  5 23:52:04.905: INFO: stdout: "service/agnhost-primary created\n"
Sep  5 23:52:04.907: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep  5 23:52:04.907: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml create -f - --namespace=kubectl-8243'
Sep  5 23:52:05.316: INFO: stderr: ""
Sep  5 23:52:05.316: INFO: stdout: "service/frontend created\n"
Sep  5 23:52:05.316: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Sep  5 23:52:05.316: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml create -f - --namespace=kubectl-8243'
Sep  5 23:52:05.868: INFO: stderr: ""
Sep  5 23:52:05.868: INFO: stdout: "deployment.apps/frontend created\n"
Sep  5 23:52:05.868: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  5 23:52:05.868: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml create -f - --namespace=kubectl-8243'
Sep  5 23:52:06.273: INFO: stderr: ""
Sep  5 23:52:06.273: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Sep  5 23:52:06.274: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: k8s.gcr.io/e2e-test-images/agnhost:2.20
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep  5 23:52:06.274: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml create -f - --namespace=kubectl-8243'
Sep  5 23:52:06.650: INFO: stderr: ""
Sep  5 23:52:06.650: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app
Sep  5 23:52:06.650: INFO: Waiting for all frontend pods to be Running.
Sep  5 23:52:41.703: INFO: Waiting for frontend to serve content.
Sep  5 23:52:41.817: INFO: Trying to add a new entry to the guestbook.
Sep  5 23:52:41.896: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Sep  5 23:52:41.992: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml delete --grace-period=0 --force -f - --namespace=kubectl-8243'
Sep  5 23:52:42.200: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 23:52:42.200: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 23:52:42.200: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml delete --grace-period=0 --force -f - --namespace=kubectl-8243'
Sep  5 23:52:42.438: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 23:52:42.438: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 23:52:42.438: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml delete --grace-period=0 --force -f - --namespace=kubectl-8243'
Sep  5 23:52:42.632: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 23:52:42.632: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 23:52:42.633: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml delete --grace-period=0 --force -f - --namespace=kubectl-8243'
Sep  5 23:52:42.747: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 23:52:42.747: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 23:52:42.747: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml delete --grace-period=0 --force -f - --namespace=kubectl-8243'
Sep  5 23:52:42.859: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 23:52:42.859: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources
Sep  5 23:52:42.859: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml delete --grace-period=0 --force -f - --namespace=kubectl-8243'
Sep  5 23:52:42.984: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  5 23:52:42.984: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:52:42.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8243" for this suite.

â€¢ [SLOW TEST:40.225 seconds]
[sig-cli] Kubectl client
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:351
    should create and stop a working application  [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","total":291,"completed":224,"skipped":3802,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-api-machinery] Events 
  should delete a collection of events [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Events
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:52:43.214: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-1527
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of events [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of events
Sep  5 23:52:43.656: INFO: created test-event-1
Sep  5 23:52:43.678: INFO: created test-event-2
Sep  5 23:52:43.696: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace
STEP: delete collection of events
Sep  5 23:52:43.719: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity
Sep  5 23:52:43.842: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-api-machinery] Events
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:52:43.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1527" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Events should delete a collection of events [Conformance]","total":291,"completed":225,"skipped":3811,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-api-machinery] server version 
  should find the server version [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] server version
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:52:44.058: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename server-version
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in server-version-2716
STEP: Waiting for a default service account to be provisioned in namespace
[It] should find the server version [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Request ServerVersion
STEP: Confirm major version
Sep  5 23:52:44.771: INFO: Major version: 1
STEP: Confirm minor version
Sep  5 23:52:44.771: INFO: cleanMinorVersion: 19
Sep  5 23:52:44.771: INFO: Minor version: 19
[AfterEach] [sig-api-machinery] server version
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:52:44.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-2716" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","total":291,"completed":226,"skipped":3820,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:52:44.973: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-810
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir volume type on node default medium
Sep  5 23:52:45.441: INFO: Waiting up to 5m0s for pod "pod-5366606d-e793-4403-8ffa-80dc6888b97d" in namespace "emptydir-810" to be "Succeeded or Failed"
Sep  5 23:52:45.456: INFO: Pod "pod-5366606d-e793-4403-8ffa-80dc6888b97d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.145811ms
Sep  5 23:52:47.463: INFO: Pod "pod-5366606d-e793-4403-8ffa-80dc6888b97d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022213133s
Sep  5 23:52:49.476: INFO: Pod "pod-5366606d-e793-4403-8ffa-80dc6888b97d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03547109s
Sep  5 23:52:51.519: INFO: Pod "pod-5366606d-e793-4403-8ffa-80dc6888b97d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.078335033s
Sep  5 23:52:53.531: INFO: Pod "pod-5366606d-e793-4403-8ffa-80dc6888b97d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.090044969s
Sep  5 23:52:55.538: INFO: Pod "pod-5366606d-e793-4403-8ffa-80dc6888b97d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.097195505s
Sep  5 23:52:57.545: INFO: Pod "pod-5366606d-e793-4403-8ffa-80dc6888b97d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.104074667s
Sep  5 23:52:59.576: INFO: Pod "pod-5366606d-e793-4403-8ffa-80dc6888b97d": Phase="Pending", Reason="", readiness=false. Elapsed: 14.134891904s
Sep  5 23:53:01.598: INFO: Pod "pod-5366606d-e793-4403-8ffa-80dc6888b97d": Phase="Pending", Reason="", readiness=false. Elapsed: 16.157180888s
Sep  5 23:53:03.609: INFO: Pod "pod-5366606d-e793-4403-8ffa-80dc6888b97d": Phase="Pending", Reason="", readiness=false. Elapsed: 18.168092138s
Sep  5 23:53:05.618: INFO: Pod "pod-5366606d-e793-4403-8ffa-80dc6888b97d": Phase="Pending", Reason="", readiness=false. Elapsed: 20.176938702s
Sep  5 23:53:07.627: INFO: Pod "pod-5366606d-e793-4403-8ffa-80dc6888b97d": Phase="Running", Reason="", readiness=true. Elapsed: 22.186352754s
Sep  5 23:53:09.642: INFO: Pod "pod-5366606d-e793-4403-8ffa-80dc6888b97d": Phase="Running", Reason="", readiness=true. Elapsed: 24.201541861s
Sep  5 23:53:11.653: INFO: Pod "pod-5366606d-e793-4403-8ffa-80dc6888b97d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.212290124s
STEP: Saw pod success
Sep  5 23:53:11.653: INFO: Pod "pod-5366606d-e793-4403-8ffa-80dc6888b97d" satisfied condition "Succeeded or Failed"
Sep  5 23:53:11.668: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-5366606d-e793-4403-8ffa-80dc6888b97d container test-container: <nil>
STEP: delete the pod
Sep  5 23:53:11.757: INFO: Waiting for pod pod-5366606d-e793-4403-8ffa-80dc6888b97d to disappear
Sep  5 23:53:11.792: INFO: Pod pod-5366606d-e793-4403-8ffa-80dc6888b97d still exists
Sep  5 23:53:13.793: INFO: Waiting for pod pod-5366606d-e793-4403-8ffa-80dc6888b97d to disappear
Sep  5 23:53:13.800: INFO: Pod pod-5366606d-e793-4403-8ffa-80dc6888b97d still exists
Sep  5 23:53:15.793: INFO: Waiting for pod pod-5366606d-e793-4403-8ffa-80dc6888b97d to disappear
Sep  5 23:53:15.802: INFO: Pod pod-5366606d-e793-4403-8ffa-80dc6888b97d still exists
Sep  5 23:53:17.794: INFO: Waiting for pod pod-5366606d-e793-4403-8ffa-80dc6888b97d to disappear
Sep  5 23:53:17.799: INFO: Pod pod-5366606d-e793-4403-8ffa-80dc6888b97d still exists
Sep  5 23:53:19.793: INFO: Waiting for pod pod-5366606d-e793-4403-8ffa-80dc6888b97d to disappear
Sep  5 23:53:19.801: INFO: Pod pod-5366606d-e793-4403-8ffa-80dc6888b97d still exists
Sep  5 23:53:21.793: INFO: Waiting for pod pod-5366606d-e793-4403-8ffa-80dc6888b97d to disappear
Sep  5 23:53:21.878: INFO: Pod pod-5366606d-e793-4403-8ffa-80dc6888b97d still exists
Sep  5 23:53:23.793: INFO: Waiting for pod pod-5366606d-e793-4403-8ffa-80dc6888b97d to disappear
Sep  5 23:53:23.799: INFO: Pod pod-5366606d-e793-4403-8ffa-80dc6888b97d still exists
Sep  5 23:53:25.793: INFO: Waiting for pod pod-5366606d-e793-4403-8ffa-80dc6888b97d to disappear
Sep  5 23:53:25.801: INFO: Pod pod-5366606d-e793-4403-8ffa-80dc6888b97d still exists
Sep  5 23:53:27.794: INFO: Waiting for pod pod-5366606d-e793-4403-8ffa-80dc6888b97d to disappear
Sep  5 23:53:27.802: INFO: Pod pod-5366606d-e793-4403-8ffa-80dc6888b97d still exists
Sep  5 23:53:29.793: INFO: Waiting for pod pod-5366606d-e793-4403-8ffa-80dc6888b97d to disappear
Sep  5 23:53:29.803: INFO: Pod pod-5366606d-e793-4403-8ffa-80dc6888b97d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:53:29.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-810" for this suite.

â€¢ [SLOW TEST:45.043 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":227,"skipped":3821,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:53:30.016: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3491
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-756
STEP: Creating secret with name secret-test-85309388-3ff3-4dcc-8e36-ac2b25f226dd
STEP: Creating a pod to test consume secrets
Sep  5 23:53:30.851: INFO: Waiting up to 5m0s for pod "pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730" in namespace "secrets-3491" to be "Succeeded or Failed"
Sep  5 23:53:30.863: INFO: Pod "pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730": Phase="Pending", Reason="", readiness=false. Elapsed: 11.563294ms
Sep  5 23:53:32.874: INFO: Pod "pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022523715s
Sep  5 23:53:34.882: INFO: Pod "pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030736527s
Sep  5 23:53:36.894: INFO: Pod "pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043076342s
Sep  5 23:53:38.901: INFO: Pod "pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730": Phase="Pending", Reason="", readiness=false. Elapsed: 8.049665642s
Sep  5 23:53:40.907: INFO: Pod "pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730": Phase="Pending", Reason="", readiness=false. Elapsed: 10.055221967s
Sep  5 23:53:42.924: INFO: Pod "pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730": Phase="Pending", Reason="", readiness=false. Elapsed: 12.072297655s
Sep  5 23:53:44.936: INFO: Pod "pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730": Phase="Pending", Reason="", readiness=false. Elapsed: 14.084626315s
Sep  5 23:53:46.948: INFO: Pod "pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730": Phase="Pending", Reason="", readiness=false. Elapsed: 16.096754396s
Sep  5 23:53:48.957: INFO: Pod "pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730": Phase="Pending", Reason="", readiness=false. Elapsed: 18.105199919s
Sep  5 23:53:50.972: INFO: Pod "pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730": Phase="Pending", Reason="", readiness=false. Elapsed: 20.120206356s
Sep  5 23:53:52.979: INFO: Pod "pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.127579525s
STEP: Saw pod success
Sep  5 23:53:52.979: INFO: Pod "pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730" satisfied condition "Succeeded or Failed"
Sep  5 23:53:52.988: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730 container secret-volume-test: <nil>
STEP: delete the pod
Sep  5 23:53:57.626: INFO: Waiting for pod pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730 to disappear
Sep  5 23:53:57.642: INFO: Pod pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730 still exists
Sep  5 23:53:59.642: INFO: Waiting for pod pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730 to disappear
Sep  5 23:53:59.652: INFO: Pod pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730 still exists
Sep  5 23:54:01.642: INFO: Waiting for pod pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730 to disappear
Sep  5 23:54:01.651: INFO: Pod pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730 still exists
Sep  5 23:54:03.644: INFO: Waiting for pod pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730 to disappear
Sep  5 23:54:03.651: INFO: Pod pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730 still exists
Sep  5 23:54:05.643: INFO: Waiting for pod pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730 to disappear
Sep  5 23:54:05.651: INFO: Pod pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730 still exists
Sep  5 23:54:07.643: INFO: Waiting for pod pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730 to disappear
Sep  5 23:54:07.648: INFO: Pod pod-secrets-41cbf1cd-4a81-4940-9988-623ae81d0730 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:54:07.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3491" for this suite.
STEP: Destroying namespace "secret-namespace-756" for this suite.

â€¢ [SLOW TEST:38.134 seconds]
[sig-storage] Secrets
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","total":291,"completed":228,"skipped":3831,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:54:08.150: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9041
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  5 23:54:08.551: INFO: Creating deployment "test-recreate-deployment"
Sep  5 23:54:08.568: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep  5 23:54:08.625: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep  5 23:54:10.644: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep  5 23:54:10.649: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:54:12.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:54:14.660: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:54:16.655: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:54:18.658: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:54:20.655: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:54:22.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:54:24.659: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 5, 23, 54, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-c96cf48f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  5 23:54:26.656: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep  5 23:54:26.688: INFO: Updating deployment test-recreate-deployment
Sep  5 23:54:26.688: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep  5 23:54:39.169: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9041 /apis/apps/v1/namespaces/deployment-9041/deployments/test-recreate-deployment d75eea8c-bae7-4ca7-abb9-824b8246fd0c 195544 2 2021-09-05 23:54:08 -0700 PDT <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-09-05 23:54:26 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-05 23:54:39 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a54e5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-09-05 23:54:26 -0700 PDT,LastTransitionTime:2021-09-05 23:54:26 -0700 PDT,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-f756bcb56" is progressing.,LastUpdateTime:2021-09-05 23:54:39 -0700 PDT,LastTransitionTime:2021-09-05 23:54:08 -0700 PDT,},},ReadyReplicas:0,CollisionCount:nil,},}

Sep  5 23:54:39.177: INFO: New ReplicaSet "test-recreate-deployment-f756bcb56" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-f756bcb56  deployment-9041 /apis/apps/v1/namespaces/deployment-9041/replicasets/test-recreate-deployment-f756bcb56 60175aaf-df53-40f2-97b9-dc281d88c2ef 195542 1 2021-09-05 23:54:39 -0700 PDT <nil> <nil> map[name:sample-pod-3 pod-template-hash:f756bcb56] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment d75eea8c-bae7-4ca7-abb9-824b8246fd0c 0xc00a54eaf0 0xc00a54eaf1}] []  [{kube-controller-manager Update apps/v1 2021-09-05 23:54:39 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75eea8c-bae7-4ca7-abb9-824b8246fd0c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: f756bcb56,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:f756bcb56] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a54eb68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  5 23:54:39.177: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep  5 23:54:39.177: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-c96cf48f  deployment-9041 /apis/apps/v1/namespaces/deployment-9041/replicasets/test-recreate-deployment-c96cf48f b9dacb36-e158-4b51-8819-84de69d071b5 195408 2 2021-09-05 23:54:08 -0700 PDT <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment d75eea8c-bae7-4ca7-abb9-824b8246fd0c 0xc00a54e9ff 0xc00a54ea10}] []  [{kube-controller-manager Update apps/v1 2021-09-05 23:54:26 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d75eea8c-bae7-4ca7-abb9-824b8246fd0c\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c96cf48f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:c96cf48f] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a54ea88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  5 23:54:39.184: INFO: Pod "test-recreate-deployment-f756bcb56-kfrps" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-f756bcb56-kfrps test-recreate-deployment-f756bcb56- deployment-9041 /api/v1/namespaces/deployment-9041/pods/test-recreate-deployment-f756bcb56-kfrps 9101005c-7074-4248-a932-4ca35534c1c9 195539 0 2021-09-05 23:54:39 -0700 PDT <nil> <nil> map[name:sample-pod-3 pod-template-hash:f756bcb56] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet test-recreate-deployment-f756bcb56 60175aaf-df53-40f2-97b9-dc281d88c2ef 0xc004287680 0xc004287681}] []  [{kube-controller-manager Update v1 2021-09-05 23:54:39 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"60175aaf-df53-40f2-97b9-dc281d88c2ef\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-n78kn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-n78kn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-n78kn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:54:39.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9041" for this suite.

â€¢ [SLOW TEST:31.271 seconds]
[sig-apps] Deployment
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","total":291,"completed":229,"skipped":3845,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:54:39.422: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2874
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should add annotations for pods in rc  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating Agnhost RC
Sep  5 23:54:39.826: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml create -f - --namespace=kubectl-2874'
Sep  5 23:54:40.247: INFO: stderr: ""
Sep  5 23:54:40.247: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start.
Sep  5 23:54:41.257: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:54:41.257: INFO: Found 0 / 1
Sep  5 23:54:42.253: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:54:42.253: INFO: Found 0 / 1
Sep  5 23:54:43.255: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:54:43.255: INFO: Found 0 / 1
Sep  5 23:54:44.316: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:54:44.316: INFO: Found 0 / 1
Sep  5 23:54:45.257: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:54:45.257: INFO: Found 0 / 1
Sep  5 23:54:46.256: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:54:46.256: INFO: Found 0 / 1
Sep  5 23:54:47.265: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:54:47.265: INFO: Found 0 / 1
Sep  5 23:54:48.258: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:54:48.258: INFO: Found 0 / 1
Sep  5 23:54:49.257: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:54:49.257: INFO: Found 0 / 1
Sep  5 23:54:50.258: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:54:50.258: INFO: Found 0 / 1
Sep  5 23:54:51.255: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:54:51.255: INFO: Found 0 / 1
Sep  5 23:54:52.256: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:54:52.256: INFO: Found 0 / 1
Sep  5 23:54:53.265: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:54:53.265: INFO: Found 0 / 1
Sep  5 23:54:54.258: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:54:54.258: INFO: Found 0 / 1
Sep  5 23:54:55.255: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:54:55.256: INFO: Found 0 / 1
Sep  5 23:54:56.255: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:54:56.255: INFO: Found 0 / 1
Sep  5 23:54:57.255: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:54:57.255: INFO: Found 0 / 1
Sep  5 23:54:58.256: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:54:58.256: INFO: Found 0 / 1
Sep  5 23:54:59.260: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:54:59.260: INFO: Found 0 / 1
Sep  5 23:55:00.262: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:55:00.262: INFO: Found 0 / 1
Sep  5 23:55:01.256: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:55:01.256: INFO: Found 0 / 1
Sep  5 23:55:02.256: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:55:02.256: INFO: Found 1 / 1
Sep  5 23:55:02.256: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep  5 23:55:02.264: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:55:02.264: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep  5 23:55:02.264: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml patch pod agnhost-primary-dbz5m --namespace=kubectl-2874 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep  5 23:55:02.412: INFO: stderr: ""
Sep  5 23:55:02.412: INFO: stdout: "pod/agnhost-primary-dbz5m patched\n"
STEP: checking annotations
Sep  5 23:55:02.420: INFO: Selector matched 1 pods for map[app:agnhost]
Sep  5 23:55:02.420: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:55:02.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2874" for this suite.

â€¢ [SLOW TEST:23.212 seconds]
[sig-cli] Kubectl client
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1490
    should add annotations for pods in rc  [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","total":291,"completed":230,"skipped":3847,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Networking
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:55:02.634: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7577
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Performing setup for networking test in namespace pod-network-test-7577
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep  5 23:55:03.087: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Sep  5 23:55:03.240: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:55:05.248: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:55:07.249: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:55:09.254: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:55:11.266: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:55:13.252: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:55:15.249: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:55:17.253: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:55:19.248: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:55:21.259: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:55:23.269: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:55:25.250: INFO: The status of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Sep  5 23:55:27.253: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 23:55:29.251: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 23:55:31.249: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 23:55:33.248: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 23:55:35.279: INFO: The status of Pod netserver-0 is Running (Ready = false)
Sep  5 23:55:37.258: INFO: The status of Pod netserver-0 is Running (Ready = true)
Sep  5 23:55:37.272: INFO: The status of Pod netserver-1 is Running (Ready = true)
Sep  5 23:55:37.286: INFO: The status of Pod netserver-2 is Running (Ready = false)
Sep  5 23:55:39.294: INFO: The status of Pod netserver-2 is Running (Ready = false)
Sep  5 23:55:41.298: INFO: The status of Pod netserver-2 is Running (Ready = false)
Sep  5 23:55:43.298: INFO: The status of Pod netserver-2 is Running (Ready = true)
STEP: Creating test pods
Sep  5 23:56:03.372: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.26.1.213:8080/dial?request=hostname&protocol=udp&host=172.26.1.210&port=8081&tries=1'] Namespace:pod-network-test-7577 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 23:56:03.372: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 23:56:03.706: INFO: Waiting for responses: map[]
Sep  5 23:56:03.713: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.26.1.213:8080/dial?request=hostname&protocol=udp&host=172.26.1.211&port=8081&tries=1'] Namespace:pod-network-test-7577 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 23:56:03.713: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 23:56:03.881: INFO: Waiting for responses: map[]
Sep  5 23:56:03.889: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.26.1.213:8080/dial?request=hostname&protocol=udp&host=172.26.1.212&port=8081&tries=1'] Namespace:pod-network-test-7577 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep  5 23:56:03.889: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  5 23:56:04.022: INFO: Waiting for responses: map[]
[AfterEach] [sig-network] Networking
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:56:04.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7577" for this suite.

â€¢ [SLOW TEST:61.620 seconds]
[sig-network] Networking
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:26
  Granular Checks: Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:29
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","total":291,"completed":231,"skipped":3852,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:56:04.255: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1364
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:58:57.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1364" for this suite.

â€¢ [SLOW TEST:173.481 seconds]
[k8s.io] Container Runtime
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  blackbox test
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:41
    when starting a container that exits
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:42
      should run with the expected status [NodeConformance] [Conformance]
      /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","total":291,"completed":232,"skipped":3871,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:58:57.736: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7323
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-map-0d090712-6e84-402f-91d7-23a9a7fd89aa
STEP: Creating a pod to test consume configMaps
Sep  5 23:58:58.339: INFO: Waiting up to 5m0s for pod "pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6" in namespace "configmap-7323" to be "Succeeded or Failed"
Sep  5 23:58:58.359: INFO: Pod "pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6": Phase="Pending", Reason="", readiness=false. Elapsed: 19.918653ms
Sep  5 23:59:00.377: INFO: Pod "pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038115653s
Sep  5 23:59:02.387: INFO: Pod "pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047781421s
Sep  5 23:59:04.413: INFO: Pod "pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073521641s
Sep  5 23:59:06.425: INFO: Pod "pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.085950867s
Sep  5 23:59:08.434: INFO: Pod "pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.09542459s
Sep  5 23:59:10.450: INFO: Pod "pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.110538665s
Sep  5 23:59:12.459: INFO: Pod "pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.120317411s
Sep  5 23:59:14.468: INFO: Pod "pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6": Phase="Pending", Reason="", readiness=false. Elapsed: 16.129083793s
Sep  5 23:59:16.477: INFO: Pod "pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.138394416s
Sep  5 23:59:18.486: INFO: Pod "pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6": Phase="Pending", Reason="", readiness=false. Elapsed: 20.146751343s
Sep  5 23:59:20.498: INFO: Pod "pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6": Phase="Pending", Reason="", readiness=false. Elapsed: 22.159222773s
Sep  5 23:59:22.505: INFO: Pod "pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6": Phase="Pending", Reason="", readiness=false. Elapsed: 24.166201271s
Sep  5 23:59:24.516: INFO: Pod "pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.177266181s
STEP: Saw pod success
Sep  5 23:59:24.516: INFO: Pod "pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6" satisfied condition "Succeeded or Failed"
Sep  5 23:59:24.525: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  5 23:59:30.793: INFO: Waiting for pod pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6 to disappear
Sep  5 23:59:30.818: INFO: Pod pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6 still exists
Sep  5 23:59:32.819: INFO: Waiting for pod pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6 to disappear
Sep  5 23:59:32.829: INFO: Pod pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6 still exists
Sep  5 23:59:34.819: INFO: Waiting for pod pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6 to disappear
Sep  5 23:59:34.827: INFO: Pod pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6 still exists
Sep  5 23:59:36.819: INFO: Waiting for pod pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6 to disappear
Sep  5 23:59:36.831: INFO: Pod pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6 still exists
Sep  5 23:59:38.819: INFO: Waiting for pod pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6 to disappear
Sep  5 23:59:38.827: INFO: Pod pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6 still exists
Sep  5 23:59:40.818: INFO: Waiting for pod pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6 to disappear
Sep  5 23:59:40.827: INFO: Pod pod-configmaps-9c3eef1f-9d9f-47e3-8f17-d81bd55815b6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  5 23:59:40.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7323" for this suite.

â€¢ [SLOW TEST:43.647 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","total":291,"completed":233,"skipped":3875,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  5 23:59:41.383: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1939
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1939.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1939.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 00:00:14.091: INFO: DNS probes using dns-1939/dns-test-e2b4426e-9c1d-4c42-838d-85ae42ba5eeb succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:00:14.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1939" for this suite.

â€¢ [SLOW TEST:33.018 seconds]
[sig-network] DNS
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","total":291,"completed":234,"skipped":3878,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:00:14.401: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5263
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/rc.go:54
[It] should release no longer matching pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep  6 00:00:14.935: INFO: Pod name pod-release: Found 0 pods out of 1
Sep  6 00:00:19.955: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:00:21.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5263" for this suite.

â€¢ [SLOW TEST:6.877 seconds]
[sig-apps] ReplicationController
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","total":291,"completed":235,"skipped":3904,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:00:21.279: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1293
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name s-test-opt-del-9474dc12-9079-4b4f-ba30-79aec9967777
STEP: Creating secret with name s-test-opt-upd-6460af68-4f0a-4c69-b884-7123122bb2fd
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9474dc12-9079-4b4f-ba30-79aec9967777
STEP: Updating secret s-test-opt-upd-6460af68-4f0a-4c69-b884-7123122bb2fd
STEP: Creating secret with name s-test-opt-create-703abbf8-bb8e-4522-93ef-d436585128dc
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:01:10.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1293" for this suite.

â€¢ [SLOW TEST:49.706 seconds]
[sig-storage] Secrets
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","total":291,"completed":236,"skipped":3906,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:01:10.985: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-watch-7268
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 00:01:11.391: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Creating first CR 
Sep  6 00:01:12.092: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-06T07:01:12Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-06T07:01:12Z]] name:name1 resourceVersion:200491 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:f7958a6e-6eed-49bb-b710-8a32385dbe0f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Sep  6 00:01:22.104: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-06T07:01:22Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-06T07:01:22Z]] name:name2 resourceVersion:200640 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:ae4f8e6c-7ab1-4e49-a9fc-80375a91a3f4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Sep  6 00:01:32.116: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-06T07:01:12Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-06T07:01:32Z]] name:name1 resourceVersion:200749 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:f7958a6e-6eed-49bb-b710-8a32385dbe0f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Sep  6 00:01:42.135: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-06T07:01:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-06T07:01:42Z]] name:name2 resourceVersion:200857 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:ae4f8e6c-7ab1-4e49-a9fc-80375a91a3f4] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Sep  6 00:01:52.169: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-06T07:01:12Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-06T07:01:32Z]] name:name1 resourceVersion:200968 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:f7958a6e-6eed-49bb-b710-8a32385dbe0f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Sep  6 00:02:02.191: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2021-09-06T07:01:22Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2021-09-06T07:01:42Z]] name:name2 resourceVersion:201078 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:ae4f8e6c-7ab1-4e49-a9fc-80375a91a3f4] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:02:12.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-7268" for this suite.

â€¢ [SLOW TEST:62.064 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","total":291,"completed":237,"skipped":3907,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
[sig-network] IngressClass API 
   should support creating IngressClass API operations [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] IngressClass API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:02:13.049: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename ingressclass
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in ingressclass-6549
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] IngressClass API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/ingressclass.go:148
[It]  should support creating IngressClass API operations [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: getting /apis
STEP: getting /apis/networking.k8s.io
STEP: getting /apis/networking.k8s.iov1
STEP: creating
STEP: getting
STEP: listing
STEP: watching
Sep  6 00:02:13.503: INFO: starting watch
STEP: patching
STEP: updating
Sep  6 00:02:13.546: INFO: waiting for watch events with expected annotations
Sep  6 00:02:13.546: INFO: saw patched and updated annotations
STEP: deleting
STEP: deleting a collection
[AfterEach] [sig-network] IngressClass API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:02:13.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-6549" for this suite.
â€¢{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","total":291,"completed":238,"skipped":3907,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:02:14.171: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6782
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should support proxy with --port 0  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting the proxy server
Sep  6 00:02:14.635: INFO: Asynchronously running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:02:14.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6782" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","total":291,"completed":239,"skipped":3918,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
S
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:02:14.921: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6725
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:02:39.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6725" for this suite.

â€¢ [SLOW TEST:24.801 seconds]
[k8s.io] Kubelet
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  when scheduling a read only busybox container
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:188
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":240,"skipped":3919,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:02:39.723: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2119
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 00:02:40.515: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ea8a8bee-9041-4c4e-856a-954d19080d59", Controller:(*bool)(0xc001f8feba), BlockOwnerDeletion:(*bool)(0xc001f8febb)}}
Sep  6 00:02:40.640: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"2fb4808a-2ee2-44a5-b5ec-9948c179d656", Controller:(*bool)(0xc0033880ba), BlockOwnerDeletion:(*bool)(0xc0033880bb)}}
Sep  6 00:02:40.688: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"f239733a-f99c-4f1c-ad76-816ccb9ac9f9", Controller:(*bool)(0xc0033882c2), BlockOwnerDeletion:(*bool)(0xc0033882c3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:02:45.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2119" for this suite.

â€¢ [SLOW TEST:6.276 seconds]
[sig-api-machinery] Garbage collector
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","total":291,"completed":241,"skipped":3966,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:02:45.999: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename resourcequota
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in resourcequota-7869
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:03:00.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7869" for this suite.

â€¢ [SLOW TEST:15.092 seconds]
[sig-api-machinery] ResourceQuota
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","total":291,"completed":242,"skipped":3970,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:03:01.092: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9022
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support rollover [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 00:03:01.571: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep  6 00:03:06.588: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep  6 00:03:24.625: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep  6 00:03:26.637: INFO: Creating deployment "test-rollover-deployment"
Sep  6 00:03:26.674: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep  6 00:03:28.699: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep  6 00:03:28.716: INFO: Ensure that both replica sets have 1 created replica
Sep  6 00:03:28.728: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep  6 00:03:28.757: INFO: Updating deployment test-rollover-deployment
Sep  6 00:03:28.757: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep  6 00:03:30.831: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep  6 00:03:30.854: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep  6 00:03:30.880: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 00:03:30.881: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:03:32.895: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 00:03:32.895: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:03:34.898: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 00:03:34.899: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:03:36.895: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 00:03:36.895: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:03:38.901: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 00:03:38.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:03:40.902: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 00:03:40.902: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:03:43.196: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 00:03:43.196: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:03:44.894: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 00:03:44.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:03:46.894: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 00:03:46.894: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:03:48.895: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 00:03:48.895: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:03:50.895: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 00:03:50.895: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:03:52.909: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 00:03:52.909: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 29, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:03:54.900: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 00:03:54.900: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:03:56.893: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 00:03:56.893: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:03:58.896: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 00:03:58.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:04:00.946: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 00:04:00.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:04:02.896: INFO: all replica sets need to contain the pod-template-hash label
Sep  6 00:04:02.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 3, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 3, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5797c7764\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:04:04.929: INFO: 
Sep  6 00:04:04.929: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep  6 00:04:04.961: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9022 /apis/apps/v1/namespaces/deployment-9022/deployments/test-rollover-deployment 2e9e924c-59eb-423b-8b01-7f36caab4188 202823 2 2021-09-06 00:03:26 -0700 PDT <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-09-06 00:03:28 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-06 00:04:03 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0083f3128 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2021-09-06 00:03:26 -0700 PDT,LastTransitionTime:2021-09-06 00:03:26 -0700 PDT,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-5797c7764" has successfully progressed.,LastUpdateTime:2021-09-06 00:04:03 -0700 PDT,LastTransitionTime:2021-09-06 00:03:26 -0700 PDT,},},ReadyReplicas:1,CollisionCount:nil,},}

Sep  6 00:04:04.970: INFO: New ReplicaSet "test-rollover-deployment-5797c7764" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-5797c7764  deployment-9022 /apis/apps/v1/namespaces/deployment-9022/replicasets/test-rollover-deployment-5797c7764 dc958486-cf8b-45f5-8d38-9691c70817fb 202813 2 2021-09-06 00:03:28 -0700 PDT <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 2e9e924c-59eb-423b-8b01-7f36caab4188 0xc0083f3680 0xc0083f3681}] []  [{kube-controller-manager Update apps/v1 2021-09-06 00:04:03 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e9e924c-59eb-423b-8b01-7f36caab4188\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5797c7764,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[] [] []  []} {[] [] [{agnhost k8s.gcr.io/e2e-test-images/agnhost:2.20 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0083f3708 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Sep  6 00:04:04.970: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep  6 00:04:04.970: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9022 /apis/apps/v1/namespaces/deployment-9022/replicasets/test-rollover-controller b8f064fa-8fa9-4266-aac0-9434621d28fc 202821 2 2021-09-06 00:03:01 -0700 PDT <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 2e9e924c-59eb-423b-8b01-7f36caab4188 0xc0083f3557 0xc0083f3558}] []  [{e2e.test Update apps/v1 2021-09-06 00:03:01 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-06 00:04:03 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e9e924c-59eb-423b-8b01-7f36caab4188\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0083f3618 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 00:04:04.971: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-78bc8b888c  deployment-9022 /apis/apps/v1/namespaces/deployment-9022/replicasets/test-rollover-deployment-78bc8b888c 25f23838-4b9e-4dea-b7cf-a538ab6d2baa 202356 2 2021-09-06 00:03:26 -0700 PDT <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 2e9e924c-59eb-423b-8b01-7f36caab4188 0xc0083f3777 0xc0083f3778}] []  [{kube-controller-manager Update apps/v1 2021-09-06 00:03:28 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2e9e924c-59eb-423b-8b01-7f36caab4188\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 78bc8b888c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:78bc8b888c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0083f3808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 00:04:04.983: INFO: Pod "test-rollover-controller-qj4gk" is available:
&Pod{ObjectMeta:{test-rollover-controller-qj4gk test-rollover-controller- deployment-9022 /api/v1/namespaces/deployment-9022/pods/test-rollover-controller-qj4gk 48dee446-dbd9-4687-bce2-56b02d14e2ae 202817 0 2021-09-06 00:03:01 -0700 PDT 2021-09-06 00:04:03 -0700 PDT 0xc0083f3d58 map[name:rollover-pod pod:httpd] map[attachment_id:d01bbd5e-9140-4871-bcc0-ced778518b77 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:60:18 vlan:None vmware-system-ephemeral-disk-uuid:6000C295-6bee-6e9f-b035-98f55451d0fe vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v16877"} vmware-system-vm-moid:vm-1378:7badc608-dd2f-42a9-952d-0d4c30d5f283 vmware-system-vm-uuid:5017db07-9af5-eb65-1f4c-ceacd30a9f20] [{apps/v1 ReplicaSet test-rollover-controller b8f064fa-8fa9-4266-aac0-9434621d28fc 0xc0083f3da7 0xc0083f3da8}] [lifecycle-controller/system.vmware.com]  [{image-controller Update v1 2021-09-06 00:03:01 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {kube-controller-manager Update v1 2021-09-06 00:03:01 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8f064fa-8fa9-4266-aac0-9434621d28fc\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-9mqvr Update v1 2021-09-06 00:03:10 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-06 00:03:15 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:03:23 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wkqwn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wkqwn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wkqwn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-34-149.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:03:01 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:03:24 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:03:24 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:03:24 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.34.149,PodIP:172.26.1.194,StartTime:2021-09-06 00:03:20 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:03:21 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v16877,ContainerID:d3e4ce91-9dd4-4a30-aa7b-216df2a3d3a4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:04:04.983: INFO: Pod "test-rollover-deployment-5797c7764-lwntx" is available:
&Pod{ObjectMeta:{test-rollover-deployment-5797c7764-lwntx test-rollover-deployment-5797c7764- deployment-9022 /api/v1/namespaces/deployment-9022/pods/test-rollover-deployment-5797c7764-lwntx bb5d7048-081c-4c9d-89fc-3137c3df2521 202697 0 2021-09-06 00:03:28 -0700 PDT <nil> <nil> map[name:rollover-pod pod-template-hash:5797c7764] map[attachment_id:b3e6b7d6-7d0c-415a-b143-7b2f04805e7e kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:60:1f vlan:None vmware-system-ephemeral-disk-uuid:6000C290-d9bc-65e0-5cdb-b1199e8c7fd4 vmware-system-image-references:{"agnhost":"agnhost-e4bbf9f419e02f0332193ee525b99e9c3c927d53-v58863"} vmware-system-vm-moid:vm-1381:7badc608-dd2f-42a9-952d-0d4c30d5f283 vmware-system-vm-uuid:5017646e-b7e9-1d43-ebda-e2bbabcd1c1c] [{apps/v1 ReplicaSet test-rollover-deployment-5797c7764 dc958486-cf8b-45f5-8d38-9691c70817fb 0xc0083f3fa7 0xc0083f3fa8}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:03:28 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dc958486-cf8b-45f5-8d38-9691c70817fb\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-06 00:03:29 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {nsx-ncp-6d7f7bf559-9mqvr Update v1 2021-09-06 00:03:31 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-06 00:03:45 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:03:53 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-wkqwn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-wkqwn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-wkqwn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-43-208.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:03:29 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:03:53 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:03:53 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:03:53 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.43.208,PodIP:172.26.1.196,StartTime:2021-09-06 00:03:48 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:03:49 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:k8s.gcr.io/e2e-test-images/agnhost:2.20,ImageID:agnhost-e4bbf9f419e02f0332193ee525b99e9c3c927d53-v58863,ContainerID:595db1dc-a760-48a9-8048-3a7e320ad301,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.196,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:04:04.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9022" for this suite.

â€¢ [SLOW TEST:64.149 seconds]
[sig-apps] Deployment
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","total":291,"completed":243,"skipped":3987,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:04:05.242: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-7930
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 00:04:05.596: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep  6 00:04:13.237: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7930 create -f -'
Sep  6 00:04:14.318: INFO: stderr: ""
Sep  6 00:04:14.318: INFO: stdout: "e2e-test-crd-publish-openapi-1890-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  6 00:04:14.318: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7930 delete e2e-test-crd-publish-openapi-1890-crds test-cr'
Sep  6 00:04:14.436: INFO: stderr: ""
Sep  6 00:04:14.436: INFO: stdout: "e2e-test-crd-publish-openapi-1890-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Sep  6 00:04:14.436: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7930 apply -f -'
Sep  6 00:04:14.812: INFO: stderr: ""
Sep  6 00:04:14.812: INFO: stdout: "e2e-test-crd-publish-openapi-1890-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Sep  6 00:04:14.812: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-7930 delete e2e-test-crd-publish-openapi-1890-crds test-cr'
Sep  6 00:04:14.946: INFO: stderr: ""
Sep  6 00:04:14.946: INFO: stdout: "e2e-test-crd-publish-openapi-1890-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Sep  6 00:04:14.946: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml explain e2e-test-crd-publish-openapi-1890-crds'
Sep  6 00:04:15.279: INFO: stderr: ""
Sep  6 00:04:15.279: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1890-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:04:22.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7930" for this suite.

â€¢ [SLOW TEST:17.324 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","total":291,"completed":244,"skipped":3994,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run 
  should check if kubectl can dry-run update Pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:04:22.566: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9804
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[It] should check if kubectl can dry-run update Pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image mirror.gcr.io/library/httpd:2.4.38-alpine
Sep  6 00:04:22.993: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml run e2e-test-httpd-pod --image=mirror.gcr.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-9804'
Sep  6 00:04:23.149: INFO: stderr: ""
Sep  6 00:04:23.149: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run
Sep  6 00:04:23.149: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pod e2e-test-httpd-pod -o json --namespace=kubectl-9804'
Sep  6 00:04:23.260: INFO: stderr: ""
Sep  6 00:04:23.260: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2021-09-06T07:04:23Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9804\",\n        \"resourceVersion\": \"203141\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9804/pods/e2e-test-httpd-pod\",\n        \"uid\": \"6b890b0a-e74d-4289-bfbc-09d4a175e27b\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"mirror.gcr.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-sw9fq\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-sw9fq\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-sw9fq\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"phase\": \"Pending\",\n        \"qosClass\": \"BestEffort\"\n    }\n}\n"
Sep  6 00:04:23.260: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml replace -f - --dry-run server --namespace=kubectl-9804'
Sep  6 00:04:23.889: INFO: stderr: "W0906 00:04:23.372603    2677 helpers.go:557] --dry-run is deprecated and can be replaced with --dry-run=client.\n"
Sep  6 00:04:23.889: INFO: stdout: "pod/e2e-test-httpd-pod replaced (dry run)\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image mirror.gcr.io/library/httpd:2.4.38-alpine
Sep  6 00:04:23.895: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml delete pods e2e-test-httpd-pod --namespace=kubectl-9804'
Sep  6 00:04:24.098: INFO: stderr: ""
Sep  6 00:04:24.098: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:04:24.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9804" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","total":291,"completed":245,"skipped":4026,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:04:24.557: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-8642
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 00:04:25.257: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Sep  6 00:04:33.485: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-8642 create -f -'
Sep  6 00:04:34.535: INFO: stderr: ""
Sep  6 00:04:34.535: INFO: stdout: "e2e-test-crd-publish-openapi-4057-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  6 00:04:34.535: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-8642 delete e2e-test-crd-publish-openapi-4057-crds test-cr'
Sep  6 00:04:34.672: INFO: stderr: ""
Sep  6 00:04:34.672: INFO: stdout: "e2e-test-crd-publish-openapi-4057-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Sep  6 00:04:34.672: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-8642 apply -f -'
Sep  6 00:04:35.150: INFO: stderr: ""
Sep  6 00:04:35.150: INFO: stdout: "e2e-test-crd-publish-openapi-4057-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Sep  6 00:04:35.150: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml --namespace=crd-publish-openapi-8642 delete e2e-test-crd-publish-openapi-4057-crds test-cr'
Sep  6 00:04:35.312: INFO: stderr: ""
Sep  6 00:04:35.312: INFO: stdout: "e2e-test-crd-publish-openapi-4057-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Sep  6 00:04:35.312: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml explain e2e-test-crd-publish-openapi-4057-crds'
Sep  6 00:04:35.721: INFO: stderr: ""
Sep  6 00:04:35.721: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-4057-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:04:43.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8642" for this suite.

â€¢ [SLOW TEST:19.583 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","total":291,"completed":246,"skipped":4057,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:04:44.140: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5876
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:135
[It] should rollback without unnecessary restarts [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 00:04:44.732: INFO: Create a RollingUpdate DaemonSet
Sep  6 00:04:44.762: INFO: Check that daemon pods launch on every node of the cluster
Sep  6 00:04:44.775: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:44.775: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:44.775: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:44.787: INFO: Number of nodes with available pods: 0
Sep  6 00:04:44.787: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:04:45.799: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:45.799: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:45.799: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:45.811: INFO: Number of nodes with available pods: 0
Sep  6 00:04:45.811: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:04:46.799: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:46.799: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:46.799: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:46.806: INFO: Number of nodes with available pods: 0
Sep  6 00:04:46.806: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:04:47.797: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:47.798: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:47.798: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:47.804: INFO: Number of nodes with available pods: 0
Sep  6 00:04:47.804: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:04:48.798: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:48.798: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:48.798: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:48.808: INFO: Number of nodes with available pods: 0
Sep  6 00:04:48.809: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:04:49.819: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:49.819: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:49.819: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:49.840: INFO: Number of nodes with available pods: 0
Sep  6 00:04:49.840: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:04:50.811: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:50.811: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:50.811: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:50.819: INFO: Number of nodes with available pods: 0
Sep  6 00:04:50.819: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:04:51.809: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:51.809: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:51.809: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:51.841: INFO: Number of nodes with available pods: 0
Sep  6 00:04:51.841: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:04:52.796: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:52.796: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:52.796: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:52.815: INFO: Number of nodes with available pods: 0
Sep  6 00:04:52.815: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:04:53.805: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:53.805: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:53.805: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:53.815: INFO: Number of nodes with available pods: 0
Sep  6 00:04:53.815: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:04:54.859: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:54.860: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:54.860: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:54.877: INFO: Number of nodes with available pods: 0
Sep  6 00:04:54.877: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:04:55.835: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:55.835: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:55.835: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:55.848: INFO: Number of nodes with available pods: 0
Sep  6 00:04:55.848: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:04:56.802: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:56.802: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:56.802: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:56.812: INFO: Number of nodes with available pods: 0
Sep  6 00:04:56.812: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:04:57.798: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:57.798: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:57.798: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:57.806: INFO: Number of nodes with available pods: 0
Sep  6 00:04:57.806: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:04:58.798: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:58.798: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:58.798: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:58.805: INFO: Number of nodes with available pods: 0
Sep  6 00:04:58.805: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:04:59.797: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:59.797: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:59.797: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:04:59.803: INFO: Number of nodes with available pods: 0
Sep  6 00:04:59.803: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:05:00.801: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:00.801: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:00.801: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:00.810: INFO: Number of nodes with available pods: 0
Sep  6 00:05:00.810: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:05:01.804: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:01.804: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:01.804: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:01.812: INFO: Number of nodes with available pods: 0
Sep  6 00:05:01.812: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:05:02.797: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:02.797: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:02.797: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:02.807: INFO: Number of nodes with available pods: 0
Sep  6 00:05:02.807: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:05:03.805: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:03.805: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:03.805: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:03.815: INFO: Number of nodes with available pods: 0
Sep  6 00:05:03.815: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:05:04.798: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:04.798: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:04.798: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:04.807: INFO: Number of nodes with available pods: 0
Sep  6 00:05:04.807: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:05:05.804: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:05.804: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:05.804: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:05.813: INFO: Number of nodes with available pods: 0
Sep  6 00:05:05.813: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:05:06.799: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:06.799: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:06.799: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:06.809: INFO: Number of nodes with available pods: 0
Sep  6 00:05:06.809: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:05:07.798: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:07.798: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:07.798: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:07.811: INFO: Number of nodes with available pods: 0
Sep  6 00:05:07.811: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:05:08.798: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:08.798: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:08.798: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:08.805: INFO: Number of nodes with available pods: 0
Sep  6 00:05:08.805: INFO: Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com is running more than one daemon pod
Sep  6 00:05:09.804: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:09.804: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:09.804: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:09.813: INFO: Number of nodes with available pods: 3
Sep  6 00:05:09.813: INFO: Number of running nodes: 3, number of available pods: 3
Sep  6 00:05:09.813: INFO: Update the DaemonSet to trigger a rollout
Sep  6 00:05:09.839: INFO: Updating DaemonSet daemon-set
Sep  6 00:05:21.920: INFO: Roll back the DaemonSet before rollout is complete
Sep  6 00:05:21.963: INFO: Updating DaemonSet daemon-set
Sep  6 00:05:21.963: INFO: Make sure DaemonSet rollback is complete
Sep  6 00:05:21.987: INFO: Wrong image for pod: daemon-set-8mtw6. Expected: mirror.gcr.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Sep  6 00:05:21.987: INFO: Pod daemon-set-8mtw6 is not available
Sep  6 00:05:22.010: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:22.010: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:22.010: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:23.021: INFO: Pod daemon-set-jsw88 is not available
Sep  6 00:05:23.039: INFO: DaemonSet pods can't tolerate node 42174da95d8c532b15b7283e9031a350 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:23.039: INFO: DaemonSet pods can't tolerate node 4217572e1dd2cd00e2d13546f91cde38 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Sep  6 00:05:23.039: INFO: DaemonSet pods can't tolerate node 4217cb448371cdb65f43561dccee265e with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:101
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5876, will wait for the garbage collector to delete the pods
Sep  6 00:05:23.220: INFO: Deleting DaemonSet.extensions daemon-set took: 96.601476ms
Sep  6 00:05:25.421: INFO: Terminating DaemonSet.extensions daemon-set pods took: 2.20087824s
Sep  6 00:05:41.929: INFO: Number of nodes with available pods: 0
Sep  6 00:05:41.929: INFO: Number of running nodes: 0, number of available pods: 0
Sep  6 00:05:41.934: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5876/daemonsets","resourceVersion":"204188"},"items":null}

Sep  6 00:05:41.940: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5876/pods","resourceVersion":"204188"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:05:41.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5876" for this suite.

â€¢ [SLOW TEST:58.022 seconds]
[sig-apps] Daemon set [Serial]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","total":291,"completed":247,"skipped":4074,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:05:42.163: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5150
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 00:05:42.682: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765" in namespace "projected-5150" to be "Succeeded or Failed"
Sep  6 00:05:42.702: INFO: Pod "downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765": Phase="Pending", Reason="", readiness=false. Elapsed: 19.823365ms
Sep  6 00:05:44.711: INFO: Pod "downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028825033s
Sep  6 00:05:46.722: INFO: Pod "downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039903729s
Sep  6 00:05:48.734: INFO: Pod "downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765": Phase="Pending", Reason="", readiness=false. Elapsed: 6.052185921s
Sep  6 00:05:50.751: INFO: Pod "downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765": Phase="Pending", Reason="", readiness=false. Elapsed: 8.069157437s
Sep  6 00:05:52.758: INFO: Pod "downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765": Phase="Pending", Reason="", readiness=false. Elapsed: 10.076154993s
Sep  6 00:05:54.788: INFO: Pod "downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765": Phase="Pending", Reason="", readiness=false. Elapsed: 12.10626492s
Sep  6 00:05:56.819: INFO: Pod "downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765": Phase="Pending", Reason="", readiness=false. Elapsed: 14.13693793s
Sep  6 00:05:58.844: INFO: Pod "downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765": Phase="Pending", Reason="", readiness=false. Elapsed: 16.16198736s
Sep  6 00:06:00.857: INFO: Pod "downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765": Phase="Pending", Reason="", readiness=false. Elapsed: 18.174754007s
Sep  6 00:06:02.868: INFO: Pod "downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765": Phase="Pending", Reason="", readiness=false. Elapsed: 20.185698512s
Sep  6 00:06:04.889: INFO: Pod "downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765": Phase="Pending", Reason="", readiness=false. Elapsed: 22.207183206s
Sep  6 00:06:06.903: INFO: Pod "downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765": Phase="Running", Reason="", readiness=true. Elapsed: 24.220444293s
Sep  6 00:06:08.922: INFO: Pod "downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765": Phase="Running", Reason="", readiness=true. Elapsed: 26.239964252s
Sep  6 00:06:10.947: INFO: Pod "downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.264795683s
STEP: Saw pod success
Sep  6 00:06:10.947: INFO: Pod "downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765" satisfied condition "Succeeded or Failed"
Sep  6 00:06:10.972: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com pod downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765 container client-container: <nil>
STEP: delete the pod
Sep  6 00:06:11.118: INFO: Waiting for pod downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765 to disappear
Sep  6 00:06:11.162: INFO: Pod downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765 still exists
Sep  6 00:06:13.163: INFO: Waiting for pod downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765 to disappear
Sep  6 00:06:13.174: INFO: Pod downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765 still exists
Sep  6 00:06:15.162: INFO: Waiting for pod downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765 to disappear
Sep  6 00:06:15.172: INFO: Pod downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765 still exists
Sep  6 00:06:17.162: INFO: Waiting for pod downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765 to disappear
Sep  6 00:06:17.172: INFO: Pod downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765 still exists
Sep  6 00:06:19.162: INFO: Waiting for pod downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765 to disappear
Sep  6 00:06:19.171: INFO: Pod downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765 still exists
Sep  6 00:06:21.162: INFO: Waiting for pod downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765 to disappear
Sep  6 00:06:21.195: INFO: Pod downwardapi-volume-7fb9eca8-3755-450c-bb1f-2daa36836765 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:06:21.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5150" for this suite.

â€¢ [SLOW TEST:39.277 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":291,"completed":248,"skipped":4107,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:06:21.444: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6173
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with configMap that has name projected-configmap-test-upd-5b790367-df3d-4218-ae3a-db69f27b107d
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-5b790367-df3d-4218-ae3a-db69f27b107d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:07:08.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6173" for this suite.

â€¢ [SLOW TEST:47.490 seconds]
[sig-storage] Projected configMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:36
  updates should be reflected in volume [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","total":291,"completed":249,"skipped":4126,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:07:08.934: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7496
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Update Demo
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:308
[It] should scale a replication controller  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a replication controller
Sep  6 00:07:09.368: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml create -f - --namespace=kubectl-7496'
Sep  6 00:07:10.245: INFO: stderr: ""
Sep  6 00:07:10.245: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 00:07:10.245: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7496'
Sep  6 00:07:10.460: INFO: stderr: ""
Sep  6 00:07:10.460: INFO: stdout: "update-demo-nautilus-24vkd update-demo-nautilus-hkfqs "
Sep  6 00:07:10.460: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-24vkd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:07:10.670: INFO: stderr: ""
Sep  6 00:07:10.670: INFO: stdout: ""
Sep  6 00:07:10.670: INFO: update-demo-nautilus-24vkd is created but not running
Sep  6 00:07:15.672: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7496'
Sep  6 00:07:15.772: INFO: stderr: ""
Sep  6 00:07:15.772: INFO: stdout: "update-demo-nautilus-24vkd update-demo-nautilus-hkfqs "
Sep  6 00:07:15.772: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-24vkd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:07:15.886: INFO: stderr: ""
Sep  6 00:07:15.886: INFO: stdout: ""
Sep  6 00:07:15.886: INFO: update-demo-nautilus-24vkd is created but not running
Sep  6 00:07:20.887: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7496'
Sep  6 00:07:21.000: INFO: stderr: ""
Sep  6 00:07:21.000: INFO: stdout: "update-demo-nautilus-24vkd update-demo-nautilus-hkfqs "
Sep  6 00:07:21.001: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-24vkd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:07:21.110: INFO: stderr: ""
Sep  6 00:07:21.110: INFO: stdout: ""
Sep  6 00:07:21.110: INFO: update-demo-nautilus-24vkd is created but not running
Sep  6 00:07:26.111: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7496'
Sep  6 00:07:26.228: INFO: stderr: ""
Sep  6 00:07:26.228: INFO: stdout: "update-demo-nautilus-24vkd update-demo-nautilus-hkfqs "
Sep  6 00:07:26.228: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-24vkd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:07:26.329: INFO: stderr: ""
Sep  6 00:07:26.329: INFO: stdout: ""
Sep  6 00:07:26.329: INFO: update-demo-nautilus-24vkd is created but not running
Sep  6 00:07:31.329: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7496'
Sep  6 00:07:31.445: INFO: stderr: ""
Sep  6 00:07:31.445: INFO: stdout: "update-demo-nautilus-24vkd update-demo-nautilus-hkfqs "
Sep  6 00:07:31.445: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-24vkd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:07:31.552: INFO: stderr: ""
Sep  6 00:07:31.552: INFO: stdout: ""
Sep  6 00:07:31.552: INFO: update-demo-nautilus-24vkd is created but not running
Sep  6 00:07:36.552: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7496'
Sep  6 00:07:36.687: INFO: stderr: ""
Sep  6 00:07:36.687: INFO: stdout: "update-demo-nautilus-24vkd update-demo-nautilus-hkfqs "
Sep  6 00:07:36.687: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-24vkd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:07:36.791: INFO: stderr: ""
Sep  6 00:07:36.791: INFO: stdout: "true"
Sep  6 00:07:36.791: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-24vkd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:07:36.911: INFO: stderr: ""
Sep  6 00:07:36.911: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 00:07:36.911: INFO: validating pod update-demo-nautilus-24vkd
Sep  6 00:07:36.981: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 00:07:36.986: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 00:07:36.986: INFO: update-demo-nautilus-24vkd is verified up and running
Sep  6 00:07:36.986: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-hkfqs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:07:37.116: INFO: stderr: ""
Sep  6 00:07:37.116: INFO: stdout: "true"
Sep  6 00:07:37.116: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-hkfqs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:07:37.222: INFO: stderr: ""
Sep  6 00:07:37.222: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 00:07:37.222: INFO: validating pod update-demo-nautilus-hkfqs
Sep  6 00:07:37.271: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 00:07:37.271: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 00:07:37.271: INFO: update-demo-nautilus-hkfqs is verified up and running
STEP: scaling down the replication controller
Sep  6 00:07:37.295: INFO: scanned /home/worker for discovery docs: <nil>
Sep  6 00:07:37.295: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-7496'
Sep  6 00:07:38.462: INFO: stderr: ""
Sep  6 00:07:38.462: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 00:07:38.462: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7496'
Sep  6 00:07:38.580: INFO: stderr: ""
Sep  6 00:07:38.580: INFO: stdout: "update-demo-nautilus-24vkd update-demo-nautilus-hkfqs "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep  6 00:07:43.581: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7496'
Sep  6 00:07:43.696: INFO: stderr: ""
Sep  6 00:07:43.696: INFO: stdout: "update-demo-nautilus-24vkd update-demo-nautilus-hkfqs "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep  6 00:07:48.696: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7496'
Sep  6 00:07:48.814: INFO: stderr: ""
Sep  6 00:07:48.814: INFO: stdout: "update-demo-nautilus-hkfqs "
Sep  6 00:07:48.814: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-hkfqs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:07:48.917: INFO: stderr: ""
Sep  6 00:07:48.917: INFO: stdout: "true"
Sep  6 00:07:48.917: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-hkfqs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:07:49.033: INFO: stderr: ""
Sep  6 00:07:49.033: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 00:07:49.033: INFO: validating pod update-demo-nautilus-hkfqs
Sep  6 00:07:49.044: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 00:07:49.044: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 00:07:49.044: INFO: update-demo-nautilus-hkfqs is verified up and running
STEP: scaling up the replication controller
Sep  6 00:07:49.049: INFO: scanned /home/worker for discovery docs: <nil>
Sep  6 00:07:49.049: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-7496'
Sep  6 00:07:50.177: INFO: stderr: ""
Sep  6 00:07:50.177: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep  6 00:07:50.177: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7496'
Sep  6 00:07:50.276: INFO: stderr: ""
Sep  6 00:07:50.276: INFO: stdout: "update-demo-nautilus-hkfqs update-demo-nautilus-sbfs7 "
Sep  6 00:07:50.276: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-hkfqs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:07:50.370: INFO: stderr: ""
Sep  6 00:07:50.370: INFO: stdout: "true"
Sep  6 00:07:50.370: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-hkfqs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:07:50.479: INFO: stderr: ""
Sep  6 00:07:50.479: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 00:07:50.480: INFO: validating pod update-demo-nautilus-hkfqs
Sep  6 00:07:50.528: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 00:07:50.528: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 00:07:50.528: INFO: update-demo-nautilus-hkfqs is verified up and running
Sep  6 00:07:50.528: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-sbfs7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:07:50.630: INFO: stderr: ""
Sep  6 00:07:50.630: INFO: stdout: ""
Sep  6 00:07:50.630: INFO: update-demo-nautilus-sbfs7 is created but not running
Sep  6 00:07:55.630: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7496'
Sep  6 00:07:55.732: INFO: stderr: ""
Sep  6 00:07:55.732: INFO: stdout: "update-demo-nautilus-hkfqs update-demo-nautilus-sbfs7 "
Sep  6 00:07:55.732: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-hkfqs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:07:55.828: INFO: stderr: ""
Sep  6 00:07:55.828: INFO: stdout: "true"
Sep  6 00:07:55.828: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-hkfqs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:07:55.937: INFO: stderr: ""
Sep  6 00:07:55.937: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 00:07:55.937: INFO: validating pod update-demo-nautilus-hkfqs
Sep  6 00:07:55.949: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 00:07:55.949: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 00:07:55.949: INFO: update-demo-nautilus-hkfqs is verified up and running
Sep  6 00:07:55.949: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-sbfs7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:07:56.049: INFO: stderr: ""
Sep  6 00:07:56.049: INFO: stdout: ""
Sep  6 00:07:56.049: INFO: update-demo-nautilus-sbfs7 is created but not running
Sep  6 00:08:01.050: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7496'
Sep  6 00:08:01.198: INFO: stderr: ""
Sep  6 00:08:01.198: INFO: stdout: "update-demo-nautilus-hkfqs update-demo-nautilus-sbfs7 "
Sep  6 00:08:01.199: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-hkfqs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:08:01.318: INFO: stderr: ""
Sep  6 00:08:01.318: INFO: stdout: "true"
Sep  6 00:08:01.318: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-hkfqs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:08:01.459: INFO: stderr: ""
Sep  6 00:08:01.459: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 00:08:01.459: INFO: validating pod update-demo-nautilus-hkfqs
Sep  6 00:08:01.498: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 00:08:01.498: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 00:08:01.498: INFO: update-demo-nautilus-hkfqs is verified up and running
Sep  6 00:08:01.498: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-sbfs7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:08:01.604: INFO: stderr: ""
Sep  6 00:08:01.604: INFO: stdout: ""
Sep  6 00:08:01.605: INFO: update-demo-nautilus-sbfs7 is created but not running
Sep  6 00:08:06.605: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7496'
Sep  6 00:08:06.710: INFO: stderr: ""
Sep  6 00:08:06.710: INFO: stdout: "update-demo-nautilus-hkfqs update-demo-nautilus-sbfs7 "
Sep  6 00:08:06.710: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-hkfqs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:08:06.806: INFO: stderr: ""
Sep  6 00:08:06.806: INFO: stdout: "true"
Sep  6 00:08:06.806: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-hkfqs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:08:06.899: INFO: stderr: ""
Sep  6 00:08:06.899: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 00:08:06.899: INFO: validating pod update-demo-nautilus-hkfqs
Sep  6 00:08:06.921: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 00:08:06.921: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 00:08:06.921: INFO: update-demo-nautilus-hkfqs is verified up and running
Sep  6 00:08:06.921: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-sbfs7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:08:07.020: INFO: stderr: ""
Sep  6 00:08:07.020: INFO: stdout: ""
Sep  6 00:08:07.020: INFO: update-demo-nautilus-sbfs7 is created but not running
Sep  6 00:08:12.021: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7496'
Sep  6 00:08:12.122: INFO: stderr: ""
Sep  6 00:08:12.122: INFO: stdout: "update-demo-nautilus-hkfqs update-demo-nautilus-sbfs7 "
Sep  6 00:08:12.122: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-hkfqs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:08:12.231: INFO: stderr: ""
Sep  6 00:08:12.231: INFO: stdout: "true"
Sep  6 00:08:12.231: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-hkfqs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:08:12.363: INFO: stderr: ""
Sep  6 00:08:12.363: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 00:08:12.364: INFO: validating pod update-demo-nautilus-hkfqs
Sep  6 00:08:12.386: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 00:08:12.386: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 00:08:12.386: INFO: update-demo-nautilus-hkfqs is verified up and running
Sep  6 00:08:12.386: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-sbfs7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:08:12.489: INFO: stderr: ""
Sep  6 00:08:12.489: INFO: stdout: "true"
Sep  6 00:08:12.489: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods update-demo-nautilus-sbfs7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7496'
Sep  6 00:08:12.589: INFO: stderr: ""
Sep  6 00:08:12.589: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep  6 00:08:12.589: INFO: validating pod update-demo-nautilus-sbfs7
Sep  6 00:08:12.661: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep  6 00:08:12.661: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep  6 00:08:12.661: INFO: update-demo-nautilus-sbfs7 is verified up and running
STEP: using delete to clean up resources
Sep  6 00:08:12.661: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml delete --grace-period=0 --force -f - --namespace=kubectl-7496'
Sep  6 00:08:12.827: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep  6 00:08:12.827: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep  6 00:08:12.827: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7496'
Sep  6 00:08:12.963: INFO: stderr: "No resources found in kubectl-7496 namespace.\n"
Sep  6 00:08:12.963: INFO: stdout: ""
Sep  6 00:08:12.963: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml get pods -l name=update-demo --namespace=kubectl-7496 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep  6 00:08:13.082: INFO: stderr: ""
Sep  6 00:08:13.082: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:08:13.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7496" for this suite.

â€¢ [SLOW TEST:64.388 seconds]
[sig-cli] Kubectl client
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:306
    should scale a replication controller  [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","total":291,"completed":250,"skipped":4140,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:08:13.322: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9988
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  6 00:08:40.358: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e1f9e0f1-1fb0-4f7c-8d23-a71cc4d291c2"
Sep  6 00:08:40.358: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e1f9e0f1-1fb0-4f7c-8d23-a71cc4d291c2" in namespace "pods-9988" to be "terminated due to deadline exceeded"
Sep  6 00:08:40.369: INFO: Pod "pod-update-activedeadlineseconds-e1f9e0f1-1fb0-4f7c-8d23-a71cc4d291c2": Phase="Running", Reason="", readiness=true. Elapsed: 10.439708ms
Sep  6 00:08:42.379: INFO: Pod "pod-update-activedeadlineseconds-e1f9e0f1-1fb0-4f7c-8d23-a71cc4d291c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.020733931s
Sep  6 00:08:44.389: INFO: Pod "pod-update-activedeadlineseconds-e1f9e0f1-1fb0-4f7c-8d23-a71cc4d291c2": Phase="Running", Reason="", readiness=true. Elapsed: 4.03113345s
Sep  6 00:08:46.401: INFO: Pod "pod-update-activedeadlineseconds-e1f9e0f1-1fb0-4f7c-8d23-a71cc4d291c2": Phase="Failed", Reason="DeadlineExceeded", readiness=true. Elapsed: 6.042759606s
Sep  6 00:08:46.401: INFO: Pod "pod-update-activedeadlineseconds-e1f9e0f1-1fb0-4f7c-8d23-a71cc4d291c2" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:08:46.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9988" for this suite.

â€¢ [SLOW TEST:33.290 seconds]
[k8s.io] Pods
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","total":291,"completed":251,"skipped":4142,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:08:46.613: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-7815
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 00:08:48.868: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  6 00:08:50.910: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:08:52.924: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:08:54.919: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:08:56.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:08:58.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:09:00.927: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:09:02.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:09:04.919: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:09:06.921: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:09:08.929: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:09:10.924: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:09:12.963: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:09:14.933: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 8, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 00:09:17.961: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:09:18.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7815" for this suite.
STEP: Destroying namespace "webhook-7815-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:32.749 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","total":291,"completed":252,"skipped":4154,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:09:19.362: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5964
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5964.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5964.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5964.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5964.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5964.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5964.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep  6 00:09:52.087: INFO: DNS probes using dns-5964/dns-test-aaa9efef-c118-4e70-84ed-57f5f7b97525 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:09:52.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5964" for this suite.

â€¢ [SLOW TEST:32.984 seconds]
[sig-network] DNS
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]","total":291,"completed":253,"skipped":4161,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:09:52.347: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 00:09:53.604: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  6 00:09:55.627: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:09:57.639: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:09:59.648: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:10:01.637: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:10:03.671: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:10:05.639: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:10:07.636: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:10:09.638: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:10:11.639: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:10:13.636: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:10:15.634: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:10:17.639: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:10:19.638: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 9, 53, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 00:10:22.707: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 00:10:22.715: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-0-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:10:25.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9577" for this suite.
STEP: Destroying namespace "webhook-9577-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:34.927 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","total":291,"completed":254,"skipped":4185,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:10:27.274: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3517
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-60e6c27a-332d-47e5-bf0b-475237164ff3
STEP: Creating a pod to test consume secrets
Sep  6 00:10:27.726: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab" in namespace "projected-3517" to be "Succeeded or Failed"
Sep  6 00:10:27.741: INFO: Pod "pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab": Phase="Pending", Reason="", readiness=false. Elapsed: 14.623792ms
Sep  6 00:10:29.749: INFO: Pod "pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023032675s
Sep  6 00:10:31.771: INFO: Pod "pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044351485s
Sep  6 00:10:33.786: INFO: Pod "pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab": Phase="Pending", Reason="", readiness=false. Elapsed: 6.060069497s
Sep  6 00:10:35.802: INFO: Pod "pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab": Phase="Pending", Reason="", readiness=false. Elapsed: 8.075997397s
Sep  6 00:10:37.821: INFO: Pod "pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab": Phase="Pending", Reason="", readiness=false. Elapsed: 10.095170723s
Sep  6 00:10:39.831: INFO: Pod "pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab": Phase="Pending", Reason="", readiness=false. Elapsed: 12.104908379s
Sep  6 00:10:41.845: INFO: Pod "pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab": Phase="Pending", Reason="", readiness=false. Elapsed: 14.118486453s
Sep  6 00:10:43.852: INFO: Pod "pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab": Phase="Pending", Reason="", readiness=false. Elapsed: 16.126185183s
Sep  6 00:10:45.860: INFO: Pod "pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab": Phase="Pending", Reason="", readiness=false. Elapsed: 18.134169033s
Sep  6 00:10:47.874: INFO: Pod "pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab": Phase="Pending", Reason="", readiness=false. Elapsed: 20.147734861s
Sep  6 00:10:49.881: INFO: Pod "pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab": Phase="Pending", Reason="", readiness=false. Elapsed: 22.154493135s
Sep  6 00:10:51.890: INFO: Pod "pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab": Phase="Pending", Reason="", readiness=false. Elapsed: 24.16335259s
Sep  6 00:10:53.897: INFO: Pod "pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.1709892s
STEP: Saw pod success
Sep  6 00:10:53.897: INFO: Pod "pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab" satisfied condition "Succeeded or Failed"
Sep  6 00:10:53.903: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com pod pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 00:10:57.997: INFO: Waiting for pod pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab to disappear
Sep  6 00:10:58.014: INFO: Pod pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab still exists
Sep  6 00:11:00.016: INFO: Waiting for pod pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab to disappear
Sep  6 00:11:00.027: INFO: Pod pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab still exists
Sep  6 00:11:02.015: INFO: Waiting for pod pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab to disappear
Sep  6 00:11:02.024: INFO: Pod pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab still exists
Sep  6 00:11:04.015: INFO: Waiting for pod pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab to disappear
Sep  6 00:11:04.022: INFO: Pod pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab still exists
Sep  6 00:11:06.015: INFO: Waiting for pod pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab to disappear
Sep  6 00:11:06.024: INFO: Pod pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab still exists
Sep  6 00:11:08.015: INFO: Waiting for pod pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab to disappear
Sep  6 00:11:08.022: INFO: Pod pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab still exists
Sep  6 00:11:10.014: INFO: Waiting for pod pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab to disappear
Sep  6 00:11:10.031: INFO: Pod pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab still exists
Sep  6 00:11:12.015: INFO: Waiting for pod pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab to disappear
Sep  6 00:11:12.026: INFO: Pod pod-projected-secrets-76765a06-aa7a-4808-b0af-0d87472aafab no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:11:12.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3517" for this suite.

â€¢ [SLOW TEST:45.008 seconds]
[sig-storage] Projected secret
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":255,"skipped":4197,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:11:12.283: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7122
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 00:11:12.719: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c" in namespace "downward-api-7122" to be "Succeeded or Failed"
Sep  6 00:11:12.728: INFO: Pod "downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.373061ms
Sep  6 00:11:14.736: INFO: Pod "downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01761942s
Sep  6 00:11:16.756: INFO: Pod "downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037529285s
Sep  6 00:11:18.764: INFO: Pod "downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.045398645s
Sep  6 00:11:20.770: INFO: Pod "downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.051176305s
Sep  6 00:11:22.784: INFO: Pod "downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.065027092s
Sep  6 00:11:24.792: INFO: Pod "downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.073132829s
Sep  6 00:11:26.800: INFO: Pod "downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.081014219s
Sep  6 00:11:28.809: INFO: Pod "downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.089819658s
Sep  6 00:11:30.814: INFO: Pod "downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.095485746s
Sep  6 00:11:32.824: INFO: Pod "downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.105366338s
Sep  6 00:11:34.830: INFO: Pod "downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.111749862s
Sep  6 00:11:36.837: INFO: Pod "downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.117996211s
STEP: Saw pod success
Sep  6 00:11:36.837: INFO: Pod "downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c" satisfied condition "Succeeded or Failed"
Sep  6 00:11:36.844: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c container client-container: <nil>
STEP: delete the pod
Sep  6 00:11:36.908: INFO: Waiting for pod downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c to disappear
Sep  6 00:11:36.924: INFO: Pod downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c still exists
Sep  6 00:11:38.924: INFO: Waiting for pod downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c to disappear
Sep  6 00:11:38.932: INFO: Pod downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c still exists
Sep  6 00:11:40.925: INFO: Waiting for pod downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c to disappear
Sep  6 00:11:40.931: INFO: Pod downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c still exists
Sep  6 00:11:42.924: INFO: Waiting for pod downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c to disappear
Sep  6 00:11:42.932: INFO: Pod downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c still exists
Sep  6 00:11:44.924: INFO: Waiting for pod downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c to disappear
Sep  6 00:11:44.933: INFO: Pod downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c still exists
Sep  6 00:11:46.925: INFO: Waiting for pod downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c to disappear
Sep  6 00:11:46.931: INFO: Pod downwardapi-volume-3021aa46-63f1-4c98-9cc2-0e7ed97b574c no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:11:46.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7122" for this suite.

â€¢ [SLOW TEST:34.846 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","total":291,"completed":256,"skipped":4212,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:11:47.130: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-8298
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep  6 00:11:47.535: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 00:11:47.569: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 00:11:47.581: INFO: 
Logging pods the apiserver thinks is on node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com before test
Sep  6 00:11:47.632: INFO: podwithpersistentvolume from storage-class-test-2 started at 2021-09-05 20:05:57 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.632: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:11:47.632: INFO: hello-web-6b97664bd5-f5452 from test-cluster-ip-service started at 2021-09-05 20:10:51 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.632: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 00:11:47.632: INFO: wcp-sanity-busybox-6f999d6849-45jct from test-dataprovider-podvms-ns started at 2021-09-05 19:57:27 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.632: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:11:47.632: INFO: wcp-sanity-busybox-6f999d6849-c27n2 from test-dataprovider-podvms-ns started at 2021-09-05 21:11:14 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.632: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:11:47.632: INFO: nginx-private from test-image-pull-secrets-ns started at 2021-09-05 19:59:11 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.632: INFO: 	Container nginx-private-container ready: true, restart count 0
Sep  6 00:11:47.632: INFO: curl-pod from test-network-policy started at 2021-09-05 20:14:04 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.632: INFO: 	Container curl-container ready: true, restart count 0
Sep  6 00:11:47.632: INFO: hello-web-1-6b97664bd5-cl9tj from test-network-policy started at 2021-09-05 21:09:17 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.632: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 00:11:47.632: INFO: schedext-test-node-selector-1 from test-node-selector started at 2021-09-05 20:00:20 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.632: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:11:47.632: INFO: busybox from test-pod-external-nw-access started at 2021-09-05 20:15:25 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.632: INFO: 	Container busybox ready: true, restart count 0
Sep  6 00:11:47.632: INFO: busybox-annotation from test-podvm-annotations started at 2021-09-05 20:03:14 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.632: INFO: 	Container busybox-annotation ready: true, restart count 0
Sep  6 00:11:47.632: INFO: helloworld from test-telemetry started at 2021-09-05 20:09:22 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.632: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:11:47.632: INFO: wcp-sanity-busybox-6f999d6849-46njm from test-update-workload-ns started at 2021-09-05 20:07:43 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.632: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:11:47.632: INFO: 
Logging pods the apiserver thinks is on node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com before test
Sep  6 00:11:47.655: INFO: curl-pod from test-cluster-ip-service started at 2021-09-05 20:10:11 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.655: INFO: 	Container curl-container ready: true, restart count 0
Sep  6 00:11:47.655: INFO: helloworld from test-exec-ns started at 2021-09-05 19:58:27 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.655: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:11:47.655: INFO: schedext-test-node-selector-2 from test-node-selector started at 2021-09-05 20:00:22 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.655: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:11:47.655: INFO: schedext-test-affinity-1 from test-pod-affinity started at 2021-09-05 20:00:51 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.655: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:11:47.655: INFO: schedext-test-affinity-2 from test-pod-affinity started at 2021-09-05 20:01:16 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.655: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:11:47.655: INFO: test-docker-registry from test-private-image-registry-ns started at 2021-09-05 20:16:11 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.655: INFO: 	Container test-docker-registry ready: true, restart count 0
Sep  6 00:11:47.655: INFO: helloworld from test-update-workload-ns started at 2021-09-05 20:08:34 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.655: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:11:47.655: INFO: 
Logging pods the apiserver thinks is on node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com before test
Sep  6 00:11:47.675: INFO: hello-web-2-f779cbdff-hffpj from test-network-policy started at 2021-09-05 21:09:20 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.675: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 00:11:47.675: INFO: wcp-sanity-busybox-6f999d6849-856nv from test-update-workload-ns started at 2021-09-05 21:10:00 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.675: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:11:47.675: INFO: wcp-sanity-busybox-6f999d6849-mts92 from test-update-workload-ns started at 2021-09-05 21:10:00 -0700 PDT (1 container statuses recorded)
Sep  6 00:11:47.675: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: verifying the node has the label node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
STEP: verifying the node has the label node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com
STEP: verifying the node has the label node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod podwithpersistentvolume requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod curl-pod requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod hello-web-6b97664bd5-f5452 requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod wcp-sanity-busybox-6f999d6849-45jct requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod wcp-sanity-busybox-6f999d6849-c27n2 requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod helloworld requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod nginx-private requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod curl-pod requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod hello-web-1-6b97664bd5-cl9tj requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod hello-web-2-f779cbdff-hffpj requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod schedext-test-node-selector-1 requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod schedext-test-node-selector-2 requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod schedext-test-affinity-1 requesting resource cpu=500m on Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod schedext-test-affinity-2 requesting resource cpu=500m on Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod busybox requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod busybox-annotation requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod test-docker-registry requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod helloworld requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod helloworld requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod wcp-sanity-busybox-6f999d6849-46njm requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod wcp-sanity-busybox-6f999d6849-856nv requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com
Sep  6 00:11:47.898: INFO: Pod wcp-sanity-busybox-6f999d6849-mts92 requesting resource cpu=0m on Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com
STEP: Starting Pods to consume most of the cluster CPU.
Sep  6 00:11:47.898: INFO: Creating a pod which consumes cpu=5600m on Node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com
Sep  6 00:11:47.933: INFO: Creating a pod which consumes cpu=7000m on Node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com
Sep  6 00:11:47.956: INFO: Creating a pod which consumes cpu=7000m on Node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d4227fd-cd21-4459-ba3c-ae292f050ea7.1630912316], Reason = [SuccessfulRealizeNSXResource], Message = [Successfully realized NSX resource for Pod]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d4227fd-cd21-4459-ba3c-ae292f050ea7.16a22a1a764e0811], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8298/filler-pod-5d4227fd-cd21-4459-ba3c-ae292f050ea7 to sc2-rdops-vm09-dhcp-34-149.eng.vmware.com]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d4227fd-cd21-4459-ba3c-ae292f050ea7.16a22a1ceb064a42], Reason = [Image], Message = [Image pause-54467452d21a00c28aa4cce37d0850173b528ca6-v27168 bound successfully]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d4227fd-cd21-4459-ba3c-ae292f050ea7.16a22a1dbb634078], Reason = [Pulling], Message = [Waiting for Image sched-pred-8298/pause-54467452d21a00c28aa4cce37d0850173b528ca6-v27168]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d4227fd-cd21-4459-ba3c-ae292f050ea7.16a22a1dbb666910], Reason = [Pulled], Message = [Image sched-pred-8298/pause-54467452d21a00c28aa4cce37d0850173b528ca6-v27168 is ready]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d4227fd-cd21-4459-ba3c-ae292f050ea7.16a22a1f06f2cc88], Reason = [SuccessfulMountVolume], Message = [Successfully mounted volume default-token-wfs5s]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d4227fd-cd21-4459-ba3c-ae292f050ea7.16a22a1f06f38420], Reason = [Created], Message = [Created container filler-pod-5d4227fd-cd21-4459-ba3c-ae292f050ea7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5d4227fd-cd21-4459-ba3c-ae292f050ea7.16a22a1f06f3f180], Reason = [Started], Message = [Started container filler-pod-5d4227fd-cd21-4459-ba3c-ae292f050ea7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c5c12cd3-b769-425a-be9b-c02ee80e0c00.1630912316], Reason = [SuccessfulRealizeNSXResource], Message = [Successfully realized NSX resource for Pod]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c5c12cd3-b769-425a-be9b-c02ee80e0c00.16a22a1a7254f74f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8298/filler-pod-c5c12cd3-b769-425a-be9b-c02ee80e0c00 to sc2-rdops-vm09-dhcp-43-208.eng.vmware.com]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c5c12cd3-b769-425a-be9b-c02ee80e0c00.16a22a1ce9060d12], Reason = [Image], Message = [Image pause-54467452d21a00c28aa4cce37d0850173b528ca6-v27168 bound successfully]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c5c12cd3-b769-425a-be9b-c02ee80e0c00.16a22a1e33b09990], Reason = [Pulling], Message = [Waiting for Image sched-pred-8298/pause-54467452d21a00c28aa4cce37d0850173b528ca6-v27168]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c5c12cd3-b769-425a-be9b-c02ee80e0c00.16a22a1e33b39348], Reason = [Pulled], Message = [Image sched-pred-8298/pause-54467452d21a00c28aa4cce37d0850173b528ca6-v27168 is ready]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c5c12cd3-b769-425a-be9b-c02ee80e0c00.16a22a1fd9410678], Reason = [Started], Message = [Started container filler-pod-c5c12cd3-b769-425a-be9b-c02ee80e0c00]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c5c12cd3-b769-425a-be9b-c02ee80e0c00.16a22a1fd941e520], Reason = [SuccessfulMountVolume], Message = [Successfully mounted volume default-token-wfs5s]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c5c12cd3-b769-425a-be9b-c02ee80e0c00.16a22a1fd94246c8], Reason = [Created], Message = [Created container filler-pod-c5c12cd3-b769-425a-be9b-c02ee80e0c00]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cb0e9d2c-a764-4a62-9678-9c0fb67cc72f.1630912316], Reason = [SuccessfulRealizeNSXResource], Message = [Successfully realized NSX resource for Pod]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cb0e9d2c-a764-4a62-9678-9c0fb67cc72f.16a22a1a6c9b6174], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8298/filler-pod-cb0e9d2c-a764-4a62-9678-9c0fb67cc72f to sc2-rdops-vm09-dhcp-39-55.eng.vmware.com]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cb0e9d2c-a764-4a62-9678-9c0fb67cc72f.16a22a1a702cbcc6], Reason = [Image], Message = [Image pause-54467452d21a00c28aa4cce37d0850173b528ca6-v27168 bound successfully]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cb0e9d2c-a764-4a62-9678-9c0fb67cc72f.16a22a1dfd816638], Reason = [Pulling], Message = [Waiting for Image sched-pred-8298/pause-54467452d21a00c28aa4cce37d0850173b528ca6-v27168]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cb0e9d2c-a764-4a62-9678-9c0fb67cc72f.16a22a1dfd83eac0], Reason = [Pulled], Message = [Image sched-pred-8298/pause-54467452d21a00c28aa4cce37d0850173b528ca6-v27168 is ready]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cb0e9d2c-a764-4a62-9678-9c0fb67cc72f.16a22a1fc88f7738], Reason = [SuccessfulMountVolume], Message = [Successfully mounted volume default-token-wfs5s]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cb0e9d2c-a764-4a62-9678-9c0fb67cc72f.16a22a1fc8902ae8], Reason = [Created], Message = [Created container filler-pod-cb0e9d2c-a764-4a62-9678-9c0fb67cc72f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cb0e9d2c-a764-4a62-9678-9c0fb67cc72f.16a22a1fc890abd0], Reason = [Started], Message = [Started container filler-pod-cb0e9d2c-a764-4a62-9678-9c0fb67cc72f]
STEP: Considering event: 
Type = [Normal], Name = [pause-54467452d21a00c28aa4cce37d0850173b528ca6-v27168.16a22a1afa543dd0], Reason = [Status], Message = [sc2-rdops-vm09-dhcp-39-55.eng.vmware.com: Image status changed to Resolving]
STEP: Considering event: 
Type = [Normal], Name = [pause-54467452d21a00c28aa4cce37d0850173b528ca6-v27168.16a22a1c6a1ea910], Reason = [Resolve], Message = [sc2-rdops-vm09-dhcp-39-55.eng.vmware.com: Image resolved to ChainID sha256:ba0dae6243cc9fa2890df40a625721fdbea5c94ca6da897acdd814d710149770]
STEP: Considering event: 
Type = [Warning], Name = [pause-54467452d21a00c28aa4cce37d0850173b528ca6-v27168.16a22a1c72282029], Reason = [Bind], Message = [Imagedisk bind failed: Operation cannot be fulfilled on images.imagecontroller.vmware.com "pause-54467452d21a00c28aa4cce37d0850173b528ca6-v27168": the object has been modified; please apply your changes to the latest version and try again]
STEP: Considering event: 
Type = [Normal], Name = [pause-54467452d21a00c28aa4cce37d0850173b528ca6-v27168.16a22a1c76e6ae71], Reason = [Bind], Message = [Imagedisk ba0dae6243cc9fa2890df40a625721fdbea5c94ca6da897acdd814d710149770-v25236853 successfully bound]
STEP: Considering event: 
Type = [Normal], Name = [pause-54467452d21a00c28aa4cce37d0850173b528ca6-v27168.16a22a1c782deb8e], Reason = [Status], Message = [Image status changed to Ready]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16a22a200d27fad5], Reason = [FailedScheduling], Message = [Insufficient resources.]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.16a22a2014e95053], Reason = [FailedScheduling], Message = [Insufficient resources.]
STEP: removing the label node off the node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:12:13.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8298" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

â€¢ [SLOW TEST:26.228 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","total":291,"completed":257,"skipped":4219,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:12:13.358: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename security-context-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in security-context-test-4249
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:41
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 00:12:13.751: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-a89614e6-1b10-4e54-bfb8-88cb627fe9d6" in namespace "security-context-test-4249" to be "Succeeded or Failed"
Sep  6 00:12:13.758: INFO: Pod "busybox-privileged-false-a89614e6-1b10-4e54-bfb8-88cb627fe9d6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.726681ms
Sep  6 00:12:15.766: INFO: Pod "busybox-privileged-false-a89614e6-1b10-4e54-bfb8-88cb627fe9d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014555923s
Sep  6 00:12:17.774: INFO: Pod "busybox-privileged-false-a89614e6-1b10-4e54-bfb8-88cb627fe9d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022345724s
Sep  6 00:12:19.782: INFO: Pod "busybox-privileged-false-a89614e6-1b10-4e54-bfb8-88cb627fe9d6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030768196s
Sep  6 00:12:21.789: INFO: Pod "busybox-privileged-false-a89614e6-1b10-4e54-bfb8-88cb627fe9d6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.0372237s
Sep  6 00:12:23.799: INFO: Pod "busybox-privileged-false-a89614e6-1b10-4e54-bfb8-88cb627fe9d6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047583543s
Sep  6 00:12:25.814: INFO: Pod "busybox-privileged-false-a89614e6-1b10-4e54-bfb8-88cb627fe9d6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.062376383s
Sep  6 00:12:27.824: INFO: Pod "busybox-privileged-false-a89614e6-1b10-4e54-bfb8-88cb627fe9d6": Phase="Pending", Reason="", readiness=false. Elapsed: 14.072141736s
Sep  6 00:12:29.848: INFO: Pod "busybox-privileged-false-a89614e6-1b10-4e54-bfb8-88cb627fe9d6": Phase="Pending", Reason="", readiness=false. Elapsed: 16.096866203s
Sep  6 00:12:31.856: INFO: Pod "busybox-privileged-false-a89614e6-1b10-4e54-bfb8-88cb627fe9d6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.104533171s
Sep  6 00:12:33.870: INFO: Pod "busybox-privileged-false-a89614e6-1b10-4e54-bfb8-88cb627fe9d6": Phase="Pending", Reason="", readiness=false. Elapsed: 20.118633422s
Sep  6 00:12:35.884: INFO: Pod "busybox-privileged-false-a89614e6-1b10-4e54-bfb8-88cb627fe9d6": Phase="Pending", Reason="", readiness=false. Elapsed: 22.132867354s
Sep  6 00:12:37.894: INFO: Pod "busybox-privileged-false-a89614e6-1b10-4e54-bfb8-88cb627fe9d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.142687673s
Sep  6 00:12:37.894: INFO: Pod "busybox-privileged-false-a89614e6-1b10-4e54-bfb8-88cb627fe9d6" satisfied condition "Succeeded or Failed"
Sep  6 00:12:43.413: INFO: Got logs for pod "busybox-privileged-false-a89614e6-1b10-4e54-bfb8-88cb627fe9d6": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:12:43.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4249" for this suite.

â€¢ [SLOW TEST:30.325 seconds]
[k8s.io] Security Context
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  When creating a pod with privileged
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:227
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":258,"skipped":4241,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:12:43.684: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2720
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating projection with secret that has name projected-secret-test-49d2e959-da46-49af-a910-3390fb05ae09
STEP: Creating a pod to test consume secrets
Sep  6 00:12:44.154: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e" in namespace "projected-2720" to be "Succeeded or Failed"
Sep  6 00:12:44.179: INFO: Pod "pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e": Phase="Pending", Reason="", readiness=false. Elapsed: 24.655942ms
Sep  6 00:12:46.188: INFO: Pod "pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033222207s
Sep  6 00:12:48.202: INFO: Pod "pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047876919s
Sep  6 00:12:50.217: INFO: Pod "pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.062266079s
Sep  6 00:12:52.226: INFO: Pod "pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.07198668s
Sep  6 00:12:54.298: INFO: Pod "pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.144121606s
Sep  6 00:12:56.306: INFO: Pod "pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.151590857s
Sep  6 00:12:58.313: INFO: Pod "pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.158711061s
Sep  6 00:13:00.324: INFO: Pod "pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e": Phase="Pending", Reason="", readiness=false. Elapsed: 16.16993835s
Sep  6 00:13:02.336: INFO: Pod "pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.18185175s
Sep  6 00:13:04.348: INFO: Pod "pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e": Phase="Pending", Reason="", readiness=false. Elapsed: 20.193312119s
Sep  6 00:13:06.360: INFO: Pod "pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e": Phase="Pending", Reason="", readiness=false. Elapsed: 22.205859493s
Sep  6 00:13:08.368: INFO: Pod "pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.214144972s
STEP: Saw pod success
Sep  6 00:13:08.369: INFO: Pod "pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e" satisfied condition "Succeeded or Failed"
Sep  6 00:13:08.381: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep  6 00:13:13.058: INFO: Waiting for pod pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e to disappear
Sep  6 00:13:13.101: INFO: Pod pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e still exists
Sep  6 00:13:15.101: INFO: Waiting for pod pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e to disappear
Sep  6 00:13:15.108: INFO: Pod pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e still exists
Sep  6 00:13:17.101: INFO: Waiting for pod pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e to disappear
Sep  6 00:13:17.112: INFO: Pod pod-projected-secrets-747c4913-bcb1-4064-8ef0-d4ccd07a884e no longer exists
[AfterEach] [sig-storage] Projected secret
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:13:17.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2720" for this suite.

â€¢ [SLOW TEST:33.653 seconds]
[sig-storage] Projected secret
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":259,"skipped":4243,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Docker Containers
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:13:17.337: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-267
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Docker Containers
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:13:37.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-267" for this suite.

â€¢ [SLOW TEST:20.708 seconds]
[k8s.io] Docker Containers
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","total":291,"completed":260,"skipped":4256,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:13:38.046: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5420
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward api env vars
Sep  6 00:13:38.533: INFO: Waiting up to 5m0s for pod "downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139" in namespace "downward-api-5420" to be "Succeeded or Failed"
Sep  6 00:13:38.559: INFO: Pod "downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139": Phase="Pending", Reason="", readiness=false. Elapsed: 26.023864ms
Sep  6 00:13:40.570: INFO: Pod "downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036855472s
Sep  6 00:13:42.579: INFO: Pod "downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045801467s
Sep  6 00:13:44.594: INFO: Pod "downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139": Phase="Pending", Reason="", readiness=false. Elapsed: 6.061191534s
Sep  6 00:13:46.602: INFO: Pod "downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139": Phase="Pending", Reason="", readiness=false. Elapsed: 8.068359019s
Sep  6 00:13:48.626: INFO: Pod "downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139": Phase="Pending", Reason="", readiness=false. Elapsed: 10.09307442s
Sep  6 00:13:50.632: INFO: Pod "downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139": Phase="Pending", Reason="", readiness=false. Elapsed: 12.098917674s
Sep  6 00:13:52.640: INFO: Pod "downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139": Phase="Pending", Reason="", readiness=false. Elapsed: 14.107120876s
Sep  6 00:13:54.649: INFO: Pod "downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139": Phase="Pending", Reason="", readiness=false. Elapsed: 16.116130435s
Sep  6 00:13:56.657: INFO: Pod "downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139": Phase="Pending", Reason="", readiness=false. Elapsed: 18.123666466s
Sep  6 00:13:58.667: INFO: Pod "downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139": Phase="Pending", Reason="", readiness=false. Elapsed: 20.133475144s
Sep  6 00:14:00.712: INFO: Pod "downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139": Phase="Pending", Reason="", readiness=false. Elapsed: 22.179193105s
Sep  6 00:14:02.721: INFO: Pod "downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.187563894s
STEP: Saw pod success
Sep  6 00:14:02.721: INFO: Pod "downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139" satisfied condition "Succeeded or Failed"
Sep  6 00:14:02.732: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 container dapi-container: <nil>
STEP: delete the pod
Sep  6 00:14:02.819: INFO: Waiting for pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 to disappear
Sep  6 00:14:02.836: INFO: Pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 still exists
Sep  6 00:14:04.837: INFO: Waiting for pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 to disappear
Sep  6 00:14:04.845: INFO: Pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 still exists
Sep  6 00:14:06.838: INFO: Waiting for pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 to disappear
Sep  6 00:14:06.848: INFO: Pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 still exists
Sep  6 00:14:08.839: INFO: Waiting for pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 to disappear
Sep  6 00:14:08.850: INFO: Pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 still exists
Sep  6 00:14:10.837: INFO: Waiting for pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 to disappear
Sep  6 00:14:10.848: INFO: Pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 still exists
Sep  6 00:14:12.837: INFO: Waiting for pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 to disappear
Sep  6 00:14:12.848: INFO: Pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 still exists
Sep  6 00:14:14.837: INFO: Waiting for pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 to disappear
Sep  6 00:14:14.846: INFO: Pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 still exists
Sep  6 00:14:16.837: INFO: Waiting for pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 to disappear
Sep  6 00:14:16.845: INFO: Pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 still exists
Sep  6 00:14:18.837: INFO: Waiting for pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 to disappear
Sep  6 00:14:18.843: INFO: Pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 still exists
Sep  6 00:14:20.837: INFO: Waiting for pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 to disappear
Sep  6 00:14:20.846: INFO: Pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 still exists
Sep  6 00:14:22.838: INFO: Waiting for pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 to disappear
Sep  6 00:14:22.851: INFO: Pod downward-api-92e07ca6-1dc8-4868-8be5-cee3f1abe139 no longer exists
[AfterEach] [sig-node] Downward API
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:14:22.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5420" for this suite.

â€¢ [SLOW TEST:45.041 seconds]
[sig-node] Downward API
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:34
  should provide host IP as an env var [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","total":291,"completed":261,"skipped":4289,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:14:23.087: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5705
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:161
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:14:23.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5705" for this suite.
â€¢{"msg":"PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","total":291,"completed":262,"skipped":4308,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] version v1
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:14:23.774: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-556
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-gcjpc in namespace proxy-556
Sep  6 00:14:52.619: INFO: setup took 28.323639585s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep  6 00:14:52.668: INFO: (0) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 44.572904ms)
Sep  6 00:14:52.668: INFO: (0) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 45.433263ms)
Sep  6 00:14:52.672: INFO: (0) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 48.555639ms)
Sep  6 00:14:52.673: INFO: (0) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 49.036971ms)
Sep  6 00:14:52.696: INFO: (0) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 72.739038ms)
Sep  6 00:14:52.696: INFO: (0) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 72.513286ms)
Sep  6 00:14:52.696: INFO: (0) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 72.870508ms)
Sep  6 00:14:52.697: INFO: (0) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 73.153461ms)
Sep  6 00:14:52.699: INFO: (0) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 74.997251ms)
Sep  6 00:14:52.699: INFO: (0) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 74.481124ms)
Sep  6 00:14:52.702: INFO: (0) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 78.741998ms)
Sep  6 00:14:52.703: INFO: (0) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 79.220949ms)
Sep  6 00:14:52.704: INFO: (0) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 80.810712ms)
Sep  6 00:14:52.704: INFO: (0) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 80.499388ms)
Sep  6 00:14:52.704: INFO: (0) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 80.724957ms)
Sep  6 00:14:52.704: INFO: (0) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 80.504389ms)
Sep  6 00:14:52.750: INFO: (1) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 45.041917ms)
Sep  6 00:14:52.750: INFO: (1) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 45.767528ms)
Sep  6 00:14:52.750: INFO: (1) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 45.754272ms)
Sep  6 00:14:52.750: INFO: (1) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 46.034365ms)
Sep  6 00:14:52.751: INFO: (1) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 45.459377ms)
Sep  6 00:14:52.751: INFO: (1) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 45.827116ms)
Sep  6 00:14:52.751: INFO: (1) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 46.023558ms)
Sep  6 00:14:52.754: INFO: (1) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 49.662184ms)
Sep  6 00:14:52.755: INFO: (1) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 49.395432ms)
Sep  6 00:14:52.755: INFO: (1) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 50.00869ms)
Sep  6 00:14:52.755: INFO: (1) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 49.992109ms)
Sep  6 00:14:52.778: INFO: (1) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 73.016107ms)
Sep  6 00:14:52.783: INFO: (1) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 77.842976ms)
Sep  6 00:14:52.783: INFO: (1) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 78.374133ms)
Sep  6 00:14:52.783: INFO: (1) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 77.917375ms)
Sep  6 00:14:52.784: INFO: (1) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 78.846578ms)
Sep  6 00:14:52.800: INFO: (2) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 15.983206ms)
Sep  6 00:14:52.814: INFO: (2) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 28.977514ms)
Sep  6 00:14:52.814: INFO: (2) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 29.167887ms)
Sep  6 00:14:52.814: INFO: (2) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 30.139087ms)
Sep  6 00:14:52.814: INFO: (2) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 29.286769ms)
Sep  6 00:14:52.814: INFO: (2) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 29.669612ms)
Sep  6 00:14:52.814: INFO: (2) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 29.852403ms)
Sep  6 00:14:52.814: INFO: (2) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 29.741601ms)
Sep  6 00:14:52.815: INFO: (2) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 30.584101ms)
Sep  6 00:14:52.817: INFO: (2) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 31.59065ms)
Sep  6 00:14:52.819: INFO: (2) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 34.174633ms)
Sep  6 00:14:52.825: INFO: (2) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 40.121864ms)
Sep  6 00:14:52.833: INFO: (2) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 47.895811ms)
Sep  6 00:14:52.835: INFO: (2) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 50.6448ms)
Sep  6 00:14:52.877: INFO: (2) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 92.256891ms)
Sep  6 00:14:52.877: INFO: (2) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 92.263504ms)
Sep  6 00:14:52.895: INFO: (3) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 18.511151ms)
Sep  6 00:14:52.901: INFO: (3) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 24.308641ms)
Sep  6 00:14:52.902: INFO: (3) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 24.225538ms)
Sep  6 00:14:52.907: INFO: (3) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 29.165561ms)
Sep  6 00:14:52.910: INFO: (3) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 33.142044ms)
Sep  6 00:14:52.910: INFO: (3) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 32.51768ms)
Sep  6 00:14:52.911: INFO: (3) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 33.814664ms)
Sep  6 00:14:52.911: INFO: (3) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 33.762595ms)
Sep  6 00:14:52.911: INFO: (3) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 33.472733ms)
Sep  6 00:14:52.911: INFO: (3) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 33.980243ms)
Sep  6 00:14:52.911: INFO: (3) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 33.605426ms)
Sep  6 00:14:52.911: INFO: (3) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 33.741492ms)
Sep  6 00:14:52.915: INFO: (3) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 37.041993ms)
Sep  6 00:14:52.916: INFO: (3) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 38.407031ms)
Sep  6 00:14:52.917: INFO: (3) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 39.694092ms)
Sep  6 00:14:52.918: INFO: (3) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 40.669378ms)
Sep  6 00:14:52.941: INFO: (4) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 22.357758ms)
Sep  6 00:14:52.942: INFO: (4) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 23.952383ms)
Sep  6 00:14:52.942: INFO: (4) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 23.358622ms)
Sep  6 00:14:52.942: INFO: (4) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 23.860194ms)
Sep  6 00:14:52.944: INFO: (4) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 25.287752ms)
Sep  6 00:14:52.946: INFO: (4) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 26.952155ms)
Sep  6 00:14:52.946: INFO: (4) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 27.793747ms)
Sep  6 00:14:52.946: INFO: (4) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 27.587105ms)
Sep  6 00:14:52.946: INFO: (4) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 27.635529ms)
Sep  6 00:14:52.946: INFO: (4) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 27.562491ms)
Sep  6 00:14:52.946: INFO: (4) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 27.531908ms)
Sep  6 00:14:52.949: INFO: (4) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 30.085586ms)
Sep  6 00:14:52.954: INFO: (4) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 35.584779ms)
Sep  6 00:14:52.954: INFO: (4) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 35.971588ms)
Sep  6 00:14:52.955: INFO: (4) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 36.318919ms)
Sep  6 00:14:52.955: INFO: (4) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 36.270096ms)
Sep  6 00:14:52.985: INFO: (5) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 29.065628ms)
Sep  6 00:14:52.985: INFO: (5) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 30.048195ms)
Sep  6 00:14:52.986: INFO: (5) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 29.895051ms)
Sep  6 00:14:52.986: INFO: (5) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 30.905459ms)
Sep  6 00:14:52.986: INFO: (5) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 30.513398ms)
Sep  6 00:14:52.987: INFO: (5) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 31.308523ms)
Sep  6 00:14:52.993: INFO: (5) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 37.529576ms)
Sep  6 00:14:52.994: INFO: (5) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 37.381836ms)
Sep  6 00:14:52.994: INFO: (5) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 38.56173ms)
Sep  6 00:14:53.007: INFO: (5) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 51.530625ms)
Sep  6 00:14:53.009: INFO: (5) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 53.826786ms)
Sep  6 00:14:53.010: INFO: (5) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 53.512649ms)
Sep  6 00:14:53.010: INFO: (5) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 54.133563ms)
Sep  6 00:14:53.010: INFO: (5) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 54.675147ms)
Sep  6 00:14:53.011: INFO: (5) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 54.759727ms)
Sep  6 00:14:53.012: INFO: (5) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 56.326724ms)
Sep  6 00:14:53.038: INFO: (6) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 25.384874ms)
Sep  6 00:14:53.038: INFO: (6) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 25.807357ms)
Sep  6 00:14:53.038: INFO: (6) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 26.460376ms)
Sep  6 00:14:53.038: INFO: (6) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 25.942796ms)
Sep  6 00:14:53.039: INFO: (6) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 27.106705ms)
Sep  6 00:14:53.039: INFO: (6) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 26.711289ms)
Sep  6 00:14:53.041: INFO: (6) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 28.792748ms)
Sep  6 00:14:53.041: INFO: (6) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 28.962826ms)
Sep  6 00:14:53.042: INFO: (6) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 30.273267ms)
Sep  6 00:14:53.043: INFO: (6) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 31.546241ms)
Sep  6 00:14:53.069: INFO: (6) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 57.238112ms)
Sep  6 00:14:53.073: INFO: (6) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 61.458768ms)
Sep  6 00:14:53.073: INFO: (6) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 61.175147ms)
Sep  6 00:14:53.074: INFO: (6) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 61.430969ms)
Sep  6 00:14:53.074: INFO: (6) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 62.359415ms)
Sep  6 00:14:53.081: INFO: (6) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 68.905161ms)
Sep  6 00:14:53.098: INFO: (7) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 17.259444ms)
Sep  6 00:14:53.105: INFO: (7) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 23.542299ms)
Sep  6 00:14:53.106: INFO: (7) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 24.125814ms)
Sep  6 00:14:53.106: INFO: (7) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 24.354888ms)
Sep  6 00:14:53.106: INFO: (7) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 24.46069ms)
Sep  6 00:14:53.108: INFO: (7) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 26.62893ms)
Sep  6 00:14:53.108: INFO: (7) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 26.570419ms)
Sep  6 00:14:53.110: INFO: (7) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 28.315687ms)
Sep  6 00:14:53.110: INFO: (7) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 27.879039ms)
Sep  6 00:14:53.111: INFO: (7) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 29.321569ms)
Sep  6 00:14:53.112: INFO: (7) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 30.78961ms)
Sep  6 00:14:53.114: INFO: (7) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 32.507228ms)
Sep  6 00:14:53.115: INFO: (7) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 33.853855ms)
Sep  6 00:14:53.116: INFO: (7) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 34.423054ms)
Sep  6 00:14:53.117: INFO: (7) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 35.713734ms)
Sep  6 00:14:53.130: INFO: (7) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 49.166729ms)
Sep  6 00:14:53.162: INFO: (8) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 31.552299ms)
Sep  6 00:14:53.163: INFO: (8) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 32.718482ms)
Sep  6 00:14:53.163: INFO: (8) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 32.81163ms)
Sep  6 00:14:53.164: INFO: (8) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 33.696465ms)
Sep  6 00:14:53.170: INFO: (8) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 39.171811ms)
Sep  6 00:14:53.170: INFO: (8) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 39.247764ms)
Sep  6 00:14:53.170: INFO: (8) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 39.741068ms)
Sep  6 00:14:53.172: INFO: (8) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 41.398169ms)
Sep  6 00:14:53.175: INFO: (8) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 44.757358ms)
Sep  6 00:14:53.181: INFO: (8) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 50.175185ms)
Sep  6 00:14:53.185: INFO: (8) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 54.523159ms)
Sep  6 00:14:53.185: INFO: (8) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 54.767636ms)
Sep  6 00:14:53.185: INFO: (8) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 54.539337ms)
Sep  6 00:14:53.185: INFO: (8) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 54.762182ms)
Sep  6 00:14:53.185: INFO: (8) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 54.498674ms)
Sep  6 00:14:53.185: INFO: (8) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 55.048377ms)
Sep  6 00:14:53.202: INFO: (9) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 16.681612ms)
Sep  6 00:14:53.203: INFO: (9) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 16.877009ms)
Sep  6 00:14:53.213: INFO: (9) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 27.382649ms)
Sep  6 00:14:53.214: INFO: (9) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 27.815975ms)
Sep  6 00:14:53.214: INFO: (9) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 27.65327ms)
Sep  6 00:14:53.214: INFO: (9) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 27.887161ms)
Sep  6 00:14:53.214: INFO: (9) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 28.103245ms)
Sep  6 00:14:53.214: INFO: (9) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 28.15605ms)
Sep  6 00:14:53.214: INFO: (9) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 28.205374ms)
Sep  6 00:14:53.215: INFO: (9) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 28.914651ms)
Sep  6 00:14:53.215: INFO: (9) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 29.692ms)
Sep  6 00:14:53.216: INFO: (9) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 29.77211ms)
Sep  6 00:14:53.219: INFO: (9) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 33.574676ms)
Sep  6 00:14:53.220: INFO: (9) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 34.855534ms)
Sep  6 00:14:53.221: INFO: (9) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 34.677409ms)
Sep  6 00:14:53.224: INFO: (9) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 38.847567ms)
Sep  6 00:14:53.243: INFO: (10) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 17.63157ms)
Sep  6 00:14:53.245: INFO: (10) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 20.078574ms)
Sep  6 00:14:53.245: INFO: (10) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 20.44337ms)
Sep  6 00:14:53.246: INFO: (10) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 21.006422ms)
Sep  6 00:14:53.246: INFO: (10) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 21.565924ms)
Sep  6 00:14:53.248: INFO: (10) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 23.293734ms)
Sep  6 00:14:53.248: INFO: (10) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 24.050837ms)
Sep  6 00:14:53.248: INFO: (10) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 23.660213ms)
Sep  6 00:14:53.248: INFO: (10) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 23.914485ms)
Sep  6 00:14:53.249: INFO: (10) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 23.126941ms)
Sep  6 00:14:53.249: INFO: (10) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 24.164183ms)
Sep  6 00:14:53.251: INFO: (10) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 25.974369ms)
Sep  6 00:14:53.256: INFO: (10) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 31.507653ms)
Sep  6 00:14:53.256: INFO: (10) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 31.48353ms)
Sep  6 00:14:53.258: INFO: (10) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 32.941043ms)
Sep  6 00:14:53.262: INFO: (10) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 37.154063ms)
Sep  6 00:14:53.298: INFO: (11) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 34.515286ms)
Sep  6 00:14:53.306: INFO: (11) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 42.99587ms)
Sep  6 00:14:53.307: INFO: (11) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 44.342482ms)
Sep  6 00:14:53.307: INFO: (11) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 44.116677ms)
Sep  6 00:14:53.309: INFO: (11) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 45.304485ms)
Sep  6 00:14:53.309: INFO: (11) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 46.414099ms)
Sep  6 00:14:53.309: INFO: (11) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 46.03429ms)
Sep  6 00:14:53.313: INFO: (11) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 49.920601ms)
Sep  6 00:14:53.316: INFO: (11) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 53.252098ms)
Sep  6 00:14:53.317: INFO: (11) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 53.607792ms)
Sep  6 00:14:53.317: INFO: (11) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 53.693003ms)
Sep  6 00:14:53.318: INFO: (11) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 54.595241ms)
Sep  6 00:14:53.319: INFO: (11) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 56.604186ms)
Sep  6 00:14:53.322: INFO: (11) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 58.544798ms)
Sep  6 00:14:53.322: INFO: (11) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 58.24316ms)
Sep  6 00:14:53.332: INFO: (11) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 69.063656ms)
Sep  6 00:14:53.403: INFO: (12) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 71.115525ms)
Sep  6 00:14:53.417: INFO: (12) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 84.192948ms)
Sep  6 00:14:53.417: INFO: (12) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 84.506866ms)
Sep  6 00:14:53.417: INFO: (12) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 84.365803ms)
Sep  6 00:14:53.417: INFO: (12) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 84.601342ms)
Sep  6 00:14:53.418: INFO: (12) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 85.375887ms)
Sep  6 00:14:53.423: INFO: (12) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 90.643308ms)
Sep  6 00:14:53.424: INFO: (12) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 91.608981ms)
Sep  6 00:14:53.424: INFO: (12) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 91.70575ms)
Sep  6 00:14:53.425: INFO: (12) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 92.470072ms)
Sep  6 00:14:53.433: INFO: (12) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 100.263654ms)
Sep  6 00:14:53.436: INFO: (12) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 103.01978ms)
Sep  6 00:14:53.436: INFO: (12) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 103.263482ms)
Sep  6 00:14:53.436: INFO: (12) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 103.811614ms)
Sep  6 00:14:53.436: INFO: (12) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 103.898916ms)
Sep  6 00:14:53.437: INFO: (12) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 104.797288ms)
Sep  6 00:14:53.468: INFO: (13) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 30.954517ms)
Sep  6 00:14:53.469: INFO: (13) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 31.733992ms)
Sep  6 00:14:53.469: INFO: (13) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 31.986716ms)
Sep  6 00:14:53.470: INFO: (13) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 32.764706ms)
Sep  6 00:14:53.471: INFO: (13) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 33.709312ms)
Sep  6 00:14:53.474: INFO: (13) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 36.241676ms)
Sep  6 00:14:53.474: INFO: (13) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 36.524363ms)
Sep  6 00:14:53.474: INFO: (13) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 37.038837ms)
Sep  6 00:14:53.475: INFO: (13) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 37.262946ms)
Sep  6 00:14:53.476: INFO: (13) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 38.702086ms)
Sep  6 00:14:53.476: INFO: (13) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 38.685923ms)
Sep  6 00:14:53.476: INFO: (13) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 38.603876ms)
Sep  6 00:14:53.477: INFO: (13) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 39.506204ms)
Sep  6 00:14:53.489: INFO: (13) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 51.432844ms)
Sep  6 00:14:53.493: INFO: (13) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 55.423001ms)
Sep  6 00:14:53.497: INFO: (13) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 59.579837ms)
Sep  6 00:14:53.541: INFO: (14) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 43.108126ms)
Sep  6 00:14:53.543: INFO: (14) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 45.003263ms)
Sep  6 00:14:53.548: INFO: (14) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 50.054985ms)
Sep  6 00:14:53.548: INFO: (14) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 49.71593ms)
Sep  6 00:14:53.548: INFO: (14) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 50.341217ms)
Sep  6 00:14:53.548: INFO: (14) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 50.063342ms)
Sep  6 00:14:53.548: INFO: (14) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 50.042748ms)
Sep  6 00:14:53.548: INFO: (14) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 49.925783ms)
Sep  6 00:14:53.548: INFO: (14) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 50.013334ms)
Sep  6 00:14:53.548: INFO: (14) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 49.735649ms)
Sep  6 00:14:53.548: INFO: (14) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 50.156025ms)
Sep  6 00:14:53.548: INFO: (14) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 50.844603ms)
Sep  6 00:14:53.550: INFO: (14) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 51.897795ms)
Sep  6 00:14:53.550: INFO: (14) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 52.086152ms)
Sep  6 00:14:53.551: INFO: (14) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 53.141569ms)
Sep  6 00:14:53.551: INFO: (14) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 52.766418ms)
Sep  6 00:14:53.573: INFO: (15) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 22.667922ms)
Sep  6 00:14:53.587: INFO: (15) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 34.996976ms)
Sep  6 00:14:53.590: INFO: (15) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 37.723751ms)
Sep  6 00:14:53.591: INFO: (15) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 39.361784ms)
Sep  6 00:14:53.591: INFO: (15) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 40.258208ms)
Sep  6 00:14:53.592: INFO: (15) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 39.779725ms)
Sep  6 00:14:53.592: INFO: (15) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 39.862899ms)
Sep  6 00:14:53.592: INFO: (15) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 40.803393ms)
Sep  6 00:14:53.592: INFO: (15) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 40.581835ms)
Sep  6 00:14:53.592: INFO: (15) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 41.238829ms)
Sep  6 00:14:53.616: INFO: (15) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 64.403832ms)
Sep  6 00:14:53.617: INFO: (15) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 64.968112ms)
Sep  6 00:14:53.617: INFO: (15) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 64.96114ms)
Sep  6 00:14:53.617: INFO: (15) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 65.417335ms)
Sep  6 00:14:53.617: INFO: (15) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 64.958808ms)
Sep  6 00:14:53.636: INFO: (15) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 85.069231ms)
Sep  6 00:14:53.682: INFO: (16) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 44.668272ms)
Sep  6 00:14:53.683: INFO: (16) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 44.898024ms)
Sep  6 00:14:53.685: INFO: (16) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 48.459004ms)
Sep  6 00:14:53.685: INFO: (16) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 49.264564ms)
Sep  6 00:14:53.685: INFO: (16) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 47.752131ms)
Sep  6 00:14:53.685: INFO: (16) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 48.809066ms)
Sep  6 00:14:53.685: INFO: (16) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 48.777701ms)
Sep  6 00:14:53.686: INFO: (16) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 49.267395ms)
Sep  6 00:14:53.703: INFO: (16) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 65.345584ms)
Sep  6 00:14:53.708: INFO: (16) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 69.844216ms)
Sep  6 00:14:53.712: INFO: (16) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 76.142357ms)
Sep  6 00:14:53.715: INFO: (16) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 77.760064ms)
Sep  6 00:14:53.715: INFO: (16) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 78.664766ms)
Sep  6 00:14:53.716: INFO: (16) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 77.943357ms)
Sep  6 00:14:53.721: INFO: (16) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 83.200181ms)
Sep  6 00:14:53.725: INFO: (16) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 87.582053ms)
Sep  6 00:14:53.738: INFO: (17) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 13.055944ms)
Sep  6 00:14:53.757: INFO: (17) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 31.668485ms)
Sep  6 00:14:53.763: INFO: (17) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 37.543668ms)
Sep  6 00:14:53.763: INFO: (17) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 38.41874ms)
Sep  6 00:14:53.765: INFO: (17) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 39.60302ms)
Sep  6 00:14:53.765: INFO: (17) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 40.082202ms)
Sep  6 00:14:53.768: INFO: (17) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 43.1768ms)
Sep  6 00:14:53.769: INFO: (17) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 43.387671ms)
Sep  6 00:14:53.769: INFO: (17) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 43.778263ms)
Sep  6 00:14:53.771: INFO: (17) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 46.031351ms)
Sep  6 00:14:53.771: INFO: (17) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 46.142159ms)
Sep  6 00:14:53.771: INFO: (17) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 46.307874ms)
Sep  6 00:14:53.771: INFO: (17) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 46.161159ms)
Sep  6 00:14:53.771: INFO: (17) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 46.483453ms)
Sep  6 00:14:53.771: INFO: (17) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 46.318138ms)
Sep  6 00:14:53.771: INFO: (17) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 46.300323ms)
Sep  6 00:14:53.789: INFO: (18) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 17.344236ms)
Sep  6 00:14:53.791: INFO: (18) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 18.569084ms)
Sep  6 00:14:53.791: INFO: (18) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 19.192442ms)
Sep  6 00:14:53.791: INFO: (18) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 18.838ms)
Sep  6 00:14:53.791: INFO: (18) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 19.15699ms)
Sep  6 00:14:53.791: INFO: (18) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 19.445113ms)
Sep  6 00:14:53.792: INFO: (18) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 19.850154ms)
Sep  6 00:14:53.793: INFO: (18) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 21.117336ms)
Sep  6 00:14:53.793: INFO: (18) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 21.217058ms)
Sep  6 00:14:53.793: INFO: (18) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 21.528894ms)
Sep  6 00:14:53.797: INFO: (18) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 25.432106ms)
Sep  6 00:14:53.801: INFO: (18) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 29.128393ms)
Sep  6 00:14:53.801: INFO: (18) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 29.111552ms)
Sep  6 00:14:53.806: INFO: (18) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 33.771006ms)
Sep  6 00:14:53.807: INFO: (18) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 35.36625ms)
Sep  6 00:14:53.808: INFO: (18) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 36.532714ms)
Sep  6 00:14:53.820: INFO: (19) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 11.068164ms)
Sep  6 00:14:53.832: INFO: (19) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">t... (200; 22.695762ms)
Sep  6 00:14:53.833: INFO: (19) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:1080/proxy/rewriteme">test</... (200; 24.129134ms)
Sep  6 00:14:53.833: INFO: (19) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:443/proxy/tlsrewriteme... (200; 23.830064ms)
Sep  6 00:14:53.833: INFO: (19) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 23.905334ms)
Sep  6 00:14:53.833: INFO: (19) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:162/proxy/: bar (200; 24.254285ms)
Sep  6 00:14:53.836: INFO: (19) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:462/proxy/: tls qux (200; 27.198615ms)
Sep  6 00:14:53.836: INFO: (19) /api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/: <a href="/api/v1/namespaces/proxy-556/pods/proxy-service-gcjpc-z26nf/proxy/rewriteme">test</a> (200; 27.173022ms)
Sep  6 00:14:53.837: INFO: (19) /api/v1/namespaces/proxy-556/pods/http:proxy-service-gcjpc-z26nf:160/proxy/: foo (200; 27.899205ms)
Sep  6 00:14:53.837: INFO: (19) /api/v1/namespaces/proxy-556/pods/https:proxy-service-gcjpc-z26nf:460/proxy/: tls baz (200; 28.12192ms)
Sep  6 00:14:53.848: INFO: (19) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname1/proxy/: foo (200; 38.893175ms)
Sep  6 00:14:53.848: INFO: (19) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname2/proxy/: tls qux (200; 39.328309ms)
Sep  6 00:14:53.849: INFO: (19) /api/v1/namespaces/proxy-556/services/http:proxy-service-gcjpc:portname2/proxy/: bar (200; 39.473298ms)
Sep  6 00:14:53.849: INFO: (19) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname1/proxy/: foo (200; 40.358946ms)
Sep  6 00:14:53.849: INFO: (19) /api/v1/namespaces/proxy-556/services/proxy-service-gcjpc:portname2/proxy/: bar (200; 40.424991ms)
Sep  6 00:14:53.854: INFO: (19) /api/v1/namespaces/proxy-556/services/https:proxy-service-gcjpc:tlsportname1/proxy/: tls baz (200; 44.735538ms)
STEP: deleting ReplicationController proxy-service-gcjpc in namespace proxy-556, will wait for the garbage collector to delete the pods
Sep  6 00:14:53.939: INFO: Deleting ReplicationController proxy-service-gcjpc took: 16.748914ms
Sep  6 00:14:56.040: INFO: Terminating ReplicationController proxy-service-gcjpc pods took: 2.100591983s
[AfterEach] version v1
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:15:10.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-556" for this suite.

â€¢ [SLOW TEST:46.812 seconds]
[sig-network] Proxy
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:59
    should proxy through a service and a pod  [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","total":291,"completed":263,"skipped":4314,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:15:10.586: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2234
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:38
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:82
[It] should be possible to delete [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Kubelet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:15:11.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2234" for this suite.
â€¢{"msg":"PASSED [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","total":291,"completed":264,"skipped":4329,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:15:11.308: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4812
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod with failed condition
STEP: updating the pod
Sep  6 00:17:12.440: INFO: Successfully updated pod "var-expansion-b3dcb5e4-296f-4c9a-a4be-d4552d79a92f"
STEP: waiting for pod running
STEP: deleting the pod gracefully
Sep  6 00:18:08.464: INFO: Deleting pod "var-expansion-b3dcb5e4-296f-4c9a-a4be-d4552d79a92f" in namespace "var-expansion-4812"
Sep  6 00:18:08.481: INFO: Wait up to 5m0s for pod "var-expansion-b3dcb5e4-296f-4c9a-a4be-d4552d79a92f" to be fully deleted
[AfterEach] [k8s.io] Variable Expansion
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:18:50.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4812" for this suite.

â€¢ [SLOW TEST:219.478 seconds]
[k8s.io] Variable Expansion
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [sig-storage][Slow] [Conformance]","total":291,"completed":265,"skipped":4340,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:18:50.786: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename tables
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in tables-951
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:18:51.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-951" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","total":291,"completed":266,"skipped":4345,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:18:51.341: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4280
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:181
[It] should be updated [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep  6 00:19:20.317: INFO: Successfully updated pod "pod-update-b465f79e-4a60-4bac-ad16-4fd47053d6e7"
STEP: verifying the updated pod is in kubernetes
Sep  6 00:19:20.342: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:19:20.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4280" for this suite.

â€¢ [SLOW TEST:29.274 seconds]
[k8s.io] Pods
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should be updated [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]","total":291,"completed":267,"skipped":4373,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:19:20.615: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-webhook-8445
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Sep  6 00:19:22.106: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Sep  6 00:19:24.125: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:19:26.137: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:19:28.241: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:19:30.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:19:32.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:19:34.132: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:19:36.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:19:38.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:19:40.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:19:42.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:19:44.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:19:46.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 19, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-85d57b96d6\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 00:19:49.173: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 00:19:49.179: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:19:50.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8445" for this suite.
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

â€¢ [SLOW TEST:31.809 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","total":291,"completed":268,"skipped":4388,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] PreStop
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:19:52.425: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-3236
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:171
[It] should call prestop when killing a pod  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating server pod server in namespace prestop-3236
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3236
STEP: Deleting pre-stop pod
Sep  6 00:20:48.072: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:20:48.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3236" for this suite.

â€¢ [SLOW TEST:55.925 seconds]
[k8s.io] [sig-node] PreStop
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  should call prestop when killing a pod  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]","total":291,"completed":269,"skipped":4408,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:20:48.351: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3268
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating secret with name secret-test-bac92a27-a814-4bfc-ad89-d7a24e432a84
STEP: Creating a pod to test consume secrets
Sep  6 00:20:48.813: INFO: Waiting up to 5m0s for pod "pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740" in namespace "secrets-3268" to be "Succeeded or Failed"
Sep  6 00:20:48.819: INFO: Pod "pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740": Phase="Pending", Reason="", readiness=false. Elapsed: 6.081475ms
Sep  6 00:20:50.829: INFO: Pod "pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015990965s
Sep  6 00:20:52.838: INFO: Pod "pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025448446s
Sep  6 00:20:54.851: INFO: Pod "pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740": Phase="Pending", Reason="", readiness=false. Elapsed: 6.03769467s
Sep  6 00:20:56.859: INFO: Pod "pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740": Phase="Pending", Reason="", readiness=false. Elapsed: 8.046495832s
Sep  6 00:20:58.865: INFO: Pod "pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740": Phase="Pending", Reason="", readiness=false. Elapsed: 10.052494177s
Sep  6 00:21:00.881: INFO: Pod "pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740": Phase="Pending", Reason="", readiness=false. Elapsed: 12.067781478s
Sep  6 00:21:02.889: INFO: Pod "pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740": Phase="Pending", Reason="", readiness=false. Elapsed: 14.076019335s
Sep  6 00:21:04.897: INFO: Pod "pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740": Phase="Pending", Reason="", readiness=false. Elapsed: 16.084102742s
Sep  6 00:21:06.907: INFO: Pod "pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740": Phase="Pending", Reason="", readiness=false. Elapsed: 18.093763275s
Sep  6 00:21:08.919: INFO: Pod "pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740": Phase="Pending", Reason="", readiness=false. Elapsed: 20.106288549s
Sep  6 00:21:10.927: INFO: Pod "pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740": Phase="Pending", Reason="", readiness=false. Elapsed: 22.11436574s
Sep  6 00:21:12.945: INFO: Pod "pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740": Phase="Pending", Reason="", readiness=false. Elapsed: 24.131841968s
Sep  6 00:21:14.958: INFO: Pod "pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.14549179s
STEP: Saw pod success
Sep  6 00:21:14.958: INFO: Pod "pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740" satisfied condition "Succeeded or Failed"
Sep  6 00:21:14.975: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740 container secret-volume-test: <nil>
STEP: delete the pod
Sep  6 00:21:19.695: INFO: Waiting for pod pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740 to disappear
Sep  6 00:21:19.708: INFO: Pod pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740 still exists
Sep  6 00:21:21.709: INFO: Waiting for pod pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740 to disappear
Sep  6 00:21:21.727: INFO: Pod pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740 still exists
Sep  6 00:21:23.709: INFO: Waiting for pod pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740 to disappear
Sep  6 00:21:23.716: INFO: Pod pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740 still exists
Sep  6 00:21:25.709: INFO: Waiting for pod pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740 to disappear
Sep  6 00:21:25.722: INFO: Pod pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740 still exists
Sep  6 00:21:27.710: INFO: Waiting for pod pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740 to disappear
Sep  6 00:21:27.729: INFO: Pod pod-secrets-2fe7f3f8-1911-4038-956c-4dab57855740 no longer exists
[AfterEach] [sig-storage] Secrets
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:21:27.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3268" for this suite.

â€¢ [SLOW TEST:39.582 seconds]
[sig-storage] Secrets
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:36
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":270,"skipped":4447,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:21:27.932: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1024
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-projected-7wll
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 00:21:28.399: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-7wll" in namespace "subpath-1024" to be "Succeeded or Failed"
Sep  6 00:21:28.427: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Pending", Reason="", readiness=false. Elapsed: 28.146848ms
Sep  6 00:21:30.435: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036919813s
Sep  6 00:21:32.446: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047038072s
Sep  6 00:21:34.454: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055616063s
Sep  6 00:21:36.462: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Pending", Reason="", readiness=false. Elapsed: 8.063115526s
Sep  6 00:21:38.470: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Pending", Reason="", readiness=false. Elapsed: 10.071625415s
Sep  6 00:21:40.482: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Pending", Reason="", readiness=false. Elapsed: 12.083179108s
Sep  6 00:21:42.495: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Pending", Reason="", readiness=false. Elapsed: 14.096203744s
Sep  6 00:21:44.501: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Pending", Reason="", readiness=false. Elapsed: 16.102480775s
Sep  6 00:21:46.515: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Pending", Reason="", readiness=false. Elapsed: 18.116875981s
Sep  6 00:21:48.525: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Running", Reason="", readiness=true. Elapsed: 20.126708716s
Sep  6 00:21:50.548: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Running", Reason="", readiness=true. Elapsed: 22.149659003s
Sep  6 00:21:52.558: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Running", Reason="", readiness=true. Elapsed: 24.159113375s
Sep  6 00:21:54.566: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Running", Reason="", readiness=true. Elapsed: 26.167683863s
Sep  6 00:21:56.574: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Running", Reason="", readiness=true. Elapsed: 28.175011612s
Sep  6 00:21:58.582: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Running", Reason="", readiness=true. Elapsed: 30.183614307s
Sep  6 00:22:00.592: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Running", Reason="", readiness=true. Elapsed: 32.193325459s
Sep  6 00:22:02.601: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Running", Reason="", readiness=true. Elapsed: 34.202601109s
Sep  6 00:22:04.612: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Running", Reason="", readiness=true. Elapsed: 36.213035242s
Sep  6 00:22:06.619: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Running", Reason="", readiness=true. Elapsed: 38.220342549s
Sep  6 00:22:08.627: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Running", Reason="", readiness=true. Elapsed: 40.228735003s
Sep  6 00:22:10.947: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Running", Reason="", readiness=true. Elapsed: 42.548755467s
Sep  6 00:22:12.955: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Running", Reason="", readiness=true. Elapsed: 44.556304558s
Sep  6 00:22:14.969: INFO: Pod "pod-subpath-test-projected-7wll": Phase="Succeeded", Reason="", readiness=false. Elapsed: 46.570571447s
STEP: Saw pod success
Sep  6 00:22:14.969: INFO: Pod "pod-subpath-test-projected-7wll" satisfied condition "Succeeded or Failed"
Sep  6 00:22:14.980: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-subpath-test-projected-7wll container test-container-subpath-projected-7wll: <nil>
STEP: delete the pod
Sep  6 00:22:19.762: INFO: Waiting for pod pod-subpath-test-projected-7wll to disappear
Sep  6 00:22:19.822: INFO: Pod pod-subpath-test-projected-7wll still exists
Sep  6 00:22:21.822: INFO: Waiting for pod pod-subpath-test-projected-7wll to disappear
Sep  6 00:22:21.829: INFO: Pod pod-subpath-test-projected-7wll still exists
Sep  6 00:22:23.823: INFO: Waiting for pod pod-subpath-test-projected-7wll to disappear
Sep  6 00:22:23.834: INFO: Pod pod-subpath-test-projected-7wll still exists
Sep  6 00:22:25.822: INFO: Waiting for pod pod-subpath-test-projected-7wll to disappear
Sep  6 00:22:25.832: INFO: Pod pod-subpath-test-projected-7wll still exists
Sep  6 00:22:27.823: INFO: Waiting for pod pod-subpath-test-projected-7wll to disappear
Sep  6 00:22:27.828: INFO: Pod pod-subpath-test-projected-7wll no longer exists
STEP: Deleting pod pod-subpath-test-projected-7wll
Sep  6 00:22:27.828: INFO: Deleting pod "pod-subpath-test-projected-7wll" in namespace "subpath-1024"
[AfterEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:22:27.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1024" for this suite.

â€¢ [SLOW TEST:60.095 seconds]
[sig-storage] Subpath
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]","total":291,"completed":271,"skipped":4454,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:22:28.028: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-812
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:78
[It] deployment should support proportional scaling [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 00:22:28.413: INFO: Creating deployment "webserver-deployment"
Sep  6 00:22:28.442: INFO: Waiting for observed generation 1
Sep  6 00:22:30.476: INFO: Waiting for all required pods to come up
Sep  6 00:22:30.514: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep  6 00:23:00.567: INFO: Waiting for deployment "webserver-deployment" to complete
Sep  6 00:23:00.591: INFO: Updating deployment "webserver-deployment" with a non-existent image
Sep  6 00:23:00.624: INFO: Updating deployment webserver-deployment
Sep  6 00:23:00.624: INFO: Waiting for observed generation 2
Sep  6 00:23:02.654: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep  6 00:23:02.664: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep  6 00:23:02.675: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  6 00:23:02.711: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep  6 00:23:02.711: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep  6 00:23:02.720: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Sep  6 00:23:02.741: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Sep  6 00:23:02.741: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Sep  6 00:23:02.794: INFO: Updating deployment webserver-deployment
Sep  6 00:23:02.794: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Sep  6 00:23:02.834: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep  6 00:23:02.865: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
Sep  6 00:23:03.142: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-812 /apis/apps/v1/namespaces/deployment-812/deployments/webserver-deployment 789cc729-7728-4b34-bb49-d4e2fa59e4a1 217721 3 2021-09-06 00:22:28 -0700 PDT <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  [{e2e.test Update apps/v1 2021-09-06 00:22:28 -0700 PDT FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{}}},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}}} {kube-controller-manager Update apps/v1 2021-09-06 00:23:01 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}}}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0036fa0e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-795d758f88" is progressing.,LastUpdateTime:2021-09-06 00:23:01 -0700 PDT,LastTransitionTime:2021-09-06 00:22:28 -0700 PDT,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2021-09-06 00:23:02 -0700 PDT,LastTransitionTime:2021-09-06 00:23:02 -0700 PDT,},},ReadyReplicas:8,CollisionCount:nil,},}

Sep  6 00:23:03.244: INFO: New ReplicaSet "webserver-deployment-795d758f88" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-795d758f88  deployment-812 /apis/apps/v1/namespaces/deployment-812/replicasets/webserver-deployment-795d758f88 e4af4d48-9967-4e3e-b43b-8d4f28cc714e 217713 3 2021-09-06 00:23:00 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 789cc729-7728-4b34-bb49-d4e2fa59e4a1 0xc0036fa567 0xc0036fa568}] []  [{kube-controller-manager Update apps/v1 2021-09-06 00:23:00 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"789cc729-7728-4b34-bb49-d4e2fa59e4a1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 795d758f88,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0036fa5e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Sep  6 00:23:03.245: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Sep  6 00:23:03.245: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-867f44f6fb  deployment-812 /apis/apps/v1/namespaces/deployment-812/replicasets/webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 217711 3 2021-09-06 00:22:28 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 789cc729-7728-4b34-bb49-d4e2fa59e4a1 0xc0036fa647 0xc0036fa648}] []  [{kube-controller-manager Update apps/v1 2021-09-06 00:22:53 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"789cc729-7728-4b34-bb49-d4e2fa59e4a1\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:replicas":{},"f:selector":{"f:matchLabels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}},"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}}}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 867f44f6fb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[] [] []  []} {[] [] [{httpd mirror.gcr.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0036fa6b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Sep  6 00:23:03.391: INFO: Pod "webserver-deployment-795d758f88-22pm4" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-22pm4 webserver-deployment-795d758f88- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-795d758f88-22pm4 bad6c579-3b0b-48d6-aa75-bf1b8247b85c 217694 0 2021-09-06 00:23:00 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[attachment_id:8d8744f9-0caa-4fb0-8637-32d2917ea91b kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:60:0a vlan:None vmware-system-image-references:{"httpd":"webserver-ea8d19c56b2978338502181157e293b3d96a2b18-v76104"}] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e4af4d48-9967-4e3e-b43b-8d4f28cc714e 0xc00338877f 0xc003388790}] []  [{image-controller Update v1 2021-09-06 00:23:00 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {kube-controller-manager Update v1 2021-09-06 00:23:00 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4af4d48-9967-4e3e-b43b-8d4f28cc714e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-9mqvr Update v1 2021-09-06 00:23:01 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-43-208.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:23:00 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.391: INFO: Pod "webserver-deployment-795d758f88-29fc5" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-29fc5 webserver-deployment-795d758f88- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-795d758f88-29fc5 9ed695b6-222c-41c2-99df-dff4629c06ac 217742 0 2021-09-06 00:23:03 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e4af4d48-9967-4e3e-b43b-8d4f28cc714e 0xc0033888f0 0xc0033888f1}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:03 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4af4d48-9967-4e3e-b43b-8d4f28cc714e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.393: INFO: Pod "webserver-deployment-795d758f88-79qtc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-79qtc webserver-deployment-795d758f88- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-795d758f88-79qtc 874a15c9-fe59-4b6a-bc14-b499c4f4bf88 217743 0 2021-09-06 00:23:03 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e4af4d48-9967-4e3e-b43b-8d4f28cc714e 0xc003388a10 0xc003388a11}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:03 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4af4d48-9967-4e3e-b43b-8d4f28cc714e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.393: INFO: Pod "webserver-deployment-795d758f88-d7bgf" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-d7bgf webserver-deployment-795d758f88- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-795d758f88-d7bgf 203dc1d6-4f57-4654-943b-0868ca1d631a 217730 0 2021-09-06 00:23:02 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e4af4d48-9967-4e3e-b43b-8d4f28cc714e 0xc003388b30 0xc003388b31}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:02 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4af4d48-9967-4e3e-b43b-8d4f28cc714e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-34-149.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:23:03 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.393: INFO: Pod "webserver-deployment-795d758f88-fhgmc" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-fhgmc webserver-deployment-795d758f88- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-795d758f88-fhgmc 17ea3e80-7669-472d-b4d1-51e0e4bc1a97 217708 0 2021-09-06 00:23:01 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[attachment_id:ff0e8f07-7947-463a-aabc-0fdc5d06e089 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:60:2b vlan:None] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e4af4d48-9967-4e3e-b43b-8d4f28cc714e 0xc003388c77 0xc003388c78}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:01 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4af4d48-9967-4e3e-b43b-8d4f28cc714e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-9mqvr Update v1 2021-09-06 00:23:02 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-43-208.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:23:01 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.393: INFO: Pod "webserver-deployment-795d758f88-kzzjg" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-kzzjg webserver-deployment-795d758f88- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-795d758f88-kzzjg 2ae4d198-fe6e-4220-b0f0-aae325a9604d 217737 0 2021-09-06 00:23:03 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e4af4d48-9967-4e3e-b43b-8d4f28cc714e 0xc003388db0 0xc003388db1}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:03 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4af4d48-9967-4e3e-b43b-8d4f28cc714e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.393: INFO: Pod "webserver-deployment-795d758f88-m4gtz" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-m4gtz webserver-deployment-795d758f88- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-795d758f88-m4gtz 011ed99d-d522-4e40-9dad-0f1be172348d 217741 0 2021-09-06 00:23:03 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e4af4d48-9967-4e3e-b43b-8d4f28cc714e 0xc003388ed0 0xc003388ed1}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:03 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4af4d48-9967-4e3e-b43b-8d4f28cc714e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.394: INFO: Pod "webserver-deployment-795d758f88-nbvp8" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-nbvp8 webserver-deployment-795d758f88- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-795d758f88-nbvp8 b5592d85-63c3-4724-831c-6d60bdb544ea 217666 0 2021-09-06 00:23:00 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e4af4d48-9967-4e3e-b43b-8d4f28cc714e 0xc003388ff0 0xc003388ff1}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:00 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4af4d48-9967-4e3e-b43b-8d4f28cc714e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-39-55.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:23:01 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.394: INFO: Pod "webserver-deployment-795d758f88-rl55q" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-rl55q webserver-deployment-795d758f88- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-795d758f88-rl55q be537c4d-acc4-46ed-bd8c-25cb41d35f2c 217679 0 2021-09-06 00:23:01 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e4af4d48-9967-4e3e-b43b-8d4f28cc714e 0xc003389120 0xc003389121}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:01 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4af4d48-9967-4e3e-b43b-8d4f28cc714e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-34-149.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:23:01 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.394: INFO: Pod "webserver-deployment-795d758f88-txbjm" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-txbjm webserver-deployment-795d758f88- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-795d758f88-txbjm 14f1b61e-3689-468c-b5ad-ab5560b75b02 217701 0 2021-09-06 00:23:00 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[attachment_id:16036259-f380-4885-a250-7d16deeb6d47 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:60:17 vlan:None] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e4af4d48-9967-4e3e-b43b-8d4f28cc714e 0xc003389267 0xc003389268}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:00 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4af4d48-9967-4e3e-b43b-8d4f28cc714e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-9mqvr Update v1 2021-09-06 00:23:02 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-34-149.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:23:00 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.394: INFO: Pod "webserver-deployment-795d758f88-tzpkv" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-tzpkv webserver-deployment-795d758f88- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-795d758f88-tzpkv 5cd112b4-131c-4e64-9ab0-999e6444cb54 217723 0 2021-09-06 00:23:02 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e4af4d48-9967-4e3e-b43b-8d4f28cc714e 0xc0033893a0 0xc0033893a1}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:02 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4af4d48-9967-4e3e-b43b-8d4f28cc714e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.394: INFO: Pod "webserver-deployment-795d758f88-z9kk2" is not available:
&Pod{ObjectMeta:{webserver-deployment-795d758f88-z9kk2 webserver-deployment-795d758f88- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-795d758f88-z9kk2 909ff257-77bb-4612-a31d-6ace527074ae 217722 0 2021-09-06 00:23:02 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:795d758f88] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-795d758f88 e4af4d48-9967-4e3e-b43b-8d4f28cc714e 0xc0033894c0 0xc0033894c1}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:02 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e4af4d48-9967-4e3e-b43b-8d4f28cc714e\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.394: INFO: Pod "webserver-deployment-867f44f6fb-2gsk9" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-2gsk9 webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-2gsk9 a2155638-b738-4404-a73b-337e25f7320a 217569 0 2021-09-06 00:22:28 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:a160552d-e2e9-4394-adcf-396e35be73ba kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:60:29 vlan:None vmware-system-ephemeral-disk-uuid:6000C291-01cc-80ab-7fcf-43f580c0770b vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416"} vmware-system-vm-moid:vm-1515:7badc608-dd2f-42a9-952d-0d4c30d5f283 vmware-system-vm-uuid:5017588f-d580-7d27-7a56-d1db313959d1] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc0033895f7 0xc0033895f8}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:22:28 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-06 00:22:40 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {nsx-ncp-6d7f7bf559-9mqvr Update v1 2021-09-06 00:22:40 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-06 00:22:48 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:22:58 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-34-149.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:29 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:58 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:58 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:58 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.34.149,PodIP:172.26.1.203,StartTime:2021-09-06 00:22:52 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:22:53 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416,ContainerID:cc1be010-31e8-4457-b289-a0b748dc8b5b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.395: INFO: Pod "webserver-deployment-867f44f6fb-4z5v7" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-4z5v7 webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-4z5v7 a5196be2-08ec-405c-8fe8-14227f775cef 217502 0 2021-09-06 00:22:28 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:a96f3875-ddc9-40c3-bb56-3ddad34fbe14 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:60:21 vlan:None vmware-system-ephemeral-disk-uuid:6000C291-ffe7-4b6d-c8aa-8f26c8b0ac56 vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416"} vmware-system-vm-moid:vm-1511:7badc608-dd2f-42a9-952d-0d4c30d5f283 vmware-system-vm-uuid:50171a27-2359-73ca-421e-82a54770da99] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc0033897ff 0xc003389810}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:22:28 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-06 00:22:40 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {nsx-ncp-6d7f7bf559-9mqvr Update v1 2021-09-06 00:22:40 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-06 00:22:48 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:22:53 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-34-149.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:28 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:54 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:54 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:54 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.34.149,PodIP:172.26.1.198,StartTime:2021-09-06 00:22:53 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:22:53 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416,ContainerID:48b05ac2-a990-4de1-9c5f-ea3b025bb8eb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.198,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.396: INFO: Pod "webserver-deployment-867f44f6fb-655j7" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-655j7 webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-655j7 6a1270a3-f669-4c3e-b98f-5ae6a5a3ed64 217745 0 2021-09-06 00:23:02 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc003389a17 0xc003389a18}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:02 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-39-55.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:23:03 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.397: INFO: Pod "webserver-deployment-867f44f6fb-7q8fc" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-7q8fc webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-7q8fc f810095e-9b74-485d-9cf1-555a520411cd 217733 0 2021-09-06 00:23:03 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc003389b60 0xc003389b61}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:03 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.397: INFO: Pod "webserver-deployment-867f44f6fb-86ct8" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-86ct8 webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-86ct8 26c84fea-8262-4bcb-bb55-b4dc5de248b5 217652 0 2021-09-06 00:22:28 -0700 PDT 2021-09-06 00:23:00 -0700 PDT 0xc003389c40 map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:fadd211a-4717-41cb-bc7b-944a4465739e kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:60:1f vlan:None vmware-system-ephemeral-disk-uuid:6000C296-34aa-1956-aedf-3762a9825cac vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416"} vmware-system-vm-moid:vm-1510:7badc608-dd2f-42a9-952d-0d4c30d5f283 vmware-system-vm-uuid:5017be3f-15e3-e874-66d0-d939e59e0492] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc003389c7f 0xc003389c90}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:22:28 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-9mqvr Update v1 2021-09-06 00:22:38 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {image-controller Update v1 2021-09-06 00:22:40 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {scheduler-extender Update v1 2021-09-06 00:22:49 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:22:58 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.197\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-43-208.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:28 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:59 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:59 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:59 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.43.208,PodIP:172.26.1.197,StartTime:2021-09-06 00:22:54 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:22:55 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416,ContainerID:37cdd40a-1980-4828-bfc7-a994c9f2d20a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.197,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.398: INFO: Pod "webserver-deployment-867f44f6fb-94s7w" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-94s7w webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-94s7w c10bf57d-9d0a-4509-91e5-93fbcb9e1cd8 217596 0 2021-09-06 00:22:28 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:43ca6695-943c-45a7-a1de-2360eaec4eb3 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:60:1a vlan:None vmware-system-ephemeral-disk-uuid:6000C298-0c31-dc82-e21e-0aa30fbeb5e8 vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416"} vmware-system-vm-moid:vm-1512:7badc608-dd2f-42a9-952d-0d4c30d5f283 vmware-system-vm-uuid:5017a13b-194b-5814-ec05-f3dbd9056a83] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc003389e7f 0xc003389e90}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:22:28 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-9mqvr Update v1 2021-09-06 00:22:38 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {image-controller Update v1 2021-09-06 00:22:40 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {scheduler-extender Update v1 2021-09-06 00:22:49 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:22:58 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-43-208.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:28 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:58 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:58 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:58 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.43.208,PodIP:172.26.1.196,StartTime:2021-09-06 00:22:54 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:22:55 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416,ContainerID:414d6104-a122-43a0-a801-49ef20561b26,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.196,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.398: INFO: Pod "webserver-deployment-867f44f6fb-bh9m4" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-bh9m4 webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-bh9m4 36e1e768-0bf5-4b84-a5d7-840c5fbb4dae 217728 0 2021-09-06 00:23:02 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc007d0c107 0xc007d0c108}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:02 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.398: INFO: Pod "webserver-deployment-867f44f6fb-bm78z" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-bm78z webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-bm78z 9ac2fea5-28da-4d5c-b39c-e99fb53c28f6 217584 0 2021-09-06 00:22:28 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:8330e668-a080-4d7a-a411-a9682b249997 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:60:11 vlan:None vmware-system-ephemeral-disk-uuid:6000C294-fb32-b6ac-205c-c374a04c90f7 vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416"} vmware-system-vm-moid:vm-1509:7badc608-dd2f-42a9-952d-0d4c30d5f283 vmware-system-vm-uuid:50174e9f-e3ec-bd90-19ff-c0cc6b0e7903] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc007d0c727 0xc007d0c728}] [lifecycle-controller/system.vmware.com]  [{image-controller Update v1 2021-09-06 00:22:28 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {kube-controller-manager Update v1 2021-09-06 00:22:28 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-9mqvr Update v1 2021-09-06 00:22:37 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {scheduler-extender Update v1 2021-09-06 00:22:49 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:22:58 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-43-208.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:28 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:58 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:58 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:58 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.43.208,PodIP:172.26.1.194,StartTime:2021-09-06 00:22:56 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:22:57 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416,ContainerID:864fb4dd-a693-42e0-9786-0729e4f04f34,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.398: INFO: Pod "webserver-deployment-867f44f6fb-c8vcc" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-c8vcc webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-c8vcc 037ca33e-ca5f-4d22-a11b-5c41c11ba92e 217591 0 2021-09-06 00:22:28 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:0ca25d79-eaff-4dcc-9438-5528865d6604 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:60:15 vlan:None vmware-system-ephemeral-disk-uuid:6000C294-9b3b-547d-4728-be36b2943c68 vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416"} vmware-system-vm-moid:vm-1508:7badc608-dd2f-42a9-952d-0d4c30d5f283 vmware-system-vm-uuid:5017aa81-ceb4-b89a-3c45-6534dda9352b] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc007d0c94f 0xc007d0c960}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:22:28 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-9mqvr Update v1 2021-09-06 00:22:37 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {image-controller Update v1 2021-09-06 00:22:41 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {scheduler-extender Update v1 2021-09-06 00:22:49 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:22:58 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.195\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-43-208.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:28 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:59 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:59 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:59 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.43.208,PodIP:172.26.1.195,StartTime:2021-09-06 00:22:56 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:22:57 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416,ContainerID:3b82f825-6244-4f93-9b1b-05ca36c94ea2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.195,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.398: INFO: Pod "webserver-deployment-867f44f6fb-gfsdq" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-gfsdq webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-gfsdq 422d1df0-33a8-4d98-b988-7bb03b846908 217727 0 2021-09-06 00:23:02 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc007d0cb67 0xc007d0cb68}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:02 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.400: INFO: Pod "webserver-deployment-867f44f6fb-h6wbw" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-h6wbw webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-h6wbw 39e10cff-6601-4498-8758-5cc4a98980a4 217514 0 2021-09-06 00:22:28 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:89c74c60-940d-4885-8d97-98498ea8a679 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:60:28 vlan:None vmware-system-ephemeral-disk-uuid:6000C29c-e6df-b78e-edb8-43c0aaafb247 vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416"} vmware-system-vm-moid:vm-1513:7badc608-dd2f-42a9-952d-0d4c30d5f283 vmware-system-vm-uuid:501765e3-c263-470e-19f8-ec34de33900c] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc007d0cc87 0xc007d0cc88}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:22:28 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-9mqvr Update v1 2021-09-06 00:22:39 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {image-controller Update v1 2021-09-06 00:22:41 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {scheduler-extender Update v1 2021-09-06 00:22:47 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:22:53 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-39-55.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:29 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:54 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:54 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:54 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.39.55,PodIP:172.26.1.202,StartTime:2021-09-06 00:22:50 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:22:51 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416,ContainerID:87658c46-d7fd-4a9b-adc9-d19c429d17ac,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.400: INFO: Pod "webserver-deployment-867f44f6fb-h8kqc" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-h8kqc webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-h8kqc a821d188-0fc4-408b-9eab-9cb21317b703 217739 0 2021-09-06 00:23:03 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc007d0ce67 0xc007d0ce68}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:03 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.400: INFO: Pod "webserver-deployment-867f44f6fb-hsqsd" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-hsqsd webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-hsqsd e7c33f5c-aa3e-4a2b-9305-611851f02c21 217726 0 2021-09-06 00:23:02 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc007d0cf70 0xc007d0cf71}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:02 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.400: INFO: Pod "webserver-deployment-867f44f6fb-jvccs" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-jvccs webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-jvccs 71912217-5643-49c9-a9fe-9e9daf667a6b 217735 0 2021-09-06 00:23:02 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc007d0d080 0xc007d0d081}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:02 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-43-208.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:23:03 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.400: INFO: Pod "webserver-deployment-867f44f6fb-kmxfv" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-kmxfv webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-kmxfv 2b71a90f-2ef5-4349-b794-06b111a28463 217729 0 2021-09-06 00:23:02 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc007d0d1a0 0xc007d0d1a1}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:02 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.400: INFO: Pod "webserver-deployment-867f44f6fb-n6nx2" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-n6nx2 webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-n6nx2 6dbeefee-f32f-41d7-ac07-8ee1b0b41647 217648 0 2021-09-06 00:22:28 -0700 PDT 2021-09-06 00:23:00 -0700 PDT 0xc007d0d280 map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:5fbc87b0-0038-4af3-a7ae-826fb47896d0 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:60:22 vlan:None vmware-system-ephemeral-disk-uuid:6000C29b-a50c-431e-2d67-8474d301743b vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416"} vmware-system-vm-moid:vm-1517:7badc608-dd2f-42a9-952d-0d4c30d5f283 vmware-system-vm-uuid:50176c5c-f58b-083b-9a26-0e20dd8303fc] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc007d0d2bf 0xc007d0d2d0}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:22:28 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-9mqvr Update v1 2021-09-06 00:22:39 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {image-controller Update v1 2021-09-06 00:22:41 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {scheduler-extender Update v1 2021-09-06 00:22:49 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:22:58 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.199\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-43-208.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:29 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:59 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:59 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:59 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.43.208,PodIP:172.26.1.199,StartTime:2021-09-06 00:22:56 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:22:57 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416,ContainerID:5efa7a57-3dcb-466f-a8ef-6f337d1db7e0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.199,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.401: INFO: Pod "webserver-deployment-867f44f6fb-ql5zt" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-ql5zt webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-ql5zt 8580c046-52ba-48fa-a5a6-3132f0378d69 217567 0 2021-09-06 00:22:28 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:12e499bc-42e4-43c4-9906-e786ae4f0c9f kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:60:23 vlan:None vmware-system-ephemeral-disk-uuid:6000C29b-9272-c384-f8e1-bda37f6663bb vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416"} vmware-system-vm-moid:vm-1514:7badc608-dd2f-42a9-952d-0d4c30d5f283 vmware-system-vm-uuid:5017fa4f-506e-3059-af20-1b41ecfaf6ac] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc007d0d4bf 0xc007d0d4d0}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:22:28 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-9mqvr Update v1 2021-09-06 00:22:39 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {image-controller Update v1 2021-09-06 00:22:41 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {scheduler-extender Update v1 2021-09-06 00:22:48 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:22:58 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.200\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-34-149.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:29 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:58 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:58 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:58 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.34.149,PodIP:172.26.1.200,StartTime:2021-09-06 00:22:53 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:22:53 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416,ContainerID:aa80bd37-f87f-4cba-a3b1-b62e7cdcea41,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.200,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.401: INFO: Pod "webserver-deployment-867f44f6fb-qtvbp" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-qtvbp webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-qtvbp 02e78a9a-b1d4-478a-bec0-254167cea1b9 217738 0 2021-09-06 00:23:03 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc007d0d6b7 0xc007d0d6b8}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:03 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.401: INFO: Pod "webserver-deployment-867f44f6fb-s975j" is available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-s975j webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-s975j 0b82c12f-3ea5-4bce-80dc-3008bef3d681 217589 0 2021-09-06 00:22:28 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[attachment_id:ed675b12-e2c6-4942-a7e3-dd5bb025e5c3 kubernetes.io/psp:e2e-test-privileged-psp mac:04:50:56:00:60:26 vlan:None vmware-system-ephemeral-disk-uuid:6000C296-fb9e-25a1-9f31-5b5d7ec52d03 vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416"} vmware-system-vm-moid:vm-1516:7badc608-dd2f-42a9-952d-0d4c30d5f283 vmware-system-vm-uuid:5017f1dd-9387-8327-79aa-f179f552199a] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc007d0d7d7 0xc007d0d7d8}] [lifecycle-controller/system.vmware.com]  [{kube-controller-manager Update v1 2021-09-06 00:22:28 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {nsx-ncp-6d7f7bf559-9mqvr Update v1 2021-09-06 00:22:39 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:attachment_id":{},"f:mac":{},"f:vlan":{}}}}} {image-controller Update v1 2021-09-06 00:22:41 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}} {scheduler-extender Update v1 2021-09-06 00:22:49 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-ephemeral-disk-uuid":{},"f:vmware-system-vm-moid":{},"f:vmware-system-vm-uuid":{}},"f:finalizers":{".":{},"v:\"lifecycle-controller/system.vmware.com\"":{}}}}} {spherelet Update v1 2021-09-06 00:22:58 -0700 PDT FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.26.1.201\"}":{".":{},"f:ip":{}}},"f:startTime":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-43-208.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:29 -0700 PDT,Reason:,Message:,},PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:59 -0700 PDT,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:59 -0700 PDT,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:22:59 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:10.193.43.208,PodIP:172.26.1.201,StartTime:2021-09-06 00:22:55 -0700 PDT,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2021-09-06 00:22:56 -0700 PDT,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,ImageID:httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416,ContainerID:305f0c6c-904c-46ba-ad1d-f03a4971b695,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.26.1.201,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.401: INFO: Pod "webserver-deployment-867f44f6fb-w429l" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-w429l webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-w429l 53ff2bb5-6dae-4695-99ca-1876fe126c40 217736 0 2021-09-06 00:23:03 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc007d0d9c7 0xc007d0d9c8}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:03 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.401: INFO: Pod "webserver-deployment-867f44f6fb-w7djf" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-w7djf webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-w7djf 7b84285e-eb25-47b0-a0e0-f7b3453d64fe 217744 0 2021-09-06 00:23:02 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp vmware-system-image-references:{"httpd":"httpd-d22d0e2cfeeaa0c1d2773814f5199d564ac0eae2-v38416"}] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc007d0dad0 0xc007d0dad1}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:02 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}} {image-controller Update v1 2021-09-06 00:23:03 -0700 PDT FieldsV1 {"f:metadata":{"f:annotations":{"f:vmware-system-image-references":{}}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sc2-rdops-vm09-dhcp-39-55.eng.vmware.com,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2021-09-06 00:23:02 -0700 PDT,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Sep  6 00:23:03.401: INFO: Pod "webserver-deployment-867f44f6fb-wkmgh" is not available:
&Pod{ObjectMeta:{webserver-deployment-867f44f6fb-wkmgh webserver-deployment-867f44f6fb- deployment-812 /api/v1/namespaces/deployment-812/pods/webserver-deployment-867f44f6fb-wkmgh 02c488b8-7015-4b92-847f-bf83076e51e6 217740 0 2021-09-06 00:23:03 -0700 PDT <nil> <nil> map[name:httpd pod-template-hash:867f44f6fb] map[kubernetes.io/psp:e2e-test-privileged-psp] [{apps/v1 ReplicaSet webserver-deployment-867f44f6fb 06d26527-8f21-41f1-a490-48f056005477 0xc007d0dc30 0xc007d0dc31}] []  [{kube-controller-manager Update v1 2021-09-06 00:23:03 -0700 PDT FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06d26527-8f21-41f1-a490-48f056005477\"}":{".":{},"f:apiVersion":{},"f:blockOwnerDeletion":{},"f:controller":{},"f:kind":{},"f:name":{},"f:uid":{}}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zb5x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zb5x8,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:mirror.gcr.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zb5x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:23:03.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-812" for this suite.

â€¢ [SLOW TEST:36.371 seconds]
[sig-apps] Deployment
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","total":291,"completed":272,"skipped":4505,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:23:04.400: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2114
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:90
Sep  6 00:23:05.084: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep  6 00:23:05.474: INFO: Waiting for terminating namespaces to be deleted...
Sep  6 00:23:05.735: INFO: 
Logging pods the apiserver thinks is on node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com before test
Sep  6 00:23:06.006: INFO: webserver-deployment-795d758f88-d7bgf from deployment-812 started at <nil> (0 container statuses recorded)
Sep  6 00:23:06.006: INFO: webserver-deployment-795d758f88-rl55q from deployment-812 started at <nil> (0 container statuses recorded)
Sep  6 00:23:06.006: INFO: webserver-deployment-795d758f88-txbjm from deployment-812 started at <nil> (0 container statuses recorded)
Sep  6 00:23:06.006: INFO: webserver-deployment-795d758f88-z9kk2 from deployment-812 started at <nil> (0 container statuses recorded)
Sep  6 00:23:06.006: INFO: webserver-deployment-867f44f6fb-2gsk9 from deployment-812 started at 2021-09-06 00:22:52 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.006: INFO: 	Container httpd ready: true, restart count 0
Sep  6 00:23:06.006: INFO: webserver-deployment-867f44f6fb-4z5v7 from deployment-812 started at 2021-09-06 00:22:53 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.006: INFO: 	Container httpd ready: true, restart count 0
Sep  6 00:23:06.006: INFO: webserver-deployment-867f44f6fb-gfsdq from deployment-812 started at <nil> (0 container statuses recorded)
Sep  6 00:23:06.006: INFO: webserver-deployment-867f44f6fb-ql5zt from deployment-812 started at 2021-09-06 00:22:53 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.006: INFO: 	Container httpd ready: true, restart count 0
Sep  6 00:23:06.006: INFO: podwithpersistentvolume from storage-class-test-2 started at 2021-09-05 20:05:57 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.006: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:23:06.006: INFO: hello-web-6b97664bd5-f5452 from test-cluster-ip-service started at 2021-09-05 20:10:51 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.006: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 00:23:06.006: INFO: wcp-sanity-busybox-6f999d6849-45jct from test-dataprovider-podvms-ns started at 2021-09-05 19:57:27 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.006: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:23:06.006: INFO: wcp-sanity-busybox-6f999d6849-c27n2 from test-dataprovider-podvms-ns started at 2021-09-05 21:11:14 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.006: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:23:06.007: INFO: nginx-private from test-image-pull-secrets-ns started at 2021-09-05 19:59:11 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.007: INFO: 	Container nginx-private-container ready: true, restart count 0
Sep  6 00:23:06.007: INFO: curl-pod from test-network-policy started at 2021-09-05 20:14:04 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.007: INFO: 	Container curl-container ready: true, restart count 0
Sep  6 00:23:06.007: INFO: hello-web-1-6b97664bd5-cl9tj from test-network-policy started at 2021-09-05 21:09:17 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.007: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 00:23:06.007: INFO: schedext-test-node-selector-1 from test-node-selector started at 2021-09-05 20:00:20 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.007: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:23:06.007: INFO: busybox from test-pod-external-nw-access started at 2021-09-05 20:15:25 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.007: INFO: 	Container busybox ready: true, restart count 0
Sep  6 00:23:06.007: INFO: busybox-annotation from test-podvm-annotations started at 2021-09-05 20:03:14 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.007: INFO: 	Container busybox-annotation ready: true, restart count 0
Sep  6 00:23:06.007: INFO: helloworld from test-telemetry started at 2021-09-05 20:09:22 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.007: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:23:06.007: INFO: wcp-sanity-busybox-6f999d6849-46njm from test-update-workload-ns started at 2021-09-05 20:07:43 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.007: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:23:06.007: INFO: 
Logging pods the apiserver thinks is on node sc2-rdops-vm09-dhcp-39-55.eng.vmware.com before test
Sep  6 00:23:06.213: INFO: webserver-deployment-795d758f88-nbvp8 from deployment-812 started at <nil> (0 container statuses recorded)
Sep  6 00:23:06.213: INFO: webserver-deployment-867f44f6fb-655j7 from deployment-812 started at <nil> (0 container statuses recorded)
Sep  6 00:23:06.213: INFO: webserver-deployment-867f44f6fb-bh9m4 from deployment-812 started at <nil> (0 container statuses recorded)
Sep  6 00:23:06.213: INFO: webserver-deployment-867f44f6fb-h6wbw from deployment-812 started at 2021-09-06 00:22:50 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.213: INFO: 	Container httpd ready: true, restart count 0
Sep  6 00:23:06.213: INFO: webserver-deployment-867f44f6fb-hsqsd from deployment-812 started at <nil> (0 container statuses recorded)
Sep  6 00:23:06.213: INFO: webserver-deployment-867f44f6fb-w7djf from deployment-812 started at <nil> (0 container statuses recorded)
Sep  6 00:23:06.213: INFO: curl-pod from test-cluster-ip-service started at 2021-09-05 20:10:11 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.213: INFO: 	Container curl-container ready: true, restart count 0
Sep  6 00:23:06.213: INFO: helloworld from test-exec-ns started at 2021-09-05 19:58:27 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.213: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:23:06.213: INFO: schedext-test-node-selector-2 from test-node-selector started at 2021-09-05 20:00:22 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.213: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:23:06.213: INFO: schedext-test-affinity-1 from test-pod-affinity started at 2021-09-05 20:00:51 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.213: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:23:06.213: INFO: schedext-test-affinity-2 from test-pod-affinity started at 2021-09-05 20:01:16 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.213: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:23:06.213: INFO: test-docker-registry from test-private-image-registry-ns started at 2021-09-05 20:16:11 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.213: INFO: 	Container test-docker-registry ready: true, restart count 0
Sep  6 00:23:06.213: INFO: helloworld from test-update-workload-ns started at 2021-09-05 20:08:34 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.213: INFO: 	Container hello ready: true, restart count 0
Sep  6 00:23:06.213: INFO: 
Logging pods the apiserver thinks is on node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com before test
Sep  6 00:23:06.344: INFO: webserver-deployment-795d758f88-22pm4 from deployment-812 started at <nil> (0 container statuses recorded)
Sep  6 00:23:06.344: INFO: webserver-deployment-795d758f88-fhgmc from deployment-812 started at <nil> (0 container statuses recorded)
Sep  6 00:23:06.344: INFO: webserver-deployment-795d758f88-tzpkv from deployment-812 started at <nil> (0 container statuses recorded)
Sep  6 00:23:06.344: INFO: webserver-deployment-867f44f6fb-86ct8 from deployment-812 started at 2021-09-06 00:22:54 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.344: INFO: 	Container httpd ready: true, restart count 0
Sep  6 00:23:06.344: INFO: webserver-deployment-867f44f6fb-94s7w from deployment-812 started at 2021-09-06 00:22:54 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.344: INFO: 	Container httpd ready: true, restart count 0
Sep  6 00:23:06.344: INFO: webserver-deployment-867f44f6fb-bm78z from deployment-812 started at 2021-09-06 00:22:56 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.344: INFO: 	Container httpd ready: true, restart count 0
Sep  6 00:23:06.344: INFO: webserver-deployment-867f44f6fb-c8vcc from deployment-812 started at 2021-09-06 00:22:56 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.344: INFO: 	Container httpd ready: true, restart count 0
Sep  6 00:23:06.344: INFO: webserver-deployment-867f44f6fb-jvccs from deployment-812 started at <nil> (0 container statuses recorded)
Sep  6 00:23:06.344: INFO: webserver-deployment-867f44f6fb-n6nx2 from deployment-812 started at 2021-09-06 00:22:56 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.344: INFO: 	Container httpd ready: true, restart count 0
Sep  6 00:23:06.344: INFO: webserver-deployment-867f44f6fb-s975j from deployment-812 started at 2021-09-06 00:22:55 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.344: INFO: 	Container httpd ready: true, restart count 0
Sep  6 00:23:06.344: INFO: hello-web-2-f779cbdff-hffpj from test-network-policy started at 2021-09-05 21:09:20 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.344: INFO: 	Container hello-app ready: true, restart count 0
Sep  6 00:23:06.344: INFO: wcp-sanity-busybox-6f999d6849-856nv from test-update-workload-ns started at 2021-09-05 21:10:00 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.344: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
Sep  6 00:23:06.344: INFO: wcp-sanity-busybox-6f999d6849-mts92 from test-update-workload-ns started at 2021-09-05 21:10:00 -0700 PDT (1 container statuses recorded)
Sep  6 00:23:06.344: INFO: 	Container wcp-sanity-busybox-container ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-35a70541-30bc-4ba5-9242-cacc43332909 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-35a70541-30bc-4ba5-9242-cacc43332909 off the node sc2-rdops-vm09-dhcp-34-149.eng.vmware.com
STEP: verifying the node doesn't have the label kubernetes.io/e2e-35a70541-30bc-4ba5-9242-cacc43332909
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:24:26.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2114" for this suite.
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81

â€¢ [SLOW TEST:82.459 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","total":291,"completed":273,"skipped":4542,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSS
------------------------------
[k8s.io] Lease 
  lease API should be available [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] Lease
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:24:26.860: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename lease-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in lease-test-8111
STEP: Waiting for a default service account to be provisioned in namespace
[It] lease API should be available [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[AfterEach] [k8s.io] Lease
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:24:27.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-8111" for this suite.
â€¢{"msg":"PASSED [k8s.io] Lease lease API should be available [Conformance]","total":291,"completed":274,"skipped":4550,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:24:27.525: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2328
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating pod pod-subpath-test-configmap-c7n2
STEP: Creating a pod to test atomic-volume-subpath
Sep  6 00:24:27.951: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-c7n2" in namespace "subpath-2328" to be "Succeeded or Failed"
Sep  6 00:24:27.988: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Pending", Reason="", readiness=false. Elapsed: 36.431791ms
Sep  6 00:24:29.997: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045252255s
Sep  6 00:24:32.008: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.056862922s
Sep  6 00:24:34.026: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.074261857s
Sep  6 00:24:36.034: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082794717s
Sep  6 00:24:38.044: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.09229458s
Sep  6 00:24:40.049: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.097334321s
Sep  6 00:24:42.062: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Pending", Reason="", readiness=false. Elapsed: 14.110331679s
Sep  6 00:24:44.083: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.131989251s
Sep  6 00:24:46.100: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.148689565s
Sep  6 00:24:48.108: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.156271682s
Sep  6 00:24:50.116: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Running", Reason="", readiness=true. Elapsed: 22.164812162s
Sep  6 00:24:52.124: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Running", Reason="", readiness=true. Elapsed: 24.172364764s
Sep  6 00:24:54.132: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Running", Reason="", readiness=true. Elapsed: 26.180383749s
Sep  6 00:24:56.139: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Running", Reason="", readiness=true. Elapsed: 28.188058178s
Sep  6 00:24:58.149: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Running", Reason="", readiness=true. Elapsed: 30.19718943s
Sep  6 00:25:00.158: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Running", Reason="", readiness=true. Elapsed: 32.206471942s
Sep  6 00:25:02.167: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Running", Reason="", readiness=true. Elapsed: 34.215162934s
Sep  6 00:25:04.173: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Running", Reason="", readiness=true. Elapsed: 36.221661645s
Sep  6 00:25:06.183: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Running", Reason="", readiness=true. Elapsed: 38.231426868s
Sep  6 00:25:08.194: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Running", Reason="", readiness=true. Elapsed: 40.242684605s
Sep  6 00:25:10.204: INFO: Pod "pod-subpath-test-configmap-c7n2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 42.252141938s
STEP: Saw pod success
Sep  6 00:25:10.204: INFO: Pod "pod-subpath-test-configmap-c7n2" satisfied condition "Succeeded or Failed"
Sep  6 00:25:10.212: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-subpath-test-configmap-c7n2 container test-container-subpath-configmap-c7n2: <nil>
STEP: delete the pod
Sep  6 00:25:10.310: INFO: Waiting for pod pod-subpath-test-configmap-c7n2 to disappear
Sep  6 00:25:10.321: INFO: Pod pod-subpath-test-configmap-c7n2 still exists
Sep  6 00:25:12.323: INFO: Waiting for pod pod-subpath-test-configmap-c7n2 to disappear
Sep  6 00:25:12.329: INFO: Pod pod-subpath-test-configmap-c7n2 still exists
Sep  6 00:25:14.322: INFO: Waiting for pod pod-subpath-test-configmap-c7n2 to disappear
Sep  6 00:25:14.332: INFO: Pod pod-subpath-test-configmap-c7n2 still exists
Sep  6 00:25:16.322: INFO: Waiting for pod pod-subpath-test-configmap-c7n2 to disappear
Sep  6 00:25:16.328: INFO: Pod pod-subpath-test-configmap-c7n2 still exists
Sep  6 00:25:18.323: INFO: Waiting for pod pod-subpath-test-configmap-c7n2 to disappear
Sep  6 00:25:18.335: INFO: Pod pod-subpath-test-configmap-c7n2 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-c7n2
Sep  6 00:25:18.335: INFO: Deleting pod "pod-subpath-test-configmap-c7n2" in namespace "subpath-2328"
[AfterEach] [sig-storage] Subpath
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:25:18.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2328" for this suite.

â€¢ [SLOW TEST:51.005 seconds]
[sig-storage] Subpath
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]","total":291,"completed":275,"skipped":4559,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:25:18.531: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9017
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Sep  6 00:25:41.494: INFO: Successfully updated pod "annotationupdatefa1d873a-55b8-4b4e-a53f-ee0d0313a922"
[AfterEach] [sig-storage] Projected downwardAPI
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:25:43.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9017" for this suite.

â€¢ [SLOW TEST:25.206 seconds]
[sig-storage] Projected downwardAPI
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:36
  should update annotations on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","total":291,"completed":276,"skipped":4636,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:25:43.738: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5858
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep  6 00:25:44.232: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5858 /api/v1/namespaces/watch-5858/configmaps/e2e-watch-test-resource-version 92261017-7cb0-432d-904a-c54c1a55d585 220072 0 2021-09-06 00:25:44 -0700 PDT <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-09-06 00:25:44 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Sep  6 00:25:44.232: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5858 /api/v1/namespaces/watch-5858/configmaps/e2e-watch-test-resource-version 92261017-7cb0-432d-904a-c54c1a55d585 220073 0 2021-09-06 00:25:44 -0700 PDT <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  [{e2e.test Update v1 2021-09-06 00:25:44 -0700 PDT FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}}}]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:25:44.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5858" for this suite.
â€¢{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","total":291,"completed":277,"skipped":4660,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-apps] ReplicaSet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:25:44.415: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5873
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep  6 00:26:07.911: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:26:08.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5873" for this suite.

â€¢ [SLOW TEST:24.757 seconds]
[sig-apps] ReplicaSet
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","total":291,"completed":278,"skipped":4667,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:26:09.173: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-2361
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep  6 00:26:10.510: INFO: Pod name wrapped-volume-race-d09e2510-2767-4024-8d44-7905fd7e3908: Found 0 pods out of 5
Sep  6 00:26:15.530: INFO: Pod name wrapped-volume-race-d09e2510-2767-4024-8d44-7905fd7e3908: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d09e2510-2767-4024-8d44-7905fd7e3908 in namespace emptydir-wrapper-2361, will wait for the garbage collector to delete the pods
Sep  6 00:26:47.674: INFO: Deleting ReplicationController wrapped-volume-race-d09e2510-2767-4024-8d44-7905fd7e3908 took: 22.910672ms
Sep  6 00:26:53.375: INFO: Terminating ReplicationController wrapped-volume-race-d09e2510-2767-4024-8d44-7905fd7e3908 pods took: 5.700784723s
STEP: Creating RC which spawns configmap-volume pods
Sep  6 00:27:12.324: INFO: Pod name wrapped-volume-race-40c8511d-2c0f-4140-8f55-adf531f94185: Found 0 pods out of 5
Sep  6 00:27:17.337: INFO: Pod name wrapped-volume-race-40c8511d-2c0f-4140-8f55-adf531f94185: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-40c8511d-2c0f-4140-8f55-adf531f94185 in namespace emptydir-wrapper-2361, will wait for the garbage collector to delete the pods
Sep  6 00:27:43.514: INFO: Deleting ReplicationController wrapped-volume-race-40c8511d-2c0f-4140-8f55-adf531f94185 took: 43.753591ms
Sep  6 00:27:45.814: INFO: Terminating ReplicationController wrapped-volume-race-40c8511d-2c0f-4140-8f55-adf531f94185 pods took: 2.300260479s
STEP: Creating RC which spawns configmap-volume pods
Sep  6 00:28:04.608: INFO: Pod name wrapped-volume-race-b0fd07e9-4f8c-4b91-a853-4665afeb1412: Found 0 pods out of 5
Sep  6 00:28:09.623: INFO: Pod name wrapped-volume-race-b0fd07e9-4f8c-4b91-a853-4665afeb1412: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-b0fd07e9-4f8c-4b91-a853-4665afeb1412 in namespace emptydir-wrapper-2361, will wait for the garbage collector to delete the pods
Sep  6 00:28:37.778: INFO: Deleting ReplicationController wrapped-volume-race-b0fd07e9-4f8c-4b91-a853-4665afeb1412 took: 41.217671ms
Sep  6 00:28:39.979: INFO: Terminating ReplicationController wrapped-volume-race-b0fd07e9-4f8c-4b91-a853-4665afeb1412 pods took: 2.200826115s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:28:53.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2361" for this suite.

â€¢ [SLOW TEST:165.013 seconds]
[sig-storage] EmptyDir wrapper volumes
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","total":291,"completed":279,"skipped":4667,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:28:54.187: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2378
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:782
[It] should serve multiport endpoints from pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: creating service multi-endpoint-test in namespace services-2378
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2378 to expose endpoints map[]
Sep  6 00:28:54.939: INFO: successfully validated that service multi-endpoint-test in namespace services-2378 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2378
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2378 to expose endpoints map[pod1:[100]]
Sep  6 00:28:59.026: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]], will retry
Sep  6 00:29:04.027: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]], will retry
Sep  6 00:29:09.024: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]], will retry
Sep  6 00:29:14.024: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]], will retry
Sep  6 00:29:19.029: INFO: successfully validated that service multi-endpoint-test in namespace services-2378 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-2378
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2378 to expose endpoints map[pod1:[100] pod2:[101]]
Sep  6 00:29:23.070: INFO: Unexpected endpoints: found map[bd32b66f-2e97-4b4a-aa3e-91adacede701:[100]], expected map[pod1:[100] pod2:[101]], will retry
Sep  6 00:29:28.073: INFO: Unexpected endpoints: found map[bd32b66f-2e97-4b4a-aa3e-91adacede701:[100]], expected map[pod1:[100] pod2:[101]], will retry
Sep  6 00:29:33.068: INFO: Unexpected endpoints: found map[bd32b66f-2e97-4b4a-aa3e-91adacede701:[100]], expected map[pod1:[100] pod2:[101]], will retry
Sep  6 00:29:34.097: INFO: successfully validated that service multi-endpoint-test in namespace services-2378 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Deleting pod pod1 in namespace services-2378
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2378 to expose endpoints map[pod2:[101]]
Sep  6 00:29:35.192: INFO: successfully validated that service multi-endpoint-test in namespace services-2378 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-2378
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2378 to expose endpoints map[]
Sep  6 00:29:36.247: INFO: successfully validated that service multi-endpoint-test in namespace services-2378 exposes endpoints map[]
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:29:36.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2378" for this suite.
[AfterEach] [sig-network] Services
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/service.go:786

â€¢ [SLOW TEST:42.394 seconds]
[sig-network] Services
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","total":291,"completed":280,"skipped":4678,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:29:36.581: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3728
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test downward API volume plugin
Sep  6 00:29:36.987: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0" in namespace "downward-api-3728" to be "Succeeded or Failed"
Sep  6 00:29:36.997: INFO: Pod "downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.982585ms
Sep  6 00:29:39.005: INFO: Pod "downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018207316s
Sep  6 00:29:41.015: INFO: Pod "downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028014644s
Sep  6 00:29:43.022: INFO: Pod "downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035385281s
Sep  6 00:29:45.029: INFO: Pod "downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.041620631s
Sep  6 00:29:47.036: INFO: Pod "downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.049259067s
Sep  6 00:29:49.049: INFO: Pod "downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.062085637s
Sep  6 00:29:51.058: INFO: Pod "downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0": Phase="Pending", Reason="", readiness=false. Elapsed: 14.07141604s
Sep  6 00:29:53.066: INFO: Pod "downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.079256125s
Sep  6 00:29:55.074: INFO: Pod "downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.08650136s
Sep  6 00:29:57.081: INFO: Pod "downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.09394451s
Sep  6 00:29:59.087: INFO: Pod "downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.099558364s
STEP: Saw pod success
Sep  6 00:29:59.087: INFO: Pod "downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0" satisfied condition "Succeeded or Failed"
Sep  6 00:29:59.091: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0 container client-container: <nil>
STEP: delete the pod
Sep  6 00:30:04.537: INFO: Waiting for pod downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0 to disappear
Sep  6 00:30:04.569: INFO: Pod downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0 still exists
Sep  6 00:30:06.569: INFO: Waiting for pod downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0 to disappear
Sep  6 00:30:06.581: INFO: Pod downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0 still exists
Sep  6 00:30:08.569: INFO: Waiting for pod downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0 to disappear
Sep  6 00:30:08.577: INFO: Pod downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0 still exists
Sep  6 00:30:10.569: INFO: Waiting for pod downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0 to disappear
Sep  6 00:30:10.577: INFO: Pod downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0 still exists
Sep  6 00:30:12.569: INFO: Waiting for pod downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0 to disappear
Sep  6 00:30:12.578: INFO: Pod downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0 still exists
Sep  6 00:30:14.570: INFO: Waiting for pod downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0 to disappear
Sep  6 00:30:14.577: INFO: Pod downwardapi-volume-3876241e-2e75-4c25-a609-8c145bbd9ca0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:30:14.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3728" for this suite.

â€¢ [SLOW TEST:38.186 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","total":291,"completed":281,"skipped":4698,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:30:14.768: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename webhook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in webhook-9510
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Sep  6 00:30:16.240: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Sep  6 00:30:18.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:30:20.297: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:30:22.285: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:30:24.333: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:30:26.288: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:30:28.286: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:30:30.291: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:30:32.288: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:30:34.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:30:36.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep  6 00:30:38.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), LastTransitionTime:time.Date(2021, time.September, 6, 0, 30, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-cbccbf6bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Sep  6 00:30:41.329: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:30:52.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9510" for this suite.
STEP: Destroying namespace "webhook-9510-markers" for this suite.
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

â€¢ [SLOW TEST:38.186 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","total":291,"completed":282,"skipped":4718,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:30:52.954: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1236
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap configmap-1236/configmap-test-5f0b51dc-8646-487e-865c-fed186a7666f
STEP: Creating a pod to test consume configMaps
Sep  6 00:30:53.454: INFO: Waiting up to 5m0s for pod "pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4" in namespace "configmap-1236" to be "Succeeded or Failed"
Sep  6 00:30:53.463: INFO: Pod "pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.671311ms
Sep  6 00:30:55.473: INFO: Pod "pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019424299s
Sep  6 00:30:57.483: INFO: Pod "pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029313816s
Sep  6 00:30:59.512: INFO: Pod "pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.058584113s
Sep  6 00:31:01.545: INFO: Pod "pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.091706802s
Sep  6 00:31:03.554: INFO: Pod "pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.100675152s
Sep  6 00:31:05.575: INFO: Pod "pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.121018284s
Sep  6 00:31:07.596: INFO: Pod "pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.142369956s
Sep  6 00:31:09.612: INFO: Pod "pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.158274528s
Sep  6 00:31:11.627: INFO: Pod "pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.173015885s
Sep  6 00:31:13.636: INFO: Pod "pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.1824725s
Sep  6 00:31:15.645: INFO: Pod "pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.191372022s
Sep  6 00:31:17.653: INFO: Pod "pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4": Phase="Pending", Reason="", readiness=false. Elapsed: 24.19980911s
Sep  6 00:31:19.669: INFO: Pod "pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.215264319s
STEP: Saw pod success
Sep  6 00:31:19.669: INFO: Pod "pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4" satisfied condition "Succeeded or Failed"
Sep  6 00:31:19.677: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4 container env-test: <nil>
STEP: delete the pod
Sep  6 00:31:19.743: INFO: Waiting for pod pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4 to disappear
Sep  6 00:31:19.768: INFO: Pod pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4 still exists
Sep  6 00:31:21.769: INFO: Waiting for pod pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4 to disappear
Sep  6 00:31:21.777: INFO: Pod pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4 still exists
Sep  6 00:31:23.770: INFO: Waiting for pod pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4 to disappear
Sep  6 00:31:23.777: INFO: Pod pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4 still exists
Sep  6 00:31:25.769: INFO: Waiting for pod pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4 to disappear
Sep  6 00:31:25.777: INFO: Pod pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4 still exists
Sep  6 00:31:27.770: INFO: Waiting for pod pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4 to disappear
Sep  6 00:31:27.781: INFO: Pod pod-configmaps-af8dee06-0447-473e-b10d-22862518f8f4 no longer exists
[AfterEach] [sig-node] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:31:27.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1236" for this suite.

â€¢ [SLOW TEST:35.030 seconds]
[sig-node] ConfigMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:34
  should be consumable via environment variable [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","total":291,"completed":283,"skipped":4734,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:31:27.985: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1495
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating configMap with name configmap-test-volume-a2dc339a-e56a-4df0-8401-99d2d042c914
STEP: Creating a pod to test consume configMaps
Sep  6 00:31:28.401: INFO: Waiting up to 5m0s for pod "pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3" in namespace "configmap-1495" to be "Succeeded or Failed"
Sep  6 00:31:28.417: INFO: Pod "pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.943866ms
Sep  6 00:31:30.441: INFO: Pod "pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03986039s
Sep  6 00:31:32.453: INFO: Pod "pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051955776s
Sep  6 00:31:34.466: INFO: Pod "pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064905185s
Sep  6 00:31:36.477: INFO: Pod "pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 8.075892815s
Sep  6 00:31:38.485: INFO: Pod "pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 10.084181808s
Sep  6 00:31:40.502: INFO: Pod "pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.101067291s
Sep  6 00:31:42.517: INFO: Pod "pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.115981468s
Sep  6 00:31:44.525: INFO: Pod "pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.1241945s
Sep  6 00:31:46.532: INFO: Pod "pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 18.131373831s
Sep  6 00:31:48.538: INFO: Pod "pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 20.137689579s
Sep  6 00:31:50.545: INFO: Pod "pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 22.144320513s
Sep  6 00:31:52.561: INFO: Pod "pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 24.16007298s
Sep  6 00:31:54.568: INFO: Pod "pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.1671961s
STEP: Saw pod success
Sep  6 00:31:54.568: INFO: Pod "pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3" satisfied condition "Succeeded or Failed"
Sep  6 00:31:54.580: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3 container configmap-volume-test: <nil>
STEP: delete the pod
Sep  6 00:31:54.648: INFO: Waiting for pod pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3 to disappear
Sep  6 00:31:54.665: INFO: Pod pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3 still exists
Sep  6 00:31:56.666: INFO: Waiting for pod pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3 to disappear
Sep  6 00:31:56.673: INFO: Pod pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3 still exists
Sep  6 00:31:58.666: INFO: Waiting for pod pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3 to disappear
Sep  6 00:31:58.672: INFO: Pod pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3 still exists
Sep  6 00:32:00.666: INFO: Waiting for pod pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3 to disappear
Sep  6 00:32:00.675: INFO: Pod pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3 still exists
Sep  6 00:32:02.666: INFO: Waiting for pod pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3 to disappear
Sep  6 00:32:02.694: INFO: Pod pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3 still exists
Sep  6 00:32:04.665: INFO: Waiting for pod pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3 to disappear
Sep  6 00:32:04.716: INFO: Pod pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3 still exists
Sep  6 00:32:06.665: INFO: Waiting for pod pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3 to disappear
Sep  6 00:32:06.676: INFO: Pod pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3 still exists
Sep  6 00:32:08.666: INFO: Waiting for pod pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3 to disappear
Sep  6 00:32:08.674: INFO: Pod pod-configmaps-4376ca4d-4bdb-46f2-9a11-2768ded1a9a3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:32:08.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1495" for this suite.

â€¢ [SLOW TEST:40.931 seconds]
[sig-storage] ConfigMap
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:36
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":284,"skipped":4783,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:32:08.916: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in crd-publish-openapi-3417
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Sep  6 00:32:09.394: INFO: >>> kubeConfig: ./kconfig.yaml
Sep  6 00:32:17.986: INFO: >>> kubeConfig: ./kconfig.yaml
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:32:47.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3417" for this suite.

â€¢ [SLOW TEST:38.327 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","total":291,"completed":285,"skipped":4824,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:32:47.244: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-702
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep  6 00:32:47.674: INFO: Waiting up to 5m0s for pod "pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a" in namespace "emptydir-702" to be "Succeeded or Failed"
Sep  6 00:32:47.705: INFO: Pod "pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a": Phase="Pending", Reason="", readiness=false. Elapsed: 31.308748ms
Sep  6 00:32:49.728: INFO: Pod "pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054062936s
Sep  6 00:32:51.735: INFO: Pod "pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.06090299s
Sep  6 00:32:53.747: INFO: Pod "pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.073457429s
Sep  6 00:32:55.756: INFO: Pod "pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.082338543s
Sep  6 00:32:57.763: INFO: Pod "pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.088962565s
Sep  6 00:32:59.771: INFO: Pod "pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a": Phase="Pending", Reason="", readiness=false. Elapsed: 12.097531578s
Sep  6 00:33:01.779: INFO: Pod "pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a": Phase="Pending", Reason="", readiness=false. Elapsed: 14.105370985s
Sep  6 00:33:03.788: INFO: Pod "pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.113859262s
Sep  6 00:33:05.847: INFO: Pod "pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.173210475s
Sep  6 00:33:07.862: INFO: Pod "pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a": Phase="Pending", Reason="", readiness=false. Elapsed: 20.188075173s
Sep  6 00:33:09.869: INFO: Pod "pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.194784314s
STEP: Saw pod success
Sep  6 00:33:09.869: INFO: Pod "pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a" satisfied condition "Succeeded or Failed"
Sep  6 00:33:09.873: INFO: Trying to get logs from node sc2-rdops-vm09-dhcp-43-208.eng.vmware.com pod pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a container test-container: <nil>
STEP: delete the pod
Sep  6 00:33:09.924: INFO: Waiting for pod pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a to disappear
Sep  6 00:33:09.948: INFO: Pod pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a still exists
Sep  6 00:33:11.950: INFO: Waiting for pod pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a to disappear
Sep  6 00:33:11.964: INFO: Pod pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a still exists
Sep  6 00:33:13.950: INFO: Waiting for pod pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a to disappear
Sep  6 00:33:13.956: INFO: Pod pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a still exists
Sep  6 00:33:15.949: INFO: Waiting for pod pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a to disappear
Sep  6 00:33:15.955: INFO: Pod pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a still exists
Sep  6 00:33:17.950: INFO: Waiting for pod pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a to disappear
Sep  6 00:33:17.957: INFO: Pod pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a still exists
Sep  6 00:33:19.949: INFO: Waiting for pod pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a to disappear
Sep  6 00:33:19.955: INFO: Pod pod-31c11ff2-7ba1-4cbd-9807-2700619c9b0a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:33:19.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-702" for this suite.

â€¢ [SLOW TEST:32.934 seconds]
[sig-storage] EmptyDir volumes
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:42
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","total":291,"completed":286,"skipped":4829,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:33:20.178: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in taint-multiple-pods-5803
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/node/taints.go:345
Sep  6 00:33:20.564: INFO: Waiting up to 1m0s for all nodes to be ready
Sep  6 00:34:20.700: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
Sep  6 00:34:20.732: INFO: Starting informer...
STEP: Starting pods...
Sep  6 00:34:20.982: INFO: Pod1 is running on sc2-rdops-vm09-dhcp-43-208.eng.vmware.com. Tainting Node
Sep  6 00:34:41.279: INFO: Pod2 is running on sc2-rdops-vm09-dhcp-43-208.eng.vmware.com. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Sep  6 00:35:02.127: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Sep  6 00:35:20.586: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:35:20.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5803" for this suite.

â€¢ [SLOW TEST:120.670 seconds]
[k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:592
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [k8s.io] [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","total":291,"completed":287,"skipped":4890,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates 
  should delete a collection of pod templates [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-node] PodTemplates
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:35:20.849: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename podtemplate
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in podtemplate-7796
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a collection of pod templates [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Create set of pod templates
Sep  6 00:35:21.265: INFO: created test-podtemplate-1
Sep  6 00:35:21.287: INFO: created test-podtemplate-2
Sep  6 00:35:21.305: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace
STEP: delete collection of pod templates
Sep  6 00:35:21.328: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity
Sep  6 00:35:21.403: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:35:21.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7796" for this suite.
â€¢{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","total":291,"completed":288,"skipped":4916,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:35:21.606: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2519
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:256
[BeforeEach] Kubectl run pod
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
[It] should create a pod from an image when restart is Never  [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: running the image mirror.gcr.io/library/httpd:2.4.38-alpine
Sep  6 00:35:21.985: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml run e2e-test-httpd-pod --restart=Never --image=mirror.gcr.io/library/httpd:2.4.38-alpine --namespace=kubectl-2519'
Sep  6 00:35:22.576: INFO: stderr: ""
Sep  6 00:35:22.576: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1550
Sep  6 00:35:22.588: INFO: Running '/home/worker/workspace/conformance-nsx-1.19/bin/kubectl --server=https://10.193.38.248:6443 --kubeconfig=./kconfig.yaml delete pods e2e-test-httpd-pod --namespace=kubectl-2519'
Sep  6 00:35:22.831: INFO: stderr: ""
Sep  6 00:35:22.831: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:35:22.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2519" for this suite.
â€¢{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","total":291,"completed":289,"skipped":4933,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
S
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:174
STEP: Creating a kubernetes client
Sep  6 00:35:23.012: INFO: >>> kubeConfig: ./kconfig.yaml
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5672
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:42
[It] should update annotations on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
STEP: Creating the pod
Sep  6 00:36:02.127: INFO: Successfully updated pod "annotationupdatef8859a3f-d5f5-4ea7-a858-2ea6d4e17f7a"
[AfterEach] [sig-storage] Downward API volume
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:175
Sep  6 00:36:04.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5672" for this suite.

â€¢ [SLOW TEST:41.500 seconds]
[sig-storage] Downward API volume
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:37
  should update annotations on modification [NodeConformance] [Conformance]
  /home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:597
------------------------------
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","total":291,"completed":290,"skipped":4934,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}
SSSSSSSSep  6 00:36:04.512: INFO: Running AfterSuite actions on all nodes
Sep  6 00:36:04.513: INFO: Running AfterSuite actions on node 1
Sep  6 00:36:04.513: INFO: Dumping logs locally to: ./e2e.results
Sep  6 00:36:04.513: INFO: Error running cluster/log-dump/log-dump.sh: fork/exec kubernetes/cluster/log-dump/log-dump.sh: no such file or directory

JUnit report was created: /home/worker/workspace/conformance-nsx-1.19/e2e.results/junit_01.xml
{"msg":"Test Suite completed","total":291,"completed":290,"skipped":4941,"failed":1,"failures":["[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]"]}


Summarizing 1 Failure:

[Fail] [sig-cli] Kubectl client Kubectl cluster-info [It] should check if Kubernetes master services is included in cluster-info  [Conformance] 
/home/worker/workspace/conformance-nsx-1.19/src/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1092

Ran 291 of 5232 Specs in 13592.476 seconds
FAIL! -- 290 Passed | 1 Failed | 0 Pending | 4941 Skipped
--- FAIL: TestE2E (13592.60s)
FAIL
